{
  "107": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2730358757",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/107#issuecomment-2730358757",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/107",
      "id": 2730358757,
      "node_id": "IC_kwDOMAMQI86ivffl",
      "user": {
        "login": "cchung85",
        "id": 36524312,
        "node_id": "MDQ6VXNlcjM2NTI0MzEy",
        "avatar_url": "https://avatars.githubusercontent.com/u/36524312?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/cchung85",
        "html_url": "https://github.com/cchung85",
        "followers_url": "https://api.github.com/users/cchung85/followers",
        "following_url": "https://api.github.com/users/cchung85/following{/other_user}",
        "gists_url": "https://api.github.com/users/cchung85/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/cchung85/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/cchung85/subscriptions",
        "organizations_url": "https://api.github.com/users/cchung85/orgs",
        "repos_url": "https://api.github.com/users/cchung85/repos",
        "events_url": "https://api.github.com/users/cchung85/events{/privacy}",
        "received_events_url": "https://api.github.com/users/cchung85/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-03-17T17:37:41Z",
      "updated_at": "2025-03-17T17:37:41Z",
      "author_association": "NONE",
      "body": "Looking at the source code of train.py, in #11, I changed the dtype from torch.bfloat16 to torch.float16, and that gets around the issue and I was able to get torchrun to start loading.\nWith 2 V100, I ran into OOM pretty quickly. So in example/7B.yaml, I made the following update:\n1. Seq length from 64K to 8192.\n2. Lora Rank from 64 to 16 (41,941,943,040 out of 7,289,966,592 parameters are finetuned (0.58%))\nWith these, I can see both sharding was used to load the model, and both GPU were running at 99% most of the time. Both are using 55% of memory.\nI plan to tweaked the seq len and lora rank, and other hyperparameters later, but with these changes, I am able to proceed.\nSo this ticket could be closed.\n(BTW, further googling: looks like in 7B.yaml, one can add a line: compute_dtype: torch.float16 to do the same)\n\nOne question I have is: how to configure so that I can use mixed precision such as  qLora? I saw in train.py that mixed precision is supported, but not sure how. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2730358757/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2730576979",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/107#issuecomment-2730576979",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/107",
      "id": 2730576979,
      "node_id": "IC_kwDOMAMQI86iwUxT",
      "user": {
        "login": "cchung85",
        "id": 36524312,
        "node_id": "MDQ6VXNlcjM2NTI0MzEy",
        "avatar_url": "https://avatars.githubusercontent.com/u/36524312?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/cchung85",
        "html_url": "https://github.com/cchung85",
        "followers_url": "https://api.github.com/users/cchung85/followers",
        "following_url": "https://api.github.com/users/cchung85/following{/other_user}",
        "gists_url": "https://api.github.com/users/cchung85/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/cchung85/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/cchung85/subscriptions",
        "organizations_url": "https://api.github.com/users/cchung85/orgs",
        "repos_url": "https://api.github.com/users/cchung85/repos",
        "events_url": "https://api.github.com/users/cchung85/events{/privacy}",
        "received_events_url": "https://api.github.com/users/cchung85/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-03-17T19:12:48Z",
      "updated_at": "2025-03-17T19:12:48Z",
      "author_association": "NONE",
      "body": "Just tried the suggestion in 7B.yaml:\ncompute_dtype: torch.float16      OR\nparam_dtype: torch.float16\nNeither of them are accepted by torch run. \nLooking at train.py and args.py, I don;t see mixed precision can be supported through 7B.yaml (will need some minor code change I think).",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2730576979/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "99": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2349712684",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/99#issuecomment-2349712684",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/99",
      "id": 2349712684,
      "node_id": "IC_kwDOMAMQI86MDcUs",
      "user": {
        "login": "nvnk3",
        "id": 27855625,
        "node_id": "MDQ6VXNlcjI3ODU1NjI1",
        "avatar_url": "https://avatars.githubusercontent.com/u/27855625?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/nvnk3",
        "html_url": "https://github.com/nvnk3",
        "followers_url": "https://api.github.com/users/nvnk3/followers",
        "following_url": "https://api.github.com/users/nvnk3/following{/other_user}",
        "gists_url": "https://api.github.com/users/nvnk3/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/nvnk3/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/nvnk3/subscriptions",
        "organizations_url": "https://api.github.com/users/nvnk3/orgs",
        "repos_url": "https://api.github.com/users/nvnk3/repos",
        "events_url": "https://api.github.com/users/nvnk3/events{/privacy}",
        "received_events_url": "https://api.github.com/users/nvnk3/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-13T18:04:53Z",
      "updated_at": "2024-09-13T18:04:53Z",
      "author_association": "NONE",
      "body": "Thank you so much, this commit has the fix:\r\nhttps://github.com/mistralai/mistral-finetune/commit/656df1c94c80ca9703ebc471c9f106c9b7a0bfa7",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2349712684/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "98": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2342695121",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2342695121",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2342695121,
      "node_id": "IC_kwDOMAMQI86LorDR",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-11T06:03:47Z",
      "updated_at": "2024-09-11T07:52:44Z",
      "author_association": "NONE",
      "body": "I fixed part of my issue by running the command directly from within the `mistral-finetune` folder:\r\n```\r\ncd mistral-finetune\r\npython -m utils.validate_data --train_yaml example/7B.yaml\r\n```\r\n\r\nStill, I am now getting another error:\r\n\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 17, in <module>\r\n    from finetune.data.dataset import parse_data_sources\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/dataset.py\", line 10, in <module>\r\n    import torch.distributed as dist\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 372, in <module>\r\n    main(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 179, in main\r\n    datasets, weights = parse_data_sources(pretrain_file, instruct_file)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/dataset.py\", line 159, in parse_data_sources\r\n    assert min(n_weights) > 0\r\nValueError: min() arg is an empty sequence\r\n```\r\n\r\nDowngrading to `numpy-1.26.4` running `pip install \"numpy<2.0\"` fixed only some of the issues:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 372, in <module>\r\n    main(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 179, in main\r\n    datasets, weights = parse_data_sources(pretrain_file, instruct_file)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/dataset.py\", line 159, in parse_data_sources\r\n    assert min(n_weights) > 0\r\nValueError: min() arg is an empty sequence\r\n```\r\n\r\nAny idea?\r\n\r\nBest,\r\n\r\nC.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2342695121/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2355841528",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2355841528",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2355841528,
      "node_id": "IC_kwDOMAMQI86Ma0n4",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-17T13:39:25Z",
      "updated_at": "2024-09-17T13:39:25Z",
      "author_association": "NONE",
      "body": "Any help?\r\n\r\nI have tried also with `Python 13.11` and it fails similarly...",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2355841528/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2367915818",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2367915818",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2367915818,
      "node_id": "IC_kwDOMAMQI86NI4cq",
      "user": {
        "login": "NazimHAli",
        "id": 26750288,
        "node_id": "MDQ6VXNlcjI2NzUwMjg4",
        "avatar_url": "https://avatars.githubusercontent.com/u/26750288?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NazimHAli",
        "html_url": "https://github.com/NazimHAli",
        "followers_url": "https://api.github.com/users/NazimHAli/followers",
        "following_url": "https://api.github.com/users/NazimHAli/following{/other_user}",
        "gists_url": "https://api.github.com/users/NazimHAli/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NazimHAli/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NazimHAli/subscriptions",
        "organizations_url": "https://api.github.com/users/NazimHAli/orgs",
        "repos_url": "https://api.github.com/users/NazimHAli/repos",
        "events_url": "https://api.github.com/users/NazimHAli/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NazimHAli/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-23T11:17:27Z",
      "updated_at": "2024-09-23T11:23:29Z",
      "author_association": "NONE",
      "body": "I tried it in a new environment for python 3.10 and it worked. You have to run it as a module (`python -m`) like the example in the README instead of as a script:\r\n\r\n```shell\r\ncd $HOME/mistral-finetune\r\npython -m utils.reformat_data $HOME/data/ultrachat_chunk_train.jsonl\r\npython -m utils.reformat_data $HOME/data/ultrachat_chunk_eval.jsonl\r\n```\r\n\r\nFYI that package `finetune` you installed from https://pypi.org/project/finetune/ has nothing to do with this project. You should uninstall it (might be easier to create a new environment just for this project). When you run the commands as a module, python will execute `mistral-finetune/finetune` correctly.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2367915818/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2371235193",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2371235193",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2371235193,
      "node_id": "IC_kwDOMAMQI86NVi15",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-24T13:11:41Z",
      "updated_at": "2024-09-24T13:13:30Z",
      "author_association": "NONE",
      "body": "> I tried it in a new environment for python 3.10 and it worked. You have to run it as a module (`python -m`) like the example in the README instead of as a script:\r\n> \r\n> ```shell\r\n> cd $HOME/mistral-finetune\r\n> python -m utils.reformat_data $HOME/data/ultrachat_chunk_train.jsonl\r\n> python -m utils.reformat_data $HOME/data/ultrachat_chunk_eval.jsonl\r\n> ```\r\n> \r\n> FYI that package `finetune` you installed from https://pypi.org/project/finetune/ has nothing to do with this project. You should uninstall it (might be easier to create a new environment just for this project). When you run the commands as a module, python will execute `mistral-finetune/finetune` correctly.\r\n\r\nDear @NazimHAli, many thanks for your support.\r\n\r\nAs written above, I have fixed some of the issues by running it as a module instead of as a script (e.g., the missing finetune package).\r\n\r\nI went through the whole process once more and realized that I forgot to modify the `example\\/7B.yaml` file to contain the absolute paths to both ultrachat_chunk_eval.jsonl and ultrachat_chunk_train.jsonl files:\r\n![image](https://github.com/user-attachments/assets/b121f240-c0d7-4ce9-b942-7ad761ee6544)\r\n\r\nI could thus successfully complete the [dataset verification section](https://github.com/mistralai/mistral-finetune/#verify-dataset):\r\npython -m utils.validate_data --train_yaml example/7B.yaml\r\n\r\nNevertheless, it fails at training when running:\r\n```\r\ntorchrun --nproc-per-node 8 --master_port $RANDOM -m train example/7B.yaml\r\n```\r\n\r\nI get:\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/bin/torchrun\", line 5, in <module>\r\n    from torch.distributed.run import main\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n[2024-09-24 15:10:19,432] torch.distributed.run: [WARNING]\r\n[2024-09-24 15:10:19,432] torch.distributed.run: [WARNING] *****************************************\r\n[2024-09-24 15:10:19,432] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\r\n[2024-09-24 15:10:19,432] torch.distributed.run: [WARNING] *****************************************\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n\r\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 10, in <module>\r\n    import torch.cuda\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\r\n    from .functional import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\r\n    from .modules import *  # noqa: F403\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\r\n    from .transformer import TransformerEncoder, TransformerDecoder, \\\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\r\n    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\n/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-09-24 15:10:21 (CET) - 0:00:01 - distributed - INFO - torch.cuda.device_count: 0\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\n[2024-09-24 15:10:24,437] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3654500) of binary: /cluster/flash/wichtco/ai-fine-tuning/.venv/bin/python\r\nTraceback (most recent call last):\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 812, in main\r\n    run(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\r\n    elastic_launch(\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\ntrain FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n[1]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 3654501)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[2]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 2 (local_rank: 2)\r\n  exitcode  : 1 (pid: 3654502)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[3]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 3 (local_rank: 3)\r\n  exitcode  : 1 (pid: 3654503)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[4]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 4 (local_rank: 4)\r\n  exitcode  : 1 (pid: 3654504)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[5]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 5 (local_rank: 5)\r\n  exitcode  : 1 (pid: 3654505)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[6]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 6 (local_rank: 6)\r\n  exitcode  : 1 (pid: 3654506)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[7]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 7 (local_rank: 7)\r\n  exitcode  : 1 (pid: 3654507)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-09-24_15:10:24\r\n  host      : master.cluster\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 3654500)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\nAny idea?\r\n\r\nBest,\r\n\r\nC.\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2371235193/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2372527679",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2372527679",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2372527679,
      "node_id": "IC_kwDOMAMQI86NaeY_",
      "user": {
        "login": "NazimHAli",
        "id": 26750288,
        "node_id": "MDQ6VXNlcjI2NzUwMjg4",
        "avatar_url": "https://avatars.githubusercontent.com/u/26750288?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NazimHAli",
        "html_url": "https://github.com/NazimHAli",
        "followers_url": "https://api.github.com/users/NazimHAli/followers",
        "following_url": "https://api.github.com/users/NazimHAli/following{/other_user}",
        "gists_url": "https://api.github.com/users/NazimHAli/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NazimHAli/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NazimHAli/subscriptions",
        "organizations_url": "https://api.github.com/users/NazimHAli/orgs",
        "repos_url": "https://api.github.com/users/NazimHAli/repos",
        "events_url": "https://api.github.com/users/NazimHAli/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NazimHAli/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-24T22:55:29Z",
      "updated_at": "2024-09-24T22:55:29Z",
      "author_association": "NONE",
      "body": "Try uninstalling `numpy` and reinstall it with version 1. An easy way would be to install the requirements like this `pip install -r requirements.txt \"numpy<2\"`.\r\n\r\nIt's possible a combination of the packages + local environment is causing it to install version 2, but not have the dependencies correctly defined. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2372527679/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2372971026",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2372971026",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2372971026,
      "node_id": "IC_kwDOMAMQI86NcKoS",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-25T04:56:15Z",
      "updated_at": "2024-09-25T04:56:15Z",
      "author_association": "NONE",
      "body": "> Try uninstalling `numpy` and reinstall it with version 1. An easy way would be to install the requirements like this `pip install -r requirements.txt \"numpy<2\"`.\r\n> \r\n> It's possible a combination of the packages + local environment is causing it to install version 2, but not have the dependencies correctly defined.\r\n\r\nMany thanks for your support, that indeed fixed the \"_A module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.1.1 as it may crash_\" message.\r\n\r\nUnfortunately, when running `torchrun --nproc-per-node 8 --master_port $RANDOM -m train example/7B.yaml`, I still get the following error:\r\n```\r\n[2024-09-25 06:53:30,824] torch.distributed.run: [WARNING]\r\n[2024-09-25 06:53:30,824] torch.distributed.run: [WARNING] *****************************************\r\n[2024-09-25 06:53:30,824] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\r\n[2024-09-25 06:53:30,824] torch.distributed.run: [WARNING] *****************************************\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))args: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))args: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=8, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n\r\n\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\nTraceback (most recent call last):\r\n      File \"<frozen runpy>\", line 198, in _run_module_as_main\r\nfire.Fire(train)\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\nTraceback (most recent call last):\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\n      File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\nfire.Fire(train)\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\n2024-09-25 06:53:33 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)Traceback (most recent call last):\r\n\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n    File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n      File \"<frozen runpy>\", line 88, in _run_code\r\n fire.Fire(train)   File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n\r\n      File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  fire.Fire(train)   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n\r\n        fire.Fire(train)\r\n    File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n           File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n           fire.Fire(train)\r\n      ^fire.Fire(train)^\r\n^ component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n^   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\nfire.Fire(train)^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    ^ component_trace = _Fire(component, args, parsed_flag_args, context, name)^\r\n^ component_trace = _Fire(component, args, parsed_flag_args, context, name)   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n^\r\n ^       ^  component_trace = _Fire(component, args, parsed_flag_args, context, name)    ^^\r\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)^\r\n ^  ^ ^^    ^^        ^^   component_trace = _Fire(component, args, parsed_flag_args, context, name)  ^^\r\n  ^^^     ^^      ^^      ^^ ^      ^ ^     ^ ^     ^ ^      ^^      ^^      ^ ^ ^    ^  ^ ^   ^   ^    ^  ^^    ^^ ^^ ^  ^^ ^ ^ ^ ^^ ^  ^ ^^^ ^ ^^^^^^ ^ ^^^^^ ^ ^^^^^^^^ ^^^^^^^ ^^^^^^^ ^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n^^^^^^^^^^^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n\r\n^^^^^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^^^^component, remaining_args = _CallAndUpdateTrace(^^^^^^^^\r\n^^^^^^^component, remaining_args = _CallAndUpdateTrace(^^^^^^\r\n^ ^^^^^^ ^^^^^^  ^^^^^^  ^^^^^^  ^^^^^^  ^^^^^^  ^^^^^^^   ^^^^^^  ^^^^^^  ^^^^^^^  ^^^^^^  ^ ^^^^\r\n ^  ^ ^^\r\n^^ ^  ^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n^^  ^   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n^^^\r\n^ ^^ ^ ^^ ^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n^ ^\r\n^ ^ ^ ^^   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n\r\n^  ^        File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n^   component, remaining_args = _CallAndUpdateTrace(^\r\n^      ^ component, remaining_args = _CallAndUpdateTrace(\r\n\r\n        File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n  component, remaining_args = _CallAndUpdateTrace(\r\n^ component, remaining_args = _CallAndUpdateTrace(  ^^\r\n  ^ ^component, remaining_args = _CallAndUpdateTrace(  ^ ^\r\n    ^ ^    ^ ^^        ^ ^component, remaining_args = _CallAndUpdateTrace(    ^ ^^\r\n     ^^      ^^      ^^      ^^      ^^      ^^ ^     ^ ^      ^^      ^ ^     ^ ^     ^\r\n^\r\n             File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n   File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n                                               ^     ^              ^component = fn(*varargs, **kwargs)   component = fn(*varargs, **kwargs)^ ^\r\n\r\n^ ^^^    ^^^     ^ ^^ ^^  ^ ^ ^^ ^^^ ^ ^ ^^^ ^ ^ ^^^^ ^  ^^^^ ^ ^^^^^ ^  ^^^^^^^  ^^^^^^  ^^^^^^ ^ ^^^^^^^ ^ ^^^^^ ^^ ^^^^^ ^^ ^^^\r\n^^^ ^^^^^^^^^ ^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n^^^^ ^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n^^\r\n^^^^^^^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n^^^\r\n\r\n^^^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n^^^^^^^^^    ^component = fn(*varargs, **kwargs)^\r\n^\r\n^^^^      File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n^^component = fn(*varargs, **kwargs)^^\r\n ^    ^ ^component = fn(*varargs, **kwargs)^^ ^\r\n ^          ^ ^component = fn(*varargs, **kwargs) component = fn(*varargs, **kwargs)\r\n\r\n ^\r\n   ^    File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n ^\r\n     component = fn(*varargs, **kwargs)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n                 _train(args, exit_stack)\r\n                 _train(args, exit_stack)^ ^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n\r\n  ^ ^       File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n^         ^ ^   ^set_device() ^  ^   ^\r\n ^    ^  ^ ^set_device()  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n^ ^^ ^\r\n^^^^ ^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n^ ^^^^^ ^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n^^^^^^^^^^^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^^^\r\n^^_train(args, exit_stack)^^^\r\n\r\n^^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    ^^^logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n\r\n^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n^\r\n^  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n ^      File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n     ^_train(args, exit_stack) logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")^\r\n\r\n\r\n       File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n     _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\nset_device() _train(args, exit_stack)\r\n\r\n  _train(args, exit_stack)  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n\r\n          File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\nset_device()_train(args, exit_stack)\r\n\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n                  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\nset_device()  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\") set_device()\r\n\r\n     set_device()\r\n    File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n\r\n          File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n      set_device()   File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n\r\n         File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n   logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n  logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n            logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                                                                         ~          ~        ~        ~ ~        ~       ~       ~       ~ ~       ^       ^  ~    ^  ~ ~        ^ ^ ~    ~ ~^ ~     ~^ ~     ~^ ~    ~^~ ~     ^~ ~    ^~ ~ ^   ^~  ^   ^~  ^    ^^ ^    ^^ ^     ^^ ^    ^^ ^^^ ~ ~ ^ ^^~~~ ^ ^^~~~ ^ ^^~~~ ^ ^^~~~ ^~^^~~~~^~^^~^~^~~^~^~~^~~\r\n~^~~^~~~^~~^^^~  File \"<frozen os>\", line 678, in __getitem__\r\n~^~~^^~^~~KeyError~^: ^^^^^~~'CUDA_VISIBLE_DEVICES'^^^^^~^~\r\n^^^^^~^^^^^^^\r\n^^^^^\r\n^^^^  File \"<frozen os>\", line 678, in __getitem__\r\n^^^^^^  File \"<frozen os>\", line 678, in __getitem__\r\n^^^^^KeyError^^^^^KeyError: ^^^^^: ^'CUDA_VISIBLE_DEVICES'^\r\n^^^'CUDA_VISIBLE_DEVICES'^^^^^\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\n^^\r\n^^^^  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError\r\n^  File \"<frozen os>\", line 678, in __getitem__\r\n: ^KeyError'CUDA_VISIBLE_DEVICES'  File \"<frozen os>\", line 678, in __getitem__\r\n\r\n: KeyError\r\n'CUDA_VISIBLE_DEVICES':\r\n'CUDA_VISIBLE_DEVICES'KeyError  File \"<frozen os>\", line 678, in __getitem__\r\n\r\n: 'CUDA_VISIBLE_DEVICES'KeyError\r\n: 'CUDA_VISIBLE_DEVICES'\r\n[2024-09-25 06:53:35,830] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1755254) of binary: /cluster/flash/wichtco/ai-fine-tuning/.venv/bin/python\r\nTraceback (most recent call last):\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 812, in main\r\n    run(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\r\n    elastic_launch(\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\ntrain FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n[1]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 1755255)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[2]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 2 (local_rank: 2)\r\n  exitcode  : 1 (pid: 1755256)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[3]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 3 (local_rank: 3)\r\n  exitcode  : 1 (pid: 1755257)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[4]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 4 (local_rank: 4)\r\n  exitcode  : 1 (pid: 1755258)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[5]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 5 (local_rank: 5)\r\n  exitcode  : 1 (pid: 1755259)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[6]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 6 (local_rank: 6)\r\n  exitcode  : 1 (pid: 1755260)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n[7]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 7 (local_rank: 7)\r\n  exitcode  : 1 (pid: 1755261)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-09-25_06:53:35\r\n  host      : node03.cluster\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 1755254)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\n_FYI I am trying to run the scripts on our University GPU Cluster._",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2372971026/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2381426638",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2381426638",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2381426638,
      "node_id": "IC_kwDOMAMQI86N8a_O",
      "user": {
        "login": "NazimHAli",
        "id": 26750288,
        "node_id": "MDQ6VXNlcjI2NzUwMjg4",
        "avatar_url": "https://avatars.githubusercontent.com/u/26750288?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NazimHAli",
        "html_url": "https://github.com/NazimHAli",
        "followers_url": "https://api.github.com/users/NazimHAli/followers",
        "following_url": "https://api.github.com/users/NazimHAli/following{/other_user}",
        "gists_url": "https://api.github.com/users/NazimHAli/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NazimHAli/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NazimHAli/subscriptions",
        "organizations_url": "https://api.github.com/users/NazimHAli/orgs",
        "repos_url": "https://api.github.com/users/NazimHAli/repos",
        "events_url": "https://api.github.com/users/NazimHAli/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NazimHAli/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-29T16:59:36Z",
      "updated_at": "2024-09-29T16:59:36Z",
      "author_association": "NONE",
      "body": "I don't have experience with this, so not sure how to debug because it could be specific to your cluster - you can try first getting it to run with a single GPU and go from there. This might be a better question in the `torch` repo as they would have more experience. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2381426638/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2385540899",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2385540899",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2385540899,
      "node_id": "IC_kwDOMAMQI86OMHcj",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-10-01T11:35:13Z",
      "updated_at": "2024-10-01T11:35:13Z",
      "author_association": "NONE",
      "body": "> I don't have experience with this, so not sure how to debug because it could be specific to your cluster - you can try first getting it to run with a single GPU and go from there. This might be a better question in the `torch` repo as they would have more experience.\r\n\r\nDear @NazimHAli,\r\n\r\nThanks for the suggestion, unfortunately it fails similarly:\r\n```\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project='Mistral-finetune', offline=False, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-10-01 13:33:54 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 8\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 78, in _train\r\n    set_device()\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/distributed.py\", line 30, in set_device\r\n    logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\r\n                                         ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen os>\", line 678, in __getitem__\r\nKeyError: 'CUDA_VISIBLE_DEVICES'\r\n[2024-10-01 13:33:57,332] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1779407) of binary: /cluster/flash/wichtco/ai-fine-tuning/.venv/bin/python\r\nTraceback (most recent call last):\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 812, in main\r\n    run(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\r\n    elastic_launch(\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\ntrain FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-10-01_13:33:57\r\n  host      : node03.cluster\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 1779407)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\nI will thus open a thread on the `torch` repo and reference it here if anyone might encounter the same issue.\r\n\r\nBest,\r\n\r\nC.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2385540899/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2387676445",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2387676445",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2387676445,
      "node_id": "IC_kwDOMAMQI86OUQ0d",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-10-02T05:46:30Z",
      "updated_at": "2024-10-02T06:28:51Z",
      "author_association": "NONE",
      "body": "@NazimHAli according to PyTorch developpers the issue is coming from your code and not from their package: https://github.com/pytorch/pytorch/issues/137082.\r\n\r\nSo I could go a step further by setting up `CUDA_VISIBLE_DEVICES` manually (see [this thread](https://stackoverflow.com/questions/39649102/how-do-i-select-which-gpu-to-run-a-job-on)) :\r\n`CUDA_VISIBLE_DEVICES=1 torchrun --nproc-per-node 1 --master_port $RANDOM -m train example/7B.yaml`\r\n\r\nThough it still fails later on...\r\n\r\n```\r\nargs: TrainArgs(data=DataArgs(data='', shuffle=False, instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl', eval_instruct_data='/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/cluster/flash/wichtco/ai-fine-tuning/mistral_models', run_dir='/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test', optim=OptimArgs(lr=6e-05, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=1, seq_len=32768, batch_size=1, max_norm=1.0, max_steps=300, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project='Mistral-finetune', offline=True, key='81eab917d15e15c70c653f96b000838fcbb6bad5', run_name=''), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\r\n2024-10-02 08:24:00 (CET) - 0:00:02 - distributed - INFO - torch.cuda.device_count: 1\r\n2024-10-02 08:24:00 (CET) - 0:00:02 - distributed - INFO - CUDA_VISIBLE_DEVICES: 1\r\n2024-10-02 08:24:00 (CET) - 0:00:02 - distributed - INFO - local rank: 0\r\n2024-10-02 08:24:00 (CET) - 0:00:02 - train - INFO - Going to init comms...\r\n2024-10-02 08:24:00 (CET) - 0:00:02 - train - INFO - Run dir: /cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test\r\n2024-10-02 08:24:01 (CET) - 0:00:02 - train - INFO - TrainArgs: {'batch_size': 1,\r\n 'checkpoint': True,\r\n 'ckpt_freq': 100,\r\n 'data': {'data': '',\r\n          'eval_instruct_data': '/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl',\r\n          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\r\n          'instruct_data': '/cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl',\r\n          'shuffle': False},\r\n 'eval_freq': 100,\r\n 'log_freq': 1,\r\n 'lora': {'dropout': 0.0, 'enable': True, 'rank': 64, 'scaling': 2.0},\r\n 'max_norm': 1.0,\r\n 'max_steps': 300,\r\n 'mlflow': {'experiment_name': None, 'tracking_uri': None},\r\n 'model_id_or_path': '/cluster/flash/wichtco/ai-fine-tuning/mistral_models',\r\n 'no_ckpt': False,\r\n 'no_eval': False,\r\n 'num_ckpt_keep': 3,\r\n 'num_microbatches': 1,\r\n 'optim': {'lr': 6e-05, 'pct_start': 0.05, 'weight_decay': 0.1},\r\n 'run_dir': '/cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test',\r\n 'save_adapters': True,\r\n 'seed': 0,\r\n 'seq_len': 32768,\r\n 'wandb': {'key': '81eab917d15e15c70c653f96b000838fcbb6bad5',\r\n           'offline': True,\r\n           'project': 'Mistral-finetune',\r\n           'run_name': ''},\r\n 'world_size': 1}\r\nwandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\r\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\r\nwandb: Appending key for api.wandb.ai to your netrc file: /cluster/raid/home/wichtco/.netrc\r\n2024-10-02 08:24:11 (CET) - 0:00:12 - metrics_logger - INFO - initializing wandb\r\nwandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.\r\nwandb: Tracking run with wandb version 0.18.1\r\nwandb: W&B syncing is set to `offline` in this directory.\r\nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\r\n2024-10-02 08:24:11 (CET) - 0:00:12 - utils - INFO - Closing: eval_logger\r\nwandb:\r\nwandb: You can sync this run to the cloud by running:\r\nwandb: wandb sync /cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test/wandb/offline-run-20241002_082411-8idg077y\r\nwandb: Find logs at: /cluster/flash/wichtco/ai-fine-tuning/ultra_chat_test/wandb/offline-run-20241002_082411-8idg077y/logs\r\n2024-10-02 08:24:14 (CET) - 0:00:15 - utils - INFO - Closed: eval_logger\r\n2024-10-02 08:24:14 (CET) - 0:00:15 - utils - INFO - Closing: metrics_logger\r\n2024-10-02 08:24:14 (CET) - 0:00:15 - utils - INFO - Closed: metrics_logger\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 328, in <module>\r\n    fire.Fire(train)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 65, in train\r\n    _train(args, exit_stack)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/train.py\", line 171, in _train\r\n    eval_batches = list(eval_data_loader)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/data_loader.py\", line 122, in build_data_loader\r\n    batch: Batch = batch_list.create_batch()\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/data_loader.py\", line 77, in create_batch\r\n    x_np: np.ndarray = self.flatten_to_numpy(self.x, dtype=np.int64)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/data_loader.py\", line 74, in flatten_to_numpy\r\n    return np.array([el for sublist in list_of_lists for el in sublist], dtype=dtype)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32768,) + inhomogeneous part.\r\n[2024-10-02 08:24:18,791] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1785821) of binary: /cluster/flash/wichtco/ai-fine-tuning/.venv/bin/python\r\nTraceback (most recent call last):\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 812, in main\r\n    run(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\r\n    elastic_launch(\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\ntrain FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-10-02_08:24:18\r\n  host      : node03.cluster\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : 1 (pid: 1785821)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n============================================================\r\n```\r\n\r\nAny idea what went wrong this time?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2387676445/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2409420389",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2409420389",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2409420389,
      "node_id": "IC_kwDOMAMQI86PnNZl",
      "user": {
        "login": "NazimHAli",
        "id": 26750288,
        "node_id": "MDQ6VXNlcjI2NzUwMjg4",
        "avatar_url": "https://avatars.githubusercontent.com/u/26750288?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NazimHAli",
        "html_url": "https://github.com/NazimHAli",
        "followers_url": "https://api.github.com/users/NazimHAli/followers",
        "following_url": "https://api.github.com/users/NazimHAli/following{/other_user}",
        "gists_url": "https://api.github.com/users/NazimHAli/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NazimHAli/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NazimHAli/subscriptions",
        "organizations_url": "https://api.github.com/users/NazimHAli/orgs",
        "repos_url": "https://api.github.com/users/NazimHAli/repos",
        "events_url": "https://api.github.com/users/NazimHAli/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NazimHAli/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-10-14T00:07:18Z",
      "updated_at": "2024-10-14T00:07:18Z",
      "author_association": "NONE",
      "body": "Hey,\r\n\r\nSorry for the late reply, lost track of things. From this error, it's complaining about your dataset:\r\n\r\n```python\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32768,) + inhomogeneous part.\r\n```\r\n\r\nCan you create a public repo with reproducible code and sample data?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2409420389/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2416581578",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2416581578",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2416581578,
      "node_id": "IC_kwDOMAMQI86QChvK",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-10-16T11:47:08Z",
      "updated_at": "2024-10-16T11:58:31Z",
      "author_association": "NONE",
      "body": "> Hey,\r\n> \r\n> Sorry for the late reply, lost track of things. From this error, it's complaining about your dataset:\r\n> \r\n> ```python\r\n> ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32768,) + inhomogeneous part.\r\n> ```\r\n> \r\n> Can you create a public repo with reproducible code and sample data?\r\n\r\n@NazimHAli, no worries and thanks for the reply.\r\n\r\nIn fact, I have strictly followed your [README](https://github.com/mistralai/mistral-finetune/?tab=readme-ov-file#instruction-following) and downloaded the [Ultrachat_200k dataset from HuggingFace](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet) using Python:\r\n```\r\n# Packages\r\nimport pandas as pd\r\n\r\n# Load the data into a Pandas Dataframe\r\ndf = pd.read_parquet('https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet')\r\n\r\n# Split into train and eval\r\ndf_train=df.sample(frac=0.95,random_state=200)\r\ndf_eval=df.drop(df_train.index)\r\n\r\n# Save data to jsonl\r\ndf_train.to_json(\"./data/ultrachat_chunk_train.jsonl\", orient=\"records\", lines=True)\r\ndf_eval.to_json(\"./data/ultrachat_chunk_eval.jsonl\", orient=\"records\", lines=True)\r\n```\r\n\r\nThen, again as suggested in your README, I made use of the [./utils/reformat_data.py](https://github.com/mistralai/mistral-finetune/blob/main/utils/reformat_data.py) to correct the data:\r\n```\r\npython -m utils.reformat_data /cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl\r\npython -m utils.reformat_data /cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_eval.jsonl\r\n```\r\n\r\nMaybe this last step corrupted the dataset ?\r\n\r\nEDIT:\r\n----\r\n\r\nThere seems to be something wrong in the [Ultrachat_200k dataset from HuggingFace](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k/resolve/main/data/test_gen-00000-of-00001-3d4cd8309148a71f.parquet) dataset because when I verify the training yaml to make sure the data is correctly formatted running `python -m utils.validate_data --train_yaml example/7B.yaml` I get the following error:\r\n```\r\n0it [00:00, ?it/s]Validating /cluster/flash/wichtco/ai-fine-tuning/data/ultrachat_chunk_train.jsonl ...\r\n  0%|                                                                                         | 0/26889 [00:00<?, ?it/s]\r\n0it [00:00, ?it/s]                                                                            | 0/26889 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 372, in <module>\r\n    main(args)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/utils/validate_data.py\", line 214, in main\r\n    sample = build_instruct_sample(data)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/mistral-finetune/finetune/data/tokenize.py\", line 180, in build_instruct_sample\r\n    validator.validate_messages(messages)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/mistral_common/protocol/instruct/validator.py\", line 50, in validate_messages\r\n    self._validate_message_list_structure(messages)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/mistral_common/protocol/instruct/validator.py\", line 259, in _validate_message_list_structure\r\n    self._validate_last_message(messages[-1])\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/mistral_common/protocol/instruct/validator.py\", line 323, in _validate_last_message\r\n    super()._validate_last_message(message)\r\n  File \"/cluster/flash/wichtco/ai-fine-tuning/.venv/lib/python3.11/site-packages/mistral_common/protocol/instruct/validator.py\", line 231, in _validate_last_message\r\n    f\"Expected last role Assistant for finetuning but got {last_message_role.value}\"\r\n                                                           ^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'str' object has no attribute 'value'\r\n```\r\n\r\nWhich is not what's to be expected as described in your README, namely:\r\n```\r\nThe data in line 1412 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formatted. Expected last role to be one of: [assistant] but got user\r\nThe data in line 1413 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formatted. Expected last role to be one of: [assistant] but got user\r\nThe data in line 1414 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formatted. Expected last role to be one of: [assistant] but got user\r\nThe data in line 1415 of dataset /Users/johndoe/data/ultrachat_chunk_eval.jsonl is incorrectly formatted. Expected last role to be one of: [assistant] but got user\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2416581578/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2548637559",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/98#issuecomment-2548637559",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/98",
      "id": 2548637559,
      "node_id": "IC_kwDOMAMQI86X6R93",
      "user": {
        "login": "CorentinWicht",
        "id": 52161904,
        "node_id": "MDQ6VXNlcjUyMTYxOTA0",
        "avatar_url": "https://avatars.githubusercontent.com/u/52161904?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CorentinWicht",
        "html_url": "https://github.com/CorentinWicht",
        "followers_url": "https://api.github.com/users/CorentinWicht/followers",
        "following_url": "https://api.github.com/users/CorentinWicht/following{/other_user}",
        "gists_url": "https://api.github.com/users/CorentinWicht/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CorentinWicht/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CorentinWicht/subscriptions",
        "organizations_url": "https://api.github.com/users/CorentinWicht/orgs",
        "repos_url": "https://api.github.com/users/CorentinWicht/repos",
        "events_url": "https://api.github.com/users/CorentinWicht/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CorentinWicht/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-12-17T14:40:25Z",
      "updated_at": "2024-12-17T14:40:25Z",
      "author_association": "NONE",
      "body": "@NazimHAli any idea ? I have actually strictly followed your [README](https://github.com/mistralai/mistral-finetune/?tab=readme-ov-file#instruction-following) as written above and cannot replicate your results, can you? \r\n\r\nBest,\r\n\r\nC.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2548637559/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "92": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2297803617",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/92#issuecomment-2297803617",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/92",
      "id": 2297803617,
      "node_id": "IC_kwDOMAMQI86I9bNh",
      "user": {
        "login": "leloss",
        "id": 8379486,
        "node_id": "MDQ6VXNlcjgzNzk0ODY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8379486?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/leloss",
        "html_url": "https://github.com/leloss",
        "followers_url": "https://api.github.com/users/leloss/followers",
        "following_url": "https://api.github.com/users/leloss/following{/other_user}",
        "gists_url": "https://api.github.com/users/leloss/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/leloss/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/leloss/subscriptions",
        "organizations_url": "https://api.github.com/users/leloss/orgs",
        "repos_url": "https://api.github.com/users/leloss/repos",
        "events_url": "https://api.github.com/users/leloss/events{/privacy}",
        "received_events_url": "https://api.github.com/users/leloss/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-20T01:41:49Z",
      "updated_at": "2024-08-20T05:22:10Z",
      "author_association": "NONE",
      "body": "Update: This seems to be an issue with lack of (individual) GPU memory which I could get around by reducing the context length in .yaml for a smaller training footprint. I couldn't find anything suggesting that in your error messages though. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2297803617/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "87": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2272887197",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/87#issuecomment-2272887197",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/87",
      "id": 2272887197,
      "node_id": "IC_kwDOMAMQI86HeYGd",
      "user": {
        "login": "acodercat",
        "id": 16076835,
        "node_id": "MDQ6VXNlcjE2MDc2ODM1",
        "avatar_url": "https://avatars.githubusercontent.com/u/16076835?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/acodercat",
        "html_url": "https://github.com/acodercat",
        "followers_url": "https://api.github.com/users/acodercat/followers",
        "following_url": "https://api.github.com/users/acodercat/following{/other_user}",
        "gists_url": "https://api.github.com/users/acodercat/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/acodercat/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/acodercat/subscriptions",
        "organizations_url": "https://api.github.com/users/acodercat/orgs",
        "repos_url": "https://api.github.com/users/acodercat/repos",
        "events_url": "https://api.github.com/users/acodercat/events{/privacy}",
        "received_events_url": "https://api.github.com/users/acodercat/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-07T08:14:27Z",
      "updated_at": "2024-08-07T08:14:27Z",
      "author_association": "NONE",
      "body": "+1",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2272887197/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2273825367",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/87#issuecomment-2273825367",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/87",
      "id": 2273825367,
      "node_id": "IC_kwDOMAMQI86Hh9JX",
      "user": {
        "login": "ciscodoojung",
        "id": 136669108,
        "node_id": "U_kgDOCCVntA",
        "avatar_url": "https://avatars.githubusercontent.com/u/136669108?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ciscodoojung",
        "html_url": "https://github.com/ciscodoojung",
        "followers_url": "https://api.github.com/users/ciscodoojung/followers",
        "following_url": "https://api.github.com/users/ciscodoojung/following{/other_user}",
        "gists_url": "https://api.github.com/users/ciscodoojung/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ciscodoojung/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ciscodoojung/subscriptions",
        "organizations_url": "https://api.github.com/users/ciscodoojung/orgs",
        "repos_url": "https://api.github.com/users/ciscodoojung/repos",
        "events_url": "https://api.github.com/users/ciscodoojung/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ciscodoojung/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-07T16:07:51Z",
      "updated_at": "2024-08-07T16:07:51Z",
      "author_association": "NONE",
      "body": "Just to update the thread (not related to this repo). \r\n\r\nHuggingface transformers v4.44.0 was released yesterday to support codestral-mamba training from HF. \r\nRelease: https://github.com/huggingface/transformers/releases/tag/v4.44.0\r\nPR: https://github.com/huggingface/transformers/pull/32080\r\n\r\nHope this helps! Thank you.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2273825367/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "85": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2371748234",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/85#issuecomment-2371748234",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/85",
      "id": 2371748234,
      "node_id": "IC_kwDOMAMQI86NXgGK",
      "user": {
        "login": "one-and-only",
        "id": 43624528,
        "node_id": "MDQ6VXNlcjQzNjI0NTI4",
        "avatar_url": "https://avatars.githubusercontent.com/u/43624528?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/one-and-only",
        "html_url": "https://github.com/one-and-only",
        "followers_url": "https://api.github.com/users/one-and-only/followers",
        "following_url": "https://api.github.com/users/one-and-only/following{/other_user}",
        "gists_url": "https://api.github.com/users/one-and-only/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/one-and-only/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/one-and-only/subscriptions",
        "organizations_url": "https://api.github.com/users/one-and-only/orgs",
        "repos_url": "https://api.github.com/users/one-and-only/repos",
        "events_url": "https://api.github.com/users/one-and-only/events{/privacy}",
        "received_events_url": "https://api.github.com/users/one-and-only/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-24T16:17:18Z",
      "updated_at": "2024-09-24T16:17:18Z",
      "author_association": "NONE",
      "body": "Did you remove the `no_eval` declaration that is already included in the `#other` section of the examle YAML? This seemed to solve that error for me.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2371748234/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "82": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2242817358",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/82#issuecomment-2242817358",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/82",
      "id": 2242817358,
      "node_id": "IC_kwDOMAMQI86Frq1O",
      "user": {
        "login": "ZTAP0011",
        "id": 17852197,
        "node_id": "MDQ6VXNlcjE3ODUyMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/17852197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ZTAP0011",
        "html_url": "https://github.com/ZTAP0011",
        "followers_url": "https://api.github.com/users/ZTAP0011/followers",
        "following_url": "https://api.github.com/users/ZTAP0011/following{/other_user}",
        "gists_url": "https://api.github.com/users/ZTAP0011/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ZTAP0011/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ZTAP0011/subscriptions",
        "organizations_url": "https://api.github.com/users/ZTAP0011/orgs",
        "repos_url": "https://api.github.com/users/ZTAP0011/repos",
        "events_url": "https://api.github.com/users/ZTAP0011/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ZTAP0011/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-22T12:17:42Z",
      "updated_at": "2024-07-22T12:17:42Z",
      "author_association": "NONE",
      "body": "![7B_yaml](https://github.com/user-attachments/assets/d63c9f07-f2da-4e7c-83b7-272b10804e31)\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2242817358/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2280410725",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/82#issuecomment-2280410725",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/82",
      "id": 2280410725,
      "node_id": "IC_kwDOMAMQI86H7E5l",
      "user": {
        "login": "kiranshivaraju",
        "id": 51982134,
        "node_id": "MDQ6VXNlcjUxOTgyMTM0",
        "avatar_url": "https://avatars.githubusercontent.com/u/51982134?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/kiranshivaraju",
        "html_url": "https://github.com/kiranshivaraju",
        "followers_url": "https://api.github.com/users/kiranshivaraju/followers",
        "following_url": "https://api.github.com/users/kiranshivaraju/following{/other_user}",
        "gists_url": "https://api.github.com/users/kiranshivaraju/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/kiranshivaraju/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/kiranshivaraju/subscriptions",
        "organizations_url": "https://api.github.com/users/kiranshivaraju/orgs",
        "repos_url": "https://api.github.com/users/kiranshivaraju/repos",
        "events_url": "https://api.github.com/users/kiranshivaraju/events{/privacy}",
        "received_events_url": "https://api.github.com/users/kiranshivaraju/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-10T08:51:02Z",
      "updated_at": "2024-08-10T08:51:02Z",
      "author_association": "NONE",
      "body": "can i know how you merged the model, because i am not able to merge the model in a 24GB VRAM, always run out of memory when i merge model\r\ninstance i am using: ml g5 12x large",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2280410725/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2283870129",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/82#issuecomment-2283870129",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/82",
      "id": 2283870129,
      "node_id": "IC_kwDOMAMQI86IIRex",
      "user": {
        "login": "pandora-s-git",
        "id": 128635000,
        "node_id": "U_kgDOB6rQeA",
        "avatar_url": "https://avatars.githubusercontent.com/u/128635000?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pandora-s-git",
        "html_url": "https://github.com/pandora-s-git",
        "followers_url": "https://api.github.com/users/pandora-s-git/followers",
        "following_url": "https://api.github.com/users/pandora-s-git/following{/other_user}",
        "gists_url": "https://api.github.com/users/pandora-s-git/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/pandora-s-git/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/pandora-s-git/subscriptions",
        "organizations_url": "https://api.github.com/users/pandora-s-git/orgs",
        "repos_url": "https://api.github.com/users/pandora-s-git/repos",
        "events_url": "https://api.github.com/users/pandora-s-git/events{/privacy}",
        "received_events_url": "https://api.github.com/users/pandora-s-git/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-12T12:39:40Z",
      "updated_at": "2024-08-12T12:40:16Z",
      "author_association": "COLLABORATOR",
      "body": "The model was trained with your data, and I've taken a look and I dont see any big difference, its just that after fine tuned it answers more closely to what your data suggested.\r\nI also noticed that you have a huge amount of cases where there is a single category, and this might also be playing a role. You do not have the category \"Tees\" at all in your dataset for example. There might be missing diversity.\r\nThe output from the fine tuned is not wrong per see neither- it does seem indeed like it was properly fine tuned with your data.\r\n\r\nOtherwise it can also depend on hyperparameters and the quantity of data you have. But your case is more related to the data itself.\r\n\r\nAlso, I noticed that your dataset has a lot of \"You are a shopping assistant for the user.\\n\\n input: viewed products\", what is \"viewed products\" here? \ud83e\udd14\r\n\r\nIm also confused why your expected result is to have the same output as the mistral 7b model, as usually we want to improve it?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2283870129/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2285550090",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/82#issuecomment-2285550090",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/82",
      "id": 2285550090,
      "node_id": "IC_kwDOMAMQI86IOroK",
      "user": {
        "login": "ZTAP0011",
        "id": 17852197,
        "node_id": "MDQ6VXNlcjE3ODUyMTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/17852197?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ZTAP0011",
        "html_url": "https://github.com/ZTAP0011",
        "followers_url": "https://api.github.com/users/ZTAP0011/followers",
        "following_url": "https://api.github.com/users/ZTAP0011/following{/other_user}",
        "gists_url": "https://api.github.com/users/ZTAP0011/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ZTAP0011/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ZTAP0011/subscriptions",
        "organizations_url": "https://api.github.com/users/ZTAP0011/orgs",
        "repos_url": "https://api.github.com/users/ZTAP0011/repos",
        "events_url": "https://api.github.com/users/ZTAP0011/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ZTAP0011/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-13T07:32:31Z",
      "updated_at": "2024-08-13T07:32:31Z",
      "author_association": "NONE",
      "body": "> can i know how you merged the model, because i am not able to merge the model in a 24GB VRAM, always run out of memory when i merge model instance i am using: ml g5 12x large\r\n\r\n@kiranshivaraju  we were able to do it using https://github.com/mistralai/mistral-finetune and using vast ai 1x A100 SXM4 instance having 80 GB VRAM.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2285550090/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "79": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2226547410",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/79#issuecomment-2226547410",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/79",
      "id": 2226547410,
      "node_id": "IC_kwDOMAMQI86EtmrS",
      "user": {
        "login": "bpcanedo",
        "id": 17790428,
        "node_id": "MDQ6VXNlcjE3NzkwNDI4",
        "avatar_url": "https://avatars.githubusercontent.com/u/17790428?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bpcanedo",
        "html_url": "https://github.com/bpcanedo",
        "followers_url": "https://api.github.com/users/bpcanedo/followers",
        "following_url": "https://api.github.com/users/bpcanedo/following{/other_user}",
        "gists_url": "https://api.github.com/users/bpcanedo/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/bpcanedo/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/bpcanedo/subscriptions",
        "organizations_url": "https://api.github.com/users/bpcanedo/orgs",
        "repos_url": "https://api.github.com/users/bpcanedo/repos",
        "events_url": "https://api.github.com/users/bpcanedo/events{/privacy}",
        "received_events_url": "https://api.github.com/users/bpcanedo/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-12T23:59:04Z",
      "updated_at": "2024-07-12T23:59:04Z",
      "author_association": "NONE",
      "body": "Similar issue using colab's T4\r\n\r\n![image](https://github.com/user-attachments/assets/367f71b8-2e62-49d0-b3e7-7376c051860a)\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2226547410/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "77": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2209200162",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/77#issuecomment-2209200162",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/77",
      "id": 2209200162,
      "node_id": "IC_kwDOMAMQI86Drbgi",
      "user": {
        "login": "pandora-s-git",
        "id": 128635000,
        "node_id": "U_kgDOB6rQeA",
        "avatar_url": "https://avatars.githubusercontent.com/u/128635000?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pandora-s-git",
        "html_url": "https://github.com/pandora-s-git",
        "followers_url": "https://api.github.com/users/pandora-s-git/followers",
        "following_url": "https://api.github.com/users/pandora-s-git/following{/other_user}",
        "gists_url": "https://api.github.com/users/pandora-s-git/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/pandora-s-git/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/pandora-s-git/subscriptions",
        "organizations_url": "https://api.github.com/users/pandora-s-git/orgs",
        "repos_url": "https://api.github.com/users/pandora-s-git/repos",
        "events_url": "https://api.github.com/users/pandora-s-git/events{/privacy}",
        "received_events_url": "https://api.github.com/users/pandora-s-git/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-04T15:07:29Z",
      "updated_at": "2024-07-04T15:07:29Z",
      "author_association": "COLLABORATOR",
      "body": "Thanks a lot and nice catch! We merged your fix, closing the issue now ;>",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2209200162/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "75": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2210985007",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/75#issuecomment-2210985007",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/75",
      "id": 2210985007,
      "node_id": "IC_kwDOMAMQI86DyPQv",
      "user": {
        "login": "tensimixt",
        "id": 174665635,
        "node_id": "U_kgDOCmkvow",
        "avatar_url": "https://avatars.githubusercontent.com/u/174665635?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/tensimixt",
        "html_url": "https://github.com/tensimixt",
        "followers_url": "https://api.github.com/users/tensimixt/followers",
        "following_url": "https://api.github.com/users/tensimixt/following{/other_user}",
        "gists_url": "https://api.github.com/users/tensimixt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/tensimixt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/tensimixt/subscriptions",
        "organizations_url": "https://api.github.com/users/tensimixt/orgs",
        "repos_url": "https://api.github.com/users/tensimixt/repos",
        "events_url": "https://api.github.com/users/tensimixt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/tensimixt/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-05T14:32:04Z",
      "updated_at": "2024-07-05T14:32:04Z",
      "author_association": "NONE",
      "body": "@patrickvonplaten Hi do you know if mistral-inference works for  lora+mixtral8x7b instruct v0.1? It does work for lora+mistral-7b v0.3  but getting error about LoRA weights file being loaded missing an expected key for one of the model layers when trying for lora+mixtral8x7b instruct v0.1 \r\n\r\nIs there something else required to make it work? \r\n\r\nThank you",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2210985007/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2219896063",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/75#issuecomment-2219896063",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/75",
      "id": 2219896063,
      "node_id": "IC_kwDOMAMQI86EUOz_",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-10T08:34:33Z",
      "updated_at": "2024-07-10T08:34:33Z",
      "author_association": "COLLABORATOR",
      "body": "Nice catch! We should fix this indeed",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2219896063/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2220326047",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/75#issuecomment-2220326047",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/75",
      "id": 2220326047,
      "node_id": "IC_kwDOMAMQI86EV3yf",
      "user": {
        "login": "tensimixt",
        "id": 174665635,
        "node_id": "U_kgDOCmkvow",
        "avatar_url": "https://avatars.githubusercontent.com/u/174665635?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/tensimixt",
        "html_url": "https://github.com/tensimixt",
        "followers_url": "https://api.github.com/users/tensimixt/followers",
        "following_url": "https://api.github.com/users/tensimixt/following{/other_user}",
        "gists_url": "https://api.github.com/users/tensimixt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/tensimixt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/tensimixt/subscriptions",
        "organizations_url": "https://api.github.com/users/tensimixt/orgs",
        "repos_url": "https://api.github.com/users/tensimixt/repos",
        "events_url": "https://api.github.com/users/tensimixt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/tensimixt/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-10T11:58:21Z",
      "updated_at": "2024-07-10T11:58:48Z",
      "author_association": "NONE",
      "body": "> Nice catch! We should fix this indeed\r\n\r\nThank you! Do you think that mistral-finetune is creating bad LoRA's for when finetuning mixtral 8x7b v0.1 instruct? \r\n Is there a place in the repo worth checking and updating where you think this issue is arising from? ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2220326047/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2229478492",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/75#issuecomment-2229478492",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/75",
      "id": 2229478492,
      "node_id": "IC_kwDOMAMQI86E4yRc",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-15T21:32:53Z",
      "updated_at": "2024-07-15T21:32:53Z",
      "author_association": "COLLABORATOR",
      "body": "Sorry to reply only now. We'll make an update to mistral-inference that should make sure that a LoRA trained with the 8x7B will work correctly with mistral-chat. Sorry to have you waiting for so long",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2229478492/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 1,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2231440332",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/75#issuecomment-2231440332",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/75",
      "id": 2231440332,
      "node_id": "IC_kwDOMAMQI86FARPM",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-16T17:21:10Z",
      "updated_at": "2024-07-16T17:21:10Z",
      "author_association": "COLLABORATOR",
      "body": "Can you check if you still encounter the problem when installing `pip install mistral_inference>=1.2.0` ? ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2231440332/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "74": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2174511998",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2174511998",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2174511998,
      "node_id": "IC_kwDOMAMQI86BnGt-",
      "user": {
        "login": "mkserge",
        "id": 2992022,
        "node_id": "MDQ6VXNlcjI5OTIwMjI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2992022?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mkserge",
        "html_url": "https://github.com/mkserge",
        "followers_url": "https://api.github.com/users/mkserge/followers",
        "following_url": "https://api.github.com/users/mkserge/following{/other_user}",
        "gists_url": "https://api.github.com/users/mkserge/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mkserge/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mkserge/subscriptions",
        "organizations_url": "https://api.github.com/users/mkserge/orgs",
        "repos_url": "https://api.github.com/users/mkserge/repos",
        "events_url": "https://api.github.com/users/mkserge/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mkserge/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-17T22:05:04Z",
      "updated_at": "2024-06-17T22:05:04Z",
      "author_association": "NONE",
      "body": "You can do something like this\r\n```\r\nfrom mistral_inference.model import Transformer\r\nmodel = Transformer.from_folder(args.model_path, device=f\"cuda:0\")\r\nmodel.load_lora(\"/path/to/lora.safetensors\", device=f\"cuda:0\")\r\nsafetensors.torch.save_model(model, \"/path/to/merged.safetensors\")\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2174511998/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2175597435",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2175597435",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2175597435,
      "node_id": "IC_kwDOMAMQI86BrPt7",
      "user": {
        "login": "forest520",
        "id": 339245,
        "node_id": "MDQ6VXNlcjMzOTI0NQ==",
        "avatar_url": "https://avatars.githubusercontent.com/u/339245?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/forest520",
        "html_url": "https://github.com/forest520",
        "followers_url": "https://api.github.com/users/forest520/followers",
        "following_url": "https://api.github.com/users/forest520/following{/other_user}",
        "gists_url": "https://api.github.com/users/forest520/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/forest520/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/forest520/subscriptions",
        "organizations_url": "https://api.github.com/users/forest520/orgs",
        "repos_url": "https://api.github.com/users/forest520/repos",
        "events_url": "https://api.github.com/users/forest520/events{/privacy}",
        "received_events_url": "https://api.github.com/users/forest520/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-18T09:09:51Z",
      "updated_at": "2024-06-18T09:09:51Z",
      "author_association": "NONE",
      "body": "How to perform inference with a LoRA model using Python code, if save_adapters = True?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2175597435/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2233634206",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2233634206",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2233634206,
      "node_id": "IC_kwDOMAMQI86FIo2e",
      "user": {
        "login": "kehuitt",
        "id": 174309856,
        "node_id": "U_kgDOCmPB4A",
        "avatar_url": "https://avatars.githubusercontent.com/u/174309856?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/kehuitt",
        "html_url": "https://github.com/kehuitt",
        "followers_url": "https://api.github.com/users/kehuitt/followers",
        "following_url": "https://api.github.com/users/kehuitt/following{/other_user}",
        "gists_url": "https://api.github.com/users/kehuitt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/kehuitt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/kehuitt/subscriptions",
        "organizations_url": "https://api.github.com/users/kehuitt/orgs",
        "repos_url": "https://api.github.com/users/kehuitt/repos",
        "events_url": "https://api.github.com/users/kehuitt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/kehuitt/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-17T15:45:28Z",
      "updated_at": "2024-07-17T15:45:51Z",
      "author_association": "NONE",
      "body": "> You can do something like this\r\n> \r\n> ```\r\n> from mistral_inference.model import Transformer\r\n> model = Transformer.from_folder(args.model_path, device=f\"cuda:0\")\r\n> model.load_lora(\"/path/to/lora.safetensors\", device=f\"cuda:0\")\r\n> safetensors.torch.save_model(model, \"/path/to/merged.safetensors\")\r\n> ```\r\n\r\nWhen I run this, I got 'ImportError: cannot import name 'Transformer' from 'mistral_inference.model'', the version of mistral_inference=1.2.0, how can I fix this problem? Thx!",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2233634206/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2233648073",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2233648073",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2233648073,
      "node_id": "IC_kwDOMAMQI86FIsPJ",
      "user": {
        "login": "pandora-s-git",
        "id": 128635000,
        "node_id": "U_kgDOB6rQeA",
        "avatar_url": "https://avatars.githubusercontent.com/u/128635000?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/pandora-s-git",
        "html_url": "https://github.com/pandora-s-git",
        "followers_url": "https://api.github.com/users/pandora-s-git/followers",
        "following_url": "https://api.github.com/users/pandora-s-git/following{/other_user}",
        "gists_url": "https://api.github.com/users/pandora-s-git/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/pandora-s-git/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/pandora-s-git/subscriptions",
        "organizations_url": "https://api.github.com/users/pandora-s-git/orgs",
        "repos_url": "https://api.github.com/users/pandora-s-git/repos",
        "events_url": "https://api.github.com/users/pandora-s-git/events{/privacy}",
        "received_events_url": "https://api.github.com/users/pandora-s-git/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-17T15:51:41Z",
      "updated_at": "2024-07-17T15:51:41Z",
      "author_association": "COLLABORATOR",
      "body": "Try with `from mistral_inference.transformer import Transformer` as it was very recently updated with the codestral mamba release! ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2233648073/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2240901211",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2240901211",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2240901211,
      "node_id": "IC_kwDOMAMQI86FkXBb",
      "user": {
        "login": "kehuitt",
        "id": 174309856,
        "node_id": "U_kgDOCmPB4A",
        "avatar_url": "https://avatars.githubusercontent.com/u/174309856?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/kehuitt",
        "html_url": "https://github.com/kehuitt",
        "followers_url": "https://api.github.com/users/kehuitt/followers",
        "following_url": "https://api.github.com/users/kehuitt/following{/other_user}",
        "gists_url": "https://api.github.com/users/kehuitt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/kehuitt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/kehuitt/subscriptions",
        "organizations_url": "https://api.github.com/users/kehuitt/orgs",
        "repos_url": "https://api.github.com/users/kehuitt/repos",
        "events_url": "https://api.github.com/users/kehuitt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/kehuitt/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-20T03:58:40Z",
      "updated_at": "2024-07-20T03:58:40Z",
      "author_association": "NONE",
      "body": "A single GPU doesn't seem to be able to load the entire Mixtral-8x7B-v0.1-Instruct model, how should I merge the model using multiple cards? Thanks!\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2240901211/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2297838291",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2297838291",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2297838291,
      "node_id": "IC_kwDOMAMQI86I9jrT",
      "user": {
        "login": "leloss",
        "id": 8379486,
        "node_id": "MDQ6VXNlcjgzNzk0ODY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8379486?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/leloss",
        "html_url": "https://github.com/leloss",
        "followers_url": "https://api.github.com/users/leloss/followers",
        "following_url": "https://api.github.com/users/leloss/following{/other_user}",
        "gists_url": "https://api.github.com/users/leloss/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/leloss/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/leloss/subscriptions",
        "organizations_url": "https://api.github.com/users/leloss/orgs",
        "repos_url": "https://api.github.com/users/leloss/repos",
        "events_url": "https://api.github.com/users/leloss/events{/privacy}",
        "received_events_url": "https://api.github.com/users/leloss/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-20T02:20:17Z",
      "updated_at": "2024-08-20T02:20:17Z",
      "author_association": "NONE",
      "body": "> A single GPU doesn't seem to be able to load the entire Mixtral-8x7B-v0.1-Instruct model, how should I merge the model using multiple cards? Thanks!\r\n\r\nApparently, the only merging method available today relies on loading everything on the same device, which forces us to rent out a 40GB GPU instance like the p4d.24xlarge for the 7B model. Someone (please) correct me if I'm wrong.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2297838291/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2339374465",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/74#issuecomment-2339374465",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/74",
      "id": 2339374465,
      "node_id": "IC_kwDOMAMQI86LcAWB",
      "user": {
        "login": "abhishekdhankar95",
        "id": 21701086,
        "node_id": "MDQ6VXNlcjIxNzAxMDg2",
        "avatar_url": "https://avatars.githubusercontent.com/u/21701086?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/abhishekdhankar95",
        "html_url": "https://github.com/abhishekdhankar95",
        "followers_url": "https://api.github.com/users/abhishekdhankar95/followers",
        "following_url": "https://api.github.com/users/abhishekdhankar95/following{/other_user}",
        "gists_url": "https://api.github.com/users/abhishekdhankar95/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/abhishekdhankar95/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/abhishekdhankar95/subscriptions",
        "organizations_url": "https://api.github.com/users/abhishekdhankar95/orgs",
        "repos_url": "https://api.github.com/users/abhishekdhankar95/repos",
        "events_url": "https://api.github.com/users/abhishekdhankar95/events{/privacy}",
        "received_events_url": "https://api.github.com/users/abhishekdhankar95/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-09-10T00:20:27Z",
      "updated_at": "2024-09-10T00:20:48Z",
      "author_association": "NONE",
      "body": "mistral-finetune has a requirement of torch==2.2, whereas mistral-inference has a requirement of torch==2.3.0 for all but the first release.\r\nIs there anyway to have the two of them in the same conda environment without conflicting requirements?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2339374465/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "71": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2167788394",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/71#issuecomment-2167788394",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/71",
      "id": 2167788394,
      "node_id": "IC_kwDOMAMQI86BNdNq",
      "user": {
        "login": "banalg",
        "id": 2079896,
        "node_id": "MDQ6VXNlcjIwNzk4OTY=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2079896?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/banalg",
        "html_url": "https://github.com/banalg",
        "followers_url": "https://api.github.com/users/banalg/followers",
        "following_url": "https://api.github.com/users/banalg/following{/other_user}",
        "gists_url": "https://api.github.com/users/banalg/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/banalg/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/banalg/subscriptions",
        "organizations_url": "https://api.github.com/users/banalg/orgs",
        "repos_url": "https://api.github.com/users/banalg/repos",
        "events_url": "https://api.github.com/users/banalg/events{/privacy}",
        "received_events_url": "https://api.github.com/users/banalg/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-14T11:04:29Z",
      "updated_at": "2024-06-14T11:04:29Z",
      "author_association": "NONE",
      "body": "It's working now. We simply halted the instance for the night, and after restarting it in the morning, the fine-tuning with all 4 GPUs worked. It's \"tomb\u00e9 en marche,\" as we usually say, but I would prefer to understand why we had issues in the first place. Our instance likely started on another server than yesterday. Could you please recommend some checks to detect the hardware and software configurations of the server that could impact the parallel GPU fine-tuning?\r\n\r\nWe'll wait a few days before closing this issue.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2167788394/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2185339120",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/71#issuecomment-2185339120",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/71",
      "id": 2185339120,
      "node_id": "IC_kwDOMAMQI86CQaDw",
      "user": {
        "login": "Aniket-J",
        "id": 54844902,
        "node_id": "MDQ6VXNlcjU0ODQ0OTAy",
        "avatar_url": "https://avatars.githubusercontent.com/u/54844902?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Aniket-J",
        "html_url": "https://github.com/Aniket-J",
        "followers_url": "https://api.github.com/users/Aniket-J/followers",
        "following_url": "https://api.github.com/users/Aniket-J/following{/other_user}",
        "gists_url": "https://api.github.com/users/Aniket-J/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/Aniket-J/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/Aniket-J/subscriptions",
        "organizations_url": "https://api.github.com/users/Aniket-J/orgs",
        "repos_url": "https://api.github.com/users/Aniket-J/repos",
        "events_url": "https://api.github.com/users/Aniket-J/events{/privacy}",
        "received_events_url": "https://api.github.com/users/Aniket-J/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-23T22:12:43Z",
      "updated_at": "2024-06-23T22:12:43Z",
      "author_association": "NONE",
      "body": "Did you figure out what's been prompting this? Similar setup as yours, tried with NCCL_P2P_DISABLE set to 1, however we're using g4.12xlarge and not g5",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2185339120/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2270267684",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/71#issuecomment-2270267684",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/71",
      "id": 2270267684,
      "node_id": "IC_kwDOMAMQI86HUYkk",
      "user": {
        "login": "SaiKrishnaBala",
        "id": 4848584,
        "node_id": "MDQ6VXNlcjQ4NDg1ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4848584?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/SaiKrishnaBala",
        "html_url": "https://github.com/SaiKrishnaBala",
        "followers_url": "https://api.github.com/users/SaiKrishnaBala/followers",
        "following_url": "https://api.github.com/users/SaiKrishnaBala/following{/other_user}",
        "gists_url": "https://api.github.com/users/SaiKrishnaBala/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/SaiKrishnaBala/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/SaiKrishnaBala/subscriptions",
        "organizations_url": "https://api.github.com/users/SaiKrishnaBala/orgs",
        "repos_url": "https://api.github.com/users/SaiKrishnaBala/repos",
        "events_url": "https://api.github.com/users/SaiKrishnaBala/events{/privacy}",
        "received_events_url": "https://api.github.com/users/SaiKrishnaBala/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-06T02:47:23Z",
      "updated_at": "2024-08-06T02:47:23Z",
      "author_association": "NONE",
      "body": "Is it possible to run these scripts on a ray cluster as a training job?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2270267684/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "69": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2172362611",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2172362611",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2172362611,
      "node_id": "IC_kwDOMAMQI86Be59z",
      "user": {
        "login": "CodeWithOz",
        "id": 28525986,
        "node_id": "MDQ6VXNlcjI4NTI1OTg2",
        "avatar_url": "https://avatars.githubusercontent.com/u/28525986?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CodeWithOz",
        "html_url": "https://github.com/CodeWithOz",
        "followers_url": "https://api.github.com/users/CodeWithOz/followers",
        "following_url": "https://api.github.com/users/CodeWithOz/following{/other_user}",
        "gists_url": "https://api.github.com/users/CodeWithOz/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CodeWithOz/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CodeWithOz/subscriptions",
        "organizations_url": "https://api.github.com/users/CodeWithOz/orgs",
        "repos_url": "https://api.github.com/users/CodeWithOz/repos",
        "events_url": "https://api.github.com/users/CodeWithOz/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CodeWithOz/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-17T06:08:16Z",
      "updated_at": "2024-06-17T06:08:16Z",
      "author_association": "NONE",
      "body": "**UPDATE**: I tried using an A100 single GPU with 40GB GPU memory and the same error happened. Seems there's a memory leak somewhere because the process just used up all the available memory. Here's the updated error message:\r\n> torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.33 GiB is free. Process 35938 has 38.05 GiB memory in use. Of the allocated memory 35.03 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2172362611/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2172364203",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2172364203",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2172364203,
      "node_id": "IC_kwDOMAMQI86Be6Wr",
      "user": {
        "login": "CodeWithOz",
        "id": 28525986,
        "node_id": "MDQ6VXNlcjI4NTI1OTg2",
        "avatar_url": "https://avatars.githubusercontent.com/u/28525986?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CodeWithOz",
        "html_url": "https://github.com/CodeWithOz",
        "followers_url": "https://api.github.com/users/CodeWithOz/followers",
        "following_url": "https://api.github.com/users/CodeWithOz/following{/other_user}",
        "gists_url": "https://api.github.com/users/CodeWithOz/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CodeWithOz/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CodeWithOz/subscriptions",
        "organizations_url": "https://api.github.com/users/CodeWithOz/orgs",
        "repos_url": "https://api.github.com/users/CodeWithOz/repos",
        "events_url": "https://api.github.com/users/CodeWithOz/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CodeWithOz/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-17T06:09:37Z",
      "updated_at": "2024-06-17T06:09:37Z",
      "author_association": "NONE",
      "body": "I'm renting my GPUs from [brev.dev](https://console.brev.dev/) by the way. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2172364203/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2189281030",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2189281030",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2189281030,
      "node_id": "IC_kwDOMAMQI86CfccG",
      "user": {
        "login": "zazabap",
        "id": 97461865,
        "node_id": "U_kgDOBc8maQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/97461865?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/zazabap",
        "html_url": "https://github.com/zazabap",
        "followers_url": "https://api.github.com/users/zazabap/followers",
        "following_url": "https://api.github.com/users/zazabap/following{/other_user}",
        "gists_url": "https://api.github.com/users/zazabap/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/zazabap/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/zazabap/subscriptions",
        "organizations_url": "https://api.github.com/users/zazabap/orgs",
        "repos_url": "https://api.github.com/users/zazabap/repos",
        "events_url": "https://api.github.com/users/zazabap/events{/privacy}",
        "received_events_url": "https://api.github.com/users/zazabap/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-25T15:35:46Z",
      "updated_at": "2024-06-25T15:35:46Z",
      "author_association": "NONE",
      "body": "Is this issue resolved? I kind of encounter exactly same issue. ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2189281030/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2192841949",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2192841949",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2192841949,
      "node_id": "IC_kwDOMAMQI86CtBzd",
      "user": {
        "login": "matheus-prandini",
        "id": 152647513,
        "node_id": "U_kgDOCRk3WQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/152647513?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/matheus-prandini",
        "html_url": "https://github.com/matheus-prandini",
        "followers_url": "https://api.github.com/users/matheus-prandini/followers",
        "following_url": "https://api.github.com/users/matheus-prandini/following{/other_user}",
        "gists_url": "https://api.github.com/users/matheus-prandini/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/matheus-prandini/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/matheus-prandini/subscriptions",
        "organizations_url": "https://api.github.com/users/matheus-prandini/orgs",
        "repos_url": "https://api.github.com/users/matheus-prandini/repos",
        "events_url": "https://api.github.com/users/matheus-prandini/events{/privacy}",
        "received_events_url": "https://api.github.com/users/matheus-prandini/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-27T00:34:42Z",
      "updated_at": "2024-06-27T00:34:59Z",
      "author_association": "CONTRIBUTOR",
      "body": "@CodeWithOz @zazabap Was the GPU running only the training? Can you provide the command you used to run the training? Additionally, what libraries and versions are you using?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2192841949/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298732755",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2298732755",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2298732755,
      "node_id": "IC_kwDOMAMQI86JA-DT",
      "user": {
        "login": "C3po-D2rd2",
        "id": 172624326,
        "node_id": "U_kgDOCkoJxg",
        "avatar_url": "https://avatars.githubusercontent.com/u/172624326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/C3po-D2rd2",
        "html_url": "https://github.com/C3po-D2rd2",
        "followers_url": "https://api.github.com/users/C3po-D2rd2/followers",
        "following_url": "https://api.github.com/users/C3po-D2rd2/following{/other_user}",
        "gists_url": "https://api.github.com/users/C3po-D2rd2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/C3po-D2rd2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/C3po-D2rd2/subscriptions",
        "organizations_url": "https://api.github.com/users/C3po-D2rd2/orgs",
        "repos_url": "https://api.github.com/users/C3po-D2rd2/repos",
        "events_url": "https://api.github.com/users/C3po-D2rd2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/C3po-D2rd2/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-20T12:25:14Z",
      "updated_at": "2024-08-20T12:28:49Z",
      "author_association": "NONE",
      "body": "Hello,\r\n  It seems I have the same issue:\r\n  ``torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 806.12 MiB is free. Including non-PyTorch memory, this process has 21.16 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)``\r\n  \r\n  \r\nDid you find a solution?\r\n\r\n\r\nI am on NVIDIA L4 with 24Go RAM\r\n\r\n  ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298732755/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298964065",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2298964065",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2298964065,
      "node_id": "IC_kwDOMAMQI86JB2hh",
      "user": {
        "login": "C3po-D2rd2",
        "id": 172624326,
        "node_id": "U_kgDOCkoJxg",
        "avatar_url": "https://avatars.githubusercontent.com/u/172624326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/C3po-D2rd2",
        "html_url": "https://github.com/C3po-D2rd2",
        "followers_url": "https://api.github.com/users/C3po-D2rd2/followers",
        "following_url": "https://api.github.com/users/C3po-D2rd2/following{/other_user}",
        "gists_url": "https://api.github.com/users/C3po-D2rd2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/C3po-D2rd2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/C3po-D2rd2/subscriptions",
        "organizations_url": "https://api.github.com/users/C3po-D2rd2/orgs",
        "repos_url": "https://api.github.com/users/C3po-D2rd2/repos",
        "events_url": "https://api.github.com/users/C3po-D2rd2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/C3po-D2rd2/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-20T14:11:32Z",
      "updated_at": "2024-08-20T14:11:32Z",
      "author_association": "NONE",
      "body": "for record, I could manage by setting seq_len: 4192 #32768                                                                                                                   \r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298964065/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2477008483",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/69#issuecomment-2477008483",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/69",
      "id": 2477008483,
      "node_id": "IC_kwDOMAMQI86TpCZj",
      "user": {
        "login": "danigarciaoca",
        "id": 27818263,
        "node_id": "MDQ6VXNlcjI3ODE4MjYz",
        "avatar_url": "https://avatars.githubusercontent.com/u/27818263?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/danigarciaoca",
        "html_url": "https://github.com/danigarciaoca",
        "followers_url": "https://api.github.com/users/danigarciaoca/followers",
        "following_url": "https://api.github.com/users/danigarciaoca/following{/other_user}",
        "gists_url": "https://api.github.com/users/danigarciaoca/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/danigarciaoca/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/danigarciaoca/subscriptions",
        "organizations_url": "https://api.github.com/users/danigarciaoca/orgs",
        "repos_url": "https://api.github.com/users/danigarciaoca/repos",
        "events_url": "https://api.github.com/users/danigarciaoca/events{/privacy}",
        "received_events_url": "https://api.github.com/users/danigarciaoca/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-11-14T17:24:38Z",
      "updated_at": "2024-11-14T17:24:38Z",
      "author_association": "NONE",
      "body": "Hi! For me it worked export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2477008483/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "66": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2167321871",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/66#issuecomment-2167321871",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/66",
      "id": 2167321871,
      "node_id": "IC_kwDOMAMQI86BLrUP",
      "user": {
        "login": "noviljohnson",
        "id": 41065220,
        "node_id": "MDQ6VXNlcjQxMDY1MjIw",
        "avatar_url": "https://avatars.githubusercontent.com/u/41065220?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/noviljohnson",
        "html_url": "https://github.com/noviljohnson",
        "followers_url": "https://api.github.com/users/noviljohnson/followers",
        "following_url": "https://api.github.com/users/noviljohnson/following{/other_user}",
        "gists_url": "https://api.github.com/users/noviljohnson/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/noviljohnson/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/noviljohnson/subscriptions",
        "organizations_url": "https://api.github.com/users/noviljohnson/orgs",
        "repos_url": "https://api.github.com/users/noviljohnson/repos",
        "events_url": "https://api.github.com/users/noviljohnson/events{/privacy}",
        "received_events_url": "https://api.github.com/users/noviljohnson/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-14T06:33:30Z",
      "updated_at": "2024-06-14T06:33:30Z",
      "author_association": "NONE",
      "body": "\"",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2167321871/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "64": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2157864717",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/64#issuecomment-2157864717",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/64",
      "id": 2157864717,
      "node_id": "IC_kwDOMAMQI86AnmcN",
      "user": {
        "login": "alxtkeng",
        "id": 152399456,
        "node_id": "U_kgDOCRVuYA",
        "avatar_url": "https://avatars.githubusercontent.com/u/152399456?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/alxtkeng",
        "html_url": "https://github.com/alxtkeng",
        "followers_url": "https://api.github.com/users/alxtkeng/followers",
        "following_url": "https://api.github.com/users/alxtkeng/following{/other_user}",
        "gists_url": "https://api.github.com/users/alxtkeng/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/alxtkeng/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/alxtkeng/subscriptions",
        "organizations_url": "https://api.github.com/users/alxtkeng/orgs",
        "repos_url": "https://api.github.com/users/alxtkeng/repos",
        "events_url": "https://api.github.com/users/alxtkeng/events{/privacy}",
        "received_events_url": "https://api.github.com/users/alxtkeng/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-10T09:43:31Z",
      "updated_at": "2024-06-10T09:43:31Z",
      "author_association": "NONE",
      "body": "I am having the same question",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2157864717/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2159595511",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/64#issuecomment-2159595511",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/64",
      "id": 2159595511,
      "node_id": "IC_kwDOMAMQI86AuM_3",
      "user": {
        "login": "bensonbs",
        "id": 120996184,
        "node_id": "U_kgDOBzZBWA",
        "avatar_url": "https://avatars.githubusercontent.com/u/120996184?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/bensonbs",
        "html_url": "https://github.com/bensonbs",
        "followers_url": "https://api.github.com/users/bensonbs/followers",
        "following_url": "https://api.github.com/users/bensonbs/following{/other_user}",
        "gists_url": "https://api.github.com/users/bensonbs/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/bensonbs/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/bensonbs/subscriptions",
        "organizations_url": "https://api.github.com/users/bensonbs/orgs",
        "repos_url": "https://api.github.com/users/bensonbs/repos",
        "events_url": "https://api.github.com/users/bensonbs/events{/privacy}",
        "received_events_url": "https://api.github.com/users/bensonbs/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-11T01:27:03Z",
      "updated_at": "2024-06-11T01:28:14Z",
      "author_association": "NONE",
      "body": "I am using Llama.cpp to convert the model format to GGUF, but it seems to only support the Mistral v0.1 format. Therefore, I made some name conversions.\r\n\r\n```python\r\nfrom mistral_inference.model import Transformer\r\nfrom safetensors.torch import save_file, load_file\r\n\r\n# Define the reverse layer name conversion rules\r\ndef reverse_convert_layer_name(name):\r\n    reverse_layer_mapping = {\r\n        \"tok_embeddings.weight\": \"model.embed_tokens.weight\",\r\n        \"norm.weight\": \"model.norm.weight\",\r\n        \"output.weight\": \"lm_head.weight\"\r\n    }\r\n    \r\n    if name in reverse_layer_mapping:\r\n        return reverse_layer_mapping[name]\r\n    \r\n    parts = name.split(\".\")\r\n    if len(parts) < 3:\r\n        return name\r\n    \r\n    layer_num = parts[1]\r\n    if parts[2] == \"ffn_norm\":\r\n        return f\"model.layers.{layer_num}.input_layernorm.weight\"\r\n    elif parts[2] == \"attention_norm\":\r\n        return f\"model.layers.{layer_num}.post_attention_layernorm.weight\"\r\n    elif parts[2] == \"attention\":\r\n        attn_reverse_mapping = {\r\n            \"wk\": \"k_proj\",\r\n            \"wv\": \"v_proj\",\r\n            \"wq\": \"q_proj\",\r\n            \"wo\": \"o_proj\"\r\n        }\r\n        if parts[3] in attn_reverse_mapping:\r\n            return f\"model.layers.{layer_num}.self_attn.{attn_reverse_mapping[parts[3]]}.weight\"\r\n    elif parts[2] == \"feed_forward\":\r\n        mlp_reverse_mapping = {\r\n            \"w2\": \"down_proj\",\r\n            \"w1\": \"gate_proj\",\r\n            \"w3\": \"up_proj\"\r\n        }\r\n        if parts[3] in mlp_reverse_mapping:\r\n            return f\"model.layers.{layer_num}.mlp.{mlp_reverse_mapping[parts[3]]}.weight\"\r\n    \r\n    return name\r\n\r\n# Load the original model\r\nmodel = Transformer.from_folder(\"/mnt/share/LLM/Breeze-7B-Instruct-v1_0\")\r\nmodel.to('cpu')\r\n\r\n# Load the LoRA weights\r\nlora_weights = load_file(\"/mnt/share/LLM/mistral_models/breeze-7b-lora/checkpoints/checkpoint_000100/consolidated/lora.safetensors\")\r\n\r\n# Apply the LoRA weights to the model\r\nfor name, param in model.named_parameters():\r\n    if name in lora_weights:\r\n        param.data += lora_weights[name].data\r\n\r\n# Extract the model's state_dict\r\nstate_dict = model.state_dict()\r\n\r\n# Create a new dictionary to store the converted layer names\r\nnew_state_dict = {}\r\nfor name, param in state_dict.items():\r\n    new_name = reverse_convert_layer_name(name)\r\n    new_state_dict[new_name] = param\r\n\r\n# Save the new model as a safetensors file\r\nsave_file(new_state_dict, \"/mnt/share/LLM/Breeze-7B-z0.1/model.safetensors\")\r\n\r\n# Confirm the save was successful\r\nprint(\"Model has been successfully saved as a safetensors file.\")\r\n```\r\n\r\n### Config.json Changes\r\n\r\nThe `config.json` needs to be changed to the following:\r\n\r\n```json\r\n{\r\n  \"architectures\": [\r\n    \"MistralForCausalLM\"\r\n  ],\r\n  \"attention_dropout\": 0.0,\r\n  \"bos_token_id\": 1,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 4096,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 14336,\r\n  \"max_position_embeddings\": 32768,\r\n  \"model_type\": \"mistral\",\r\n  \"num_attention_heads\": 32,\r\n  \"num_hidden_layers\": 32,\r\n  \"num_key_value_heads\": 8,\r\n  \"output_router_logits\": true,\r\n  \"pretraining_tp\": 1,\r\n  \"rms_norm_eps\": 1e-05,\r\n  \"rope_theta\": 10000.0,\r\n  \"sliding_window\": 4096,\r\n  \"tie_word_embeddings\": false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.37.2\",\r\n  \"use_cache\": false,\r\n  \"vocab_size\": xxxxxx\r\n}\r\n```\r\n\r\nPlease replace `xxxxxx` with the actual vocabulary size.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2159595511/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2162644101",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/64#issuecomment-2162644101",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/64",
      "id": 2162644101,
      "node_id": "IC_kwDOMAMQI86A51SF",
      "user": {
        "login": "alexsgit",
        "id": 2076932,
        "node_id": "MDQ6VXNlcjIwNzY5MzI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2076932?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/alexsgit",
        "html_url": "https://github.com/alexsgit",
        "followers_url": "https://api.github.com/users/alexsgit/followers",
        "following_url": "https://api.github.com/users/alexsgit/following{/other_user}",
        "gists_url": "https://api.github.com/users/alexsgit/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/alexsgit/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/alexsgit/subscriptions",
        "organizations_url": "https://api.github.com/users/alexsgit/orgs",
        "repos_url": "https://api.github.com/users/alexsgit/repos",
        "events_url": "https://api.github.com/users/alexsgit/events{/privacy}",
        "received_events_url": "https://api.github.com/users/alexsgit/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-12T10:18:35Z",
      "updated_at": "2024-06-12T10:18:35Z",
      "author_association": "NONE",
      "body": "You can also use [convert_mistral_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/convert_mistral_weights_to_hf.py) \r\nto save the model as hugging face model and then use llama.cpp to convert from hf to gguf.\r\n```\r\npip install sentencepiece accelerate\r\npython [path to the HF repo]/src/transformers/models/mistral/convert_mistral_weights_to_hf.py --input_dir [model dir] --model_size 7B --is_v3 --output_dir [new hf model]\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2162644101/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "28": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2143809112",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/28#issuecomment-2143809112",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/28",
      "id": 2143809112,
      "node_id": "IC_kwDOMAMQI85_x-5Y",
      "user": {
        "login": "thegallier",
        "id": 12978650,
        "node_id": "MDQ6VXNlcjEyOTc4NjUw",
        "avatar_url": "https://avatars.githubusercontent.com/u/12978650?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/thegallier",
        "html_url": "https://github.com/thegallier",
        "followers_url": "https://api.github.com/users/thegallier/followers",
        "following_url": "https://api.github.com/users/thegallier/following{/other_user}",
        "gists_url": "https://api.github.com/users/thegallier/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/thegallier/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/thegallier/subscriptions",
        "organizations_url": "https://api.github.com/users/thegallier/orgs",
        "repos_url": "https://api.github.com/users/thegallier/repos",
        "events_url": "https://api.github.com/users/thegallier/events{/privacy}",
        "received_events_url": "https://api.github.com/users/thegallier/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-02T11:21:49Z",
      "updated_at": "2024-06-02T11:21:49Z",
      "author_association": "NONE",
      "body": "Same with the save_pretrained.  I did not see that functionality in the current code base and hence can not leverage open source packages.  ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2143809112/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "25": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2141967391",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/25#issuecomment-2141967391",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/25",
      "id": 2141967391,
      "node_id": "IC_kwDOMAMQI85_q9Qf",
      "user": {
        "login": "Praveenmurali986",
        "id": 89091306,
        "node_id": "MDQ6VXNlcjg5MDkxMzA2",
        "avatar_url": "https://avatars.githubusercontent.com/u/89091306?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Praveenmurali986",
        "html_url": "https://github.com/Praveenmurali986",
        "followers_url": "https://api.github.com/users/Praveenmurali986/followers",
        "following_url": "https://api.github.com/users/Praveenmurali986/following{/other_user}",
        "gists_url": "https://api.github.com/users/Praveenmurali986/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/Praveenmurali986/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/Praveenmurali986/subscriptions",
        "organizations_url": "https://api.github.com/users/Praveenmurali986/orgs",
        "repos_url": "https://api.github.com/users/Praveenmurali986/repos",
        "events_url": "https://api.github.com/users/Praveenmurali986/events{/privacy}",
        "received_events_url": "https://api.github.com/users/Praveenmurali986/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-31T12:23:17Z",
      "updated_at": "2024-05-31T12:23:17Z",
      "author_association": "NONE",
      "body": "I have the same error",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2141967391/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "24": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2138387303",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/24#issuecomment-2138387303",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/24",
      "id": 2138387303,
      "node_id": "IC_kwDOMAMQI85_dTNn",
      "user": {
        "login": "donwany",
        "id": 10055091,
        "node_id": "MDQ6VXNlcjEwMDU1MDkx",
        "avatar_url": "https://avatars.githubusercontent.com/u/10055091?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/donwany",
        "html_url": "https://github.com/donwany",
        "followers_url": "https://api.github.com/users/donwany/followers",
        "following_url": "https://api.github.com/users/donwany/following{/other_user}",
        "gists_url": "https://api.github.com/users/donwany/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/donwany/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/donwany/subscriptions",
        "organizations_url": "https://api.github.com/users/donwany/orgs",
        "repos_url": "https://api.github.com/users/donwany/repos",
        "events_url": "https://api.github.com/users/donwany/events{/privacy}",
        "received_events_url": "https://api.github.com/users/donwany/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-29T22:58:10Z",
      "updated_at": "2024-05-29T22:59:05Z",
      "author_association": "NONE",
      "body": "SET THIS IN THE YAML FILE\r\n\r\nmax_steps: 300\r\nrun_dir: \"/Users/johndoe/ultra_chat_test\"\r\nwandb.project: ultra_chat\r\n\r\n\r\nmake sure u install:  pip install wandb",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2138387303/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2140791082",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/24#issuecomment-2140791082",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/24",
      "id": 2140791082,
      "node_id": "IC_kwDOMAMQI85_meEq",
      "user": {
        "login": "jagilley",
        "id": 37783831,
        "node_id": "MDQ6VXNlcjM3NzgzODMx",
        "avatar_url": "https://avatars.githubusercontent.com/u/37783831?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jagilley",
        "html_url": "https://github.com/jagilley",
        "followers_url": "https://api.github.com/users/jagilley/followers",
        "following_url": "https://api.github.com/users/jagilley/following{/other_user}",
        "gists_url": "https://api.github.com/users/jagilley/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jagilley/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jagilley/subscriptions",
        "organizations_url": "https://api.github.com/users/jagilley/orgs",
        "repos_url": "https://api.github.com/users/jagilley/repos",
        "events_url": "https://api.github.com/users/jagilley/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jagilley/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-30T20:12:02Z",
      "updated_at": "2024-05-30T20:12:02Z",
      "author_association": "NONE",
      "body": "I did all that",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2140791082/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2145214427",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/24#issuecomment-2145214427",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/24",
      "id": 2145214427,
      "node_id": "IC_kwDOMAMQI85_3V_b",
      "user": {
        "login": "SaiKrishnaBala",
        "id": 4848584,
        "node_id": "MDQ6VXNlcjQ4NDg1ODQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/4848584?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/SaiKrishnaBala",
        "html_url": "https://github.com/SaiKrishnaBala",
        "followers_url": "https://api.github.com/users/SaiKrishnaBala/followers",
        "following_url": "https://api.github.com/users/SaiKrishnaBala/following{/other_user}",
        "gists_url": "https://api.github.com/users/SaiKrishnaBala/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/SaiKrishnaBala/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/SaiKrishnaBala/subscriptions",
        "organizations_url": "https://api.github.com/users/SaiKrishnaBala/orgs",
        "repos_url": "https://api.github.com/users/SaiKrishnaBala/repos",
        "events_url": "https://api.github.com/users/SaiKrishnaBala/events{/privacy}",
        "received_events_url": "https://api.github.com/users/SaiKrishnaBala/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-03T13:31:16Z",
      "updated_at": "2024-06-03T13:31:16Z",
      "author_association": "NONE",
      "body": "Can you check if you are using the below format?\r\n```\r\nwandb:\r\n  project: \"mistral_tuning\" # your wandb project name\r\n  run_name: \"\" # your wandb run name\r\n  key: \"XXXXXXXXXXXXXX\" # your wandb api key\r\n  offline: False\r\n```",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2145214427/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "23": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2296427756",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/23#issuecomment-2296427756",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/23",
      "id": 2296427756,
      "node_id": "IC_kwDOMAMQI86I4LTs",
      "user": {
        "login": "C3po-D2rd2",
        "id": 172624326,
        "node_id": "U_kgDOCkoJxg",
        "avatar_url": "https://avatars.githubusercontent.com/u/172624326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/C3po-D2rd2",
        "html_url": "https://github.com/C3po-D2rd2",
        "followers_url": "https://api.github.com/users/C3po-D2rd2/followers",
        "following_url": "https://api.github.com/users/C3po-D2rd2/following{/other_user}",
        "gists_url": "https://api.github.com/users/C3po-D2rd2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/C3po-D2rd2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/C3po-D2rd2/subscriptions",
        "organizations_url": "https://api.github.com/users/C3po-D2rd2/orgs",
        "repos_url": "https://api.github.com/users/C3po-D2rd2/repos",
        "events_url": "https://api.github.com/users/C3po-D2rd2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/C3po-D2rd2/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-19T12:12:46Z",
      "updated_at": "2024-08-19T12:12:46Z",
      "author_association": "NONE",
      "body": "hello,\r\n  Did you solve this issue?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2296427756/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2296441281",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/23#issuecomment-2296441281",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/23",
      "id": 2296441281,
      "node_id": "IC_kwDOMAMQI86I4OnB",
      "user": {
        "login": "C3po-D2rd2",
        "id": 172624326,
        "node_id": "U_kgDOCkoJxg",
        "avatar_url": "https://avatars.githubusercontent.com/u/172624326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/C3po-D2rd2",
        "html_url": "https://github.com/C3po-D2rd2",
        "followers_url": "https://api.github.com/users/C3po-D2rd2/followers",
        "following_url": "https://api.github.com/users/C3po-D2rd2/following{/other_user}",
        "gists_url": "https://api.github.com/users/C3po-D2rd2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/C3po-D2rd2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/C3po-D2rd2/subscriptions",
        "organizations_url": "https://api.github.com/users/C3po-D2rd2/orgs",
        "repos_url": "https://api.github.com/users/C3po-D2rd2/repos",
        "events_url": "https://api.github.com/users/C3po-D2rd2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/C3po-D2rd2/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-19T12:20:10Z",
      "updated_at": "2024-08-19T12:20:10Z",
      "author_association": "NONE",
      "body": "Alrights for noobs like me, you just need to set in example/7B.yaml the model_id_or_path to the directory where you downloaded data",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2296441281/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "18": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133098605",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/18#issuecomment-2133098605",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/18",
      "id": 2133098605,
      "node_id": "IC_kwDOMAMQI85_JIBt",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T09:48:02Z",
      "updated_at": "2024-05-27T09:48:02Z",
      "author_association": "COLLABORATOR",
      "body": "Thanks for spotting! This was indeed badly designed. Put it in the .gitignore file for now. Will soon updated with better, actual fixture.\r\n\r\ncc https://github.com/mistralai/mistral-finetune/pull/19",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133098605/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "17": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133100440",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/17#issuecomment-2133100440",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/17",
      "id": 2133100440,
      "node_id": "IC_kwDOMAMQI85_JIeY",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T09:48:57Z",
      "updated_at": "2024-05-27T09:48:57Z",
      "author_association": "COLLABORATOR",
      "body": "Make to fix out `example/7B.yaml` with a valid path to a dataset",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133100440/reactions",
        "total_count": 1,
        "+1": 1,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "14": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2132898754",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2132898754",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2132898754,
      "node_id": "IC_kwDOMAMQI85_IXPC",
      "user": {
        "login": "danielhanchen",
        "id": 23090290,
        "node_id": "MDQ6VXNlcjIzMDkwMjkw",
        "avatar_url": "https://avatars.githubusercontent.com/u/23090290?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/danielhanchen",
        "html_url": "https://github.com/danielhanchen",
        "followers_url": "https://api.github.com/users/danielhanchen/followers",
        "following_url": "https://api.github.com/users/danielhanchen/following{/other_user}",
        "gists_url": "https://api.github.com/users/danielhanchen/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/danielhanchen/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/danielhanchen/subscriptions",
        "organizations_url": "https://api.github.com/users/danielhanchen/orgs",
        "repos_url": "https://api.github.com/users/danielhanchen/repos",
        "events_url": "https://api.github.com/users/danielhanchen/events{/privacy}",
        "received_events_url": "https://api.github.com/users/danielhanchen/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T08:09:24Z",
      "updated_at": "2024-05-27T08:09:24Z",
      "author_association": "NONE",
      "body": "Not a Mistral official product, and shameless promotion, but if you're having OOM issues, have a go with Unsloth :) You get 70%+ memory reduction, 2x faster and no accuracy degradation! https://github.com/unslothai/unsloth Mistral v0.3 7b via a free Colab: https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2132898754/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133515954",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2133515954",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2133515954,
      "node_id": "IC_kwDOMAMQI85_Kt6y",
      "user": {
        "login": "mosh98",
        "id": 48658042,
        "node_id": "MDQ6VXNlcjQ4NjU4MDQy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48658042?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mosh98",
        "html_url": "https://github.com/mosh98",
        "followers_url": "https://api.github.com/users/mosh98/followers",
        "following_url": "https://api.github.com/users/mosh98/following{/other_user}",
        "gists_url": "https://api.github.com/users/mosh98/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mosh98/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mosh98/subscriptions",
        "organizations_url": "https://api.github.com/users/mosh98/orgs",
        "repos_url": "https://api.github.com/users/mosh98/repos",
        "events_url": "https://api.github.com/users/mosh98/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mosh98/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T13:43:16Z",
      "updated_at": "2024-05-27T13:43:16Z",
      "author_association": "NONE",
      "body": "reducing the sequence length to under 8000 helped me",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133515954/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133559961",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2133559961",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2133559961,
      "node_id": "IC_kwDOMAMQI85_K4qZ",
      "user": {
        "login": "yuvalshachaf",
        "id": 9166517,
        "node_id": "MDQ6VXNlcjkxNjY1MTc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/9166517?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/yuvalshachaf",
        "html_url": "https://github.com/yuvalshachaf",
        "followers_url": "https://api.github.com/users/yuvalshachaf/followers",
        "following_url": "https://api.github.com/users/yuvalshachaf/following{/other_user}",
        "gists_url": "https://api.github.com/users/yuvalshachaf/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/yuvalshachaf/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/yuvalshachaf/subscriptions",
        "organizations_url": "https://api.github.com/users/yuvalshachaf/orgs",
        "repos_url": "https://api.github.com/users/yuvalshachaf/repos",
        "events_url": "https://api.github.com/users/yuvalshachaf/events{/privacy}",
        "received_events_url": "https://api.github.com/users/yuvalshachaf/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T14:06:53Z",
      "updated_at": "2024-05-27T14:06:53Z",
      "author_association": "NONE",
      "body": "Hi thx\r\nOf course\r\nBut the main problem is that Lora model of 80mb becomes 10gb and gets me to\r\nmemory error\r\nThe base model is 13.5gb on disk on on GPU. How come the Lora model becomes\r\nto big?\r\n\r\nOn Mon, May 27, 2024, 16:43 Mosleh Mahamud ***@***.***> wrote:\r\n\r\n> reducing the sequence length to under 8000 helped me\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2133515954>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ACF55NPVTTZAVWWHCIUPNJ3ZEM2AXAVCNFSM6AAAAABIJ6W7O6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCMZTGUYTKOJVGQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133559961/reactions",
        "total_count": 3,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2134040684",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2134040684",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2134040684,
      "node_id": "IC_kwDOMAMQI85_MuBs",
      "user": {
        "login": "NathanMayPro",
        "id": 90243707,
        "node_id": "MDQ6VXNlcjkwMjQzNzA3",
        "avatar_url": "https://avatars.githubusercontent.com/u/90243707?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NathanMayPro",
        "html_url": "https://github.com/NathanMayPro",
        "followers_url": "https://api.github.com/users/NathanMayPro/followers",
        "following_url": "https://api.github.com/users/NathanMayPro/following{/other_user}",
        "gists_url": "https://api.github.com/users/NathanMayPro/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NathanMayPro/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NathanMayPro/subscriptions",
        "organizations_url": "https://api.github.com/users/NathanMayPro/orgs",
        "repos_url": "https://api.github.com/users/NathanMayPro/repos",
        "events_url": "https://api.github.com/users/NathanMayPro/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NathanMayPro/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T20:57:18Z",
      "updated_at": "2024-05-27T20:57:18Z",
      "author_association": "NONE",
      "body": "Lora is not the cause of you're error\r\nWhen inference:\r\n- Empty model (13.5gb) + self_attention (depending of the size of you're input) +  activation (depending of number of parameters)\r\nWhen training:\r\n - same as previous + gradient (depending of number of parameters) + optimizer \r\nWith LoRa during training:\r\n - the gradient and optimizer are no longer depending of you're original model size but the LoRa size (A,rank), (rank, B)\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2134040684/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2156316000",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2156316000",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2156316000,
      "node_id": "IC_kwDOMAMQI86AhsVg",
      "user": {
        "login": "halilergul1",
        "id": 90449997,
        "node_id": "MDQ6VXNlcjkwNDQ5OTk3",
        "avatar_url": "https://avatars.githubusercontent.com/u/90449997?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/halilergul1",
        "html_url": "https://github.com/halilergul1",
        "followers_url": "https://api.github.com/users/halilergul1/followers",
        "following_url": "https://api.github.com/users/halilergul1/following{/other_user}",
        "gists_url": "https://api.github.com/users/halilergul1/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/halilergul1/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/halilergul1/subscriptions",
        "organizations_url": "https://api.github.com/users/halilergul1/orgs",
        "repos_url": "https://api.github.com/users/halilergul1/repos",
        "events_url": "https://api.github.com/users/halilergul1/events{/privacy}",
        "received_events_url": "https://api.github.com/users/halilergul1/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-09T04:41:04Z",
      "updated_at": "2024-06-09T19:28:27Z",
      "author_association": "NONE",
      "body": "> reducing the sequence length to under 8000 helped me\r\n\r\n\r\n\r\nHello @mosh98, you mean reducing seq length while training? For sure it helps during training.\r\n\r\n\r\n\r\nAny other updates on this issue? I get the same cuda error while trying to get inferences from the fine-tuned model. The problem arises due to load_lora method I think. Because just like @yuvalshachaf said, the lora model becomes 10gb when loading!\r\n\r\n\r\n\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2156316000/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2257951052",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2257951052",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2257951052,
      "node_id": "IC_kwDOMAMQI86GlZlM",
      "user": {
        "login": "BlahBlah314",
        "id": 36380340,
        "node_id": "MDQ6VXNlcjM2MzgwMzQw",
        "avatar_url": "https://avatars.githubusercontent.com/u/36380340?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/BlahBlah314",
        "html_url": "https://github.com/BlahBlah314",
        "followers_url": "https://api.github.com/users/BlahBlah314/followers",
        "following_url": "https://api.github.com/users/BlahBlah314/following{/other_user}",
        "gists_url": "https://api.github.com/users/BlahBlah314/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/BlahBlah314/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/BlahBlah314/subscriptions",
        "organizations_url": "https://api.github.com/users/BlahBlah314/orgs",
        "repos_url": "https://api.github.com/users/BlahBlah314/repos",
        "events_url": "https://api.github.com/users/BlahBlah314/events{/privacy}",
        "received_events_url": "https://api.github.com/users/BlahBlah314/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-30T09:54:01Z",
      "updated_at": "2024-07-30T09:54:01Z",
      "author_association": "NONE",
      "body": "Hello,\r\n\r\nI have the same problem. The inference with Nemo-instruct is OK on an A100. But when finetuned, using the inference method described in the tutorial with model.load_lora, I get an OOM with the same A100. How did you solve it if you indeed solved it ? Maybe merging the adapter with peft before the inference is a solution ?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2257951052/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298972761",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2298972761",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2298972761,
      "node_id": "IC_kwDOMAMQI86JB4pZ",
      "user": {
        "login": "C3po-D2rd2",
        "id": 172624326,
        "node_id": "U_kgDOCkoJxg",
        "avatar_url": "https://avatars.githubusercontent.com/u/172624326?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/C3po-D2rd2",
        "html_url": "https://github.com/C3po-D2rd2",
        "followers_url": "https://api.github.com/users/C3po-D2rd2/followers",
        "following_url": "https://api.github.com/users/C3po-D2rd2/following{/other_user}",
        "gists_url": "https://api.github.com/users/C3po-D2rd2/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/C3po-D2rd2/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/C3po-D2rd2/subscriptions",
        "organizations_url": "https://api.github.com/users/C3po-D2rd2/orgs",
        "repos_url": "https://api.github.com/users/C3po-D2rd2/repos",
        "events_url": "https://api.github.com/users/C3po-D2rd2/events{/privacy}",
        "received_events_url": "https://api.github.com/users/C3po-D2rd2/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-08-20T14:15:21Z",
      "updated_at": "2024-08-20T14:15:21Z",
      "author_association": "NONE",
      "body": "Same issue here, did anyone found a way?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2298972761/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2493753624",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2493753624",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2493753624,
      "node_id": "IC_kwDOMAMQI86Uo6kY",
      "user": {
        "login": "Luis-gd",
        "id": 56192640,
        "node_id": "MDQ6VXNlcjU2MTkyNjQw",
        "avatar_url": "https://avatars.githubusercontent.com/u/56192640?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Luis-gd",
        "html_url": "https://github.com/Luis-gd",
        "followers_url": "https://api.github.com/users/Luis-gd/followers",
        "following_url": "https://api.github.com/users/Luis-gd/following{/other_user}",
        "gists_url": "https://api.github.com/users/Luis-gd/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/Luis-gd/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/Luis-gd/subscriptions",
        "organizations_url": "https://api.github.com/users/Luis-gd/orgs",
        "repos_url": "https://api.github.com/users/Luis-gd/repos",
        "events_url": "https://api.github.com/users/Luis-gd/events{/privacy}",
        "received_events_url": "https://api.github.com/users/Luis-gd/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-11-22T13:19:00Z",
      "updated_at": "2024-11-22T13:19:00Z",
      "author_association": "NONE",
      "body": "> Lora is not the cause of you're error When inference:\r\n> \r\n> * Empty model (13.5gb) + self_attention (depending of the size of you're input) +  activation (depending of number of parameters)\r\n>   When training:\r\n> * same as previous + gradient (depending of number of parameters) + optimizer\r\n>   With LoRa during training:\r\n> * the gradient and optimizer are no longer depending of you're original model size but the LoRa size (A,rank), (rank, B)\r\n\r\nAnd what's the issue? It happens to me too and I don't know what to do, IDK if it's something related to load_lora function or what",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2493753624/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2493798817",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2493798817",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2493798817,
      "node_id": "IC_kwDOMAMQI86UpFmh",
      "user": {
        "login": "NathanMayPro",
        "id": 90243707,
        "node_id": "MDQ6VXNlcjkwMjQzNzA3",
        "avatar_url": "https://avatars.githubusercontent.com/u/90243707?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/NathanMayPro",
        "html_url": "https://github.com/NathanMayPro",
        "followers_url": "https://api.github.com/users/NathanMayPro/followers",
        "following_url": "https://api.github.com/users/NathanMayPro/following{/other_user}",
        "gists_url": "https://api.github.com/users/NathanMayPro/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/NathanMayPro/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/NathanMayPro/subscriptions",
        "organizations_url": "https://api.github.com/users/NathanMayPro/orgs",
        "repos_url": "https://api.github.com/users/NathanMayPro/repos",
        "events_url": "https://api.github.com/users/NathanMayPro/events{/privacy}",
        "received_events_url": "https://api.github.com/users/NathanMayPro/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-11-22T13:42:13Z",
      "updated_at": "2024-11-22T13:42:13Z",
      "author_association": "NONE",
      "body": "Lora is something that you add to your model so he cannot reduce the memory usage during loading and inference steps.\r\nHe can during training because it reduce the number of trainable parameters (instead of training the huge amount of neurons of the original it create a small model that correct the first one). \r\nIn terms of GPU requirements:\r\n        - Load the model < Inference < Training (In terms of GPU Usage)\r\nBasically you need a bigger GPU.\r\nIf you want reduce the GPU memory usage look Quantization",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2493798817/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2494168213",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2494168213",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2494168213,
      "node_id": "IC_kwDOMAMQI86UqfyV",
      "user": {
        "login": "Luis-gd",
        "id": 56192640,
        "node_id": "MDQ6VXNlcjU2MTkyNjQw",
        "avatar_url": "https://avatars.githubusercontent.com/u/56192640?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Luis-gd",
        "html_url": "https://github.com/Luis-gd",
        "followers_url": "https://api.github.com/users/Luis-gd/followers",
        "following_url": "https://api.github.com/users/Luis-gd/following{/other_user}",
        "gists_url": "https://api.github.com/users/Luis-gd/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/Luis-gd/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/Luis-gd/subscriptions",
        "organizations_url": "https://api.github.com/users/Luis-gd/orgs",
        "repos_url": "https://api.github.com/users/Luis-gd/repos",
        "events_url": "https://api.github.com/users/Luis-gd/events{/privacy}",
        "received_events_url": "https://api.github.com/users/Luis-gd/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-11-22T16:34:14Z",
      "updated_at": "2024-11-22T16:34:14Z",
      "author_association": "NONE",
      "body": "> Lora is something that you add to your model so he cannot reduce the memory usage during loading and inference steps.\n> He can during training because it reduce the number of trainable parameters (instead of training the huge amount of neurons of the original it create a small model that correct the first one). \n> In terms of GPU requirements:\n>         - Load the model < Inference < Training (In terms of GPU Usage)\n> Basically you need a bigger GPU.\n> If you want reduce the GPU memory usage look Quantization\n\nMy problem is not the training process. I trained the model and I have my lora.savetensors.\n\nMy problem comes when I use mistral-inference to load lora.savetensors. Before I load lora, my GPU VRAM usage is about 14GB, but when I load lora it gives me a VRAM usage error. My GPU has  24GB, it's good enough.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2494168213/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2494171642",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/14#issuecomment-2494171642",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/14",
      "id": 2494171642,
      "node_id": "IC_kwDOMAMQI86Uqgn6",
      "user": {
        "login": "Luis-gd",
        "id": 56192640,
        "node_id": "MDQ6VXNlcjU2MTkyNjQw",
        "avatar_url": "https://avatars.githubusercontent.com/u/56192640?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/Luis-gd",
        "html_url": "https://github.com/Luis-gd",
        "followers_url": "https://api.github.com/users/Luis-gd/followers",
        "following_url": "https://api.github.com/users/Luis-gd/following{/other_user}",
        "gists_url": "https://api.github.com/users/Luis-gd/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/Luis-gd/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/Luis-gd/subscriptions",
        "organizations_url": "https://api.github.com/users/Luis-gd/orgs",
        "repos_url": "https://api.github.com/users/Luis-gd/repos",
        "events_url": "https://api.github.com/users/Luis-gd/events{/privacy}",
        "received_events_url": "https://api.github.com/users/Luis-gd/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-11-22T16:35:33Z",
      "updated_at": "2024-11-22T16:35:33Z",
      "author_association": "NONE",
      "body": "> > Lora is something that you add to your model so he cannot reduce the memory usage during loading and inference steps.\n> > He can during training because it reduce the number of trainable parameters (instead of training the huge amount of neurons of the original it create a small model that correct the first one). \n> > In terms of GPU requirements:\n> >         - Load the model < Inference < Training (In terms of GPU Usage)\n> > Basically you need a bigger GPU.\n> > If you want reduce the GPU memory usage look Quantization\n> \n> My problem is not the training process. I trained the model and I have my lora.savetensors.\n> \n> My problem comes when I use mistral-inference to load lora.savetensors. Before I load lora, my GPU VRAM usage is about 14GB, but when I load lora it gives me a VRAM usage error. My GPU has  24GB, it's good enough.\n\nThe only conclusion I can get is the problem I'm facing is made by this load lora method",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2494171642/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "12": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2192837339",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/12#issuecomment-2192837339",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/12",
      "id": 2192837339,
      "node_id": "IC_kwDOMAMQI86CtArb",
      "user": {
        "login": "matheus-prandini",
        "id": 152647513,
        "node_id": "U_kgDOCRk3WQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/152647513?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/matheus-prandini",
        "html_url": "https://github.com/matheus-prandini",
        "followers_url": "https://api.github.com/users/matheus-prandini/followers",
        "following_url": "https://api.github.com/users/matheus-prandini/following{/other_user}",
        "gists_url": "https://api.github.com/users/matheus-prandini/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/matheus-prandini/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/matheus-prandini/subscriptions",
        "organizations_url": "https://api.github.com/users/matheus-prandini/orgs",
        "repos_url": "https://api.github.com/users/matheus-prandini/repos",
        "events_url": "https://api.github.com/users/matheus-prandini/events{/privacy}",
        "received_events_url": "https://api.github.com/users/matheus-prandini/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-27T00:29:21Z",
      "updated_at": "2024-06-27T00:29:21Z",
      "author_association": "CONTRIBUTOR",
      "body": "@CrispStrobe What are you trying to achieve? Do you want to add specific new tokens?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2192837339/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2193932548",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/12#issuecomment-2193932548",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/12",
      "id": 2193932548,
      "node_id": "IC_kwDOMAMQI86CxMEE",
      "user": {
        "login": "CrispStrobe",
        "id": 154636388,
        "node_id": "U_kgDOCTeQZA",
        "avatar_url": "https://avatars.githubusercontent.com/u/154636388?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CrispStrobe",
        "html_url": "https://github.com/CrispStrobe",
        "followers_url": "https://api.github.com/users/CrispStrobe/followers",
        "following_url": "https://api.github.com/users/CrispStrobe/following{/other_user}",
        "gists_url": "https://api.github.com/users/CrispStrobe/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CrispStrobe/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CrispStrobe/subscriptions",
        "organizations_url": "https://api.github.com/users/CrispStrobe/orgs",
        "repos_url": "https://api.github.com/users/CrispStrobe/repos",
        "events_url": "https://api.github.com/users/CrispStrobe/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CrispStrobe/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-06-27T06:52:29Z",
      "updated_at": "2024-06-27T06:52:29Z",
      "author_association": "NONE",
      "body": "would it be possible to adapt an existing eg 7b mistral v0.1/0.2 based finetuned model so that it works with the new extended 32768 vocabulary and the v3 tokenizer, and so that we could merge it with v3 models?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2193932548/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2249715233",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/12#issuecomment-2249715233",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/12",
      "id": 2249715233,
      "node_id": "IC_kwDOMAMQI86GF-4h",
      "user": {
        "login": "kehuitt",
        "id": 174309856,
        "node_id": "U_kgDOCmPB4A",
        "avatar_url": "https://avatars.githubusercontent.com/u/174309856?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/kehuitt",
        "html_url": "https://github.com/kehuitt",
        "followers_url": "https://api.github.com/users/kehuitt/followers",
        "following_url": "https://api.github.com/users/kehuitt/following{/other_user}",
        "gists_url": "https://api.github.com/users/kehuitt/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/kehuitt/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/kehuitt/subscriptions",
        "organizations_url": "https://api.github.com/users/kehuitt/orgs",
        "repos_url": "https://api.github.com/users/kehuitt/repos",
        "events_url": "https://api.github.com/users/kehuitt/events{/privacy}",
        "received_events_url": "https://api.github.com/users/kehuitt/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-07-25T08:06:46Z",
      "updated_at": "2024-07-25T08:06:46Z",
      "author_association": "NONE",
      "body": "@matheus-prandini Hi! I met the same problem. I merged the base model and lora by the script below\r\n```\r\npython utils/merge_lora.py \\\r\n    --initial_model_ckpt Mixtral-8x7B-v0.1-Instruct_extended/consolidated.00.pth \\\r\n    --lora_ckpt mistral_8x7B/snapshots/f7ef571615ca95187c38d1e5451758532cf795c3/checkpoint_000948/consolidated/lora.safetensors \\\r\n    --dump_ckpt Model/merged_model.safetensors\r\n```\r\nThen I converted its weights to hf, but when I inferred from this merged model, I got the same output like [control_29][control_698][control_638]...\r\n\r\nI would like to know which step I am having problems with, is it the merge step that is the problem?\r\nThanks!!\r\n\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2249715233/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "11": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2131415953",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/11#issuecomment-2131415953",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/11",
      "id": 2131415953,
      "node_id": "IC_kwDOMAMQI85_CtOR",
      "user": {
        "login": "CyberTimon",
        "id": 78795905,
        "node_id": "MDQ6VXNlcjc4Nzk1OTA1",
        "avatar_url": "https://avatars.githubusercontent.com/u/78795905?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/CyberTimon",
        "html_url": "https://github.com/CyberTimon",
        "followers_url": "https://api.github.com/users/CyberTimon/followers",
        "following_url": "https://api.github.com/users/CyberTimon/following{/other_user}",
        "gists_url": "https://api.github.com/users/CyberTimon/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/CyberTimon/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/CyberTimon/subscriptions",
        "organizations_url": "https://api.github.com/users/CyberTimon/orgs",
        "repos_url": "https://api.github.com/users/CyberTimon/repos",
        "events_url": "https://api.github.com/users/CyberTimon/events{/privacy}",
        "received_events_url": "https://api.github.com/users/CyberTimon/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-25T19:46:18Z",
      "updated_at": "2024-05-25T19:46:18Z",
      "author_association": "NONE",
      "body": "I'm interested too. +1",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2131415953/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136241145",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/11#issuecomment-2136241145",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/11",
      "id": 2136241145,
      "node_id": "IC_kwDOMAMQI85_VHP5",
      "user": {
        "login": "patrickvonplaten",
        "id": 23423619,
        "node_id": "MDQ6VXNlcjIzNDIzNjE5",
        "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/patrickvonplaten",
        "html_url": "https://github.com/patrickvonplaten",
        "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
        "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
        "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
        "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
        "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
        "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
        "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-28T23:00:33Z",
      "updated_at": "2024-05-28T23:00:33Z",
      "author_association": "COLLABORATOR",
      "body": "Hey @ehartford,\r\n\r\nYou can fine-tune 8x7B and 8x22B exactly the same way you train the 7B. For the 8x7B you should probably use 2-4 80GB GPUs and for the 8x22B you will need 8x 80GB GPUs.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136241145/reactions",
        "total_count": 2,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 2,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136260787",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/11#issuecomment-2136260787",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/11",
      "id": 2136260787,
      "node_id": "IC_kwDOMAMQI85_VMCz",
      "user": {
        "login": "ehartford",
        "id": 1117701,
        "node_id": "MDQ6VXNlcjExMTc3MDE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1117701?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ehartford",
        "html_url": "https://github.com/ehartford",
        "followers_url": "https://api.github.com/users/ehartford/followers",
        "following_url": "https://api.github.com/users/ehartford/following{/other_user}",
        "gists_url": "https://api.github.com/users/ehartford/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ehartford/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ehartford/subscriptions",
        "organizations_url": "https://api.github.com/users/ehartford/orgs",
        "repos_url": "https://api.github.com/users/ehartford/repos",
        "events_url": "https://api.github.com/users/ehartford/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ehartford/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-28T23:25:33Z",
      "updated_at": "2024-05-28T23:25:33Z",
      "author_association": "NONE",
      "body": "> Hey @ehartford,\n> \n> You can fine-tune 8x7B and 8x22B exactly the same way you train the 7B. For the 8x7B you should probably use 2-4 80GB GPUs and for the 8x22B you will need 8x 80GB GPUs.\n\nI have my tricks \ud83d\ude1c ",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136260787/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136280719",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/11#issuecomment-2136280719",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/11",
      "id": 2136280719,
      "node_id": "IC_kwDOMAMQI85_VQ6P",
      "user": {
        "login": "jagilley",
        "id": 37783831,
        "node_id": "MDQ6VXNlcjM3NzgzODMx",
        "avatar_url": "https://avatars.githubusercontent.com/u/37783831?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/jagilley",
        "html_url": "https://github.com/jagilley",
        "followers_url": "https://api.github.com/users/jagilley/followers",
        "following_url": "https://api.github.com/users/jagilley/following{/other_user}",
        "gists_url": "https://api.github.com/users/jagilley/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/jagilley/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/jagilley/subscriptions",
        "organizations_url": "https://api.github.com/users/jagilley/orgs",
        "repos_url": "https://api.github.com/users/jagilley/repos",
        "events_url": "https://api.github.com/users/jagilley/events{/privacy}",
        "received_events_url": "https://api.github.com/users/jagilley/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-28T23:52:12Z",
      "updated_at": "2024-05-28T23:52:12Z",
      "author_association": "NONE",
      "body": "@ehartford it seems like there's built-in support for the way that Mistral implemented the MoE routing etc etc - is there some other implementation detail you were looking for?",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136280719/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    },
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136293425",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/11#issuecomment-2136293425",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/11",
      "id": 2136293425,
      "node_id": "IC_kwDOMAMQI85_VUAx",
      "user": {
        "login": "ehartford",
        "id": 1117701,
        "node_id": "MDQ6VXNlcjExMTc3MDE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1117701?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ehartford",
        "html_url": "https://github.com/ehartford",
        "followers_url": "https://api.github.com/users/ehartford/followers",
        "following_url": "https://api.github.com/users/ehartford/following{/other_user}",
        "gists_url": "https://api.github.com/users/ehartford/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ehartford/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ehartford/subscriptions",
        "organizations_url": "https://api.github.com/users/ehartford/orgs",
        "repos_url": "https://api.github.com/users/ehartford/repos",
        "events_url": "https://api.github.com/users/ehartford/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ehartford/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-29T00:07:36Z",
      "updated_at": "2024-05-29T00:07:36Z",
      "author_association": "NONE",
      "body": "no; I'm happy with that!  I'll try it out.",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2136293425/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ],
  "6": [
    {
      "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133514393",
      "html_url": "https://github.com/mistralai/mistral-finetune/issues/6#issuecomment-2133514393",
      "issue_url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/6",
      "id": 2133514393,
      "node_id": "IC_kwDOMAMQI85_KtiZ",
      "user": {
        "login": "mosh98",
        "id": 48658042,
        "node_id": "MDQ6VXNlcjQ4NjU4MDQy",
        "avatar_url": "https://avatars.githubusercontent.com/u/48658042?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mosh98",
        "html_url": "https://github.com/mosh98",
        "followers_url": "https://api.github.com/users/mosh98/followers",
        "following_url": "https://api.github.com/users/mosh98/following{/other_user}",
        "gists_url": "https://api.github.com/users/mosh98/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mosh98/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mosh98/subscriptions",
        "organizations_url": "https://api.github.com/users/mosh98/orgs",
        "repos_url": "https://api.github.com/users/mosh98/repos",
        "events_url": "https://api.github.com/users/mosh98/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mosh98/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2024-05-27T13:42:20Z",
      "updated_at": "2024-05-27T13:42:20Z",
      "author_association": "NONE",
      "body": "Hi, i made one here: https://youtu.be/bO-b5Soxzxk\r\n",
      "reactions": {
        "url": "https://api.github.com/repos/mistralai/mistral-finetune/issues/comments/2133514393/reactions",
        "total_count": 2,
        "+1": 2,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": null
    }
  ]
}