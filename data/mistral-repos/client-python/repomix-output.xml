This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
- Git diffs from the worktree and staged changes are included
</notes>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    bug_report_client.yml
    bug_report_model.yml
    config.yml
  workflows/
    lint_custom_code.yaml
    run_example_scripts.yaml
    sdk_generation_mistralai_azure_sdk.yaml
    sdk_generation_mistralai_gcp_sdk.yaml
    sdk_generation_mistralai_sdk.yaml
    sdk_publish_mistralai_sdk.yaml
    test_custom_code.yaml
.speakeasy/
  gen.lock
  gen.yaml
  workflow.lock
  workflow.yaml
docs/
  models/
    utils/
      retryconfig.md
    agent.md
    agentconversation.md
    agentconversationobject.md
    agentcreationrequest.md
    agentcreationrequesttools.md
    agenthandoffdoneevent.md
    agenthandoffdoneeventtype.md
    agenthandoffentry.md
    agenthandoffentryobject.md
    agenthandoffentrytype.md
    agenthandoffstartedevent.md
    agenthandoffstartedeventtype.md
    agentobject.md
    agentsapiv1agentsgetrequest.md
    agentsapiv1agentslistrequest.md
    agentsapiv1agentsupdaterequest.md
    agentsapiv1agentsupdateversionrequest.md
    agentsapiv1conversationsappendrequest.md
    agentsapiv1conversationsappendstreamrequest.md
    agentsapiv1conversationsgetrequest.md
    agentsapiv1conversationsgetresponsev1conversationsget.md
    agentsapiv1conversationshistoryrequest.md
    agentsapiv1conversationslistrequest.md
    agentsapiv1conversationsmessagesrequest.md
    agentsapiv1conversationsrestartrequest.md
    agentsapiv1conversationsrestartstreamrequest.md
    agentscompletionrequest.md
    agentscompletionrequestmessages.md
    agentscompletionrequeststop.md
    agentscompletionrequesttoolchoice.md
    agentscompletionstreamrequest.md
    agentscompletionstreamrequestmessages.md
    agentscompletionstreamrequeststop.md
    agentscompletionstreamrequesttoolchoice.md
    agenttools.md
    agentupdaterequest.md
    agentupdaterequesttools.md
    apiendpoint.md
    archiveftmodelout.md
    archiveftmodeloutobject.md
    arguments.md
    assistantmessage.md
    assistantmessagecontent.md
    assistantmessagerole.md
    basemodelcard.md
    batcherror.md
    batchjobin.md
    batchjobout.md
    batchjoboutobject.md
    batchjobsout.md
    batchjobsoutobject.md
    batchjobstatus.md
    builtinconnectors.md
    chatclassificationrequest.md
    chatcompletionchoice.md
    chatcompletionrequest.md
    chatcompletionrequesttoolchoice.md
    chatcompletionresponse.md
    chatcompletionstreamrequest.md
    chatcompletionstreamrequestmessages.md
    chatcompletionstreamrequeststop.md
    chatcompletionstreamrequesttoolchoice.md
    chatmoderationrequest.md
    chatmoderationrequestinputs.md
    checkpointout.md
    classificationrequest.md
    classificationrequestinputs.md
    classificationresponse.md
    classificationtargetresult.md
    classifierdetailedjobout.md
    classifierdetailedjoboutintegrations.md
    classifierdetailedjoboutjobtype.md
    classifierdetailedjoboutobject.md
    classifierdetailedjoboutstatus.md
    classifierftmodelout.md
    classifierftmodeloutmodeltype.md
    classifierftmodeloutobject.md
    classifierjobout.md
    classifierjoboutintegrations.md
    classifierjoboutjobtype.md
    classifierjoboutobject.md
    classifierjoboutstatus.md
    classifiertargetin.md
    classifiertargetout.md
    classifiertrainingparameters.md
    classifiertrainingparametersin.md
    codeinterpretertool.md
    codeinterpretertooltype.md
    completionargs.md
    completionargsstop.md
    completionchunk.md
    completiondetailedjobout.md
    completiondetailedjoboutintegrations.md
    completiondetailedjoboutjobtype.md
    completiondetailedjoboutobject.md
    completiondetailedjoboutrepositories.md
    completiondetailedjoboutstatus.md
    completionevent.md
    completionftmodelout.md
    completionftmodeloutobject.md
    completionjobout.md
    completionjoboutobject.md
    completionresponsestreamchoice.md
    completionresponsestreamchoicefinishreason.md
    completiontrainingparameters.md
    completiontrainingparametersin.md
    content.md
    contentchunk.md
    conversationappendrequest.md
    conversationappendrequesthandoffexecution.md
    conversationappendstreamrequest.md
    conversationappendstreamrequesthandoffexecution.md
    conversationevents.md
    conversationeventsdata.md
    conversationhistory.md
    conversationhistoryobject.md
    conversationinputs.md
    conversationmessages.md
    conversationmessagesobject.md
    conversationrequest.md
    conversationresponse.md
    conversationresponseobject.md
    conversationrestartrequest.md
    conversationrestartrequesthandoffexecution.md
    conversationrestartstreamrequest.md
    conversationrestartstreamrequesthandoffexecution.md
    conversationstreamrequest.md
    conversationstreamrequesthandoffexecution.md
    conversationstreamrequesttools.md
    conversationusageinfo.md
    data.md
    deletefileout.md
    deletemodelout.md
    deletemodelv1modelsmodeliddeleterequest.md
    deltamessage.md
    document.md
    documentlibrarytool.md
    documentlibrarytooltype.md
    documenturlchunk.md
    documenturlchunktype.md
    embeddingdtype.md
    embeddingrequest.md
    embeddingrequestinputs.md
    embeddingresponse.md
    embeddingresponsedata.md
    entries.md
    eventout.md
    file.md
    filepurpose.md
    filesapiroutesdeletefilerequest.md
    filesapiroutesdownloadfilerequest.md
    filesapiroutesgetsignedurlrequest.md
    filesapirouteslistfilesrequest.md
    filesapiroutesretrievefilerequest.md
    filesapiroutesuploadfilemultipartbodyparams.md
    fileschema.md
    filesignedurl.md
    fimcompletionrequest.md
    fimcompletionrequeststop.md
    fimcompletionresponse.md
    fimcompletionstreamrequest.md
    fimcompletionstreamrequeststop.md
    finetuneablemodeltype.md
    finishreason.md
    ftclassifierlossfunction.md
    ftmodelcapabilitiesout.md
    ftmodelcard.md
    ftmodelcardtype.md
    function.md
    functioncall.md
    functioncallentry.md
    functioncallentryarguments.md
    functioncallentryobject.md
    functioncallentrytype.md
    functioncallevent.md
    functioncalleventtype.md
    functionname.md
    functionresultentry.md
    functionresultentryobject.md
    functionresultentrytype.md
    functiontool.md
    functiontooltype.md
    githubrepositoryin.md
    githubrepositoryintype.md
    githubrepositoryout.md
    githubrepositoryouttype.md
    handoffexecution.md
    httpvalidationerror.md
    hyperparameters.md
    imagegenerationtool.md
    imagegenerationtooltype.md
    imageurl.md
    imageurlchunk.md
    imageurlchunkimageurl.md
    imageurlchunktype.md
    inputentries.md
    inputs.md
    instructrequest.md
    instructrequestinputs.md
    instructrequestinputsmessages.md
    instructrequestmessages.md
    integrations.md
    jobin.md
    jobinintegrations.md
    jobinrepositories.md
    jobmetadataout.md
    jobsapiroutesbatchcancelbatchjobrequest.md
    jobsapiroutesbatchgetbatchjobrequest.md
    jobsapiroutesbatchgetbatchjobsrequest.md
    jobsapiroutesfinetuningarchivefinetunedmodelrequest.md
    jobsapiroutesfinetuningcancelfinetuningjobrequest.md
    jobsapiroutesfinetuningcancelfinetuningjobresponse.md
    jobsapiroutesfinetuningcreatefinetuningjobresponse.md
    jobsapiroutesfinetuninggetfinetuningjobrequest.md
    jobsapiroutesfinetuninggetfinetuningjobresponse.md
    jobsapiroutesfinetuninggetfinetuningjobsrequest.md
    jobsapiroutesfinetuningstartfinetuningjobrequest.md
    jobsapiroutesfinetuningstartfinetuningjobresponse.md
    jobsapiroutesfinetuningunarchivefinetunedmodelrequest.md
    jobsapiroutesfinetuningupdatefinetunedmodelrequest.md
    jobsapiroutesfinetuningupdatefinetunedmodelresponse.md
    jobsout.md
    jobsoutdata.md
    jobsoutobject.md
    jobtype.md
    jsonschema.md
    legacyjobmetadataout.md
    legacyjobmetadataoutobject.md
    listfilesout.md
    loc.md
    messageentries.md
    messageinputcontentchunks.md
    messageinputentry.md
    messageinputentrycontent.md
    messageinputentryrole.md
    messageinputentrytype.md
    messageoutputcontentchunks.md
    messageoutputentry.md
    messageoutputentrycontent.md
    messageoutputentryobject.md
    messageoutputentryrole.md
    messageoutputentrytype.md
    messageoutputevent.md
    messageoutputeventcontent.md
    messageoutputeventrole.md
    messageoutputeventtype.md
    messages.md
    metricout.md
    mistralpromptmode.md
    modelcapabilities.md
    modelconversation.md
    modelconversationobject.md
    modelconversationtools.md
    modellist.md
    modeltype.md
    moderationobject.md
    moderationresponse.md
    object.md
    ocrimageobject.md
    ocrpagedimensions.md
    ocrpageobject.md
    ocrrequest.md
    ocrresponse.md
    ocrusageinfo.md
    one.md
    outputcontentchunks.md
    outputs.md
    prediction.md
    queryparamstatus.md
    referencechunk.md
    referencechunktype.md
    repositories.md
    response1.md
    responsebody.md
    responsedoneevent.md
    responsedoneeventtype.md
    responseerrorevent.md
    responseerroreventtype.md
    responseformat.md
    responseformats.md
    responsestartedevent.md
    responsestartedeventtype.md
    retrievefileout.md
    retrievemodelv1modelsmodelidgetrequest.md
    retrievemodelv1modelsmodelidgetresponseretrievemodelv1modelsmodelidget.md
    role.md
    sampletype.md
    security.md
    source.md
    ssetypes.md
    status.md
    stop.md
    systemmessage.md
    systemmessagecontent.md
    textchunk.md
    textchunktype.md
    tool.md
    toolcall.md
    toolchoice.md
    toolchoiceenum.md
    toolexecutiondoneevent.md
    toolexecutiondoneeventtype.md
    toolexecutionentry.md
    toolexecutionentryobject.md
    toolexecutionentrytype.md
    toolexecutionstartedevent.md
    toolexecutionstartedeventtype.md
    toolfilechunk.md
    toolfilechunktype.md
    toolmessage.md
    toolmessagecontent.md
    toolmessagerole.md
    toolreferencechunk.md
    toolreferencechunktype.md
    tools.md
    tooltypes.md
    trainingfile.md
    two.md
    type.md
    unarchiveftmodelout.md
    unarchiveftmodeloutobject.md
    updateftmodelin.md
    uploadfileout.md
    usageinfo.md
    usermessage.md
    usermessagecontent.md
    usermessagerole.md
    validationerror.md
    wandbintegration.md
    wandbintegrationout.md
    wandbintegrationouttype.md
    wandbintegrationtype.md
    websearchpremiumtool.md
    websearchpremiumtooltype.md
    websearchtool.md
    websearchtooltype.md
  sdks/
    agents/
      README.md
    batch/
      README.md
    beta/
      README.md
    chat/
      README.md
    classifiers/
      README.md
    conversations/
      README.md
    embeddings/
      README.md
    files/
      README.md
    fim/
      README.md
    finetuning/
      README.md
    jobs/
      README.md
    mistral/
      README.md
    mistralagents/
      README.md
    mistraljobs/
      README.md
    models/
      README.md
    ocr/
      README.md
examples/
  azure/
    az_chat_no_streaming.py.py
    chat_no_streaming.py.py
  fixtures/
    classifier_sentiments.jsonl
    ft_training_file.jsonl
    ft_validation_file.jsonl
  gcp/
    async_chat_no_streaming.py
    gcp_async_chat_no_streaming.py
  mcp_servers/
    sse_server.py
    stdio_server.py
  async_agents_no_streaming.py
  async_chat_no_streaming.py
  async_chat_with_image_no_streaming.py
  async_chat_with_streaming.py
  async_classifier.py
  async_code_completion.py
  async_conversation_agent.py
  async_conversation_run_mcp_remote_auth.py
  async_conversation_run_mcp_remote.py
  async_conversation_run_mcp.py
  async_conversation_run_stream.py
  async_conversation_run.py
  async_embeddings.py
  async_files.py
  async_jobs_chat.py
  async_jobs.py
  async_list_models.py
  async_structured_outputs.py
  chat_no_streaming.py
  chat_prediction.py
  chat_with_streaming.py
  chatbot_with_streaming.py
  code_completion.py
  completion_with_streaming.py
  dry_run_job.py
  embeddings.py
  files.py
  function_calling.py
  jobs.py
  json_format.py
  list_models.py
  ocr_process_from_file.py
  ocr_process_from_url.py
  structured_outputs_with_json_schema.py
  structured_outputs_with_pydantic.py
  structured_outputs.py
packages/
  mistralai_azure/
    .speakeasy/
      gen.lock
      gen.yaml
    docs/
      models/
        utils/
          retryconfig.md
        arguments.md
        assistantmessage.md
        assistantmessagecontent.md
        assistantmessagerole.md
        chatcompletionchoice.md
        chatcompletionchoicefinishreason.md
        chatcompletionrequest.md
        chatcompletionrequestmessages.md
        chatcompletionrequeststop.md
        chatcompletionrequesttoolchoice.md
        chatcompletionresponse.md
        chatcompletionstreamrequest.md
        chatcompletionstreamrequesttoolchoice.md
        completionchunk.md
        completionevent.md
        completionresponsestreamchoice.md
        content.md
        contentchunk.md
        deltamessage.md
        finishreason.md
        function.md
        functioncall.md
        functionname.md
        httpvalidationerror.md
        imageurl.md
        imageurlchunk.md
        imageurlchunkimageurl.md
        imageurlchunktype.md
        jsonschema.md
        loc.md
        messages.md
        prediction.md
        referencechunk.md
        referencechunktype.md
        responseformat.md
        responseformats.md
        role.md
        security.md
        stop.md
        systemmessage.md
        systemmessagecontent.md
        textchunk.md
        tool.md
        toolcall.md
        toolchoice.md
        toolchoiceenum.md
        toolmessage.md
        toolmessagecontent.md
        toolmessagerole.md
        tooltypes.md
        type.md
        usageinfo.md
        usermessage.md
        usermessagecontent.md
        usermessagerole.md
        validationerror.md
      sdks/
        chat/
          README.md
        mistralazure/
          README.md
    scripts/
      prepare_readme.py
      publish.sh
    src/
      mistralai_azure/
        _hooks/
          __init__.py
          custom_user_agent.py
          registration.py
          sdkhooks.py
          types.py
        models/
          __init__.py
          assistantmessage.py
          chatcompletionchoice.py
          chatcompletionrequest.py
          chatcompletionresponse.py
          chatcompletionstreamrequest.py
          completionchunk.py
          completionevent.py
          completionresponsestreamchoice.py
          contentchunk.py
          deltamessage.py
          function.py
          functioncall.py
          functionname.py
          httpvalidationerror.py
          imageurl.py
          imageurlchunk.py
          jsonschema.py
          prediction.py
          referencechunk.py
          responseformat.py
          responseformats.py
          sdkerror.py
          security.py
          systemmessage.py
          textchunk.py
          tool.py
          toolcall.py
          toolchoice.py
          toolchoiceenum.py
          toolmessage.py
          tooltypes.py
          usageinfo.py
          usermessage.py
          validationerror.py
        types/
          __init__.py
          basemodel.py
        utils/
          __init__.py
          annotations.py
          enums.py
          eventstreaming.py
          forms.py
          headers.py
          logger.py
          metadata.py
          queryparams.py
          requestbodies.py
          retries.py
          security.py
          serializers.py
          url.py
          values.py
        __init__.py
        _version.py
        basesdk.py
        chat.py
        httpclient.py
        py.typed
        sdk.py
        sdkconfiguration.py
    .genignore
    .gitattributes
    .gitignore
    CONTRIBUTING.md
    poetry.toml
    py.typed
    pylintrc
    pyproject.toml
    README.md
    USAGE.md
  mistralai_gcp/
    .speakeasy/
      gen.lock
      gen.yaml
    docs/
      models/
        utils/
          retryconfig.md
        arguments.md
        assistantmessage.md
        assistantmessagecontent.md
        assistantmessagerole.md
        chatcompletionchoice.md
        chatcompletionchoicefinishreason.md
        chatcompletionrequest.md
        chatcompletionrequestmessages.md
        chatcompletionrequeststop.md
        chatcompletionrequesttoolchoice.md
        chatcompletionresponse.md
        chatcompletionstreamrequest.md
        chatcompletionstreamrequesttoolchoice.md
        completionchunk.md
        completionevent.md
        completionresponsestreamchoice.md
        content.md
        contentchunk.md
        deltamessage.md
        fimcompletionrequest.md
        fimcompletionrequeststop.md
        fimcompletionresponse.md
        fimcompletionstreamrequest.md
        fimcompletionstreamrequeststop.md
        finishreason.md
        function.md
        functioncall.md
        functionname.md
        httpvalidationerror.md
        imageurl.md
        imageurlchunk.md
        imageurlchunkimageurl.md
        imageurlchunktype.md
        jsonschema.md
        loc.md
        messages.md
        prediction.md
        referencechunk.md
        referencechunktype.md
        responseformat.md
        responseformats.md
        role.md
        security.md
        stop.md
        systemmessage.md
        systemmessagecontent.md
        textchunk.md
        tool.md
        toolcall.md
        toolchoice.md
        toolchoiceenum.md
        toolmessage.md
        toolmessagecontent.md
        toolmessagerole.md
        tooltypes.md
        type.md
        usageinfo.md
        usermessage.md
        usermessagecontent.md
        usermessagerole.md
        validationerror.md
      sdks/
        chat/
          README.md
        fim/
          README.md
        mistralgcp/
          README.md
    scripts/
      prepare_readme.py
      publish.sh
    src/
      mistralai_gcp/
        _hooks/
          __init__.py
          custom_user_agent.py
          registration.py
          sdkhooks.py
          types.py
        models/
          __init__.py
          assistantmessage.py
          chatcompletionchoice.py
          chatcompletionrequest.py
          chatcompletionresponse.py
          chatcompletionstreamrequest.py
          completionchunk.py
          completionevent.py
          completionresponsestreamchoice.py
          contentchunk.py
          deltamessage.py
          fimcompletionrequest.py
          fimcompletionresponse.py
          fimcompletionstreamrequest.py
          function.py
          functioncall.py
          functionname.py
          httpvalidationerror.py
          imageurl.py
          imageurlchunk.py
          jsonschema.py
          prediction.py
          referencechunk.py
          responseformat.py
          responseformats.py
          sdkerror.py
          security.py
          systemmessage.py
          textchunk.py
          tool.py
          toolcall.py
          toolchoice.py
          toolchoiceenum.py
          toolmessage.py
          tooltypes.py
          usageinfo.py
          usermessage.py
          validationerror.py
        types/
          __init__.py
          basemodel.py
        utils/
          __init__.py
          annotations.py
          enums.py
          eventstreaming.py
          forms.py
          headers.py
          logger.py
          metadata.py
          queryparams.py
          requestbodies.py
          retries.py
          security.py
          serializers.py
          url.py
          values.py
        __init__.py
        _version.py
        basesdk.py
        chat.py
        fim.py
        httpclient.py
        py.typed
        sdk.py
        sdkconfiguration.py
    .genignore
    .gitattributes
    .gitignore
    CONTRIBUTING.md
    poetry.toml
    py.typed
    pylintrc
    pyproject.toml
    README.md
    USAGE.md
scripts/
  lint_custom_code.sh
  prepare_readme.py
  publish.sh
  run_examples.sh
src/
  mistralai/
    _hooks/
      __init__.py
      custom_user_agent.py
      deprecation_warning.py
      registration.py
      sdkhooks.py
      types.py
    extra/
      mcp/
        auth.py
        base.py
        sse.py
        stdio.py
      run/
        context.py
        result.py
        tools.py
        utils.py
      tests/
        test_struct_chat.py
        test_utils.py
      utils/
        __init__.py
        _pydantic_helper.py
        response_format.py
      __init__.py
      exceptions.py
      README.md
      struct_chat.py
    models/
      __init__.py
      agent.py
      agentconversation.py
      agentcreationrequest.py
      agenthandoffdoneevent.py
      agenthandoffentry.py
      agenthandoffstartedevent.py
      agents_api_v1_agents_getop.py
      agents_api_v1_agents_listop.py
      agents_api_v1_agents_update_versionop.py
      agents_api_v1_agents_updateop.py
      agents_api_v1_conversations_append_streamop.py
      agents_api_v1_conversations_appendop.py
      agents_api_v1_conversations_getop.py
      agents_api_v1_conversations_historyop.py
      agents_api_v1_conversations_listop.py
      agents_api_v1_conversations_messagesop.py
      agents_api_v1_conversations_restart_streamop.py
      agents_api_v1_conversations_restartop.py
      agentscompletionrequest.py
      agentscompletionstreamrequest.py
      agentupdaterequest.py
      apiendpoint.py
      archiveftmodelout.py
      assistantmessage.py
      basemodelcard.py
      batcherror.py
      batchjobin.py
      batchjobout.py
      batchjobsout.py
      batchjobstatus.py
      builtinconnectors.py
      chatclassificationrequest.py
      chatcompletionchoice.py
      chatcompletionrequest.py
      chatcompletionresponse.py
      chatcompletionstreamrequest.py
      chatmoderationrequest.py
      checkpointout.py
      classificationrequest.py
      classificationresponse.py
      classificationtargetresult.py
      classifierdetailedjobout.py
      classifierftmodelout.py
      classifierjobout.py
      classifiertargetin.py
      classifiertargetout.py
      classifiertrainingparameters.py
      classifiertrainingparametersin.py
      codeinterpretertool.py
      completionargs.py
      completionargsstop.py
      completionchunk.py
      completiondetailedjobout.py
      completionevent.py
      completionftmodelout.py
      completionjobout.py
      completionresponsestreamchoice.py
      completiontrainingparameters.py
      completiontrainingparametersin.py
      contentchunk.py
      conversationappendrequest.py
      conversationappendstreamrequest.py
      conversationevents.py
      conversationhistory.py
      conversationinputs.py
      conversationmessages.py
      conversationrequest.py
      conversationresponse.py
      conversationrestartrequest.py
      conversationrestartstreamrequest.py
      conversationstreamrequest.py
      conversationusageinfo.py
      delete_model_v1_models_model_id_deleteop.py
      deletefileout.py
      deletemodelout.py
      deltamessage.py
      documentlibrarytool.py
      documenturlchunk.py
      embeddingdtype.py
      embeddingrequest.py
      embeddingresponse.py
      embeddingresponsedata.py
      eventout.py
      filepurpose.py
      files_api_routes_delete_fileop.py
      files_api_routes_download_fileop.py
      files_api_routes_get_signed_urlop.py
      files_api_routes_list_filesop.py
      files_api_routes_retrieve_fileop.py
      files_api_routes_upload_fileop.py
      fileschema.py
      filesignedurl.py
      fimcompletionrequest.py
      fimcompletionresponse.py
      fimcompletionstreamrequest.py
      finetuneablemodeltype.py
      ftclassifierlossfunction.py
      ftmodelcapabilitiesout.py
      ftmodelcard.py
      function.py
      functioncall.py
      functioncallentry.py
      functioncallentryarguments.py
      functioncallevent.py
      functionname.py
      functionresultentry.py
      functiontool.py
      githubrepositoryin.py
      githubrepositoryout.py
      httpvalidationerror.py
      imagegenerationtool.py
      imageurl.py
      imageurlchunk.py
      inputentries.py
      inputs.py
      instructrequest.py
      jobin.py
      jobmetadataout.py
      jobs_api_routes_batch_cancel_batch_jobop.py
      jobs_api_routes_batch_get_batch_jobop.py
      jobs_api_routes_batch_get_batch_jobsop.py
      jobs_api_routes_fine_tuning_archive_fine_tuned_modelop.py
      jobs_api_routes_fine_tuning_cancel_fine_tuning_jobop.py
      jobs_api_routes_fine_tuning_create_fine_tuning_jobop.py
      jobs_api_routes_fine_tuning_get_fine_tuning_jobop.py
      jobs_api_routes_fine_tuning_get_fine_tuning_jobsop.py
      jobs_api_routes_fine_tuning_start_fine_tuning_jobop.py
      jobs_api_routes_fine_tuning_unarchive_fine_tuned_modelop.py
      jobs_api_routes_fine_tuning_update_fine_tuned_modelop.py
      jobsout.py
      jsonschema.py
      legacyjobmetadataout.py
      listfilesout.py
      messageentries.py
      messageinputcontentchunks.py
      messageinputentry.py
      messageoutputcontentchunks.py
      messageoutputentry.py
      messageoutputevent.py
      metricout.py
      mistralpromptmode.py
      modelcapabilities.py
      modelconversation.py
      modellist.py
      moderationobject.py
      moderationresponse.py
      ocrimageobject.py
      ocrpagedimensions.py
      ocrpageobject.py
      ocrrequest.py
      ocrresponse.py
      ocrusageinfo.py
      outputcontentchunks.py
      prediction.py
      referencechunk.py
      responsedoneevent.py
      responseerrorevent.py
      responseformat.py
      responseformats.py
      responsestartedevent.py
      retrieve_model_v1_models_model_id_getop.py
      retrievefileout.py
      sampletype.py
      sdkerror.py
      security.py
      source.py
      ssetypes.py
      systemmessage.py
      textchunk.py
      tool.py
      toolcall.py
      toolchoice.py
      toolchoiceenum.py
      toolexecutiondoneevent.py
      toolexecutionentry.py
      toolexecutionstartedevent.py
      toolfilechunk.py
      toolmessage.py
      toolreferencechunk.py
      tooltypes.py
      trainingfile.py
      unarchiveftmodelout.py
      updateftmodelin.py
      uploadfileout.py
      usageinfo.py
      usermessage.py
      validationerror.py
      wandbintegration.py
      wandbintegrationout.py
      websearchpremiumtool.py
      websearchtool.py
    types/
      __init__.py
      basemodel.py
    utils/
      __init__.py
      annotations.py
      enums.py
      eventstreaming.py
      forms.py
      headers.py
      logger.py
      metadata.py
      queryparams.py
      requestbodies.py
      retries.py
      security.py
      serializers.py
      url.py
      values.py
    __init__.py
    _version.py
    agents.py
    async_client.py
    basesdk.py
    batch.py
    beta.py
    chat.py
    classifiers.py
    client.py
    conversations.py
    embeddings.py
    files.py
    fim.py
    fine_tuning.py
    httpclient.py
    jobs.py
    mistral_agents.py
    mistral_jobs.py
    models_.py
    ocr.py
    py.typed
    sdk.py
    sdkconfiguration.py
    version.py
.genignore
.gitattributes
.gitignore
.pre-commit-config.yaml
CONTRIBUTING.md
LICENSE
MIGRATION.md
OLD-README.md
poetry.toml
py.typed
pylintrc
pyproject.toml
README.md
RELEASES.md
USAGE.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/ISSUE_TEMPLATE/bug_report_client.yml">
name: Bug report related to the Python client
description: Submit a bug report that's related to the Python client
title: '[BUG CLIENT]: '
labels: ['bug client', 'triage']
body:
  - type: markdown
    attributes:
      value: |
        Thanks for taking the time to fill out this bug report!
  - type: textarea
    id: python-vv
    attributes:
      label: Python -VV
      description: Run `python -VV` from your virtual environment
      placeholder: Copy-paste the output (no need for backticks, will be formatted into code automatically)
      render: shell
    validations:
      required: true
  - type: textarea
    id: pip-freeze
    attributes:
      label: Pip Freeze
      description: Run `pip freeze` from your virtual environment
      placeholder: Copy-paste the output (no need for backticks, will be formatted into code automatically)
      render: shell
    validations:
      required: true
  - type: textarea
    id: reproduction-steps
    attributes:
      label: Reproduction Steps
      description: Provide a clear and concise description of the steps that lead to your issue.
      placeholder: |
        1. First step...
        2. Second step...
        ...
    validations:
      required: true
  - type: textarea
    id: expected-behavior
    attributes:
      label: Expected Behavior
      description: Explain briefly what you expected to happen.
    validations:
      required: true
  - type: textarea
    id: additional-context
    attributes:
      label: Additional Context
      description: Add any context about your problem that you deem relevant.
  - type: textarea
    id: suggested-solutions
    attributes:
      label: Suggested Solutions
      description: Please list any solutions you recommend we consider.
</file>

<file path=".github/ISSUE_TEMPLATE/bug_report_model.yml">
name: Bug report related to our models
description: Submit a bug report that's related to our models behavior
title: '[BUG MODEL]: '
labels: ['bug model', 'triage']
body:
  - type: markdown
    attributes:
      value: |
        Thanks for taking the time to fill out this bug report!
  - type: dropdown
    id: model
    attributes:
      label: Model
      description: What's the model?
      options:
        - open-mistral-7b
        - open-mixtral-8x7b
        - open-mixtral-8x22b
        - mistral-small-latest
        - mistral-medium-latest
        - mistral-large-latest
        - mistral-embed
        - codestral-latest
      default: 0
    validations:
      required: true
  - type: textarea
    id: model-payload
    attributes:
      label: Request Payload
      description: Please provide the full request payload.
      placeholder: | 
        Please provide a complete example of the request payload, including, if possible, parameters such as 'messages', 'max_tokens', 'temperature', 'prefix', and 'tool_calls'.
        If you can, please set the 'temperature' parameter to 0 in your example. This will greatly help us.
    validations:
      required: true
  - type: textarea
    id: model-output
    attributes:
      label: Output
      description: Provide the output of the model.
    validations:
      required: true
  - type: textarea
    id: model-expected-output
    attributes:
      label: Expected Behavior
      description: Explain briefly what you expected to happen.
    validations:
      required: true
  - type: textarea
    id: model-additional-context
    attributes:
      label: Additional Context
      description: Add any context about your problem that you deem relevant.
  - type: textarea
    id: model-suggested-solutions
    attributes:
      label: Suggested Solutions
      description: Please list any solutions you recommend we consider.
</file>

<file path=".github/ISSUE_TEMPLATE/config.yml">
blank_issues_enabled: false
contact_links:
  - name: Documentation
    url: https://docs.mistral.ai
    about: Developer documentation for the Mistral AI platform
  - name: Discord
    url: https://discord.com/invite/mistralai
    about: Chat with the Mistral community
</file>

<file path=".github/workflows/lint_custom_code.yaml">
name: Linting Python custom files

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@76e04a911780d5b312d89783f7b1cd627778900a # v1.4.1

      - name: Install dependencies
        run: |
          touch README-PYPI.md
          poetry install --all-extras

      # The init, sdkhooks.py and types.py files in the _hooks folders are generated by Speakeasy hence the exclusion
      - name: Run all linters
        run: scripts/lint_custom_code.sh
</file>

<file path=".github/workflows/run_example_scripts.yaml">
name: Running the examples

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  run_examples:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5.4.0
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install Poetry
        uses: snok/install-poetry@76e04a911780d5b312d89783f7b1cd627778900a # v1.4.1

      - name: Set VERSION
        run: |
          VERSION=$(echo ${{ matrix.python-version }} | tr -d .)
          echo "VERSION=$VERSION" >> $GITHUB_ENV

      - name: Set MISTRAL_API_KEY using VERSION
        run: |
          echo "MISTRAL_API_KEY=${{ secrets[format('CI_MISTRAL_API_KEY_PYTHON_{0}', env.VERSION)] }}" >> $GITHUB_ENV

      - name: Build the package
        run: |
          touch README-PYPI.md # Create this file since the client is not built by Speakeasy
          poetry build

      - name: For python 3.9, install the client and run examples without extra dependencies.
        if: matrix.python-version == '3.9'
        run: |
          PACKAGE="dist/$(ls dist | grep whl | head -n 1)"
          python3 -m pip install "$PACKAGE"
          ./scripts/run_examples.sh --no-extra-dep
        env:
          MISTRAL_AGENT_ID: ${{ secrets.CI_AGENT_ID }}
          MISTRAL_API_KEY: ${{ env.MISTRAL_API_KEY }}

      - name: For python 3.10+, install client with extras and run all examples.
        if: matrix.python-version != '3.9'
        run: |
          PACKAGE="dist/$(ls dist | grep whl | head -n 1)[agents]"
          python3 -m pip install "$PACKAGE"
          ./scripts/run_examples.sh
        env:
          MISTRAL_AGENT_ID: ${{ secrets.CI_AGENT_ID }}
          MISTRAL_API_KEY: ${{ env.MISTRAL_API_KEY }}
</file>

<file path=".github/workflows/sdk_generation_mistralai_azure_sdk.yaml">
name: Generate MISTRAL-PYTHON-SDK-AZURE
permissions:
  checks: write
  contents: write
  pull-requests: write
  statuses: write
"on":
  workflow_dispatch:
    inputs:
      force:
        description: Force generation of SDKs
        type: boolean
        default: false
      set_version:
        description: optionally set a specific SDK version
        type: string
jobs:
  generate:
    uses: speakeasy-api/sdk-generation-action/.github/workflows/workflow-executor.yaml@v15
    with:
      force: ${{ github.event.inputs.force }}
      mode: pr
      set_version: ${{ github.event.inputs.set_version }}
      speakeasy_version: latest
      target: mistral-python-sdk-azure
    secrets:
      github_access_token: ${{ secrets.GITHUB_TOKEN }}
      pypi_token: ${{ secrets.PYPI_TOKEN }}
      speakeasy_api_key: ${{ secrets.SPEAKEASY_API_KEY }}
</file>

<file path=".github/workflows/sdk_generation_mistralai_gcp_sdk.yaml">
name: Generate MISTRAL-PYTHON-SDK-GOOGLE-CLOUD
permissions:
  checks: write
  contents: write
  pull-requests: write
  statuses: write
"on":
  workflow_dispatch:
    inputs:
      force:
        description: Force generation of SDKs
        type: boolean
        default: false
      set_version:
        description: optionally set a specific SDK version
        type: string
jobs:
  generate:
    uses: speakeasy-api/sdk-generation-action/.github/workflows/workflow-executor.yaml@v15
    with:
      force: ${{ github.event.inputs.force }}
      mode: pr
      set_version: ${{ github.event.inputs.set_version }}
      speakeasy_version: latest
      target: mistral-python-sdk-google-cloud
    secrets:
      github_access_token: ${{ secrets.GITHUB_TOKEN }}
      pypi_token: ${{ secrets.PYPI_TOKEN }}
      speakeasy_api_key: ${{ secrets.SPEAKEASY_API_KEY }}
</file>

<file path=".github/workflows/sdk_generation_mistralai_sdk.yaml">
name: Generate MISTRALAI
permissions:
  checks: write
  contents: write
  pull-requests: write
  statuses: write
"on":
  workflow_dispatch:
    inputs:
      force:
        description: Force generation of SDKs
        type: boolean
        default: false
      set_version:
        description: optionally set a specific SDK version
        type: string
jobs:
  generate:
    uses: speakeasy-api/sdk-generation-action/.github/workflows/workflow-executor.yaml@v15
    with:
      force: ${{ github.event.inputs.force }}
      mode: pr
      set_version: ${{ github.event.inputs.set_version }}
      speakeasy_version: latest
      target: mistralai-sdk
    secrets:
      github_access_token: ${{ secrets.SPEAKEASY_WORKFLOW_GITHUB_PAT }}
      pypi_token: ${{ secrets.PYPI_TOKEN }}
      speakeasy_api_key: ${{ secrets.SPEAKEASY_API_KEY }}
</file>

<file path=".github/workflows/sdk_publish_mistralai_sdk.yaml">
name: Publish MISTRALAI-SDK
permissions:
  checks: write
  contents: write
  pull-requests: write
  statuses: write
"on":
  push:
    branches:
      - main
    paths:
      - RELEASES.md
      - "*/RELEASES.md"
jobs:
  publish:
    uses: speakeasy-api/sdk-generation-action/.github/workflows/sdk-publish.yaml@v15
    secrets:
      github_access_token: ${{ secrets.GITHUB_TOKEN }}
      pypi_token: ${{ secrets.PYPI_TOKEN }}
      speakeasy_api_key: ${{ secrets.SPEAKEASY_API_KEY }}
</file>

<file path=".github/workflows/test_custom_code.yaml">
name: Testing Python custom files

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        id: setup-python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@76e04a911780d5b312d89783f7b1cd627778900a # v1.4.1
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true
          virtualenvs-path: .venv
          installer-parallel: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        # Install dependencies if cache does not exist
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --no-root

      - name: Run the 'src/mistralai/extra' package unit tests
        run: |
          source .venv/bin/activate
          python3.12 -m unittest discover -s src/mistralai/extra/tests -t src
</file>

<file path=".speakeasy/gen.lock">
lockVersion: 2.0.0
id: 2d045ec7-2ebb-4f4d-ad25-40953b132161
management:
  docChecksum: 9c8bd4d6bf675b159a80173b97c1265c
  docVersion: 1.0.0
  speakeasyVersion: 1.517.3
  generationVersion: 2.548.6
  releaseVersion: 1.8.2
  configChecksum: 5024c28578f991eabb85310ad8df96b7
  repoURL: https://github.com/mistralai/client-python.git
  installationURL: https://github.com/mistralai/client-python.git
  published: true
features:
  python:
    additionalDependencies: 1.0.0
    constsAndDefaults: 1.0.5
    core: 5.12.3
    customCodeRegions: 0.1.1
    defaultEnabledRetries: 0.2.0
    downloadStreams: 1.0.1
    enumUnions: 0.1.0
    envVarSecurityUsage: 0.3.2
    examples: 3.0.1
    flatRequests: 1.0.1
    flattening: 3.1.1
    globalSecurity: 3.0.3
    globalSecurityCallbacks: 1.0.0
    globalSecurityFlattening: 1.0.0
    globalServerURLs: 3.1.0
    methodArguments: 1.0.2
    multipartFileContentType: 1.0.0
    nameOverrides: 3.0.1
    nullables: 1.0.1
    openEnums: 1.0.0
    responseFormat: 1.0.1
    retries: 3.0.2
    sdkHooks: 1.0.1
    serverEvents: 1.0.7
    serverEventsSentinels: 0.1.0
    serverIDs: 3.0.0
    unions: 3.0.4
    uploadStreams: 1.0.0
generatedFiles:
  - .gitattributes
  - .python-version
  - .vscode/settings.json
  - USAGE.md
  - docs/models/agent.md
  - docs/models/agentconversation.md
  - docs/models/agentconversationobject.md
  - docs/models/agentcreationrequest.md
  - docs/models/agentcreationrequesttools.md
  - docs/models/agenthandoffdoneevent.md
  - docs/models/agenthandoffdoneeventtype.md
  - docs/models/agenthandoffentry.md
  - docs/models/agenthandoffentryobject.md
  - docs/models/agenthandoffentrytype.md
  - docs/models/agenthandoffstartedevent.md
  - docs/models/agenthandoffstartedeventtype.md
  - docs/models/agentobject.md
  - docs/models/agentsapiv1agentsgetrequest.md
  - docs/models/agentsapiv1agentslistrequest.md
  - docs/models/agentsapiv1agentsupdaterequest.md
  - docs/models/agentsapiv1agentsupdateversionrequest.md
  - docs/models/agentsapiv1conversationsappendrequest.md
  - docs/models/agentsapiv1conversationsappendstreamrequest.md
  - docs/models/agentsapiv1conversationsgetrequest.md
  - docs/models/agentsapiv1conversationsgetresponsev1conversationsget.md
  - docs/models/agentsapiv1conversationshistoryrequest.md
  - docs/models/agentsapiv1conversationslistrequest.md
  - docs/models/agentsapiv1conversationsmessagesrequest.md
  - docs/models/agentsapiv1conversationsrestartrequest.md
  - docs/models/agentsapiv1conversationsrestartstreamrequest.md
  - docs/models/agentscompletionrequest.md
  - docs/models/agentscompletionrequestmessages.md
  - docs/models/agentscompletionrequeststop.md
  - docs/models/agentscompletionrequesttoolchoice.md
  - docs/models/agentscompletionstreamrequest.md
  - docs/models/agentscompletionstreamrequestmessages.md
  - docs/models/agentscompletionstreamrequeststop.md
  - docs/models/agentscompletionstreamrequesttoolchoice.md
  - docs/models/agenttools.md
  - docs/models/agentupdaterequest.md
  - docs/models/agentupdaterequesttools.md
  - docs/models/apiendpoint.md
  - docs/models/archiveftmodelout.md
  - docs/models/archiveftmodeloutobject.md
  - docs/models/arguments.md
  - docs/models/assistantmessage.md
  - docs/models/assistantmessagecontent.md
  - docs/models/assistantmessagerole.md
  - docs/models/basemodelcard.md
  - docs/models/batcherror.md
  - docs/models/batchjobin.md
  - docs/models/batchjobout.md
  - docs/models/batchjoboutobject.md
  - docs/models/batchjobsout.md
  - docs/models/batchjobsoutobject.md
  - docs/models/batchjobstatus.md
  - docs/models/builtinconnectors.md
  - docs/models/chatclassificationrequest.md
  - docs/models/chatcompletionchoice.md
  - docs/models/chatcompletionrequest.md
  - docs/models/chatcompletionrequesttoolchoice.md
  - docs/models/chatcompletionresponse.md
  - docs/models/chatcompletionstreamrequest.md
  - docs/models/chatcompletionstreamrequestmessages.md
  - docs/models/chatcompletionstreamrequeststop.md
  - docs/models/chatcompletionstreamrequesttoolchoice.md
  - docs/models/chatmoderationrequest.md
  - docs/models/chatmoderationrequestinputs.md
  - docs/models/checkpointout.md
  - docs/models/classificationrequest.md
  - docs/models/classificationrequestinputs.md
  - docs/models/classificationresponse.md
  - docs/models/classificationtargetresult.md
  - docs/models/classifierdetailedjobout.md
  - docs/models/classifierdetailedjoboutintegrations.md
  - docs/models/classifierdetailedjoboutjobtype.md
  - docs/models/classifierdetailedjoboutobject.md
  - docs/models/classifierdetailedjoboutstatus.md
  - docs/models/classifierftmodelout.md
  - docs/models/classifierftmodeloutmodeltype.md
  - docs/models/classifierftmodeloutobject.md
  - docs/models/classifierjobout.md
  - docs/models/classifierjoboutintegrations.md
  - docs/models/classifierjoboutjobtype.md
  - docs/models/classifierjoboutobject.md
  - docs/models/classifierjoboutstatus.md
  - docs/models/classifiertargetin.md
  - docs/models/classifiertargetout.md
  - docs/models/classifiertrainingparameters.md
  - docs/models/classifiertrainingparametersin.md
  - docs/models/codeinterpretertool.md
  - docs/models/codeinterpretertooltype.md
  - docs/models/completionargs.md
  - docs/models/completionargsstop.md
  - docs/models/completionchunk.md
  - docs/models/completiondetailedjobout.md
  - docs/models/completiondetailedjoboutintegrations.md
  - docs/models/completiondetailedjoboutjobtype.md
  - docs/models/completiondetailedjoboutobject.md
  - docs/models/completiondetailedjoboutrepositories.md
  - docs/models/completiondetailedjoboutstatus.md
  - docs/models/completionevent.md
  - docs/models/completionftmodelout.md
  - docs/models/completionftmodeloutobject.md
  - docs/models/completionjobout.md
  - docs/models/completionjoboutobject.md
  - docs/models/completionresponsestreamchoice.md
  - docs/models/completionresponsestreamchoicefinishreason.md
  - docs/models/completiontrainingparameters.md
  - docs/models/completiontrainingparametersin.md
  - docs/models/content.md
  - docs/models/contentchunk.md
  - docs/models/conversationappendrequest.md
  - docs/models/conversationappendrequesthandoffexecution.md
  - docs/models/conversationappendstreamrequest.md
  - docs/models/conversationappendstreamrequesthandoffexecution.md
  - docs/models/conversationevents.md
  - docs/models/conversationeventsdata.md
  - docs/models/conversationhistory.md
  - docs/models/conversationhistoryobject.md
  - docs/models/conversationinputs.md
  - docs/models/conversationmessages.md
  - docs/models/conversationmessagesobject.md
  - docs/models/conversationrequest.md
  - docs/models/conversationresponse.md
  - docs/models/conversationresponseobject.md
  - docs/models/conversationrestartrequest.md
  - docs/models/conversationrestartrequesthandoffexecution.md
  - docs/models/conversationrestartstreamrequest.md
  - docs/models/conversationrestartstreamrequesthandoffexecution.md
  - docs/models/conversationstreamrequest.md
  - docs/models/conversationstreamrequesthandoffexecution.md
  - docs/models/conversationstreamrequesttools.md
  - docs/models/conversationusageinfo.md
  - docs/models/data.md
  - docs/models/deletefileout.md
  - docs/models/deletemodelout.md
  - docs/models/deletemodelv1modelsmodeliddeleterequest.md
  - docs/models/deltamessage.md
  - docs/models/document.md
  - docs/models/documentlibrarytool.md
  - docs/models/documentlibrarytooltype.md
  - docs/models/documenturlchunk.md
  - docs/models/documenturlchunktype.md
  - docs/models/embeddingdtype.md
  - docs/models/embeddingrequest.md
  - docs/models/embeddingrequestinputs.md
  - docs/models/embeddingresponse.md
  - docs/models/embeddingresponsedata.md
  - docs/models/entries.md
  - docs/models/eventout.md
  - docs/models/file.md
  - docs/models/filepurpose.md
  - docs/models/filesapiroutesdeletefilerequest.md
  - docs/models/filesapiroutesdownloadfilerequest.md
  - docs/models/filesapiroutesgetsignedurlrequest.md
  - docs/models/filesapirouteslistfilesrequest.md
  - docs/models/filesapiroutesretrievefilerequest.md
  - docs/models/filesapiroutesuploadfilemultipartbodyparams.md
  - docs/models/fileschema.md
  - docs/models/filesignedurl.md
  - docs/models/fimcompletionrequest.md
  - docs/models/fimcompletionrequeststop.md
  - docs/models/fimcompletionresponse.md
  - docs/models/fimcompletionstreamrequest.md
  - docs/models/fimcompletionstreamrequeststop.md
  - docs/models/finetuneablemodeltype.md
  - docs/models/finishreason.md
  - docs/models/ftclassifierlossfunction.md
  - docs/models/ftmodelcapabilitiesout.md
  - docs/models/ftmodelcard.md
  - docs/models/ftmodelcardtype.md
  - docs/models/function.md
  - docs/models/functioncall.md
  - docs/models/functioncallentry.md
  - docs/models/functioncallentryarguments.md
  - docs/models/functioncallentryobject.md
  - docs/models/functioncallentrytype.md
  - docs/models/functioncallevent.md
  - docs/models/functioncalleventtype.md
  - docs/models/functionname.md
  - docs/models/functionresultentry.md
  - docs/models/functionresultentryobject.md
  - docs/models/functionresultentrytype.md
  - docs/models/functiontool.md
  - docs/models/functiontooltype.md
  - docs/models/githubrepositoryin.md
  - docs/models/githubrepositoryintype.md
  - docs/models/githubrepositoryout.md
  - docs/models/githubrepositoryouttype.md
  - docs/models/handoffexecution.md
  - docs/models/httpvalidationerror.md
  - docs/models/hyperparameters.md
  - docs/models/imagegenerationtool.md
  - docs/models/imagegenerationtooltype.md
  - docs/models/imageurl.md
  - docs/models/imageurlchunk.md
  - docs/models/imageurlchunkimageurl.md
  - docs/models/imageurlchunktype.md
  - docs/models/inputentries.md
  - docs/models/inputs.md
  - docs/models/instructrequest.md
  - docs/models/instructrequestinputs.md
  - docs/models/instructrequestinputsmessages.md
  - docs/models/instructrequestmessages.md
  - docs/models/integrations.md
  - docs/models/jobin.md
  - docs/models/jobinintegrations.md
  - docs/models/jobinrepositories.md
  - docs/models/jobmetadataout.md
  - docs/models/jobsapiroutesbatchcancelbatchjobrequest.md
  - docs/models/jobsapiroutesbatchgetbatchjobrequest.md
  - docs/models/jobsapiroutesbatchgetbatchjobsrequest.md
  - docs/models/jobsapiroutesfinetuningarchivefinetunedmodelrequest.md
  - docs/models/jobsapiroutesfinetuningcancelfinetuningjobrequest.md
  - docs/models/jobsapiroutesfinetuningcancelfinetuningjobresponse.md
  - docs/models/jobsapiroutesfinetuningcreatefinetuningjobresponse.md
  - docs/models/jobsapiroutesfinetuninggetfinetuningjobrequest.md
  - docs/models/jobsapiroutesfinetuninggetfinetuningjobresponse.md
  - docs/models/jobsapiroutesfinetuninggetfinetuningjobsrequest.md
  - docs/models/jobsapiroutesfinetuningstartfinetuningjobrequest.md
  - docs/models/jobsapiroutesfinetuningstartfinetuningjobresponse.md
  - docs/models/jobsapiroutesfinetuningunarchivefinetunedmodelrequest.md
  - docs/models/jobsapiroutesfinetuningupdatefinetunedmodelrequest.md
  - docs/models/jobsapiroutesfinetuningupdatefinetunedmodelresponse.md
  - docs/models/jobsout.md
  - docs/models/jobsoutdata.md
  - docs/models/jobsoutobject.md
  - docs/models/jobtype.md
  - docs/models/jsonschema.md
  - docs/models/legacyjobmetadataout.md
  - docs/models/legacyjobmetadataoutobject.md
  - docs/models/listfilesout.md
  - docs/models/loc.md
  - docs/models/messageentries.md
  - docs/models/messageinputcontentchunks.md
  - docs/models/messageinputentry.md
  - docs/models/messageinputentrycontent.md
  - docs/models/messageinputentryrole.md
  - docs/models/messageinputentrytype.md
  - docs/models/messageoutputcontentchunks.md
  - docs/models/messageoutputentry.md
  - docs/models/messageoutputentrycontent.md
  - docs/models/messageoutputentryobject.md
  - docs/models/messageoutputentryrole.md
  - docs/models/messageoutputentrytype.md
  - docs/models/messageoutputevent.md
  - docs/models/messageoutputeventcontent.md
  - docs/models/messageoutputeventrole.md
  - docs/models/messageoutputeventtype.md
  - docs/models/messages.md
  - docs/models/metricout.md
  - docs/models/mistralpromptmode.md
  - docs/models/modelcapabilities.md
  - docs/models/modelconversation.md
  - docs/models/modelconversationobject.md
  - docs/models/modelconversationtools.md
  - docs/models/modellist.md
  - docs/models/modeltype.md
  - docs/models/moderationobject.md
  - docs/models/moderationresponse.md
  - docs/models/object.md
  - docs/models/ocrimageobject.md
  - docs/models/ocrpagedimensions.md
  - docs/models/ocrpageobject.md
  - docs/models/ocrrequest.md
  - docs/models/ocrresponse.md
  - docs/models/ocrusageinfo.md
  - docs/models/one.md
  - docs/models/outputcontentchunks.md
  - docs/models/outputs.md
  - docs/models/prediction.md
  - docs/models/queryparamstatus.md
  - docs/models/referencechunk.md
  - docs/models/referencechunktype.md
  - docs/models/repositories.md
  - docs/models/response1.md
  - docs/models/responsebody.md
  - docs/models/responsedoneevent.md
  - docs/models/responsedoneeventtype.md
  - docs/models/responseerrorevent.md
  - docs/models/responseerroreventtype.md
  - docs/models/responseformat.md
  - docs/models/responseformats.md
  - docs/models/responsestartedevent.md
  - docs/models/responsestartedeventtype.md
  - docs/models/retrievefileout.md
  - docs/models/retrievemodelv1modelsmodelidgetrequest.md
  - docs/models/retrievemodelv1modelsmodelidgetresponseretrievemodelv1modelsmodelidget.md
  - docs/models/role.md
  - docs/models/sampletype.md
  - docs/models/security.md
  - docs/models/source.md
  - docs/models/ssetypes.md
  - docs/models/status.md
  - docs/models/stop.md
  - docs/models/systemmessage.md
  - docs/models/systemmessagecontent.md
  - docs/models/textchunk.md
  - docs/models/textchunktype.md
  - docs/models/tool.md
  - docs/models/toolcall.md
  - docs/models/toolchoice.md
  - docs/models/toolchoiceenum.md
  - docs/models/toolexecutiondoneevent.md
  - docs/models/toolexecutiondoneeventtype.md
  - docs/models/toolexecutionentry.md
  - docs/models/toolexecutionentryobject.md
  - docs/models/toolexecutionentrytype.md
  - docs/models/toolexecutionstartedevent.md
  - docs/models/toolexecutionstartedeventtype.md
  - docs/models/toolfilechunk.md
  - docs/models/toolfilechunktype.md
  - docs/models/toolmessage.md
  - docs/models/toolmessagecontent.md
  - docs/models/toolmessagerole.md
  - docs/models/toolreferencechunk.md
  - docs/models/toolreferencechunktype.md
  - docs/models/tools.md
  - docs/models/tooltypes.md
  - docs/models/trainingfile.md
  - docs/models/two.md
  - docs/models/type.md
  - docs/models/unarchiveftmodelout.md
  - docs/models/unarchiveftmodeloutobject.md
  - docs/models/updateftmodelin.md
  - docs/models/uploadfileout.md
  - docs/models/usageinfo.md
  - docs/models/usermessage.md
  - docs/models/usermessagecontent.md
  - docs/models/usermessagerole.md
  - docs/models/utils/retryconfig.md
  - docs/models/validationerror.md
  - docs/models/wandbintegration.md
  - docs/models/wandbintegrationout.md
  - docs/models/wandbintegrationouttype.md
  - docs/models/wandbintegrationtype.md
  - docs/models/websearchpremiumtool.md
  - docs/models/websearchpremiumtooltype.md
  - docs/models/websearchtool.md
  - docs/models/websearchtooltype.md
  - docs/sdks/agents/README.md
  - docs/sdks/batch/README.md
  - docs/sdks/beta/README.md
  - docs/sdks/chat/README.md
  - docs/sdks/classifiers/README.md
  - docs/sdks/conversations/README.md
  - docs/sdks/embeddings/README.md
  - docs/sdks/files/README.md
  - docs/sdks/fim/README.md
  - docs/sdks/finetuning/README.md
  - docs/sdks/jobs/README.md
  - docs/sdks/mistral/README.md
  - docs/sdks/mistralagents/README.md
  - docs/sdks/mistraljobs/README.md
  - docs/sdks/models/README.md
  - docs/sdks/ocr/README.md
  - poetry.toml
  - py.typed
  - scripts/prepare_readme.py
  - scripts/publish.sh
  - src/mistralai/__init__.py
  - src/mistralai/_hooks/__init__.py
  - src/mistralai/_hooks/sdkhooks.py
  - src/mistralai/_hooks/types.py
  - src/mistralai/_version.py
  - src/mistralai/agents.py
  - src/mistralai/basesdk.py
  - src/mistralai/batch.py
  - src/mistralai/beta.py
  - src/mistralai/chat.py
  - src/mistralai/classifiers.py
  - src/mistralai/conversations.py
  - src/mistralai/embeddings.py
  - src/mistralai/files.py
  - src/mistralai/fim.py
  - src/mistralai/fine_tuning.py
  - src/mistralai/httpclient.py
  - src/mistralai/jobs.py
  - src/mistralai/mistral_agents.py
  - src/mistralai/mistral_jobs.py
  - src/mistralai/models/__init__.py
  - src/mistralai/models/agent.py
  - src/mistralai/models/agentconversation.py
  - src/mistralai/models/agentcreationrequest.py
  - src/mistralai/models/agenthandoffdoneevent.py
  - src/mistralai/models/agenthandoffentry.py
  - src/mistralai/models/agenthandoffstartedevent.py
  - src/mistralai/models/agents_api_v1_agents_getop.py
  - src/mistralai/models/agents_api_v1_agents_listop.py
  - src/mistralai/models/agents_api_v1_agents_update_versionop.py
  - src/mistralai/models/agents_api_v1_agents_updateop.py
  - src/mistralai/models/agents_api_v1_conversations_append_streamop.py
  - src/mistralai/models/agents_api_v1_conversations_appendop.py
  - src/mistralai/models/agents_api_v1_conversations_getop.py
  - src/mistralai/models/agents_api_v1_conversations_historyop.py
  - src/mistralai/models/agents_api_v1_conversations_listop.py
  - src/mistralai/models/agents_api_v1_conversations_messagesop.py
  - src/mistralai/models/agents_api_v1_conversations_restart_streamop.py
  - src/mistralai/models/agents_api_v1_conversations_restartop.py
  - src/mistralai/models/agentscompletionrequest.py
  - src/mistralai/models/agentscompletionstreamrequest.py
  - src/mistralai/models/agentupdaterequest.py
  - src/mistralai/models/apiendpoint.py
  - src/mistralai/models/archiveftmodelout.py
  - src/mistralai/models/assistantmessage.py
  - src/mistralai/models/basemodelcard.py
  - src/mistralai/models/batcherror.py
  - src/mistralai/models/batchjobin.py
  - src/mistralai/models/batchjobout.py
  - src/mistralai/models/batchjobsout.py
  - src/mistralai/models/batchjobstatus.py
  - src/mistralai/models/builtinconnectors.py
  - src/mistralai/models/chatclassificationrequest.py
  - src/mistralai/models/chatcompletionchoice.py
  - src/mistralai/models/chatcompletionrequest.py
  - src/mistralai/models/chatcompletionresponse.py
  - src/mistralai/models/chatcompletionstreamrequest.py
  - src/mistralai/models/chatmoderationrequest.py
  - src/mistralai/models/checkpointout.py
  - src/mistralai/models/classificationrequest.py
  - src/mistralai/models/classificationresponse.py
  - src/mistralai/models/classificationtargetresult.py
  - src/mistralai/models/classifierdetailedjobout.py
  - src/mistralai/models/classifierftmodelout.py
  - src/mistralai/models/classifierjobout.py
  - src/mistralai/models/classifiertargetin.py
  - src/mistralai/models/classifiertargetout.py
  - src/mistralai/models/classifiertrainingparameters.py
  - src/mistralai/models/classifiertrainingparametersin.py
  - src/mistralai/models/codeinterpretertool.py
  - src/mistralai/models/completionargs.py
  - src/mistralai/models/completionargsstop.py
  - src/mistralai/models/completionchunk.py
  - src/mistralai/models/completiondetailedjobout.py
  - src/mistralai/models/completionevent.py
  - src/mistralai/models/completionftmodelout.py
  - src/mistralai/models/completionjobout.py
  - src/mistralai/models/completionresponsestreamchoice.py
  - src/mistralai/models/completiontrainingparameters.py
  - src/mistralai/models/completiontrainingparametersin.py
  - src/mistralai/models/contentchunk.py
  - src/mistralai/models/conversationappendrequest.py
  - src/mistralai/models/conversationappendstreamrequest.py
  - src/mistralai/models/conversationevents.py
  - src/mistralai/models/conversationhistory.py
  - src/mistralai/models/conversationinputs.py
  - src/mistralai/models/conversationmessages.py
  - src/mistralai/models/conversationrequest.py
  - src/mistralai/models/conversationresponse.py
  - src/mistralai/models/conversationrestartrequest.py
  - src/mistralai/models/conversationrestartstreamrequest.py
  - src/mistralai/models/conversationstreamrequest.py
  - src/mistralai/models/conversationusageinfo.py
  - src/mistralai/models/delete_model_v1_models_model_id_deleteop.py
  - src/mistralai/models/deletefileout.py
  - src/mistralai/models/deletemodelout.py
  - src/mistralai/models/deltamessage.py
  - src/mistralai/models/documentlibrarytool.py
  - src/mistralai/models/documenturlchunk.py
  - src/mistralai/models/embeddingdtype.py
  - src/mistralai/models/embeddingrequest.py
  - src/mistralai/models/embeddingresponse.py
  - src/mistralai/models/embeddingresponsedata.py
  - src/mistralai/models/eventout.py
  - src/mistralai/models/filepurpose.py
  - src/mistralai/models/files_api_routes_delete_fileop.py
  - src/mistralai/models/files_api_routes_download_fileop.py
  - src/mistralai/models/files_api_routes_get_signed_urlop.py
  - src/mistralai/models/files_api_routes_list_filesop.py
  - src/mistralai/models/files_api_routes_retrieve_fileop.py
  - src/mistralai/models/files_api_routes_upload_fileop.py
  - src/mistralai/models/fileschema.py
  - src/mistralai/models/filesignedurl.py
  - src/mistralai/models/fimcompletionrequest.py
  - src/mistralai/models/fimcompletionresponse.py
  - src/mistralai/models/fimcompletionstreamrequest.py
  - src/mistralai/models/finetuneablemodeltype.py
  - src/mistralai/models/ftclassifierlossfunction.py
  - src/mistralai/models/ftmodelcapabilitiesout.py
  - src/mistralai/models/ftmodelcard.py
  - src/mistralai/models/function.py
  - src/mistralai/models/functioncall.py
  - src/mistralai/models/functioncallentry.py
  - src/mistralai/models/functioncallentryarguments.py
  - src/mistralai/models/functioncallevent.py
  - src/mistralai/models/functionname.py
  - src/mistralai/models/functionresultentry.py
  - src/mistralai/models/functiontool.py
  - src/mistralai/models/githubrepositoryin.py
  - src/mistralai/models/githubrepositoryout.py
  - src/mistralai/models/httpvalidationerror.py
  - src/mistralai/models/imagegenerationtool.py
  - src/mistralai/models/imageurl.py
  - src/mistralai/models/imageurlchunk.py
  - src/mistralai/models/inputentries.py
  - src/mistralai/models/inputs.py
  - src/mistralai/models/instructrequest.py
  - src/mistralai/models/jobin.py
  - src/mistralai/models/jobmetadataout.py
  - src/mistralai/models/jobs_api_routes_batch_cancel_batch_jobop.py
  - src/mistralai/models/jobs_api_routes_batch_get_batch_jobop.py
  - src/mistralai/models/jobs_api_routes_batch_get_batch_jobsop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_archive_fine_tuned_modelop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_cancel_fine_tuning_jobop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_create_fine_tuning_jobop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_get_fine_tuning_jobop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_get_fine_tuning_jobsop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_start_fine_tuning_jobop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_unarchive_fine_tuned_modelop.py
  - src/mistralai/models/jobs_api_routes_fine_tuning_update_fine_tuned_modelop.py
  - src/mistralai/models/jobsout.py
  - src/mistralai/models/jsonschema.py
  - src/mistralai/models/legacyjobmetadataout.py
  - src/mistralai/models/listfilesout.py
  - src/mistralai/models/messageentries.py
  - src/mistralai/models/messageinputcontentchunks.py
  - src/mistralai/models/messageinputentry.py
  - src/mistralai/models/messageoutputcontentchunks.py
  - src/mistralai/models/messageoutputentry.py
  - src/mistralai/models/messageoutputevent.py
  - src/mistralai/models/metricout.py
  - src/mistralai/models/mistralpromptmode.py
  - src/mistralai/models/modelcapabilities.py
  - src/mistralai/models/modelconversation.py
  - src/mistralai/models/modellist.py
  - src/mistralai/models/moderationobject.py
  - src/mistralai/models/moderationresponse.py
  - src/mistralai/models/ocrimageobject.py
  - src/mistralai/models/ocrpagedimensions.py
  - src/mistralai/models/ocrpageobject.py
  - src/mistralai/models/ocrrequest.py
  - src/mistralai/models/ocrresponse.py
  - src/mistralai/models/ocrusageinfo.py
  - src/mistralai/models/outputcontentchunks.py
  - src/mistralai/models/prediction.py
  - src/mistralai/models/referencechunk.py
  - src/mistralai/models/responsedoneevent.py
  - src/mistralai/models/responseerrorevent.py
  - src/mistralai/models/responseformat.py
  - src/mistralai/models/responseformats.py
  - src/mistralai/models/responsestartedevent.py
  - src/mistralai/models/retrieve_model_v1_models_model_id_getop.py
  - src/mistralai/models/retrievefileout.py
  - src/mistralai/models/sampletype.py
  - src/mistralai/models/sdkerror.py
  - src/mistralai/models/security.py
  - src/mistralai/models/source.py
  - src/mistralai/models/ssetypes.py
  - src/mistralai/models/systemmessage.py
  - src/mistralai/models/textchunk.py
  - src/mistralai/models/tool.py
  - src/mistralai/models/toolcall.py
  - src/mistralai/models/toolchoice.py
  - src/mistralai/models/toolchoiceenum.py
  - src/mistralai/models/toolexecutiondoneevent.py
  - src/mistralai/models/toolexecutionentry.py
  - src/mistralai/models/toolexecutionstartedevent.py
  - src/mistralai/models/toolfilechunk.py
  - src/mistralai/models/toolmessage.py
  - src/mistralai/models/toolreferencechunk.py
  - src/mistralai/models/tooltypes.py
  - src/mistralai/models/trainingfile.py
  - src/mistralai/models/unarchiveftmodelout.py
  - src/mistralai/models/updateftmodelin.py
  - src/mistralai/models/uploadfileout.py
  - src/mistralai/models/usageinfo.py
  - src/mistralai/models/usermessage.py
  - src/mistralai/models/validationerror.py
  - src/mistralai/models/wandbintegration.py
  - src/mistralai/models/wandbintegrationout.py
  - src/mistralai/models/websearchpremiumtool.py
  - src/mistralai/models/websearchtool.py
  - src/mistralai/models_.py
  - src/mistralai/ocr.py
  - src/mistralai/py.typed
  - src/mistralai/sdk.py
  - src/mistralai/sdkconfiguration.py
  - src/mistralai/types/__init__.py
  - src/mistralai/types/basemodel.py
  - src/mistralai/utils/__init__.py
  - src/mistralai/utils/annotations.py
  - src/mistralai/utils/enums.py
  - src/mistralai/utils/eventstreaming.py
  - src/mistralai/utils/forms.py
  - src/mistralai/utils/headers.py
  - src/mistralai/utils/logger.py
  - src/mistralai/utils/metadata.py
  - src/mistralai/utils/queryparams.py
  - src/mistralai/utils/requestbodies.py
  - src/mistralai/utils/retries.py
  - src/mistralai/utils/security.py
  - src/mistralai/utils/serializers.py
  - src/mistralai/utils/url.py
  - src/mistralai/utils/values.py
examples:
  list_models_v1_models_get:
    speakeasy-default-list-models-v1-models-get:
      responses:
        "200":
          application/json: {"object": "list"}
        "422":
          application/json: {}
  retrieve_model_v1_models__model_id__get:
    "":
      parameters:
        path:
          model_id: "ft:open-mistral-7b:587a6b29:20240514:7e773925"
      responses:
        "200":
          application/json: {"id": "<id>", "object": "model", "owned_by": "mistralai", "capabilities": {"completion_chat": true, "completion_fim": false, "function_calling": true, "fine_tuning": false, "vision": false}, "max_context_length": 32768}
        "422": {}
  delete_model_v1_models__model_id__delete:
    "":
      parameters:
        path:
          model_id: "ft:open-mistral-7b:587a6b29:20240514:7e773925"
      responses:
        "200":
          application/json: {"id": "ft:open-mistral-7b:587a6b29:20240514:7e773925", "object": "model", "deleted": true}
        "422": {}
  jobs_api_routes_fine_tuning_update_fine_tuned_model:
    "":
      parameters:
        path:
          model_id: "ft:open-mistral-7b:587a6b29:20240514:7e773925"
      requestBody:
        application/json: {}
      responses:
        "200":
          application/json: {"id": "<id>", "object": "model", "created": 597129, "owned_by": "<value>", "root": "<value>", "archived": true, "capabilities": {"completion_chat": true, "completion_fim": false, "function_calling": false, "fine_tuning": false, "classification": false}, "max_context_length": 32768, "job": "fa7f0e74-32ce-447c-9b60-cc78135ddeb8", "model_type": "completion"}
  jobs_api_routes_fine_tuning_archive_fine_tuned_model:
    "":
      parameters:
        path:
          model_id: "ft:open-mistral-7b:587a6b29:20240514:7e773925"
      responses:
        "200":
          application/json: {"id": "<id>", "object": "model", "archived": true}
  jobs_api_routes_fine_tuning_unarchive_fine_tuned_model:
    "":
      parameters:
        path:
          model_id: "ft:open-mistral-7b:587a6b29:20240514:7e773925"
      responses:
        "200":
          application/json: {"id": "<id>", "object": "model", "archived": false}
  files_api_routes_upload_file:
    speakeasy-default-files-api-routes-upload-file:
      requestBody:
        multipart/form-data: {"file": {}}
      responses:
        "200":
          application/json: {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "bytes": 13000, "created_at": 1716963433, "filename": "files_upload.jsonl", "purpose": "fine-tune", "sample_type": "batch_request", "source": "repository"}
  files_api_routes_list_files:
    speakeasy-default-files-api-routes-list-files:
      parameters:
        query:
          page: 0
          page_size: 100
      responses:
        "200":
          application/json: {"data": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "bytes": 13000, "created_at": 1716963433, "filename": "files_upload.jsonl", "purpose": "batch", "sample_type": "batch_result", "source": "upload"}, {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "bytes": 13000, "created_at": 1716963433, "filename": "files_upload.jsonl", "purpose": "fine-tune", "sample_type": "pretrain", "source": "repository"}, {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "bytes": 13000, "created_at": 1716963433, "filename": "files_upload.jsonl", "purpose": "batch", "sample_type": "pretrain", "source": "mistral"}], "object": "<value>", "total": 86140}
  files_api_routes_retrieve_file:
    speakeasy-default-files-api-routes-retrieve-file:
      parameters:
        path:
          file_id: "<id>"
      responses:
        "200":
          application/json: {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "bytes": 13000, "created_at": 1716963433, "filename": "files_upload.jsonl", "purpose": "fine-tune", "sample_type": "batch_error", "source": "upload", "deleted": true}
  files_api_routes_delete_file:
    speakeasy-default-files-api-routes-delete-file:
      parameters:
        path:
          file_id: "<id>"
      responses:
        "200":
          application/json: {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f09", "object": "file", "deleted": false}
  files_api_routes_download_file:
    speakeasy-default-files-api-routes-download-file:
      parameters:
        path:
          file_id: "<id>"
      responses:
        "200":
          application/octet-stream: "x-file: example.file"
  files_api_routes_get_signed_url:
    speakeasy-default-files-api-routes-get-signed-url:
      parameters:
        path:
          file_id: "<id>"
        query:
          expiry: 24
      responses:
        "200":
          application/json: {"url": "https://scornful-daughter.com/"}
  jobs_api_routes_fine_tuning_get_fine_tuning_jobs:
    speakeasy-default-jobs-api-routes-fine-tuning-get-fine-tuning-jobs:
      parameters:
        query:
          page: 0
          page_size: 100
          created_by_me: false
      responses:
        "200":
          application/json: {"object": "list", "total": 768578}
  jobs_api_routes_fine_tuning_create_fine_tuning_job:
    speakeasy-default-jobs-api-routes-fine-tuning-create-fine-tuning-job:
      requestBody:
        application/json: {"model": "Fiesta", "invalid_sample_skip_percentage": 0, "hyperparameters": {"learning_rate": 0.0001}}
      responses:
        "200":
          application/json: {"expected_duration_seconds": 220, "cost": 10, "cost_currency": "EUR", "train_tokens_per_step": 131072, "train_tokens": 1310720, "data_tokens": 305375, "deprecated": true, "details": "<value>", "epochs": 4.2922, "training_steps": 10, "object": "job.metadata"}
  jobs_api_routes_fine_tuning_get_fine_tuning_job:
    speakeasy-default-jobs-api-routes-fine-tuning-get-fine-tuning-job:
      parameters:
        path:
          job_id: "b888f774-3e7c-4135-a18c-6b985523c4bc"
      responses:
        "200":
          application/json: {"id": "888f7743-e7c1-4351-b8c6-b985523c4bcb", "auto_start": true, "model": "2", "status": "CANCELLATION_REQUESTED", "created_at": 444836, "modified_at": 424256, "training_files": [], "object": "job", "job_type": "completion", "hyperparameters": {"learning_rate": 0.0001}, "checkpoints": [{"metrics": {}, "step_number": 550563, "created_at": 1716963433}]}
  jobs_api_routes_fine_tuning_cancel_fine_tuning_job:
    speakeasy-default-jobs-api-routes-fine-tuning-cancel-fine-tuning-job:
      parameters:
        path:
          job_id: "0f713502-9233-41c6-9ebd-c570b7edb496"
      responses:
        "200":
          application/json: {"id": "f7135029-2331-4c6e-bbdc-570b7edb4966", "auto_start": true, "model": "A4", "status": "CANCELLATION_REQUESTED", "created_at": 703131, "modified_at": 929437, "training_files": ["e3e32613-5744-4d82-8f3f-d6b3c11eb45e"], "object": "job", "job_type": "classifier", "hyperparameters": {"learning_rate": 0.0001}, "checkpoints": [{"metrics": {}, "step_number": 466651, "created_at": 1716963433}], "classifier_targets": [{"name": "<value>", "labels": ["<value>", "<value>", "<value>"], "weight": 687.66, "loss_function": "single_class"}, {"name": "<value>", "labels": ["<value>", "<value>", "<value>"], "weight": 8470.22, "loss_function": "multi_class"}]}
  jobs_api_routes_fine_tuning_start_fine_tuning_job:
    speakeasy-default-jobs-api-routes-fine-tuning-start-fine-tuning-job:
      parameters:
        path:
          job_id: "0bf0f9e6-c3e5-4d61-aac8-0e36dcac0dfc"
      responses:
        "200":
          application/json: {"id": "bf0f9e6c-3e5d-461a-ac80-e36dcac0dfc8", "auto_start": true, "model": "Explorer", "status": "RUNNING", "created_at": 961967, "modified_at": 914446, "training_files": ["82c4783e-31ec-471d-bbed-4c90a1b0dd73"], "object": "job", "job_type": "classifier", "hyperparameters": {"learning_rate": 0.0001}, "checkpoints": [{"metrics": {}, "step_number": 590686, "created_at": 1716963433}], "classifier_targets": [{"name": "<value>", "labels": ["<value>"], "weight": 5494.15, "loss_function": "single_class"}, {"name": "<value>", "labels": ["<value>", "<value>"], "weight": 7945.15, "loss_function": "single_class"}]}
  jobs_api_routes_batch_get_batch_jobs:
    speakeasy-default-jobs-api-routes-batch-get-batch-jobs:
      parameters:
        query:
          page: 0
          page_size: 100
          created_by_me: false
      responses:
        "200":
          application/json: {"object": "list", "total": 768578}
  jobs_api_routes_batch_create_batch_job:
    speakeasy-default-jobs-api-routes-batch-create-batch-job:
      requestBody:
        application/json: {"input_files": ["a621cf02-1cd9-4cf5-8403-315211a509a3"], "endpoint": "/v1/fim/completions", "model": "2", "timeout_hours": 24}
      responses:
        "200":
          application/json: {"id": "<id>", "object": "batch", "input_files": ["8e774c2b-ecc3-4769-b177-5e024985613d", "0ee803d5-6a1d-4f94-836b-fd39494798bc"], "endpoint": "<value>", "model": "Impala", "errors": [{"message": "<value>", "count": 1}, {"message": "<value>", "count": 1}, {"message": "<value>", "count": 1}], "status": "RUNNING", "created_at": 770370, "total_requests": 350586, "completed_requests": 95214, "succeeded_requests": 930830, "failed_requests": 617761}
  jobs_api_routes_batch_get_batch_job:
    speakeasy-default-jobs-api-routes-batch-get-batch-job:
      parameters:
        path:
          job_id: "b888f774-3e7c-4135-a18c-6b985523c4bc"
      responses:
        "200":
          application/json: {"id": "<id>", "object": "batch", "input_files": ["50f76228-1da8-44bc-b661-c8a99c6b71b6", "cd62b8f7-112a-4af0-bab4-e43b4cca3716", "620807aa-1f8c-4f05-ad89-d58ee381f6b4"], "endpoint": "<value>", "model": "Golf", "errors": [{"message": "<value>", "count": 1}, {"message": "<value>", "count": 1}], "status": "SUCCESS", "created_at": 790898, "total_requests": 55097, "completed_requests": 578320, "succeeded_requests": 856562, "failed_requests": 328633}
  jobs_api_routes_batch_cancel_batch_job:
    speakeasy-default-jobs-api-routes-batch-cancel-batch-job:
      parameters:
        path:
          job_id: "0f713502-9233-41c6-9ebd-c570b7edb496"
      responses:
        "200":
          application/json: {"id": "<id>", "object": "batch", "input_files": ["50fbe4e3-e326-4135-8744-d82f3fd6b3c1", "eb45e247-ac10-4cdc-8311-2f7cc9241230", "4afaa0f8-4bd4-4945-9116-89d07a64aa72"], "endpoint": "<value>", "model": "Alpine", "errors": [{"message": "<value>", "count": 1}, {"message": "<value>", "count": 1}], "status": "QUEUED", "created_at": 709109, "total_requests": 275794, "completed_requests": 158938, "succeeded_requests": 12381, "failed_requests": 11864}
  chat_completion_v1_chat_completions_post:
    speakeasy-default-chat-completion-v1-chat-completions-post:
      requestBody:
        application/json: {"model": "mistral-small-latest", "stream": false, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "mistral-small-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
  stream_chat:
    speakeasy-default-stream-chat:
      requestBody:
        application/json: {"model": "mistral-small-latest", "stream": true, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "422":
          application/json: {}
        "200": {}
  fim_completion_v1_fim_completions_post:
    speakeasy-default-fim-completion-v1-fim-completions-post:
      requestBody:
        application/json: {"model": "codestral-2405", "top_p": 1, "stream": false, "prompt": "def", "suffix": "return a+b"}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "codestral-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
  stream_fim:
    speakeasy-default-stream-fim:
      requestBody:
        application/json: {"model": "codestral-2405", "top_p": 1, "stream": true, "prompt": "def", "suffix": "return a+b"}
      responses:
        "422":
          application/json: {}
        "200": {}
  agents_completion_v1_agents_completions_post:
    speakeasy-default-agents-completion-v1-agents-completions-post:
      requestBody:
        application/json: {"stream": false, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}], "agent_id": "<id>"}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "mistral-small-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
  stream_agents:
    speakeasy-default-stream-agents:
      requestBody:
        application/json: {"stream": true, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}], "agent_id": "<id>"}
      responses:
        "422":
          application/json: {}
        "200": {}
  embeddings_v1_embeddings_post:
    speakeasy-default-embeddings-v1-embeddings-post:
      requestBody:
        application/json: {"model": "mistral-embed", "input": ["Embed this sentence.", "As well as this one."]}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "mistral-small-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "data": [{"object": "embedding", "embedding": [0.1, 0.2, 0.3], "index": 0}, {"object": "embedding", "embedding": [0.4, 0.5, 0.6], "index": 1}]}
        "422":
          application/json: {}
  moderations_v1_moderations_post:
    speakeasy-default-moderations-v1-moderations-post:
      requestBody:
        application/json: {"model": "V90", "input": ["<value>"]}
      responses:
        "200":
          application/json: {"id": "mod-e5cc70bb28c444948073e77776eb30ef", "model": "V90", "results": [{}]}
        "422":
          application/json: {}
  moderations_chat_v1_chat_moderations_post:
    speakeasy-default-moderations-chat-v1-chat-moderations-post:
      requestBody:
        application/json: {"model": "Roadster", "input": [[{"content": "<value>", "role": "tool"}, {"content": "<value>", "role": "tool"}, {"content": "<value>", "role": "tool"}], [{"prefix": false, "role": "assistant"}, {"content": "<value>", "role": "user"}, {"prefix": false, "role": "assistant"}]], "truncate_for_context_length": false}
      responses:
        "200":
          application/json: {"id": "mod-e5cc70bb28c444948073e77776eb30ef"}
        "422":
          application/json: {}
  ocr_v1_ocr_post:
    speakeasy-default-ocr-v1-ocr-post:
      requestBody:
        application/json: {"model": "Focus", "document": {"document_url": "https://dutiful-horst.org", "type": "document_url"}}
      responses:
        "200":
          application/json: {"pages": [], "model": "A4", "usage_info": {"pages_processed": 442675}}
        "422":
          application/json: {}
  chat_moderations_v1_chat_moderations_post:
    speakeasy-default-chat-moderations-v1-chat-moderations-post:
      requestBody:
        application/json: {"input": [[{"content": [], "role": "system"}, {"content": "<value>", "role": "tool"}], [{"prefix": false, "role": "assistant"}, {"content": "<value>", "role": "user"}, {"prefix": false, "role": "assistant"}], [{"content": "<value>", "role": "system"}, {"content": [{"image_url": "https://fatherly-colon.name", "type": "image_url"}], "role": "user"}, {"content": "<value>", "role": "user"}]], "model": "Model Y"}
      responses:
        "200":
          application/json: {"id": "mod-e5cc70bb28c444948073e77776eb30ef", "model": "Model Y", "results": [{}, {}]}
        "422":
          application/json: {}
  classifications_v1_classifications_post:
    speakeasy-default-classifications-v1-classifications-post:
      requestBody:
        application/json: {"model": "Altima", "input": "<value>"}
      responses:
        "200":
          application/json: {"id": "mod-e5cc70bb28c444948073e77776eb30ef", "model": "Camaro", "results": [{"key": {"scores": {"key": 6063.42, "key1": 1739.44}}, "key1": {"scores": {}}}, {"key": {"scores": {"key": 2625.67}}, "key1": {"scores": {}}}]}
        "422":
          application/json: {}
  chat_classifications_v1_chat_classifications_post:
    speakeasy-default-chat-classifications-v1-chat-classifications-post:
      requestBody:
        application/json: {"model": "Fortwo", "input": [{"messages": [{"content": "<value>", "role": "tool"}]}, {"messages": []}]}
      responses:
        "200":
          application/json: {"id": "mod-e5cc70bb28c444948073e77776eb30ef", "model": "CX-9", "results": [{"key": {"scores": {"key": 4386.53, "key1": 2974.85}}, "key1": {"scores": {"key": 7100.52, "key1": 480.47}}}]}
        "422":
          application/json: {}
  agents_api_v1_conversations_start:
    speakeasy-default-agents-api-v1-conversations-start:
      requestBody:
        application/json: {"inputs": "<value>", "stream": false}
      responses:
        "200":
          application/json: {"object": "conversation.response", "conversation_id": "<id>", "outputs": [{"object": "entry", "type": "agent.handoff", "previous_agent_id": "<id>", "previous_agent_name": "<value>", "next_agent_id": "<id>", "next_agent_name": "<value>"}, {"object": "entry", "type": "message.output", "role": "assistant", "content": [{"type": "tool_reference", "tool": "web_search_premium", "title": "<value>"}, {"document_url": "https://unrealistic-fund.org/", "type": "document_url"}]}], "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        "422":
          application/json: {}
  agents_api_v1_conversations_list:
    speakeasy-default-agents-api-v1-conversations-list:
      parameters:
        query:
          page: 0
          page_size: 100
      responses:
        "200":
          application/json: [{"object": "conversation", "id": "<id>", "created_at": "2025-01-13T10:26:00.433Z", "updated_at": "2023-07-14T18:23:27.528Z", "agent_id": "<id>"}, {"object": "conversation", "id": "<id>", "created_at": "2023-06-17T12:14:27.999Z", "updated_at": "2024-11-27T13:02:27.296Z", "model": "LeBaron"}, {"object": "conversation", "id": "<id>", "created_at": "2025-02-26T06:14:46.641Z", "updated_at": "2023-04-05T09:49:38.010Z", "model": "A8"}]
        "422":
          application/json: {}
  agents_api_v1_conversations_get:
    speakeasy-default-agents-api-v1-conversations-get:
      parameters:
        path:
          conversation_id: "<id>"
      responses:
        "200":
          application/json: {"object": "conversation", "id": "<id>", "created_at": "2024-09-04T11:33:52.011Z", "updated_at": "2024-08-19T11:11:04.610Z", "agent_id": "<id>"}
        "422":
          application/json: {}
  agents_api_v1_conversations_append:
    speakeasy-default-agents-api-v1-conversations-append:
      parameters:
        path:
          conversation_id: "<id>"
      requestBody:
        application/json: {"inputs": "<value>", "stream": false, "store": true, "handoff_execution": "server"}
      responses:
        "200":
          application/json: {"object": "conversation.response", "conversation_id": "<id>", "outputs": [{"object": "entry", "type": "agent.handoff", "previous_agent_id": "<id>", "previous_agent_name": "<value>", "next_agent_id": "<id>", "next_agent_name": "<value>"}, {"object": "entry", "type": "function.call", "tool_call_id": "<id>", "name": "<value>", "arguments": "<value>"}], "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        "422":
          application/json: {}
  agents_api_v1_conversations_history:
    speakeasy-default-agents-api-v1-conversations-history:
      parameters:
        path:
          conversation_id: "<id>"
      responses:
        "200":
          application/json: {"object": "conversation.history", "conversation_id": "<id>", "entries": [{"object": "entry", "type": "message.output", "role": "assistant", "content": [{"type": "tool_file", "tool": "web_search", "file_id": "<id>"}]}]}
        "422":
          application/json: {}
  agents_api_v1_conversations_messages:
    speakeasy-default-agents-api-v1-conversations-messages:
      parameters:
        path:
          conversation_id: "<id>"
      responses:
        "200":
          application/json: {"object": "conversation.messages", "conversation_id": "<id>", "messages": [{"object": "entry", "type": "message.input", "role": "assistant", "content": "<value>"}, {"object": "entry", "type": "message.input", "role": "assistant", "content": [{"document_url": "https://black-and-white-sauerkraut.biz", "type": "document_url"}, {"type": "tool_file", "tool": "code_interpreter", "file_id": "<id>"}, {"image_url": "https://emotional-couch.org", "type": "image_url"}]}, {"object": "entry", "type": "message.input", "role": "assistant", "content": "<value>"}]}
        "422":
          application/json: {}
  agents_api_v1_conversations_restart:
    speakeasy-default-agents-api-v1-conversations-restart:
      parameters:
        path:
          conversation_id: "<id>"
      requestBody:
        application/json: {"inputs": "<value>", "stream": false, "store": true, "handoff_execution": "server", "from_entry_id": "<id>"}
      responses:
        "200":
          application/json: {"object": "conversation.response", "conversation_id": "<id>", "outputs": [{"object": "entry", "type": "tool.execution", "name": "image_generation"}, {"object": "entry", "type": "tool.execution", "name": "web_search_premium"}], "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}}
        "422":
          application/json: {}
  agents_api_v1_conversations_start_stream:
    speakeasy-default-agents-api-v1-conversations-start-stream:
      requestBody:
        application/json: {"inputs": "<value>", "stream": true}
      responses:
        "422":
          application/json: {}
        "200": {}
  agents_api_v1_conversations_append_stream:
    speakeasy-default-agents-api-v1-conversations-append-stream:
      parameters:
        path:
          conversation_id: "<id>"
      requestBody:
        application/json: {"inputs": "<value>", "stream": true, "store": true, "handoff_execution": "server"}
      responses:
        "422":
          application/json: {}
        "200": {}
  agents_api_v1_conversations_restart_stream:
    speakeasy-default-agents-api-v1-conversations-restart-stream:
      parameters:
        path:
          conversation_id: "<id>"
      requestBody:
        application/json: {"inputs": "<value>", "stream": true, "store": true, "handoff_execution": "server", "from_entry_id": "<id>"}
      responses:
        "422":
          application/json: {}
        "200": {}
  agents_api_v1_agents_create:
    speakeasy-default-agents-api-v1-agents-create:
      requestBody:
        application/json: {"model": "Fiesta", "name": "<value>"}
      responses:
        "200":
          application/json: {"model": "LeBaron", "name": "<value>", "object": "agent", "id": "<id>", "version": 417458, "created_at": "2023-05-28T06:20:22.766Z", "updated_at": "2023-03-17T15:39:20.911Z"}
        "422":
          application/json: {}
  agents_api_v1_agents_list:
    speakeasy-default-agents-api-v1-agents-list:
      parameters:
        query:
          page: 0
          page_size: 20
      responses:
        "200":
          application/json: [{"model": "Golf", "name": "<value>", "object": "agent", "id": "<id>", "version": 678317, "created_at": "2023-07-14T18:23:27.528Z", "updated_at": "2023-09-09T18:28:08.953Z"}, {"model": "Aventador", "name": "<value>", "object": "agent", "id": "<id>", "version": 635532, "created_at": "2024-12-01T18:25:37.169Z", "updated_at": "2023-01-20T06:21:22.156Z"}, {"model": "Model T", "name": "<value>", "object": "agent", "id": "<id>", "version": 86140, "created_at": "2023-03-17T01:57:00.187Z", "updated_at": "2025-01-24T00:05:25.844Z"}]
        "422":
          application/json: {}
  agents_api_v1_agents_get:
    speakeasy-default-agents-api-v1-agents-get:
      parameters:
        path:
          agent_id: "<id>"
      responses:
        "200":
          application/json: {"model": "Model S", "name": "<value>", "object": "agent", "id": "<id>", "version": 558834, "created_at": "2024-08-19T11:11:04.610Z", "updated_at": "2024-07-25T06:33:15.810Z"}
        "422":
          application/json: {}
  agents_api_v1_agents_update:
    speakeasy-default-agents-api-v1-agents-update:
      parameters:
        path:
          agent_id: "<id>"
      requestBody:
        application/json: {}
      responses:
        "200":
          application/json: {"model": "Sentra", "name": "<value>", "object": "agent", "id": "<id>", "version": 597129, "created_at": "2024-01-13T16:52:57.274Z", "updated_at": "2025-12-22T15:27:45.882Z"}
        "422":
          application/json: {}
  agents_api_v1_agents_update_version:
    speakeasy-default-agents-api-v1-agents-update-version:
      parameters:
        path:
          agent_id: "<id>"
        query:
          version: 193920
      responses:
        "200":
          application/json: {"model": "Mercielago", "name": "<value>", "object": "agent", "id": "<id>", "version": 253661, "created_at": "2023-02-14T22:44:06.703Z", "updated_at": "2025-12-15T06:22:04.120Z"}
        "422":
          application/json: {}
examplesVersion: 1.0.0
generatedTests: {}
</file>

<file path=".speakeasy/gen.yaml">
configVersion: 2.0.0
generation:
  sdkClassName: Mistral
  maintainOpenAPIOrder: true
  usageSnippets:
    optionalPropertyRendering: withExample
  useClassNamesForArrayFields: true
  fixes:
    nameResolutionDec2023: true
    nameResolutionFeb2025: false
    parameterOrderingFeb2024: true
    requestResponseComponentNamesFeb2024: true
    securityFeb2025: false
  auth:
    oAuth2ClientCredentialsEnabled: true
    oAuth2PasswordEnabled: false
python:
  version: 1.8.2
  additionalDependencies:
    dev:
      pytest: ^8.2.2
      pytest-asyncio: ^0.23.7
  authors:
    - Mistral
  clientServerStatusCodesAsErrors: true
  defaultErrorName: SDKError
  description: Python Client SDK for the Mistral AI API.
  enableCustomCodeRegions: true
  enumFormat: union
  envVarPrefix: MISTRAL
  fixFlags:
    responseRequiredSep2024: true
  flattenGlobalSecurity: true
  flattenRequests: true
  flatteningOrder: parameters-first
  imports:
    option: openapi
    paths:
      callbacks: ""
      errors: ""
      operations: ""
      shared: ""
      webhooks: ""
  inputModelSuffix: input
  maxMethodParams: 15
  methodArguments: infer-optional-args
  outputModelSuffix: output
  packageName: mistralai
  pytestTimeout: 0
  responseFormat: flat
  templateVersion: v2
</file>

<file path=".speakeasy/workflow.lock">
speakeasyVersion: 1.517.3
sources:
    mistral-azure-source:
        sourceNamespace: mistral-openapi-azure
        sourceRevisionDigest: sha256:c5931a7e0cc2db844149d71db57dfc2178665f0400bc26c90ee113795ea2872f
        sourceBlobDigest: sha256:504fff788fdac8d781e33d85e3a04d35f6d9f7a3ef5ed40da8b4567074e94f03
        tags:
            - latest
    mistral-google-cloud-source:
        sourceNamespace: mistral-openapi-google-cloud
        sourceRevisionDigest: sha256:4a5343e63c6a78152e472b00ccc46d7bcb15594496bc94c8040039d3a9d4c5f8
        sourceBlobDigest: sha256:3327f078a11596abdcbc21cd8a1adcf0b2aa474975cd9ab1feb745a2e50d555f
        tags:
            - latest
    mistral-openapi:
        sourceNamespace: mistral-openapi
        sourceRevisionDigest: sha256:21244d618cafcc163c3aa4acbc443ca16c63b8614632b65b87fbb2c4066987f3
        sourceBlobDigest: sha256:74aeb6a2e0d466c206f983ce79581cc72d205cc7866826282c181207ebe841a2
        tags:
            - latest
            - speakeasy-sdk-regen-1749573609
targets:
    mistralai-azure-sdk:
        source: mistral-azure-source
        sourceNamespace: mistral-openapi-azure
        sourceRevisionDigest: sha256:c5931a7e0cc2db844149d71db57dfc2178665f0400bc26c90ee113795ea2872f
        sourceBlobDigest: sha256:504fff788fdac8d781e33d85e3a04d35f6d9f7a3ef5ed40da8b4567074e94f03
        codeSamplesNamespace: mistral-openapi-azure-code-samples
        codeSamplesRevisionDigest: sha256:e242a7fc42e44d2bbc8e5637d4a6455da7fb3d0307dc275ee4c64867f5c4be55
    mistralai-gcp-sdk:
        source: mistral-google-cloud-source
        sourceNamespace: mistral-openapi-google-cloud
        sourceRevisionDigest: sha256:4a5343e63c6a78152e472b00ccc46d7bcb15594496bc94c8040039d3a9d4c5f8
        sourceBlobDigest: sha256:3327f078a11596abdcbc21cd8a1adcf0b2aa474975cd9ab1feb745a2e50d555f
        codeSamplesNamespace: mistral-openapi-google-cloud-code-samples
        codeSamplesRevisionDigest: sha256:03b3e82c20d10faa8622f14696632b96b1a2e8d747b266fff345061298d5f3e4
    mistralai-sdk:
        source: mistral-openapi
        sourceNamespace: mistral-openapi
        sourceRevisionDigest: sha256:21244d618cafcc163c3aa4acbc443ca16c63b8614632b65b87fbb2c4066987f3
        sourceBlobDigest: sha256:74aeb6a2e0d466c206f983ce79581cc72d205cc7866826282c181207ebe841a2
        codeSamplesNamespace: mistral-openapi-code-samples
        codeSamplesRevisionDigest: sha256:dc4396ba994048a9f31c008dced1a46a9e54d89973e9608039a7bc37b1052957
workflow:
    workflowVersion: 1.0.0
    speakeasyVersion: 1.517.3
    sources:
        mistral-azure-source:
            inputs:
                - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-azure:main
        mistral-google-cloud-source:
            inputs:
                - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-google-cloud:main
        mistral-openapi:
            inputs:
                - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi:main
    targets:
        mistralai-azure-sdk:
            target: python
            source: mistral-azure-source
            output: ./packages/mistralai_azure
            publish:
                pypi:
                    token: $pypi_token
            codeSamples:
                registry:
                    location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-azure-code-samples
                blocking: false
        mistralai-gcp-sdk:
            target: python
            source: mistral-google-cloud-source
            output: ./packages/mistralai_gcp
            publish:
                pypi:
                    token: $pypi_token
            codeSamples:
                registry:
                    location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-google-cloud-code-samples
                blocking: false
        mistralai-sdk:
            target: python
            source: mistral-openapi
            publish:
                pypi:
                    token: $pypi_token
            codeSamples:
                registry:
                    location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-code-samples
                blocking: false
</file>

<file path=".speakeasy/workflow.yaml">
workflowVersion: 1.0.0
speakeasyVersion:  1.517.3
sources:
    mistral-azure-source:
        inputs:
            - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-azure:main
    mistral-google-cloud-source:
        inputs:
            - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-google-cloud:main
    mistral-openapi:
        inputs:
            - location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi:main
targets:
    mistralai-azure-sdk:
        target: python
        source: mistral-azure-source
        output: ./packages/mistralai_azure
        publish:
            pypi:
                token: $pypi_token
        codeSamples:
            registry:
                location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-azure-code-samples
            blocking: false
    mistralai-gcp-sdk:
        target: python
        source: mistral-google-cloud-source
        output: ./packages/mistralai_gcp
        publish:
            pypi:
                token: $pypi_token
        codeSamples:
            registry:
                location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-google-cloud-code-samples
            blocking: false
    mistralai-sdk:
        target: python
        source: mistral-openapi
        publish:
            pypi:
                token: $pypi_token
        codeSamples:
            registry:
                location: registry.speakeasyapi.dev/mistral-dev/mistral-dev/mistral-openapi-code-samples
            blocking: false
</file>

<file path="docs/models/utils/retryconfig.md">
# RetryConfig

Allows customizing the default retry configuration. Only usable with methods that mention they support retries.

## Fields

| Name                      | Type                                | Description                             | Example   |
| ------------------------- | ----------------------------------- | --------------------------------------- | --------- |
| `strategy`                | `*str*`                             | The retry strategy to use.              | `backoff` |
| `backoff`                 | [BackoffStrategy](#backoffstrategy) | Configuration for the backoff strategy. |           |
| `retry_connection_errors` | `*bool*`                            | Whether to retry on connection errors.  | `true`    |

## BackoffStrategy

The backoff strategy allows retrying a request with an exponential backoff between each retry.

### Fields

| Name               | Type      | Description                               | Example  |
| ------------------ | --------- | ----------------------------------------- | -------- |
| `initial_interval` | `*int*`   | The initial interval in milliseconds.     | `500`    |
| `max_interval`     | `*int*`   | The maximum interval in milliseconds.     | `60000`  |
| `exponent`         | `*float*` | The exponent to use for the backoff.      | `1.5`    |
| `max_elapsed_time` | `*int*`   | The maximum elapsed time in milliseconds. | `300000` |
</file>

<file path="docs/models/agent.md">
# Agent


## Fields

| Field                                                                   | Type                                                                    | Required                                                                | Description                                                             |
| ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| `model`                                                                 | *str*                                                                   | :heavy_check_mark:                                                      | N/A                                                                     |
| `name`                                                                  | *str*                                                                   | :heavy_check_mark:                                                      | N/A                                                                     |
| `id`                                                                    | *str*                                                                   | :heavy_check_mark:                                                      | N/A                                                                     |
| `version`                                                               | *int*                                                                   | :heavy_check_mark:                                                      | N/A                                                                     |
| `created_at`                                                            | [date](https://docs.python.org/3/library/datetime.html#date-objects)    | :heavy_check_mark:                                                      | N/A                                                                     |
| `updated_at`                                                            | [date](https://docs.python.org/3/library/datetime.html#date-objects)    | :heavy_check_mark:                                                      | N/A                                                                     |
| `instructions`                                                          | *OptionalNullable[str]*                                                 | :heavy_minus_sign:                                                      | Instruction prompt the model will follow during the conversation.       |
| `tools`                                                                 | List[[models.AgentTools](../models/agenttools.md)]                      | :heavy_minus_sign:                                                      | List of tools which are available to the model during the conversation. |
| `completion_args`                                                       | [Optional[models.CompletionArgs]](../models/completionargs.md)          | :heavy_minus_sign:                                                      | White-listed arguments from the completion API                          |
| `description`                                                           | *OptionalNullable[str]*                                                 | :heavy_minus_sign:                                                      | N/A                                                                     |
| `handoffs`                                                              | List[*str*]                                                             | :heavy_minus_sign:                                                      | N/A                                                                     |
| `object`                                                                | [Optional[models.AgentObject]](../models/agentobject.md)                | :heavy_minus_sign:                                                      | N/A                                                                     |
</file>

<file path="docs/models/agentconversation.md">
# AgentConversation


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `id`                                                                             | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `created_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_check_mark:                                                               | N/A                                                                              |
| `updated_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_check_mark:                                                               | N/A                                                                              |
| `agent_id`                                                                       | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `name`                                                                           | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Name given to the conversation.                                                  |
| `description`                                                                    | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Description of the what the conversation is about.                               |
| `object`                                                                         | [Optional[models.AgentConversationObject]](../models/agentconversationobject.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/agentconversationobject.md">
# AgentConversationObject


## Values

| Name           | Value          |
| -------------- | -------------- |
| `CONVERSATION` | conversation   |
</file>

<file path="docs/models/agentcreationrequest.md">
# AgentCreationRequest


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `model`                                                                          | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `name`                                                                           | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `instructions`                                                                   | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Instruction prompt the model will follow during the conversation.                |
| `tools`                                                                          | List[[models.AgentCreationRequestTools](../models/agentcreationrequesttools.md)] | :heavy_minus_sign:                                                               | List of tools which are available to the model during the conversation.          |
| `completion_args`                                                                | [Optional[models.CompletionArgs]](../models/completionargs.md)                   | :heavy_minus_sign:                                                               | White-listed arguments from the completion API                                   |
| `description`                                                                    | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | N/A                                                                              |
| `handoffs`                                                                       | List[*str*]                                                                      | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/agentcreationrequesttools.md">
# AgentCreationRequestTools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/agenthandoffdoneevent.md">
# AgentHandoffDoneEvent


## Fields

| Field                                                                                | Type                                                                                 | Required                                                                             | Description                                                                          |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ |
| `id`                                                                                 | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `next_agent_id`                                                                      | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `next_agent_name`                                                                    | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `type`                                                                               | [Optional[models.AgentHandoffDoneEventType]](../models/agenthandoffdoneeventtype.md) | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `created_at`                                                                         | [date](https://docs.python.org/3/library/datetime.html#date-objects)                 | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `output_index`                                                                       | *Optional[int]*                                                                      | :heavy_minus_sign:                                                                   | N/A                                                                                  |
</file>

<file path="docs/models/agenthandoffdoneeventtype.md">
# AgentHandoffDoneEventType


## Values

| Name                 | Value                |
| -------------------- | -------------------- |
| `AGENT_HANDOFF_DONE` | agent.handoff.done   |
</file>

<file path="docs/models/agenthandoffentry.md">
# AgentHandoffEntry


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `previous_agent_id`                                                              | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `previous_agent_name`                                                            | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `next_agent_id`                                                                  | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `next_agent_name`                                                                | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `object`                                                                         | [Optional[models.AgentHandoffEntryObject]](../models/agenthandoffentryobject.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
| `type`                                                                           | [Optional[models.AgentHandoffEntryType]](../models/agenthandoffentrytype.md)     | :heavy_minus_sign:                                                               | N/A                                                                              |
| `created_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_minus_sign:                                                               | N/A                                                                              |
| `completed_at`                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_minus_sign:                                                               | N/A                                                                              |
| `id`                                                                             | *Optional[str]*                                                                  | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/agenthandoffentryobject.md">
# AgentHandoffEntryObject


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/agenthandoffentrytype.md">
# AgentHandoffEntryType


## Values

| Name            | Value           |
| --------------- | --------------- |
| `AGENT_HANDOFF` | agent.handoff   |
</file>

<file path="docs/models/agenthandoffstartedevent.md">
# AgentHandoffStartedEvent


## Fields

| Field                                                                                      | Type                                                                                       | Required                                                                                   | Description                                                                                |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| `id`                                                                                       | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `previous_agent_id`                                                                        | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `previous_agent_name`                                                                      | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `type`                                                                                     | [Optional[models.AgentHandoffStartedEventType]](../models/agenthandoffstartedeventtype.md) | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `created_at`                                                                               | [date](https://docs.python.org/3/library/datetime.html#date-objects)                       | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `output_index`                                                                             | *Optional[int]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
</file>

<file path="docs/models/agenthandoffstartedeventtype.md">
# AgentHandoffStartedEventType


## Values

| Name                    | Value                   |
| ----------------------- | ----------------------- |
| `AGENT_HANDOFF_STARTED` | agent.handoff.started   |
</file>

<file path="docs/models/agentobject.md">
# AgentObject


## Values

| Name    | Value   |
| ------- | ------- |
| `AGENT` | agent   |
</file>

<file path="docs/models/agentsapiv1agentsgetrequest.md">
# AgentsAPIV1AgentsGetRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `agent_id`         | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/agentsapiv1agentslistrequest.md">
# AgentsAPIV1AgentsListRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `page`             | *Optional[int]*    | :heavy_minus_sign: | N/A                |
| `page_size`        | *Optional[int]*    | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/agentsapiv1agentsupdaterequest.md">
# AgentsAPIV1AgentsUpdateRequest


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `agent_id`                                                   | *str*                                                        | :heavy_check_mark:                                           | N/A                                                          |
| `agent_update_request`                                       | [models.AgentUpdateRequest](../models/agentupdaterequest.md) | :heavy_check_mark:                                           | N/A                                                          |
</file>

<file path="docs/models/agentsapiv1agentsupdateversionrequest.md">
# AgentsAPIV1AgentsUpdateVersionRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `agent_id`         | *str*              | :heavy_check_mark: | N/A                |
| `version`          | *int*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/agentsapiv1conversationsappendrequest.md">
# AgentsAPIV1ConversationsAppendRequest


## Fields

| Field                                                                      | Type                                                                       | Required                                                                   | Description                                                                |
| -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| `conversation_id`                                                          | *str*                                                                      | :heavy_check_mark:                                                         | ID of the conversation to which we append entries.                         |
| `conversation_append_request`                                              | [models.ConversationAppendRequest](../models/conversationappendrequest.md) | :heavy_check_mark:                                                         | N/A                                                                        |
</file>

<file path="docs/models/agentsapiv1conversationsappendstreamrequest.md">
# AgentsAPIV1ConversationsAppendStreamRequest


## Fields

| Field                                                                                  | Type                                                                                   | Required                                                                               | Description                                                                            |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `conversation_id`                                                                      | *str*                                                                                  | :heavy_check_mark:                                                                     | ID of the conversation to which we append entries.                                     |
| `conversation_append_stream_request`                                                   | [models.ConversationAppendStreamRequest](../models/conversationappendstreamrequest.md) | :heavy_check_mark:                                                                     | N/A                                                                                    |
</file>

<file path="docs/models/agentsapiv1conversationsgetrequest.md">
# AgentsAPIV1ConversationsGetRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `conversation_id`  | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/agentsapiv1conversationsgetresponsev1conversationsget.md">
# AgentsAPIV1ConversationsGetResponseV1ConversationsGet

Successful Response


## Supported Types

### `models.ModelConversation`

```python
value: models.ModelConversation = /* values here */
```

### `models.AgentConversation`

```python
value: models.AgentConversation = /* values here */
```
</file>

<file path="docs/models/agentsapiv1conversationshistoryrequest.md">
# AgentsAPIV1ConversationsHistoryRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `conversation_id`  | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/agentsapiv1conversationslistrequest.md">
# AgentsAPIV1ConversationsListRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `page`             | *Optional[int]*    | :heavy_minus_sign: | N/A                |
| `page_size`        | *Optional[int]*    | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/agentsapiv1conversationsmessagesrequest.md">
# AgentsAPIV1ConversationsMessagesRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `conversation_id`  | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/agentsapiv1conversationsrestartrequest.md">
# AgentsAPIV1ConversationsRestartRequest


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `conversation_id`                                                            | *str*                                                                        | :heavy_check_mark:                                                           | N/A                                                                          |
| `conversation_restart_request`                                               | [models.ConversationRestartRequest](../models/conversationrestartrequest.md) | :heavy_check_mark:                                                           | N/A                                                                          |
</file>

<file path="docs/models/agentsapiv1conversationsrestartstreamrequest.md">
# AgentsAPIV1ConversationsRestartStreamRequest


## Fields

| Field                                                                                    | Type                                                                                     | Required                                                                                 | Description                                                                              |
| ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| `conversation_id`                                                                        | *str*                                                                                    | :heavy_check_mark:                                                                       | N/A                                                                                      |
| `conversation_restart_stream_request`                                                    | [models.ConversationRestartStreamRequest](../models/conversationrestartstreamrequest.md) | :heavy_check_mark:                                                                       | N/A                                                                                      |
</file>

<file path="docs/models/agentscompletionrequest.md">
# AgentsCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                              | Type                                                                                                                                                                                                                                                                                                                               | Required                                                                                                                                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                                                        | Example                                                                                                                                                                                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                                                                         | List[[models.AgentsCompletionRequestMessages](../models/agentscompletionrequestmessages.md)]                                                                                                                                                                                                                                       | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                 | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                        | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                             |
| `agent_id`                                                                                                                                                                                                                                                                                                                         | *str*                                                                                                                                                                                                                                                                                                                              | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                 | The ID of the agent to use for this completion.                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                    |
| `max_tokens`                                                                                                                                                                                                                                                                                                                       | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                    |
| `stream`                                                                                                                                                                                                                                                                                                                           | *Optional[bool]*                                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON. |                                                                                                                                                                                                                                                                                                                                    |
| `stop`                                                                                                                                                                                                                                                                                                                             | [Optional[models.AgentsCompletionRequestStop]](../models/agentscompletionrequeststop.md)                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                    |
| `random_seed`                                                                                                                                                                                                                                                                                                                      | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                    |
| `response_format`                                                                                                                                                                                                                                                                                                                  | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `tools`                                                                                                                                                                                                                                                                                                                            | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `tool_choice`                                                                                                                                                                                                                                                                                                                      | [Optional[models.AgentsCompletionRequestToolChoice]](../models/agentscompletionrequesttoolchoice.md)                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                 | *Optional[float]*                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                              |                                                                                                                                                                                                                                                                                                                                    |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                | *Optional[float]*                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                        |                                                                                                                                                                                                                                                                                                                                    |
| `n`                                                                                                                                                                                                                                                                                                                                | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                    |
| `prediction`                                                                                                                                                                                                                                                                                                                       | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                      | [OptionalNullable[models.MistralPromptMode]](../models/mistralpromptmode.md)                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
</file>

<file path="docs/models/agentscompletionrequestmessages.md">
# AgentsCompletionRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/agentscompletionrequeststop.md">
# AgentsCompletionRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/agentscompletionrequesttoolchoice.md">
# AgentsCompletionRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="docs/models/agentscompletionstreamrequest.md">
# AgentsCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                       | Type                                                                                                                                                                                                                                                                        | Required                                                                                                                                                                                                                                                                    | Description                                                                                                                                                                                                                                                                 | Example                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                  | List[[models.AgentsCompletionStreamRequestMessages](../models/agentscompletionstreamrequestmessages.md)]                                                                                                                                                                    | :heavy_check_mark:                                                                                                                                                                                                                                                          | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                 | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                      |
| `agent_id`                                                                                                                                                                                                                                                                  | *str*                                                                                                                                                                                                                                                                       | :heavy_check_mark:                                                                                                                                                                                                                                                          | The ID of the agent to use for this completion.                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                             |
| `max_tokens`                                                                                                                                                                                                                                                                | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                      |                                                                                                                                                                                                                                                                             |
| `stream`                                                                                                                                                                                                                                                                    | *Optional[bool]*                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `stop`                                                                                                                                                                                                                                                                      | [Optional[models.AgentsCompletionStreamRequestStop]](../models/agentscompletionstreamrequeststop.md)                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                          | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                    |                                                                                                                                                                                                                                                                             |
| `random_seed`                                                                                                                                                                                                                                                               | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                           |                                                                                                                                                                                                                                                                             |
| `response_format`                                                                                                                                                                                                                                                           | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                              | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `tools`                                                                                                                                                                                                                                                                     | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `tool_choice`                                                                                                                                                                                                                                                               | [Optional[models.AgentsCompletionStreamRequestToolChoice]](../models/agentscompletionstreamrequesttoolchoice.md)                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `presence_penalty`                                                                                                                                                                                                                                                          | *Optional[float]*                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                          | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                       |                                                                                                                                                                                                                                                                             |
| `frequency_penalty`                                                                                                                                                                                                                                                         | *Optional[float]*                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                          | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition. |                                                                                                                                                                                                                                                                             |
| `n`                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                        |                                                                                                                                                                                                                                                                             |
| `prediction`                                                                                                                                                                                                                                                                | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `parallel_tool_calls`                                                                                                                                                                                                                                                       | *Optional[bool]*                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `prompt_mode`                                                                                                                                                                                                                                                               | [OptionalNullable[models.MistralPromptMode]](../models/mistralpromptmode.md)                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
</file>

<file path="docs/models/agentscompletionstreamrequestmessages.md">
# AgentsCompletionStreamRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/agentscompletionstreamrequeststop.md">
# AgentsCompletionStreamRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/agentscompletionstreamrequesttoolchoice.md">
# AgentsCompletionStreamRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="docs/models/agenttools.md">
# AgentTools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/agentupdaterequest.md">
# AgentUpdateRequest


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `instructions`                                                               | *OptionalNullable[str]*                                                      | :heavy_minus_sign:                                                           | Instruction prompt the model will follow during the conversation.            |
| `tools`                                                                      | List[[models.AgentUpdateRequestTools](../models/agentupdaterequesttools.md)] | :heavy_minus_sign:                                                           | List of tools which are available to the model during the conversation.      |
| `completion_args`                                                            | [Optional[models.CompletionArgs]](../models/completionargs.md)               | :heavy_minus_sign:                                                           | White-listed arguments from the completion API                               |
| `model`                                                                      | *OptionalNullable[str]*                                                      | :heavy_minus_sign:                                                           | N/A                                                                          |
| `name`                                                                       | *OptionalNullable[str]*                                                      | :heavy_minus_sign:                                                           | N/A                                                                          |
| `description`                                                                | *OptionalNullable[str]*                                                      | :heavy_minus_sign:                                                           | N/A                                                                          |
| `handoffs`                                                                   | List[*str*]                                                                  | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="docs/models/agentupdaterequesttools.md">
# AgentUpdateRequestTools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/apiendpoint.md">
# APIEndpoint


## Values

| Name                       | Value                      |
| -------------------------- | -------------------------- |
| `ROOT_V1_CHAT_COMPLETIONS` | /v1/chat/completions       |
| `ROOT_V1_EMBEDDINGS`       | /v1/embeddings             |
| `ROOT_V1_FIM_COMPLETIONS`  | /v1/fim/completions        |
| `ROOT_V1_MODERATIONS`      | /v1/moderations            |
| `ROOT_V1_CHAT_MODERATIONS` | /v1/chat/moderations       |
</file>

<file path="docs/models/archiveftmodelout.md">
# ArchiveFTModelOut


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `id`                                                                             | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `object`                                                                         | [Optional[models.ArchiveFTModelOutObject]](../models/archiveftmodeloutobject.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
| `archived`                                                                       | *Optional[bool]*                                                                 | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/archiveftmodeloutobject.md">
# ArchiveFTModelOutObject


## Values

| Name    | Value   |
| ------- | ------- |
| `MODEL` | model   |
</file>

<file path="docs/models/arguments.md">
# Arguments


## Supported Types

### `Dict[str, Any]`

```python
value: Dict[str, Any] = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="docs/models/assistantmessage.md">
# AssistantMessage


## Fields

| Field                                                                                                                                                                                                  | Type                                                                                                                                                                                                   | Required                                                                                                                                                                                               | Description                                                                                                                                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `content`                                                                                                                                                                                              | [OptionalNullable[models.AssistantMessageContent]](../models/assistantmessagecontent.md)                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `tool_calls`                                                                                                                                                                                           | List[[models.ToolCall](../models/toolcall.md)]                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `prefix`                                                                                                                                                                                               | *Optional[bool]*                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                     | Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message. |
| `role`                                                                                                                                                                                                 | [Optional[models.AssistantMessageRole]](../models/assistantmessagerole.md)                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
</file>

<file path="docs/models/assistantmessagecontent.md">
# AssistantMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="docs/models/assistantmessagerole.md">
# AssistantMessageRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
</file>

<file path="docs/models/basemodelcard.md">
# BaseModelCard


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `id`                                                                 | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `capabilities`                                                       | [models.ModelCapabilities](../models/modelcapabilities.md)           | :heavy_check_mark:                                                   | N/A                                                                  |
| `object`                                                             | *Optional[str]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created`                                                            | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `owned_by`                                                           | *Optional[str]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `name`                                                               | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `description`                                                        | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `max_context_length`                                                 | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `aliases`                                                            | List[*str*]                                                          | :heavy_minus_sign:                                                   | N/A                                                                  |
| `deprecation`                                                        | [date](https://docs.python.org/3/library/datetime.html#date-objects) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `default_model_temperature`                                          | *OptionalNullable[float]*                                            | :heavy_minus_sign:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.Type]](../models/type.md)                           | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/batcherror.md">
# BatchError


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `message`          | *str*              | :heavy_check_mark: | N/A                |
| `count`            | *Optional[int]*    | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/batchjobin.md">
# BatchJobIn


## Fields

| Field                                          | Type                                           | Required                                       | Description                                    |
| ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| `input_files`                                  | List[*str*]                                    | :heavy_check_mark:                             | N/A                                            |
| `endpoint`                                     | [models.APIEndpoint](../models/apiendpoint.md) | :heavy_check_mark:                             | N/A                                            |
| `model`                                        | *str*                                          | :heavy_check_mark:                             | N/A                                            |
| `metadata`                                     | Dict[str, *str*]                               | :heavy_minus_sign:                             | N/A                                            |
| `timeout_hours`                                | *Optional[int]*                                | :heavy_minus_sign:                             | N/A                                            |
</file>

<file path="docs/models/batchjobout.md">
# BatchJobOut


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `id`                                                                 | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `input_files`                                                        | List[*str*]                                                          | :heavy_check_mark:                                                   | N/A                                                                  |
| `endpoint`                                                           | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `model`                                                              | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `errors`                                                             | List[[models.BatchError](../models/batcherror.md)]                   | :heavy_check_mark:                                                   | N/A                                                                  |
| `status`                                                             | [models.BatchJobStatus](../models/batchjobstatus.md)                 | :heavy_check_mark:                                                   | N/A                                                                  |
| `created_at`                                                         | *int*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `total_requests`                                                     | *int*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `completed_requests`                                                 | *int*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `succeeded_requests`                                                 | *int*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `failed_requests`                                                    | *int*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `object`                                                             | [Optional[models.BatchJobOutObject]](../models/batchjoboutobject.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `metadata`                                                           | Dict[str, *Any*]                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `output_file`                                                        | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `error_file`                                                         | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `started_at`                                                         | *OptionalNullable[int]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `completed_at`                                                       | *OptionalNullable[int]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/batchjoboutobject.md">
# BatchJobOutObject


## Values

| Name    | Value   |
| ------- | ------- |
| `BATCH` | batch   |
</file>

<file path="docs/models/batchjobsout.md">
# BatchJobsOut


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `total`                                                                | *int*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    |
| `data`                                                                 | List[[models.BatchJobOut](../models/batchjobout.md)]                   | :heavy_minus_sign:                                                     | N/A                                                                    |
| `object`                                                               | [Optional[models.BatchJobsOutObject]](../models/batchjobsoutobject.md) | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="docs/models/batchjobsoutobject.md">
# BatchJobsOutObject


## Values

| Name   | Value  |
| ------ | ------ |
| `LIST` | list   |
</file>

<file path="docs/models/batchjobstatus.md">
# BatchJobStatus


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `RUNNING`                | RUNNING                  |
| `SUCCESS`                | SUCCESS                  |
| `FAILED`                 | FAILED                   |
| `TIMEOUT_EXCEEDED`       | TIMEOUT_EXCEEDED         |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
| `CANCELLED`              | CANCELLED                |
</file>

<file path="docs/models/builtinconnectors.md">
# BuiltInConnectors


## Values

| Name                 | Value                |
| -------------------- | -------------------- |
| `WEB_SEARCH`         | web_search           |
| `WEB_SEARCH_PREMIUM` | web_search_premium   |
| `CODE_INTERPRETER`   | code_interpreter     |
| `IMAGE_GENERATION`   | image_generation     |
| `DOCUMENT_LIBRARY`   | document_library     |
</file>

<file path="docs/models/chatclassificationrequest.md">
# ChatClassificationRequest


## Fields

| Field                                | Type                                 | Required                             | Description                          |
| ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ |
| `model`                              | *str*                                | :heavy_check_mark:                   | N/A                                  |
| `inputs`                             | [models.Inputs](../models/inputs.md) | :heavy_check_mark:                   | Chat to classify                     |
</file>

<file path="docs/models/chatcompletionchoice.md">
# ChatCompletionChoice


## Fields

| Field                                                    | Type                                                     | Required                                                 | Description                                              | Example                                                  |
| -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- |
| `index`                                                  | *int*                                                    | :heavy_check_mark:                                       | N/A                                                      | 0                                                        |
| `message`                                                | [models.AssistantMessage](../models/assistantmessage.md) | :heavy_check_mark:                                       | N/A                                                      |                                                          |
| `finish_reason`                                          | [models.FinishReason](../models/finishreason.md)         | :heavy_check_mark:                                       | N/A                                                      | stop                                                     |
</file>

<file path="docs/models/chatcompletionrequest.md">
# ChatCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.Messages](../models/messages.md)]                                                                                                                                                                                                                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.Stop]](../models/stop.md)                                                                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionRequestToolChoice]](../models/chatcompletionrequesttoolchoice.md)                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                                                                                         | [OptionalNullable[models.MistralPromptMode]](../models/mistralpromptmode.md)                                                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="docs/models/chatcompletionrequesttoolchoice.md">
# ChatCompletionRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="docs/models/chatcompletionresponse.md">
# ChatCompletionResponse


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            | Example                                                                |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `id`                                                                   | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | cmpl-e5cc70bb28c444948073e77776eb30ef                                  |
| `object`                                                               | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | chat.completion                                                        |
| `model`                                                                | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | mistral-small-latest                                                   |
| `usage`                                                                | [models.UsageInfo](../models/usageinfo.md)                             | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
| `created`                                                              | *int*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | 1702256327                                                             |
| `choices`                                                              | List[[models.ChatCompletionChoice](../models/chatcompletionchoice.md)] | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
</file>

<file path="docs/models/chatcompletionstreamrequest.md">
# ChatCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.ChatCompletionStreamRequestMessages](../models/chatcompletionstreamrequestmessages.md)]                                                                                                                                                                                                                                                                                                  | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.ChatCompletionStreamRequestStop]](../models/chatcompletionstreamrequeststop.md)                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionStreamRequestToolChoice]](../models/chatcompletionstreamrequesttoolchoice.md)                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                                                                                         | [OptionalNullable[models.MistralPromptMode]](../models/mistralpromptmode.md)                                                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="docs/models/chatcompletionstreamrequestmessages.md">
# ChatCompletionStreamRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/chatcompletionstreamrequeststop.md">
# ChatCompletionStreamRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/chatcompletionstreamrequesttoolchoice.md">
# ChatCompletionStreamRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="docs/models/chatmoderationrequest.md">
# ChatModerationRequest


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `inputs`                                                                       | [models.ChatModerationRequestInputs](../models/chatmoderationrequestinputs.md) | :heavy_check_mark:                                                             | Chat to classify                                                               |
| `model`                                                                        | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
</file>

<file path="docs/models/chatmoderationrequestinputs.md">
# ChatModerationRequestInputs

Chat to classify


## Supported Types

### `List[models.One]`

```python
value: List[models.One] = /* values here */
```

### `List[List[models.Two]]`

```python
value: List[List[models.Two]] = /* values here */
```
</file>

<file path="docs/models/checkpointout.md">
# CheckpointOut


## Fields

| Field                                                                                                                                                                        | Type                                                                                                                                                                         | Required                                                                                                                                                                     | Description                                                                                                                                                                  | Example                                                                                                                                                                      |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `metrics`                                                                                                                                                                    | [models.MetricOut](../models/metricout.md)                                                                                                                                   | :heavy_check_mark:                                                                                                                                                           | Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase). |                                                                                                                                                                              |
| `step_number`                                                                                                                                                                | *int*                                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                           | The step number that the checkpoint was created at.                                                                                                                          |                                                                                                                                                                              |
| `created_at`                                                                                                                                                                 | *int*                                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                           | The UNIX timestamp (in seconds) for when the checkpoint was created.                                                                                                         | 1716963433                                                                                                                                                                   |
</file>

<file path="docs/models/classificationrequest.md">
# ClassificationRequest


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `model`                                                                        | *str*                                                                          | :heavy_check_mark:                                                             | ID of the model to use.                                                        |
| `inputs`                                                                       | [models.ClassificationRequestInputs](../models/classificationrequestinputs.md) | :heavy_check_mark:                                                             | Text to classify.                                                              |
</file>

<file path="docs/models/classificationrequestinputs.md">
# ClassificationRequestInputs

Text to classify.


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/classificationresponse.md">
# ClassificationResponse


## Fields

| Field                                                                                         | Type                                                                                          | Required                                                                                      | Description                                                                                   | Example                                                                                       |
| --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| `id`                                                                                          | *str*                                                                                         | :heavy_check_mark:                                                                            | N/A                                                                                           | mod-e5cc70bb28c444948073e77776eb30ef                                                          |
| `model`                                                                                       | *str*                                                                                         | :heavy_check_mark:                                                                            | N/A                                                                                           |                                                                                               |
| `results`                                                                                     | List[Dict[str, [models.ClassificationTargetResult](../models/classificationtargetresult.md)]] | :heavy_check_mark:                                                                            | N/A                                                                                           |                                                                                               |
</file>

<file path="docs/models/classificationtargetresult.md">
# ClassificationTargetResult


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `scores`           | Dict[str, *float*] | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/classifierdetailedjobout.md">
# ClassifierDetailedJobOut


## Fields

| Field                                                                                                                              | Type                                                                                                                               | Required                                                                                                                           | Description                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| `id`                                                                                                                               | *str*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `auto_start`                                                                                                                       | *bool*                                                                                                                             | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `model`                                                                                                                            | *str*                                                                                                                              | :heavy_check_mark:                                                                                                                 | The name of the model to fine-tune.                                                                                                |
| `status`                                                                                                                           | [models.ClassifierDetailedJobOutStatus](../models/classifierdetailedjoboutstatus.md)                                               | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `created_at`                                                                                                                       | *int*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `modified_at`                                                                                                                      | *int*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `training_files`                                                                                                                   | List[*str*]                                                                                                                        | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `hyperparameters`                                                                                                                  | [models.ClassifierTrainingParameters](../models/classifiertrainingparameters.md)                                                   | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `classifier_targets`                                                                                                               | List[[models.ClassifierTargetOut](../models/classifiertargetout.md)]                                                               | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `validation_files`                                                                                                                 | List[*str*]                                                                                                                        | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `object`                                                                                                                           | [Optional[models.ClassifierDetailedJobOutObject]](../models/classifierdetailedjoboutobject.md)                                     | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `fine_tuned_model`                                                                                                                 | *OptionalNullable[str]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `suffix`                                                                                                                           | *OptionalNullable[str]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `integrations`                                                                                                                     | List[[models.ClassifierDetailedJobOutIntegrations](../models/classifierdetailedjoboutintegrations.md)]                             | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `trained_tokens`                                                                                                                   | *OptionalNullable[int]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `metadata`                                                                                                                         | [OptionalNullable[models.JobMetadataOut]](../models/jobmetadataout.md)                                                             | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `job_type`                                                                                                                         | [Optional[models.ClassifierDetailedJobOutJobType]](../models/classifierdetailedjoboutjobtype.md)                                   | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `events`                                                                                                                           | List[[models.EventOut](../models/eventout.md)]                                                                                     | :heavy_minus_sign:                                                                                                                 | Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here. |
| `checkpoints`                                                                                                                      | List[[models.CheckpointOut](../models/checkpointout.md)]                                                                           | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
</file>

<file path="docs/models/classifierdetailedjoboutintegrations.md">
# ClassifierDetailedJobOutIntegrations


## Supported Types

### `models.WandbIntegrationOut`

```python
value: models.WandbIntegrationOut = /* values here */
```
</file>

<file path="docs/models/classifierdetailedjoboutjobtype.md">
# ClassifierDetailedJobOutJobType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `CLASSIFIER` | classifier   |
</file>

<file path="docs/models/classifierdetailedjoboutobject.md">
# ClassifierDetailedJobOutObject


## Values

| Name  | Value |
| ----- | ----- |
| `JOB` | job   |
</file>

<file path="docs/models/classifierdetailedjoboutstatus.md">
# ClassifierDetailedJobOutStatus


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `STARTED`                | STARTED                  |
| `VALIDATING`             | VALIDATING               |
| `VALIDATED`              | VALIDATED                |
| `RUNNING`                | RUNNING                  |
| `FAILED_VALIDATION`      | FAILED_VALIDATION        |
| `FAILED`                 | FAILED                   |
| `SUCCESS`                | SUCCESS                  |
| `CANCELLED`              | CANCELLED                |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
</file>

<file path="docs/models/classifierftmodelout.md">
# ClassifierFTModelOut


## Fields

| Field                                                                                        | Type                                                                                         | Required                                                                                     | Description                                                                                  |
| -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| `id`                                                                                         | *str*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `created`                                                                                    | *int*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `owned_by`                                                                                   | *str*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `root`                                                                                       | *str*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `archived`                                                                                   | *bool*                                                                                       | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `capabilities`                                                                               | [models.FTModelCapabilitiesOut](../models/ftmodelcapabilitiesout.md)                         | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `job`                                                                                        | *str*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `classifier_targets`                                                                         | List[[models.ClassifierTargetOut](../models/classifiertargetout.md)]                         | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `object`                                                                                     | [Optional[models.ClassifierFTModelOutObject]](../models/classifierftmodeloutobject.md)       | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `name`                                                                                       | *OptionalNullable[str]*                                                                      | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `description`                                                                                | *OptionalNullable[str]*                                                                      | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `max_context_length`                                                                         | *Optional[int]*                                                                              | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `aliases`                                                                                    | List[*str*]                                                                                  | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `model_type`                                                                                 | [Optional[models.ClassifierFTModelOutModelType]](../models/classifierftmodeloutmodeltype.md) | :heavy_minus_sign:                                                                           | N/A                                                                                          |
</file>

<file path="docs/models/classifierftmodeloutmodeltype.md">
# ClassifierFTModelOutModelType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `CLASSIFIER` | classifier   |
</file>

<file path="docs/models/classifierftmodeloutobject.md">
# ClassifierFTModelOutObject


## Values

| Name    | Value   |
| ------- | ------- |
| `MODEL` | model   |
</file>

<file path="docs/models/classifierjobout.md">
# ClassifierJobOut


## Fields

| Field                                                                                                                                                                                                                                   | Type                                                                                                                                                                                                                                    | Required                                                                                                                                                                                                                                | Description                                                                                                                                                                                                                             |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`                                                                                                                                                                                                                                    | *str*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The ID of the job.                                                                                                                                                                                                                      |
| `auto_start`                                                                                                                                                                                                                            | *bool*                                                                                                                                                                                                                                  | :heavy_check_mark:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `model`                                                                                                                                                                                                                                 | *str*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The name of the model to fine-tune.                                                                                                                                                                                                     |
| `status`                                                                                                                                                                                                                                | [models.ClassifierJobOutStatus](../models/classifierjoboutstatus.md)                                                                                                                                                                    | :heavy_check_mark:                                                                                                                                                                                                                      | The current status of the fine-tuning job.                                                                                                                                                                                              |
| `created_at`                                                                                                                                                                                                                            | *int*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The UNIX timestamp (in seconds) for when the fine-tuning job was created.                                                                                                                                                               |
| `modified_at`                                                                                                                                                                                                                           | *int*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The UNIX timestamp (in seconds) for when the fine-tuning job was last modified.                                                                                                                                                         |
| `training_files`                                                                                                                                                                                                                        | List[*str*]                                                                                                                                                                                                                             | :heavy_check_mark:                                                                                                                                                                                                                      | A list containing the IDs of uploaded files that contain training data.                                                                                                                                                                 |
| `hyperparameters`                                                                                                                                                                                                                       | [models.ClassifierTrainingParameters](../models/classifiertrainingparameters.md)                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `validation_files`                                                                                                                                                                                                                      | List[*str*]                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                      | A list containing the IDs of uploaded files that contain validation data.                                                                                                                                                               |
| `object`                                                                                                                                                                                                                                | [Optional[models.ClassifierJobOutObject]](../models/classifierjoboutobject.md)                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                      | The object type of the fine-tuning job.                                                                                                                                                                                                 |
| `fine_tuned_model`                                                                                                                                                                                                                      | *OptionalNullable[str]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running.                                                                                                               |
| `suffix`                                                                                                                                                                                                                                | *OptionalNullable[str]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`. |
| `integrations`                                                                                                                                                                                                                          | List[[models.ClassifierJobOutIntegrations](../models/classifierjoboutintegrations.md)]                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                      | A list of integrations enabled for your fine-tuning job.                                                                                                                                                                                |
| `trained_tokens`                                                                                                                                                                                                                        | *OptionalNullable[int]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | Total number of tokens trained.                                                                                                                                                                                                         |
| `metadata`                                                                                                                                                                                                                              | [OptionalNullable[models.JobMetadataOut]](../models/jobmetadataout.md)                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `job_type`                                                                                                                                                                                                                              | [Optional[models.ClassifierJobOutJobType]](../models/classifierjoboutjobtype.md)                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                      | The type of job (`FT` for fine-tuning).                                                                                                                                                                                                 |
</file>

<file path="docs/models/classifierjoboutintegrations.md">
# ClassifierJobOutIntegrations


## Supported Types

### `models.WandbIntegrationOut`

```python
value: models.WandbIntegrationOut = /* values here */
```
</file>

<file path="docs/models/classifierjoboutjobtype.md">
# ClassifierJobOutJobType

The type of job (`FT` for fine-tuning).


## Values

| Name         | Value        |
| ------------ | ------------ |
| `CLASSIFIER` | classifier   |
</file>

<file path="docs/models/classifierjoboutobject.md">
# ClassifierJobOutObject

The object type of the fine-tuning job.


## Values

| Name  | Value |
| ----- | ----- |
| `JOB` | job   |
</file>

<file path="docs/models/classifierjoboutstatus.md">
# ClassifierJobOutStatus

The current status of the fine-tuning job.


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `STARTED`                | STARTED                  |
| `VALIDATING`             | VALIDATING               |
| `VALIDATED`              | VALIDATED                |
| `RUNNING`                | RUNNING                  |
| `FAILED_VALIDATION`      | FAILED_VALIDATION        |
| `FAILED`                 | FAILED                   |
| `SUCCESS`                | SUCCESS                  |
| `CANCELLED`              | CANCELLED                |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
</file>

<file path="docs/models/classifiertargetin.md">
# ClassifierTargetIn


## Fields

| Field                                                                                      | Type                                                                                       | Required                                                                                   | Description                                                                                |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| `name`                                                                                     | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `labels`                                                                                   | List[*str*]                                                                                | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `weight`                                                                                   | *Optional[float]*                                                                          | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `loss_function`                                                                            | [OptionalNullable[models.FTClassifierLossFunction]](../models/ftclassifierlossfunction.md) | :heavy_minus_sign:                                                                         | N/A                                                                                        |
</file>

<file path="docs/models/classifiertargetout.md">
# ClassifierTargetOut


## Fields

| Field                                                                    | Type                                                                     | Required                                                                 | Description                                                              |
| ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| `name`                                                                   | *str*                                                                    | :heavy_check_mark:                                                       | N/A                                                                      |
| `labels`                                                                 | List[*str*]                                                              | :heavy_check_mark:                                                       | N/A                                                                      |
| `weight`                                                                 | *float*                                                                  | :heavy_check_mark:                                                       | N/A                                                                      |
| `loss_function`                                                          | [models.FTClassifierLossFunction](../models/ftclassifierlossfunction.md) | :heavy_check_mark:                                                       | N/A                                                                      |
</file>

<file path="docs/models/classifiertrainingparameters.md">
# ClassifierTrainingParameters


## Fields

| Field                     | Type                      | Required                  | Description               |
| ------------------------- | ------------------------- | ------------------------- | ------------------------- |
| `training_steps`          | *OptionalNullable[int]*   | :heavy_minus_sign:        | N/A                       |
| `learning_rate`           | *Optional[float]*         | :heavy_minus_sign:        | N/A                       |
| `weight_decay`            | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `warmup_fraction`         | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `epochs`                  | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `seq_len`                 | *OptionalNullable[int]*   | :heavy_minus_sign:        | N/A                       |
</file>

<file path="docs/models/classifiertrainingparametersin.md">
# ClassifierTrainingParametersIn

The fine-tuning hyperparameter settings used in a classifier fine-tune job.


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                          | Type                                                                                                                                                                                                                                                                                                                                                                                                           | Required                                                                                                                                                                                                                                                                                                                                                                                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                    |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `training_steps`                                                                                                                                                                                                                                                                                                                                                                                               | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset.                                                                                                                                                                                |
| `learning_rate`                                                                                                                                                                                                                                                                                                                                                                                                | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                              | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process.                                                                                                                                                                                                                                 |
| `weight_decay`                                                                                                                                                                                                                                                                                                                                                                                                 | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | (Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large.                                                                                                                                                                                                |
| `warmup_fraction`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | (Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune) |
| `epochs`                                                                                                                                                                                                                                                                                                                                                                                                       | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | N/A                                                                                                                                                                                                                                                                                                                                                                                                            |
| `seq_len`                                                                                                                                                                                                                                                                                                                                                                                                      | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | N/A                                                                                                                                                                                                                                                                                                                                                                                                            |
</file>

<file path="docs/models/codeinterpretertool.md">
# CodeInterpreterTool


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `type`                                                                           | [Optional[models.CodeInterpreterToolType]](../models/codeinterpretertooltype.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/codeinterpretertooltype.md">
# CodeInterpreterToolType


## Values

| Name               | Value              |
| ------------------ | ------------------ |
| `CODE_INTERPRETER` | code_interpreter   |
</file>

<file path="docs/models/completionargs.md">
# CompletionArgs

White-listed arguments from the completion API


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `stop`                                                                         | [OptionalNullable[models.CompletionArgsStop]](../models/completionargsstop.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
| `presence_penalty`                                                             | *OptionalNullable[float]*                                                      | :heavy_minus_sign:                                                             | N/A                                                                            |
| `frequency_penalty`                                                            | *OptionalNullable[float]*                                                      | :heavy_minus_sign:                                                             | N/A                                                                            |
| `temperature`                                                                  | *Optional[float]*                                                              | :heavy_minus_sign:                                                             | N/A                                                                            |
| `top_p`                                                                        | *OptionalNullable[float]*                                                      | :heavy_minus_sign:                                                             | N/A                                                                            |
| `max_tokens`                                                                   | *OptionalNullable[int]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `random_seed`                                                                  | *OptionalNullable[int]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `prediction`                                                                   | [OptionalNullable[models.Prediction]](../models/prediction.md)                 | :heavy_minus_sign:                                                             | N/A                                                                            |
| `response_format`                                                              | [OptionalNullable[models.ResponseFormat]](../models/responseformat.md)         | :heavy_minus_sign:                                                             | N/A                                                                            |
| `tool_choice`                                                                  | [Optional[models.ToolChoiceEnum]](../models/toolchoiceenum.md)                 | :heavy_minus_sign:                                                             | N/A                                                                            |
</file>

<file path="docs/models/completionargsstop.md">
# CompletionArgsStop


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/completionchunk.md">
# CompletionChunk


## Fields

| Field                                                                                      | Type                                                                                       | Required                                                                                   | Description                                                                                |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| `id`                                                                                       | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `model`                                                                                    | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `choices`                                                                                  | List[[models.CompletionResponseStreamChoice](../models/completionresponsestreamchoice.md)] | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `object`                                                                                   | *Optional[str]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `created`                                                                                  | *Optional[int]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `usage`                                                                                    | [Optional[models.UsageInfo]](../models/usageinfo.md)                                       | :heavy_minus_sign:                                                                         | N/A                                                                                        |
</file>

<file path="docs/models/completiondetailedjobout.md">
# CompletionDetailedJobOut


## Fields

| Field                                                                                                                              | Type                                                                                                                               | Required                                                                                                                           | Description                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| `id`                                                                                                                               | *str*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `auto_start`                                                                                                                       | *bool*                                                                                                                             | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `model`                                                                                                                            | *str*                                                                                                                              | :heavy_check_mark:                                                                                                                 | The name of the model to fine-tune.                                                                                                |
| `status`                                                                                                                           | [models.CompletionDetailedJobOutStatus](../models/completiondetailedjoboutstatus.md)                                               | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `created_at`                                                                                                                       | *int*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `modified_at`                                                                                                                      | *int*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `training_files`                                                                                                                   | List[*str*]                                                                                                                        | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `hyperparameters`                                                                                                                  | [models.CompletionTrainingParameters](../models/completiontrainingparameters.md)                                                   | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `validation_files`                                                                                                                 | List[*str*]                                                                                                                        | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `object`                                                                                                                           | [Optional[models.CompletionDetailedJobOutObject]](../models/completiondetailedjoboutobject.md)                                     | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `fine_tuned_model`                                                                                                                 | *OptionalNullable[str]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `suffix`                                                                                                                           | *OptionalNullable[str]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `integrations`                                                                                                                     | List[[models.CompletionDetailedJobOutIntegrations](../models/completiondetailedjoboutintegrations.md)]                             | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `trained_tokens`                                                                                                                   | *OptionalNullable[int]*                                                                                                            | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `metadata`                                                                                                                         | [OptionalNullable[models.JobMetadataOut]](../models/jobmetadataout.md)                                                             | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `job_type`                                                                                                                         | [Optional[models.CompletionDetailedJobOutJobType]](../models/completiondetailedjoboutjobtype.md)                                   | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `repositories`                                                                                                                     | List[[models.CompletionDetailedJobOutRepositories](../models/completiondetailedjoboutrepositories.md)]                             | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `events`                                                                                                                           | List[[models.EventOut](../models/eventout.md)]                                                                                     | :heavy_minus_sign:                                                                                                                 | Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here. |
| `checkpoints`                                                                                                                      | List[[models.CheckpointOut](../models/checkpointout.md)]                                                                           | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
</file>

<file path="docs/models/completiondetailedjoboutintegrations.md">
# CompletionDetailedJobOutIntegrations


## Supported Types

### `models.WandbIntegrationOut`

```python
value: models.WandbIntegrationOut = /* values here */
```
</file>

<file path="docs/models/completiondetailedjoboutjobtype.md">
# CompletionDetailedJobOutJobType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `COMPLETION` | completion   |
</file>

<file path="docs/models/completiondetailedjoboutobject.md">
# CompletionDetailedJobOutObject


## Values

| Name  | Value |
| ----- | ----- |
| `JOB` | job   |
</file>

<file path="docs/models/completiondetailedjoboutrepositories.md">
# CompletionDetailedJobOutRepositories


## Supported Types

### `models.GithubRepositoryOut`

```python
value: models.GithubRepositoryOut = /* values here */
```
</file>

<file path="docs/models/completiondetailedjoboutstatus.md">
# CompletionDetailedJobOutStatus


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `STARTED`                | STARTED                  |
| `VALIDATING`             | VALIDATING               |
| `VALIDATED`              | VALIDATED                |
| `RUNNING`                | RUNNING                  |
| `FAILED_VALIDATION`      | FAILED_VALIDATION        |
| `FAILED`                 | FAILED                   |
| `SUCCESS`                | SUCCESS                  |
| `CANCELLED`              | CANCELLED                |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
</file>

<file path="docs/models/completionevent.md">
# CompletionEvent


## Fields

| Field                                                  | Type                                                   | Required                                               | Description                                            |
| ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ |
| `data`                                                 | [models.CompletionChunk](../models/completionchunk.md) | :heavy_check_mark:                                     | N/A                                                    |
</file>

<file path="docs/models/completionftmodelout.md">
# CompletionFTModelOut


## Fields

| Field                                                                                  | Type                                                                                   | Required                                                                               | Description                                                                            |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `id`                                                                                   | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `created`                                                                              | *int*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `owned_by`                                                                             | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `root`                                                                                 | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `archived`                                                                             | *bool*                                                                                 | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `capabilities`                                                                         | [models.FTModelCapabilitiesOut](../models/ftmodelcapabilitiesout.md)                   | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `job`                                                                                  | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `object`                                                                               | [Optional[models.CompletionFTModelOutObject]](../models/completionftmodeloutobject.md) | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `name`                                                                                 | *OptionalNullable[str]*                                                                | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `description`                                                                          | *OptionalNullable[str]*                                                                | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `max_context_length`                                                                   | *Optional[int]*                                                                        | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `aliases`                                                                              | List[*str*]                                                                            | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `model_type`                                                                           | [Optional[models.ModelType]](../models/modeltype.md)                                   | :heavy_minus_sign:                                                                     | N/A                                                                                    |
</file>

<file path="docs/models/completionftmodeloutobject.md">
# CompletionFTModelOutObject


## Values

| Name    | Value   |
| ------- | ------- |
| `MODEL` | model   |
</file>

<file path="docs/models/completionjobout.md">
# CompletionJobOut


## Fields

| Field                                                                                                                                                                                                                                   | Type                                                                                                                                                                                                                                    | Required                                                                                                                                                                                                                                | Description                                                                                                                                                                                                                             |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`                                                                                                                                                                                                                                    | *str*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The ID of the job.                                                                                                                                                                                                                      |
| `auto_start`                                                                                                                                                                                                                            | *bool*                                                                                                                                                                                                                                  | :heavy_check_mark:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `model`                                                                                                                                                                                                                                 | *str*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The name of the model to fine-tune.                                                                                                                                                                                                     |
| `status`                                                                                                                                                                                                                                | [models.Status](../models/status.md)                                                                                                                                                                                                    | :heavy_check_mark:                                                                                                                                                                                                                      | The current status of the fine-tuning job.                                                                                                                                                                                              |
| `created_at`                                                                                                                                                                                                                            | *int*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The UNIX timestamp (in seconds) for when the fine-tuning job was created.                                                                                                                                                               |
| `modified_at`                                                                                                                                                                                                                           | *int*                                                                                                                                                                                                                                   | :heavy_check_mark:                                                                                                                                                                                                                      | The UNIX timestamp (in seconds) for when the fine-tuning job was last modified.                                                                                                                                                         |
| `training_files`                                                                                                                                                                                                                        | List[*str*]                                                                                                                                                                                                                             | :heavy_check_mark:                                                                                                                                                                                                                      | A list containing the IDs of uploaded files that contain training data.                                                                                                                                                                 |
| `hyperparameters`                                                                                                                                                                                                                       | [models.CompletionTrainingParameters](../models/completiontrainingparameters.md)                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `validation_files`                                                                                                                                                                                                                      | List[*str*]                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                      | A list containing the IDs of uploaded files that contain validation data.                                                                                                                                                               |
| `object`                                                                                                                                                                                                                                | [Optional[models.CompletionJobOutObject]](../models/completionjoboutobject.md)                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                      | The object type of the fine-tuning job.                                                                                                                                                                                                 |
| `fine_tuned_model`                                                                                                                                                                                                                      | *OptionalNullable[str]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running.                                                                                                               |
| `suffix`                                                                                                                                                                                                                                | *OptionalNullable[str]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`. |
| `integrations`                                                                                                                                                                                                                          | List[[models.Integrations](../models/integrations.md)]                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                      | A list of integrations enabled for your fine-tuning job.                                                                                                                                                                                |
| `trained_tokens`                                                                                                                                                                                                                        | *OptionalNullable[int]*                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                      | Total number of tokens trained.                                                                                                                                                                                                         |
| `metadata`                                                                                                                                                                                                                              | [OptionalNullable[models.JobMetadataOut]](../models/jobmetadataout.md)                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
| `job_type`                                                                                                                                                                                                                              | [Optional[models.JobType]](../models/jobtype.md)                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                      | The type of job (`FT` for fine-tuning).                                                                                                                                                                                                 |
| `repositories`                                                                                                                                                                                                                          | List[[models.Repositories](../models/repositories.md)]                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                      | N/A                                                                                                                                                                                                                                     |
</file>

<file path="docs/models/completionjoboutobject.md">
# CompletionJobOutObject

The object type of the fine-tuning job.


## Values

| Name  | Value |
| ----- | ----- |
| `JOB` | job   |
</file>

<file path="docs/models/completionresponsestreamchoice.md">
# CompletionResponseStreamChoice


## Fields

| Field                                                                                                                  | Type                                                                                                                   | Required                                                                                                               | Description                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `index`                                                                                                                | *int*                                                                                                                  | :heavy_check_mark:                                                                                                     | N/A                                                                                                                    |
| `delta`                                                                                                                | [models.DeltaMessage](../models/deltamessage.md)                                                                       | :heavy_check_mark:                                                                                                     | N/A                                                                                                                    |
| `finish_reason`                                                                                                        | [Nullable[models.CompletionResponseStreamChoiceFinishReason]](../models/completionresponsestreamchoicefinishreason.md) | :heavy_check_mark:                                                                                                     | N/A                                                                                                                    |
</file>

<file path="docs/models/completionresponsestreamchoicefinishreason.md">
# CompletionResponseStreamChoiceFinishReason


## Values

| Name         | Value        |
| ------------ | ------------ |
| `STOP`       | stop         |
| `LENGTH`     | length       |
| `ERROR`      | error        |
| `TOOL_CALLS` | tool_calls   |
</file>

<file path="docs/models/completiontrainingparameters.md">
# CompletionTrainingParameters


## Fields

| Field                     | Type                      | Required                  | Description               |
| ------------------------- | ------------------------- | ------------------------- | ------------------------- |
| `training_steps`          | *OptionalNullable[int]*   | :heavy_minus_sign:        | N/A                       |
| `learning_rate`           | *Optional[float]*         | :heavy_minus_sign:        | N/A                       |
| `weight_decay`            | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `warmup_fraction`         | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `epochs`                  | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
| `seq_len`                 | *OptionalNullable[int]*   | :heavy_minus_sign:        | N/A                       |
| `fim_ratio`               | *OptionalNullable[float]* | :heavy_minus_sign:        | N/A                       |
</file>

<file path="docs/models/completiontrainingparametersin.md">
# CompletionTrainingParametersIn

The fine-tuning hyperparameter settings used in a fine-tune job.


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                          | Type                                                                                                                                                                                                                                                                                                                                                                                                           | Required                                                                                                                                                                                                                                                                                                                                                                                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                    |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `training_steps`                                                                                                                                                                                                                                                                                                                                                                                               | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset.                                                                                                                                                                                |
| `learning_rate`                                                                                                                                                                                                                                                                                                                                                                                                | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                              | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process.                                                                                                                                                                                                                                 |
| `weight_decay`                                                                                                                                                                                                                                                                                                                                                                                                 | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | (Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large.                                                                                                                                                                                                |
| `warmup_fraction`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | (Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune) |
| `epochs`                                                                                                                                                                                                                                                                                                                                                                                                       | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | N/A                                                                                                                                                                                                                                                                                                                                                                                                            |
| `seq_len`                                                                                                                                                                                                                                                                                                                                                                                                      | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | N/A                                                                                                                                                                                                                                                                                                                                                                                                            |
| `fim_ratio`                                                                                                                                                                                                                                                                                                                                                                                                    | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                             | N/A                                                                                                                                                                                                                                                                                                                                                                                                            |
</file>

<file path="docs/models/content.md">
# Content


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="docs/models/contentchunk.md">
# ContentChunk


## Supported Types

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.DocumentURLChunk`

```python
value: models.DocumentURLChunk = /* values here */
```

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ReferenceChunk`

```python
value: models.ReferenceChunk = /* values here */
```
</file>

<file path="docs/models/conversationappendrequest.md">
# ConversationAppendRequest


## Fields

| Field                                                                                                                | Type                                                                                                                 | Required                                                                                                             | Description                                                                                                          |
| -------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                             | [models.ConversationInputs](../models/conversationinputs.md)                                                         | :heavy_check_mark:                                                                                                   | N/A                                                                                                                  |
| `stream`                                                                                                             | *Optional[bool]*                                                                                                     | :heavy_minus_sign:                                                                                                   | N/A                                                                                                                  |
| `store`                                                                                                              | *Optional[bool]*                                                                                                     | :heavy_minus_sign:                                                                                                   | Whether to store the results into our servers or not.                                                                |
| `handoff_execution`                                                                                                  | [Optional[models.ConversationAppendRequestHandoffExecution]](../models/conversationappendrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                   | N/A                                                                                                                  |
| `completion_args`                                                                                                    | [Optional[models.CompletionArgs]](../models/completionargs.md)                                                       | :heavy_minus_sign:                                                                                                   | White-listed arguments from the completion API                                                                       |
</file>

<file path="docs/models/conversationappendrequesthandoffexecution.md">
# ConversationAppendRequestHandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/conversationappendstreamrequest.md">
# ConversationAppendStreamRequest


## Fields

| Field                                                                                                                            | Type                                                                                                                             | Required                                                                                                                         | Description                                                                                                                      |
| -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                                         | [models.ConversationInputs](../models/conversationinputs.md)                                                                     | :heavy_check_mark:                                                                                                               | N/A                                                                                                                              |
| `stream`                                                                                                                         | *Optional[bool]*                                                                                                                 | :heavy_minus_sign:                                                                                                               | N/A                                                                                                                              |
| `store`                                                                                                                          | *Optional[bool]*                                                                                                                 | :heavy_minus_sign:                                                                                                               | Whether to store the results into our servers or not.                                                                            |
| `handoff_execution`                                                                                                              | [Optional[models.ConversationAppendStreamRequestHandoffExecution]](../models/conversationappendstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                               | N/A                                                                                                                              |
| `completion_args`                                                                                                                | [Optional[models.CompletionArgs]](../models/completionargs.md)                                                                   | :heavy_minus_sign:                                                                                                               | White-listed arguments from the completion API                                                                                   |
</file>

<file path="docs/models/conversationappendstreamrequesthandoffexecution.md">
# ConversationAppendStreamRequestHandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/conversationevents.md">
# ConversationEvents


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `event`                                                              | [models.SSETypes](../models/ssetypes.md)                             | :heavy_check_mark:                                                   | Server side events sent when streaming a conversation response.      |
| `data`                                                               | [models.ConversationEventsData](../models/conversationeventsdata.md) | :heavy_check_mark:                                                   | N/A                                                                  |
</file>

<file path="docs/models/conversationeventsdata.md">
# ConversationEventsData


## Supported Types

### `models.AgentHandoffDoneEvent`

```python
value: models.AgentHandoffDoneEvent = /* values here */
```

### `models.AgentHandoffStartedEvent`

```python
value: models.AgentHandoffStartedEvent = /* values here */
```

### `models.ResponseDoneEvent`

```python
value: models.ResponseDoneEvent = /* values here */
```

### `models.ResponseErrorEvent`

```python
value: models.ResponseErrorEvent = /* values here */
```

### `models.ResponseStartedEvent`

```python
value: models.ResponseStartedEvent = /* values here */
```

### `models.FunctionCallEvent`

```python
value: models.FunctionCallEvent = /* values here */
```

### `models.MessageOutputEvent`

```python
value: models.MessageOutputEvent = /* values here */
```

### `models.ToolExecutionDoneEvent`

```python
value: models.ToolExecutionDoneEvent = /* values here */
```

### `models.ToolExecutionStartedEvent`

```python
value: models.ToolExecutionStartedEvent = /* values here */
```
</file>

<file path="docs/models/conversationhistory.md">
# ConversationHistory

Retrieve all entries in a conversation.


## Fields

| Field                                                                                | Type                                                                                 | Required                                                                             | Description                                                                          |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ |
| `conversation_id`                                                                    | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `entries`                                                                            | List[[models.Entries](../models/entries.md)]                                         | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `object`                                                                             | [Optional[models.ConversationHistoryObject]](../models/conversationhistoryobject.md) | :heavy_minus_sign:                                                                   | N/A                                                                                  |
</file>

<file path="docs/models/conversationhistoryobject.md">
# ConversationHistoryObject


## Values

| Name                   | Value                  |
| ---------------------- | ---------------------- |
| `CONVERSATION_HISTORY` | conversation.history   |
</file>

<file path="docs/models/conversationinputs.md">
# ConversationInputs


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.InputEntries]`

```python
value: List[models.InputEntries] = /* values here */
```
</file>

<file path="docs/models/conversationmessages.md">
# ConversationMessages

Similar to the conversation history but only keep the messages


## Fields

| Field                                                                                  | Type                                                                                   | Required                                                                               | Description                                                                            |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `conversation_id`                                                                      | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `messages`                                                                             | List[[models.MessageEntries](../models/messageentries.md)]                             | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `object`                                                                               | [Optional[models.ConversationMessagesObject]](../models/conversationmessagesobject.md) | :heavy_minus_sign:                                                                     | N/A                                                                                    |
</file>

<file path="docs/models/conversationmessagesobject.md">
# ConversationMessagesObject


## Values

| Name                    | Value                   |
| ----------------------- | ----------------------- |
| `CONVERSATION_MESSAGES` | conversation.messages   |
</file>

<file path="docs/models/conversationrequest.md">
# ConversationRequest


## Fields

| Field                                                                      | Type                                                                       | Required                                                                   | Description                                                                |
| -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| `inputs`                                                                   | [models.ConversationInputs](../models/conversationinputs.md)               | :heavy_check_mark:                                                         | N/A                                                                        |
| `stream`                                                                   | *Optional[bool]*                                                           | :heavy_minus_sign:                                                         | N/A                                                                        |
| `store`                                                                    | *OptionalNullable[bool]*                                                   | :heavy_minus_sign:                                                         | N/A                                                                        |
| `handoff_execution`                                                        | [OptionalNullable[models.HandoffExecution]](../models/handoffexecution.md) | :heavy_minus_sign:                                                         | N/A                                                                        |
| `instructions`                                                             | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | N/A                                                                        |
| `tools`                                                                    | List[[models.Tools](../models/tools.md)]                                   | :heavy_minus_sign:                                                         | N/A                                                                        |
| `completion_args`                                                          | [OptionalNullable[models.CompletionArgs]](../models/completionargs.md)     | :heavy_minus_sign:                                                         | N/A                                                                        |
| `name`                                                                     | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | N/A                                                                        |
| `description`                                                              | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | N/A                                                                        |
| `agent_id`                                                                 | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | N/A                                                                        |
| `model`                                                                    | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | N/A                                                                        |
</file>

<file path="docs/models/conversationresponse.md">
# ConversationResponse

The response after appending new entries to the conversation.


## Fields

| Field                                                                                  | Type                                                                                   | Required                                                                               | Description                                                                            |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `conversation_id`                                                                      | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `outputs`                                                                              | List[[models.Outputs](../models/outputs.md)]                                           | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `usage`                                                                                | [models.ConversationUsageInfo](../models/conversationusageinfo.md)                     | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `object`                                                                               | [Optional[models.ConversationResponseObject]](../models/conversationresponseobject.md) | :heavy_minus_sign:                                                                     | N/A                                                                                    |
</file>

<file path="docs/models/conversationresponseobject.md">
# ConversationResponseObject


## Values

| Name                    | Value                   |
| ----------------------- | ----------------------- |
| `CONVERSATION_RESPONSE` | conversation.response   |
</file>

<file path="docs/models/conversationrestartrequest.md">
# ConversationRestartRequest

Request to restart a new conversation from a given entry in the conversation.


## Fields

| Field                                                                                                                  | Type                                                                                                                   | Required                                                                                                               | Description                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                               | [models.ConversationInputs](../models/conversationinputs.md)                                                           | :heavy_check_mark:                                                                                                     | N/A                                                                                                                    |
| `from_entry_id`                                                                                                        | *str*                                                                                                                  | :heavy_check_mark:                                                                                                     | N/A                                                                                                                    |
| `stream`                                                                                                               | *Optional[bool]*                                                                                                       | :heavy_minus_sign:                                                                                                     | N/A                                                                                                                    |
| `store`                                                                                                                | *Optional[bool]*                                                                                                       | :heavy_minus_sign:                                                                                                     | Whether to store the results into our servers or not.                                                                  |
| `handoff_execution`                                                                                                    | [Optional[models.ConversationRestartRequestHandoffExecution]](../models/conversationrestartrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                     | N/A                                                                                                                    |
| `completion_args`                                                                                                      | [Optional[models.CompletionArgs]](../models/completionargs.md)                                                         | :heavy_minus_sign:                                                                                                     | White-listed arguments from the completion API                                                                         |
</file>

<file path="docs/models/conversationrestartrequesthandoffexecution.md">
# ConversationRestartRequestHandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/conversationrestartstreamrequest.md">
# ConversationRestartStreamRequest

Request to restart a new conversation from a given entry in the conversation.


## Fields

| Field                                                                                                                              | Type                                                                                                                               | Required                                                                                                                           | Description                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                                           | [models.ConversationInputs](../models/conversationinputs.md)                                                                       | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `from_entry_id`                                                                                                                    | *str*                                                                                                                              | :heavy_check_mark:                                                                                                                 | N/A                                                                                                                                |
| `stream`                                                                                                                           | *Optional[bool]*                                                                                                                   | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `store`                                                                                                                            | *Optional[bool]*                                                                                                                   | :heavy_minus_sign:                                                                                                                 | Whether to store the results into our servers or not.                                                                              |
| `handoff_execution`                                                                                                                | [Optional[models.ConversationRestartStreamRequestHandoffExecution]](../models/conversationrestartstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                                 | N/A                                                                                                                                |
| `completion_args`                                                                                                                  | [Optional[models.CompletionArgs]](../models/completionargs.md)                                                                     | :heavy_minus_sign:                                                                                                                 | White-listed arguments from the completion API                                                                                     |
</file>

<file path="docs/models/conversationrestartstreamrequesthandoffexecution.md">
# ConversationRestartStreamRequestHandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/conversationstreamrequest.md">
# ConversationStreamRequest


## Fields

| Field                                                                                                                        | Type                                                                                                                         | Required                                                                                                                     | Description                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                                     | [models.ConversationInputs](../models/conversationinputs.md)                                                                 | :heavy_check_mark:                                                                                                           | N/A                                                                                                                          |
| `stream`                                                                                                                     | *Optional[bool]*                                                                                                             | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `store`                                                                                                                      | *OptionalNullable[bool]*                                                                                                     | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `handoff_execution`                                                                                                          | [OptionalNullable[models.ConversationStreamRequestHandoffExecution]](../models/conversationstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `instructions`                                                                                                               | *OptionalNullable[str]*                                                                                                      | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `tools`                                                                                                                      | List[[models.ConversationStreamRequestTools](../models/conversationstreamrequesttools.md)]                                   | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `completion_args`                                                                                                            | [OptionalNullable[models.CompletionArgs]](../models/completionargs.md)                                                       | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `name`                                                                                                                       | *OptionalNullable[str]*                                                                                                      | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `description`                                                                                                                | *OptionalNullable[str]*                                                                                                      | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `agent_id`                                                                                                                   | *OptionalNullable[str]*                                                                                                      | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
| `model`                                                                                                                      | *OptionalNullable[str]*                                                                                                      | :heavy_minus_sign:                                                                                                           | N/A                                                                                                                          |
</file>

<file path="docs/models/conversationstreamrequesthandoffexecution.md">
# ConversationStreamRequestHandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/conversationstreamrequesttools.md">
# ConversationStreamRequestTools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/conversationusageinfo.md">
# ConversationUsageInfo


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `prompt_tokens`         | *Optional[int]*         | :heavy_minus_sign:      | N/A                     |
| `completion_tokens`     | *Optional[int]*         | :heavy_minus_sign:      | N/A                     |
| `total_tokens`          | *Optional[int]*         | :heavy_minus_sign:      | N/A                     |
| `connector_tokens`      | *OptionalNullable[int]* | :heavy_minus_sign:      | N/A                     |
| `connectors`            | Dict[str, *int*]        | :heavy_minus_sign:      | N/A                     |
</file>

<file path="docs/models/data.md">
# Data


## Supported Types

### `models.BaseModelCard`

```python
value: models.BaseModelCard = /* values here */
```

### `models.FTModelCard`

```python
value: models.FTModelCard = /* values here */
```
</file>

<file path="docs/models/deletefileout.md">
# DeleteFileOut


## Fields

| Field                                | Type                                 | Required                             | Description                          | Example                              |
| ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ |
| `id`                                 | *str*                                | :heavy_check_mark:                   | The ID of the deleted file.          | 497f6eca-6276-4993-bfeb-53cbbbba6f09 |
| `object`                             | *str*                                | :heavy_check_mark:                   | The object type that was deleted     | file                                 |
| `deleted`                            | *bool*                               | :heavy_check_mark:                   | The deletion status.                 | false                                |
</file>

<file path="docs/models/deletemodelout.md">
# DeleteModelOut


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   | Example                                       |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `id`                                          | *str*                                         | :heavy_check_mark:                            | The ID of the deleted model.                  | ft:open-mistral-7b:587a6b29:20240514:7e773925 |
| `object`                                      | *Optional[str]*                               | :heavy_minus_sign:                            | The object type that was deleted              |                                               |
| `deleted`                                     | *Optional[bool]*                              | :heavy_minus_sign:                            | The deletion status                           | true                                          |
</file>

<file path="docs/models/deletemodelv1modelsmodeliddeleterequest.md">
# DeleteModelV1ModelsModelIDDeleteRequest


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   | Example                                       |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `model_id`                                    | *str*                                         | :heavy_check_mark:                            | The ID of the model to delete.                | ft:open-mistral-7b:587a6b29:20240514:7e773925 |
</file>

<file path="docs/models/deltamessage.md">
# DeltaMessage


## Fields

| Field                                                    | Type                                                     | Required                                                 | Description                                              |
| -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- |
| `role`                                                   | *OptionalNullable[str]*                                  | :heavy_minus_sign:                                       | N/A                                                      |
| `content`                                                | [OptionalNullable[models.Content]](../models/content.md) | :heavy_minus_sign:                                       | N/A                                                      |
| `tool_calls`                                             | List[[models.ToolCall](../models/toolcall.md)]           | :heavy_minus_sign:                                       | N/A                                                      |
</file>

<file path="docs/models/document.md">
# Document

Document to run OCR on


## Supported Types

### `models.DocumentURLChunk`

```python
value: models.DocumentURLChunk = /* values here */
```

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```
</file>

<file path="docs/models/documentlibrarytool.md">
# DocumentLibraryTool


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `library_ids`                                                                    | List[*str*]                                                                      | :heavy_check_mark:                                                               | Ids of the library in which to search.                                           |
| `type`                                                                           | [Optional[models.DocumentLibraryToolType]](../models/documentlibrarytooltype.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/documentlibrarytooltype.md">
# DocumentLibraryToolType


## Values

| Name               | Value              |
| ------------------ | ------------------ |
| `DOCUMENT_LIBRARY` | document_library   |
</file>

<file path="docs/models/documenturlchunk.md">
# DocumentURLChunk


## Fields

| Field                                                                      | Type                                                                       | Required                                                                   | Description                                                                |
| -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| `document_url`                                                             | *str*                                                                      | :heavy_check_mark:                                                         | N/A                                                                        |
| `document_name`                                                            | *OptionalNullable[str]*                                                    | :heavy_minus_sign:                                                         | The filename of the document                                               |
| `type`                                                                     | [Optional[models.DocumentURLChunkType]](../models/documenturlchunktype.md) | :heavy_minus_sign:                                                         | N/A                                                                        |
</file>

<file path="docs/models/documenturlchunktype.md">
# DocumentURLChunkType


## Values

| Name           | Value          |
| -------------- | -------------- |
| `DOCUMENT_URL` | document_url   |
</file>

<file path="docs/models/embeddingdtype.md">
# EmbeddingDtype


## Values

| Name      | Value     |
| --------- | --------- |
| `FLOAT`   | float     |
| `INT8`    | int8      |
| `UINT8`   | uint8     |
| `BINARY`  | binary    |
| `UBINARY` | ubinary   |
</file>

<file path="docs/models/embeddingrequest.md">
# EmbeddingRequest


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          | Example                                                              |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `model`                                                              | *str*                                                                | :heavy_check_mark:                                                   | ID of the model to use.                                              | mistral-embed                                                        |
| `inputs`                                                             | [models.EmbeddingRequestInputs](../models/embeddingrequestinputs.md) | :heavy_check_mark:                                                   | Text to embed.                                                       | [<br/>"Embed this sentence.",<br/>"As well as this one."<br/>]       |
| `output_dimension`                                                   | *OptionalNullable[int]*                                              | :heavy_minus_sign:                                                   | The dimension of the output embeddings.                              |                                                                      |
| `output_dtype`                                                       | [Optional[models.EmbeddingDtype]](../models/embeddingdtype.md)       | :heavy_minus_sign:                                                   | N/A                                                                  |                                                                      |
</file>

<file path="docs/models/embeddingrequestinputs.md">
# EmbeddingRequestInputs

Text to embed.


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/embeddingresponse.md">
# EmbeddingResponse


## Fields

| Field                                                                    | Type                                                                     | Required                                                                 | Description                                                              | Example                                                                  |
| ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| `id`                                                                     | *str*                                                                    | :heavy_check_mark:                                                       | N/A                                                                      | cmpl-e5cc70bb28c444948073e77776eb30ef                                    |
| `object`                                                                 | *str*                                                                    | :heavy_check_mark:                                                       | N/A                                                                      | chat.completion                                                          |
| `model`                                                                  | *str*                                                                    | :heavy_check_mark:                                                       | N/A                                                                      | mistral-small-latest                                                     |
| `usage`                                                                  | [models.UsageInfo](../models/usageinfo.md)                               | :heavy_check_mark:                                                       | N/A                                                                      |                                                                          |
| `data`                                                                   | List[[models.EmbeddingResponseData](../models/embeddingresponsedata.md)] | :heavy_check_mark:                                                       | N/A                                                                      |                                                                          |
</file>

<file path="docs/models/embeddingresponsedata.md">
# EmbeddingResponseData


## Fields

| Field              | Type               | Required           | Description        | Example            |
| ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| `object`           | *Optional[str]*    | :heavy_minus_sign: | N/A                | embedding          |
| `embedding`        | List[*float*]      | :heavy_minus_sign: | N/A                | [<br/>0.1,<br/>0.2,<br/>0.3<br/>] |
| `index`            | *Optional[int]*    | :heavy_minus_sign: | N/A                | 0                  |
</file>

<file path="docs/models/entries.md">
# Entries


## Supported Types

### `models.MessageInputEntry`

```python
value: models.MessageInputEntry = /* values here */
```

### `models.MessageOutputEntry`

```python
value: models.MessageOutputEntry = /* values here */
```

### `models.FunctionResultEntry`

```python
value: models.FunctionResultEntry = /* values here */
```

### `models.FunctionCallEntry`

```python
value: models.FunctionCallEntry = /* values here */
```

### `models.ToolExecutionEntry`

```python
value: models.ToolExecutionEntry = /* values here */
```

### `models.AgentHandoffEntry`

```python
value: models.AgentHandoffEntry = /* values here */
```
</file>

<file path="docs/models/eventout.md">
# EventOut


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `name`                                        | *str*                                         | :heavy_check_mark:                            | The name of the event.                        |
| `created_at`                                  | *int*                                         | :heavy_check_mark:                            | The UNIX timestamp (in seconds) of the event. |
| `data`                                        | Dict[str, *Any*]                              | :heavy_minus_sign:                            | N/A                                           |
</file>

<file path="docs/models/file.md">
# File


## Fields

| Field                                        | Type                                         | Required                                     | Description                                  |
| -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- |
| `file_name`                                  | *str*                                        | :heavy_check_mark:                           | N/A                                          |
| `content`                                    | *Union[bytes, IO[bytes], io.BufferedReader]* | :heavy_check_mark:                           | N/A                                          |
| `content_type`                               | *Optional[str]*                              | :heavy_minus_sign:                           | N/A                                          |
</file>

<file path="docs/models/filepurpose.md">
# FilePurpose


## Values

| Name        | Value       |
| ----------- | ----------- |
| `FINE_TUNE` | fine-tune   |
| `BATCH`     | batch       |
| `OCR`       | ocr         |
</file>

<file path="docs/models/filesapiroutesdeletefilerequest.md">
# FilesAPIRoutesDeleteFileRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `file_id`          | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/filesapiroutesdownloadfilerequest.md">
# FilesAPIRoutesDownloadFileRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `file_id`          | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/filesapiroutesgetsignedurlrequest.md">
# FilesAPIRoutesGetSignedURLRequest


## Fields

| Field                                                           | Type                                                            | Required                                                        | Description                                                     |
| --------------------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------------------- |
| `file_id`                                                       | *str*                                                           | :heavy_check_mark:                                              | N/A                                                             |
| `expiry`                                                        | *Optional[int]*                                                 | :heavy_minus_sign:                                              | Number of hours before the url becomes invalid. Defaults to 24h |
</file>

<file path="docs/models/filesapirouteslistfilesrequest.md">
# FilesAPIRoutesListFilesRequest


## Fields

| Field                                                            | Type                                                             | Required                                                         | Description                                                      |
| ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| `page`                                                           | *Optional[int]*                                                  | :heavy_minus_sign:                                               | N/A                                                              |
| `page_size`                                                      | *Optional[int]*                                                  | :heavy_minus_sign:                                               | N/A                                                              |
| `sample_type`                                                    | List[[models.SampleType](../models/sampletype.md)]               | :heavy_minus_sign:                                               | N/A                                                              |
| `source`                                                         | List[[models.Source](../models/source.md)]                       | :heavy_minus_sign:                                               | N/A                                                              |
| `search`                                                         | *OptionalNullable[str]*                                          | :heavy_minus_sign:                                               | N/A                                                              |
| `purpose`                                                        | [OptionalNullable[models.FilePurpose]](../models/filepurpose.md) | :heavy_minus_sign:                                               | N/A                                                              |
</file>

<file path="docs/models/filesapiroutesretrievefilerequest.md">
# FilesAPIRoutesRetrieveFileRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `file_id`          | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/filesapiroutesuploadfilemultipartbodyparams.md">
# FilesAPIRoutesUploadFileMultiPartBodyParams


## Fields

| Field                                                                                                                                                                                                                                                                                                               | Type                                                                                                                                                                                                                                                                                                                | Required                                                                                                                                                                                                                                                                                                            | Description                                                                                                                                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `file`                                                                                                                                                                                                                                                                                                              | [models.File](../models/file.md)                                                                                                                                                                                                                                                                                    | :heavy_check_mark:                                                                                                                                                                                                                                                                                                  | The File object (not file name) to be uploaded.<br/> To upload a file and specify a custom file name you should format your request as such:<br/> ```bash<br/> file=@path/to/your/file.jsonl;filename=custom_name.jsonl<br/> ```<br/> Otherwise, you can just keep the original file name:<br/> ```bash<br/> file=@path/to/your/file.jsonl<br/> ``` |
| `purpose`                                                                                                                                                                                                                                                                                                           | [Optional[models.FilePurpose]](../models/filepurpose.md)                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                  | N/A                                                                                                                                                                                                                                                                                                                 |
</file>

<file path="docs/models/fileschema.md">
# FileSchema


## Fields

| Field                                          | Type                                           | Required                                       | Description                                    | Example                                        |
| ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| `id`                                           | *str*                                          | :heavy_check_mark:                             | The unique identifier of the file.             | 497f6eca-6276-4993-bfeb-53cbbbba6f09           |
| `object`                                       | *str*                                          | :heavy_check_mark:                             | The object type, which is always "file".       | file                                           |
| `size_bytes`                                   | *int*                                          | :heavy_check_mark:                             | The size of the file, in bytes.                | 13000                                          |
| `created_at`                                   | *int*                                          | :heavy_check_mark:                             | The UNIX timestamp (in seconds) of the event.  | 1716963433                                     |
| `filename`                                     | *str*                                          | :heavy_check_mark:                             | The name of the uploaded file.                 | files_upload.jsonl                             |
| `purpose`                                      | [models.FilePurpose](../models/filepurpose.md) | :heavy_check_mark:                             | N/A                                            |                                                |
| `sample_type`                                  | [models.SampleType](../models/sampletype.md)   | :heavy_check_mark:                             | N/A                                            |                                                |
| `source`                                       | [models.Source](../models/source.md)           | :heavy_check_mark:                             | N/A                                            |                                                |
| `num_lines`                                    | *OptionalNullable[int]*                        | :heavy_minus_sign:                             | N/A                                            |                                                |
</file>

<file path="docs/models/filesignedurl.md">
# FileSignedURL


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `url`              | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/fimcompletionrequest.md">
# FIMCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionRequestStop]](../models/fimcompletionrequeststop.md)                                                                                                                                                                                                                                                                                                                    | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="docs/models/fimcompletionrequeststop.md">
# FIMCompletionRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/fimcompletionresponse.md">
# FIMCompletionResponse


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            | Example                                                                |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `id`                                                                   | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | cmpl-e5cc70bb28c444948073e77776eb30ef                                  |
| `object`                                                               | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | chat.completion                                                        |
| `model`                                                                | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | codestral-latest                                                       |
| `usage`                                                                | [models.UsageInfo](../models/usageinfo.md)                             | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
| `created`                                                              | *int*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | 1702256327                                                             |
| `choices`                                                              | List[[models.ChatCompletionChoice](../models/chatcompletionchoice.md)] | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
</file>

<file path="docs/models/fimcompletionstreamrequest.md">
# FIMCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionStreamRequestStop]](../models/fimcompletionstreamrequeststop.md)                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="docs/models/fimcompletionstreamrequeststop.md">
# FIMCompletionStreamRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/finetuneablemodeltype.md">
# FineTuneableModelType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `COMPLETION` | completion   |
| `CLASSIFIER` | classifier   |
</file>

<file path="docs/models/finishreason.md">
# FinishReason


## Values

| Name           | Value          |
| -------------- | -------------- |
| `STOP`         | stop           |
| `LENGTH`       | length         |
| `MODEL_LENGTH` | model_length   |
| `ERROR`        | error          |
| `TOOL_CALLS`   | tool_calls     |
</file>

<file path="docs/models/ftclassifierlossfunction.md">
# FTClassifierLossFunction


## Values

| Name           | Value          |
| -------------- | -------------- |
| `SINGLE_CLASS` | single_class   |
| `MULTI_CLASS`  | multi_class    |
</file>

<file path="docs/models/ftmodelcapabilitiesout.md">
# FTModelCapabilitiesOut


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `completion_chat`  | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `completion_fim`   | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `function_calling` | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `fine_tuning`      | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `classification`   | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/ftmodelcard.md">
# FTModelCard

Extra fields for fine-tuned models.


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `id`                                                                 | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `capabilities`                                                       | [models.ModelCapabilities](../models/modelcapabilities.md)           | :heavy_check_mark:                                                   | N/A                                                                  |
| `job`                                                                | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `root`                                                               | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `object`                                                             | *Optional[str]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created`                                                            | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `owned_by`                                                           | *Optional[str]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `name`                                                               | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `description`                                                        | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `max_context_length`                                                 | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `aliases`                                                            | List[*str*]                                                          | :heavy_minus_sign:                                                   | N/A                                                                  |
| `deprecation`                                                        | [date](https://docs.python.org/3/library/datetime.html#date-objects) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `default_model_temperature`                                          | *OptionalNullable[float]*                                            | :heavy_minus_sign:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.FTModelCardType]](../models/ftmodelcardtype.md)     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `archived`                                                           | *Optional[bool]*                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/ftmodelcardtype.md">
# FTModelCardType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `FINE_TUNED` | fine-tuned   |
</file>

<file path="docs/models/function.md">
# Function


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
| `parameters`       | Dict[str, *Any*]   | :heavy_check_mark: | N/A                |
| `description`      | *Optional[str]*    | :heavy_minus_sign: | N/A                |
| `strict`           | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/functioncall.md">
# FunctionCall


## Fields

| Field                                      | Type                                       | Required                                   | Description                                |
| ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ |
| `name`                                     | *str*                                      | :heavy_check_mark:                         | N/A                                        |
| `arguments`                                | [models.Arguments](../models/arguments.md) | :heavy_check_mark:                         | N/A                                        |
</file>

<file path="docs/models/functioncallentry.md">
# FunctionCallEntry


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `tool_call_id`                                                                   | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `name`                                                                           | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `arguments`                                                                      | [models.FunctionCallEntryArguments](../models/functioncallentryarguments.md)     | :heavy_check_mark:                                                               | N/A                                                                              |
| `object`                                                                         | [Optional[models.FunctionCallEntryObject]](../models/functioncallentryobject.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
| `type`                                                                           | [Optional[models.FunctionCallEntryType]](../models/functioncallentrytype.md)     | :heavy_minus_sign:                                                               | N/A                                                                              |
| `created_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_minus_sign:                                                               | N/A                                                                              |
| `completed_at`                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_minus_sign:                                                               | N/A                                                                              |
| `id`                                                                             | *Optional[str]*                                                                  | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/functioncallentryarguments.md">
# FunctionCallEntryArguments


## Supported Types

### `Dict[str, Any]`

```python
value: Dict[str, Any] = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="docs/models/functioncallentryobject.md">
# FunctionCallEntryObject


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/functioncallentrytype.md">
# FunctionCallEntryType


## Values

| Name            | Value           |
| --------------- | --------------- |
| `FUNCTION_CALL` | function.call   |
</file>

<file path="docs/models/functioncallevent.md">
# FunctionCallEvent


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `id`                                                                         | *str*                                                                        | :heavy_check_mark:                                                           | N/A                                                                          |
| `name`                                                                       | *str*                                                                        | :heavy_check_mark:                                                           | N/A                                                                          |
| `tool_call_id`                                                               | *str*                                                                        | :heavy_check_mark:                                                           | N/A                                                                          |
| `arguments`                                                                  | *str*                                                                        | :heavy_check_mark:                                                           | N/A                                                                          |
| `type`                                                                       | [Optional[models.FunctionCallEventType]](../models/functioncalleventtype.md) | :heavy_minus_sign:                                                           | N/A                                                                          |
| `created_at`                                                                 | [date](https://docs.python.org/3/library/datetime.html#date-objects)         | :heavy_minus_sign:                                                           | N/A                                                                          |
| `output_index`                                                               | *Optional[int]*                                                              | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="docs/models/functioncalleventtype.md">
# FunctionCallEventType


## Values

| Name                  | Value                 |
| --------------------- | --------------------- |
| `FUNCTION_CALL_DELTA` | function.call.delta   |
</file>

<file path="docs/models/functionname.md">
# FunctionName

this restriction of `Function` is used to select a specific function to call


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/functionresultentry.md">
# FunctionResultEntry


## Fields

| Field                                                                                | Type                                                                                 | Required                                                                             | Description                                                                          |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ |
| `tool_call_id`                                                                       | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `result`                                                                             | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `object`                                                                             | [Optional[models.FunctionResultEntryObject]](../models/functionresultentryobject.md) | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `type`                                                                               | [Optional[models.FunctionResultEntryType]](../models/functionresultentrytype.md)     | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `created_at`                                                                         | [date](https://docs.python.org/3/library/datetime.html#date-objects)                 | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `completed_at`                                                                       | [date](https://docs.python.org/3/library/datetime.html#date-objects)                 | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `id`                                                                                 | *Optional[str]*                                                                      | :heavy_minus_sign:                                                                   | N/A                                                                                  |
</file>

<file path="docs/models/functionresultentryobject.md">
# FunctionResultEntryObject


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/functionresultentrytype.md">
# FunctionResultEntryType


## Values

| Name              | Value             |
| ----------------- | ----------------- |
| `FUNCTION_RESULT` | function.result   |
</file>

<file path="docs/models/functiontool.md">
# FunctionTool


## Fields

| Field                                                              | Type                                                               | Required                                                           | Description                                                        |
| ------------------------------------------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------------------ |
| `function`                                                         | [models.Function](../models/function.md)                           | :heavy_check_mark:                                                 | N/A                                                                |
| `type`                                                             | [Optional[models.FunctionToolType]](../models/functiontooltype.md) | :heavy_minus_sign:                                                 | N/A                                                                |
</file>

<file path="docs/models/functiontooltype.md">
# FunctionToolType


## Values

| Name       | Value      |
| ---------- | ---------- |
| `FUNCTION` | function   |
</file>

<file path="docs/models/githubrepositoryin.md">
# GithubRepositoryIn


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `name`                                                                         | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `owner`                                                                        | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `token`                                                                        | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `type`                                                                         | [Optional[models.GithubRepositoryInType]](../models/githubrepositoryintype.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
| `ref`                                                                          | *OptionalNullable[str]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `weight`                                                                       | *Optional[float]*                                                              | :heavy_minus_sign:                                                             | N/A                                                                            |
</file>

<file path="docs/models/githubrepositoryintype.md">
# GithubRepositoryInType


## Values

| Name     | Value    |
| -------- | -------- |
| `GITHUB` | github   |
</file>

<file path="docs/models/githubrepositoryout.md">
# GithubRepositoryOut


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `name`                                                                           | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `owner`                                                                          | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `commit_id`                                                                      | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `type`                                                                           | [Optional[models.GithubRepositoryOutType]](../models/githubrepositoryouttype.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
| `ref`                                                                            | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | N/A                                                                              |
| `weight`                                                                         | *Optional[float]*                                                                | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/githubrepositoryouttype.md">
# GithubRepositoryOutType


## Values

| Name     | Value    |
| -------- | -------- |
| `GITHUB` | github   |
</file>

<file path="docs/models/handoffexecution.md">
# HandoffExecution


## Values

| Name     | Value    |
| -------- | -------- |
| `CLIENT` | client   |
| `SERVER` | server   |
</file>

<file path="docs/models/httpvalidationerror.md">
# HTTPValidationError


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `detail`                                                     | List[[models.ValidationError](../models/validationerror.md)] | :heavy_minus_sign:                                           | N/A                                                          |
</file>

<file path="docs/models/hyperparameters.md">
# Hyperparameters


## Supported Types

### `models.CompletionTrainingParametersIn`

```python
value: models.CompletionTrainingParametersIn = /* values here */
```

### `models.ClassifierTrainingParametersIn`

```python
value: models.ClassifierTrainingParametersIn = /* values here */
```
</file>

<file path="docs/models/imagegenerationtool.md">
# ImageGenerationTool


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `type`                                                                           | [Optional[models.ImageGenerationToolType]](../models/imagegenerationtooltype.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/imagegenerationtooltype.md">
# ImageGenerationToolType


## Values

| Name               | Value              |
| ------------------ | ------------------ |
| `IMAGE_GENERATION` | image_generation   |
</file>

<file path="docs/models/imageurl.md">
# ImageURL


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `url`                   | *str*                   | :heavy_check_mark:      | N/A                     |
| `detail`                | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
</file>

<file path="docs/models/imageurlchunk.md">
# ImageURLChunk

{"type":"image_url","image_url":{"url":"data:image/png;base64,iVBORw0


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `image_url`                                                          | [models.ImageURLChunkImageURL](../models/imageurlchunkimageurl.md)   | :heavy_check_mark:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.ImageURLChunkType]](../models/imageurlchunktype.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/imageurlchunkimageurl.md">
# ImageURLChunkImageURL


## Supported Types

### `models.ImageURL`

```python
value: models.ImageURL = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="docs/models/imageurlchunktype.md">
# ImageURLChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `IMAGE_URL` | image_url   |
</file>

<file path="docs/models/inputentries.md">
# InputEntries


## Supported Types

### `models.MessageInputEntry`

```python
value: models.MessageInputEntry = /* values here */
```

### `models.FunctionResultEntry`

```python
value: models.FunctionResultEntry = /* values here */
```
</file>

<file path="docs/models/inputs.md">
# Inputs

Chat to classify


## Supported Types

### `models.InstructRequestInputs`

```python
value: models.InstructRequestInputs = /* values here */
```

### `List[models.InstructRequest]`

```python
value: List[models.InstructRequest] = /* values here */
```
</file>

<file path="docs/models/instructrequest.md">
# InstructRequest


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `messages`                                                                   | List[[models.InstructRequestMessages](../models/instructrequestmessages.md)] | :heavy_check_mark:                                                           | N/A                                                                          |
</file>

<file path="docs/models/instructrequestinputs.md">
# InstructRequestInputs


## Fields

| Field                                                                                    | Type                                                                                     | Required                                                                                 | Description                                                                              |
| ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| `messages`                                                                               | List[[models.InstructRequestInputsMessages](../models/instructrequestinputsmessages.md)] | :heavy_check_mark:                                                                       | N/A                                                                                      |
</file>

<file path="docs/models/instructrequestinputsmessages.md">
# InstructRequestInputsMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/instructrequestmessages.md">
# InstructRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/integrations.md">
# Integrations


## Supported Types

### `models.WandbIntegrationOut`

```python
value: models.WandbIntegrationOut = /* values here */
```
</file>

<file path="docs/models/jobin.md">
# JobIn


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                             | Type                                                                                                                                                                                                                                                                                                                                                              | Required                                                                                                                                                                                                                                                                                                                                                          | Description                                                                                                                                                                                                                                                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                           | *str*                                                                                                                                                                                                                                                                                                                                                             | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                | The name of the model to fine-tune.                                                                                                                                                                                                                                                                                                                               |
| `hyperparameters`                                                                                                                                                                                                                                                                                                                                                 | [models.Hyperparameters](../models/hyperparameters.md)                                                                                                                                                                                                                                                                                                            | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `training_files`                                                                                                                                                                                                                                                                                                                                                  | List[[models.TrainingFile](../models/trainingfile.md)]                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `validation_files`                                                                                                                                                                                                                                                                                                                                                | List[*str*]                                                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files. |
| `suffix`                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A string that will be added to your fine-tuning model name. For example, a suffix of "my-great-model" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`                                                                                                                                                                                  |
| `integrations`                                                                                                                                                                                                                                                                                                                                                    | List[[models.JobInIntegrations](../models/jobinintegrations.md)]                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A list of integrations to enable for your fine-tuning job.                                                                                                                                                                                                                                                                                                        |
| `auto_start`                                                                                                                                                                                                                                                                                                                                                      | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | This field will be required in a future release.                                                                                                                                                                                                                                                                                                                  |
| `invalid_sample_skip_percentage`                                                                                                                                                                                                                                                                                                                                  | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `job_type`                                                                                                                                                                                                                                                                                                                                                        | [OptionalNullable[models.FineTuneableModelType]](../models/finetuneablemodeltype.md)                                                                                                                                                                                                                                                                              | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `repositories`                                                                                                                                                                                                                                                                                                                                                    | List[[models.JobInRepositories](../models/jobinrepositories.md)]                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `classifier_targets`                                                                                                                                                                                                                                                                                                                                              | List[[models.ClassifierTargetIn](../models/classifiertargetin.md)]                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
</file>

<file path="docs/models/jobinintegrations.md">
# JobInIntegrations


## Supported Types

### `models.WandbIntegration`

```python
value: models.WandbIntegration = /* values here */
```
</file>

<file path="docs/models/jobinrepositories.md">
# JobInRepositories


## Supported Types

### `models.GithubRepositoryIn`

```python
value: models.GithubRepositoryIn = /* values here */
```
</file>

<file path="docs/models/jobmetadataout.md">
# JobMetadataOut


## Fields

| Field                       | Type                        | Required                    | Description                 |
| --------------------------- | --------------------------- | --------------------------- | --------------------------- |
| `expected_duration_seconds` | *OptionalNullable[int]*     | :heavy_minus_sign:          | N/A                         |
| `cost`                      | *OptionalNullable[float]*   | :heavy_minus_sign:          | N/A                         |
| `cost_currency`             | *OptionalNullable[str]*     | :heavy_minus_sign:          | N/A                         |
| `train_tokens_per_step`     | *OptionalNullable[int]*     | :heavy_minus_sign:          | N/A                         |
| `train_tokens`              | *OptionalNullable[int]*     | :heavy_minus_sign:          | N/A                         |
| `data_tokens`               | *OptionalNullable[int]*     | :heavy_minus_sign:          | N/A                         |
| `estimated_start_time`      | *OptionalNullable[int]*     | :heavy_minus_sign:          | N/A                         |
</file>

<file path="docs/models/jobsapiroutesbatchcancelbatchjobrequest.md">
# JobsAPIRoutesBatchCancelBatchJobRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `job_id`           | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/jobsapiroutesbatchgetbatchjobrequest.md">
# JobsAPIRoutesBatchGetBatchJobRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `job_id`           | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/jobsapiroutesbatchgetbatchjobsrequest.md">
# JobsAPIRoutesBatchGetBatchJobsRequest


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `page`                                                               | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `page_size`                                                          | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `model`                                                              | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `metadata`                                                           | Dict[str, *Any*]                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created_after`                                                      | [date](https://docs.python.org/3/library/datetime.html#date-objects) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created_by_me`                                                      | *Optional[bool]*                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `status`                                                             | List[[models.BatchJobStatus](../models/batchjobstatus.md)]           | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/jobsapiroutesfinetuningarchivefinetunedmodelrequest.md">
# JobsAPIRoutesFineTuningArchiveFineTunedModelRequest


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   | Example                                       |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `model_id`                                    | *str*                                         | :heavy_check_mark:                            | The ID of the model to archive.               | ft:open-mistral-7b:587a6b29:20240514:7e773925 |
</file>

<file path="docs/models/jobsapiroutesfinetuningcancelfinetuningjobrequest.md">
# JobsAPIRoutesFineTuningCancelFineTuningJobRequest


## Fields

| Field                        | Type                         | Required                     | Description                  |
| ---------------------------- | ---------------------------- | ---------------------------- | ---------------------------- |
| `job_id`                     | *str*                        | :heavy_check_mark:           | The ID of the job to cancel. |
</file>

<file path="docs/models/jobsapiroutesfinetuningcancelfinetuningjobresponse.md">
# JobsAPIRoutesFineTuningCancelFineTuningJobResponse

OK


## Supported Types

### `models.ClassifierDetailedJobOut`

```python
value: models.ClassifierDetailedJobOut = /* values here */
```

### `models.CompletionDetailedJobOut`

```python
value: models.CompletionDetailedJobOut = /* values here */
```
</file>

<file path="docs/models/jobsapiroutesfinetuningcreatefinetuningjobresponse.md">
# JobsAPIRoutesFineTuningCreateFineTuningJobResponse

OK


## Supported Types

### `models.Response1`

```python
value: models.Response1 = /* values here */
```

### `models.LegacyJobMetadataOut`

```python
value: models.LegacyJobMetadataOut = /* values here */
```
</file>

<file path="docs/models/jobsapiroutesfinetuninggetfinetuningjobrequest.md">
# JobsAPIRoutesFineTuningGetFineTuningJobRequest


## Fields

| Field                         | Type                          | Required                      | Description                   |
| ----------------------------- | ----------------------------- | ----------------------------- | ----------------------------- |
| `job_id`                      | *str*                         | :heavy_check_mark:            | The ID of the job to analyse. |
</file>

<file path="docs/models/jobsapiroutesfinetuninggetfinetuningjobresponse.md">
# JobsAPIRoutesFineTuningGetFineTuningJobResponse

OK


## Supported Types

### `models.ClassifierDetailedJobOut`

```python
value: models.ClassifierDetailedJobOut = /* values here */
```

### `models.CompletionDetailedJobOut`

```python
value: models.CompletionDetailedJobOut = /* values here */
```
</file>

<file path="docs/models/jobsapiroutesfinetuninggetfinetuningjobsrequest.md">
# JobsAPIRoutesFineTuningGetFineTuningJobsRequest


## Fields

| Field                                                                                              | Type                                                                                               | Required                                                                                           | Description                                                                                        |
| -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `page`                                                                                             | *Optional[int]*                                                                                    | :heavy_minus_sign:                                                                                 | The page number of the results to be returned.                                                     |
| `page_size`                                                                                        | *Optional[int]*                                                                                    | :heavy_minus_sign:                                                                                 | The number of items to return per page.                                                            |
| `model`                                                                                            | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The model name used for fine-tuning to filter on. When set, the other results are not displayed.   |
| `created_after`                                                                                    | [date](https://docs.python.org/3/library/datetime.html#date-objects)                               | :heavy_minus_sign:                                                                                 | The date/time to filter on. When set, the results for previous creation times are not displayed.   |
| `created_before`                                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)                               | :heavy_minus_sign:                                                                                 | N/A                                                                                                |
| `created_by_me`                                                                                    | *Optional[bool]*                                                                                   | :heavy_minus_sign:                                                                                 | When set, only return results for jobs created by the API caller. Other results are not displayed. |
| `status`                                                                                           | [OptionalNullable[models.QueryParamStatus]](../models/queryparamstatus.md)                         | :heavy_minus_sign:                                                                                 | The current job state to filter on. When set, the other results are not displayed.                 |
| `wandb_project`                                                                                    | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The Weights and Biases project to filter on. When set, the other results are not displayed.        |
| `wandb_name`                                                                                       | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The Weight and Biases run name to filter on. When set, the other results are not displayed.        |
| `suffix`                                                                                           | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The model suffix to filter on. When set, the other results are not displayed.                      |
</file>

<file path="docs/models/jobsapiroutesfinetuningstartfinetuningjobrequest.md">
# JobsAPIRoutesFineTuningStartFineTuningJobRequest


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `job_id`           | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="docs/models/jobsapiroutesfinetuningstartfinetuningjobresponse.md">
# JobsAPIRoutesFineTuningStartFineTuningJobResponse

OK


## Supported Types

### `models.ClassifierDetailedJobOut`

```python
value: models.ClassifierDetailedJobOut = /* values here */
```

### `models.CompletionDetailedJobOut`

```python
value: models.CompletionDetailedJobOut = /* values here */
```
</file>

<file path="docs/models/jobsapiroutesfinetuningunarchivefinetunedmodelrequest.md">
# JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   | Example                                       |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `model_id`                                    | *str*                                         | :heavy_check_mark:                            | The ID of the model to unarchive.             | ft:open-mistral-7b:587a6b29:20240514:7e773925 |
</file>

<file path="docs/models/jobsapiroutesfinetuningupdatefinetunedmodelrequest.md">
# JobsAPIRoutesFineTuningUpdateFineTunedModelRequest


## Fields

| Field                                                  | Type                                                   | Required                                               | Description                                            | Example                                                |
| ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ |
| `model_id`                                             | *str*                                                  | :heavy_check_mark:                                     | The ID of the model to update.                         | ft:open-mistral-7b:587a6b29:20240514:7e773925          |
| `update_ft_model_in`                                   | [models.UpdateFTModelIn](../models/updateftmodelin.md) | :heavy_check_mark:                                     | N/A                                                    |                                                        |
</file>

<file path="docs/models/jobsapiroutesfinetuningupdatefinetunedmodelresponse.md">
# JobsAPIRoutesFineTuningUpdateFineTunedModelResponse

OK


## Supported Types

### `models.ClassifierFTModelOut`

```python
value: models.ClassifierFTModelOut = /* values here */
```

### `models.CompletionFTModelOut`

```python
value: models.CompletionFTModelOut = /* values here */
```
</file>

<file path="docs/models/jobsout.md">
# JobsOut


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `total`                                                      | *int*                                                        | :heavy_check_mark:                                           | N/A                                                          |
| `data`                                                       | List[[models.JobsOutData](../models/jobsoutdata.md)]         | :heavy_minus_sign:                                           | N/A                                                          |
| `object`                                                     | [Optional[models.JobsOutObject]](../models/jobsoutobject.md) | :heavy_minus_sign:                                           | N/A                                                          |
</file>

<file path="docs/models/jobsoutdata.md">
# JobsOutData


## Supported Types

### `models.ClassifierJobOut`

```python
value: models.ClassifierJobOut = /* values here */
```

### `models.CompletionJobOut`

```python
value: models.CompletionJobOut = /* values here */
```
</file>

<file path="docs/models/jobsoutobject.md">
# JobsOutObject


## Values

| Name   | Value  |
| ------ | ------ |
| `LIST` | list   |
</file>

<file path="docs/models/jobtype.md">
# JobType

The type of job (`FT` for fine-tuning).


## Values

| Name         | Value        |
| ------------ | ------------ |
| `COMPLETION` | completion   |
</file>

<file path="docs/models/jsonschema.md">
# JSONSchema


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `name`                  | *str*                   | :heavy_check_mark:      | N/A                     |
| `schema_definition`     | Dict[str, *Any*]        | :heavy_check_mark:      | N/A                     |
| `description`           | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
| `strict`                | *Optional[bool]*        | :heavy_minus_sign:      | N/A                     |
</file>

<file path="docs/models/legacyjobmetadataout.md">
# LegacyJobMetadataOut


## Fields

| Field                                                                                                                                                                                                                           | Type                                                                                                                                                                                                                            | Required                                                                                                                                                                                                                        | Description                                                                                                                                                                                                                     | Example                                                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `details`                                                                                                                                                                                                                       | *str*                                                                                                                                                                                                                           | :heavy_check_mark:                                                                                                                                                                                                              | N/A                                                                                                                                                                                                                             |                                                                                                                                                                                                                                 |
| `expected_duration_seconds`                                                                                                                                                                                                     | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The approximated time (in seconds) for the fine-tuning process to complete.                                                                                                                                                     | 220                                                                                                                                                                                                                             |
| `cost`                                                                                                                                                                                                                          | *OptionalNullable[float]*                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                              | The cost of the fine-tuning job.                                                                                                                                                                                                | 10                                                                                                                                                                                                                              |
| `cost_currency`                                                                                                                                                                                                                 | *OptionalNullable[str]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The currency used for the fine-tuning job cost.                                                                                                                                                                                 | EUR                                                                                                                                                                                                                             |
| `train_tokens_per_step`                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The number of tokens consumed by one training step.                                                                                                                                                                             | 131072                                                                                                                                                                                                                          |
| `train_tokens`                                                                                                                                                                                                                  | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The total number of tokens used during the fine-tuning process.                                                                                                                                                                 | 1310720                                                                                                                                                                                                                         |
| `data_tokens`                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The total number of tokens in the training dataset.                                                                                                                                                                             | 305375                                                                                                                                                                                                                          |
| `estimated_start_time`                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | N/A                                                                                                                                                                                                                             |                                                                                                                                                                                                                                 |
| `deprecated`                                                                                                                                                                                                                    | *Optional[bool]*                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                              | N/A                                                                                                                                                                                                                             |                                                                                                                                                                                                                                 |
| `epochs`                                                                                                                                                                                                                        | *OptionalNullable[float]*                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                              | The number of complete passes through the entire training dataset.                                                                                                                                                              | 4.2922                                                                                                                                                                                                                          |
| `training_steps`                                                                                                                                                                                                                | *OptionalNullable[int]*                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                              | The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset. | 10                                                                                                                                                                                                                              |
| `object`                                                                                                                                                                                                                        | [Optional[models.LegacyJobMetadataOutObject]](../models/legacyjobmetadataoutobject.md)                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                              | N/A                                                                                                                                                                                                                             |                                                                                                                                                                                                                                 |
</file>

<file path="docs/models/legacyjobmetadataoutobject.md">
# LegacyJobMetadataOutObject


## Values

| Name           | Value          |
| -------------- | -------------- |
| `JOB_METADATA` | job.metadata   |
</file>

<file path="docs/models/listfilesout.md">
# ListFilesOut


## Fields

| Field                                              | Type                                               | Required                                           | Description                                        |
| -------------------------------------------------- | -------------------------------------------------- | -------------------------------------------------- | -------------------------------------------------- |
| `data`                                             | List[[models.FileSchema](../models/fileschema.md)] | :heavy_check_mark:                                 | N/A                                                |
| `object`                                           | *str*                                              | :heavy_check_mark:                                 | N/A                                                |
| `total`                                            | *int*                                              | :heavy_check_mark:                                 | N/A                                                |
</file>

<file path="docs/models/loc.md">
# Loc


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `int`

```python
value: int = /* values here */
```
</file>

<file path="docs/models/messageentries.md">
# MessageEntries


## Supported Types

### `models.MessageInputEntry`

```python
value: models.MessageInputEntry = /* values here */
```

### `models.MessageOutputEntry`

```python
value: models.MessageOutputEntry = /* values here */
```
</file>

<file path="docs/models/messageinputcontentchunks.md">
# MessageInputContentChunks


## Supported Types

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.ToolFileChunk`

```python
value: models.ToolFileChunk = /* values here */
```

### `models.DocumentURLChunk`

```python
value: models.DocumentURLChunk = /* values here */
```
</file>

<file path="docs/models/messageinputentry.md">
# MessageInputEntry

Representation of an input message inside the conversation.


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `role`                                                                       | [models.MessageInputEntryRole](../models/messageinputentryrole.md)           | :heavy_check_mark:                                                           | N/A                                                                          |
| `content`                                                                    | [models.MessageInputEntryContent](../models/messageinputentrycontent.md)     | :heavy_check_mark:                                                           | N/A                                                                          |
| `object`                                                                     | [Optional[models.Object]](../models/object.md)                               | :heavy_minus_sign:                                                           | N/A                                                                          |
| `type`                                                                       | [Optional[models.MessageInputEntryType]](../models/messageinputentrytype.md) | :heavy_minus_sign:                                                           | N/A                                                                          |
| `created_at`                                                                 | [date](https://docs.python.org/3/library/datetime.html#date-objects)         | :heavy_minus_sign:                                                           | N/A                                                                          |
| `completed_at`                                                               | [date](https://docs.python.org/3/library/datetime.html#date-objects)         | :heavy_minus_sign:                                                           | N/A                                                                          |
| `id`                                                                         | *Optional[str]*                                                              | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="docs/models/messageinputentrycontent.md">
# MessageInputEntryContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.MessageInputContentChunks]`

```python
value: List[models.MessageInputContentChunks] = /* values here */
```
</file>

<file path="docs/models/messageinputentryrole.md">
# MessageInputEntryRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
| `USER`      | user        |
</file>

<file path="docs/models/messageinputentrytype.md">
# MessageInputEntryType


## Values

| Name            | Value           |
| --------------- | --------------- |
| `MESSAGE_INPUT` | message.input   |
</file>

<file path="docs/models/messageoutputcontentchunks.md">
# MessageOutputContentChunks


## Supported Types

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.ToolFileChunk`

```python
value: models.ToolFileChunk = /* values here */
```

### `models.DocumentURLChunk`

```python
value: models.DocumentURLChunk = /* values here */
```

### `models.ToolReferenceChunk`

```python
value: models.ToolReferenceChunk = /* values here */
```
</file>

<file path="docs/models/messageoutputentry.md">
# MessageOutputEntry


## Fields

| Field                                                                              | Type                                                                               | Required                                                                           | Description                                                                        |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| `content`                                                                          | [models.MessageOutputEntryContent](../models/messageoutputentrycontent.md)         | :heavy_check_mark:                                                                 | N/A                                                                                |
| `object`                                                                           | [Optional[models.MessageOutputEntryObject]](../models/messageoutputentryobject.md) | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `type`                                                                             | [Optional[models.MessageOutputEntryType]](../models/messageoutputentrytype.md)     | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `created_at`                                                                       | [date](https://docs.python.org/3/library/datetime.html#date-objects)               | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `completed_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)               | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `id`                                                                               | *Optional[str]*                                                                    | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `agent_id`                                                                         | *OptionalNullable[str]*                                                            | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `model`                                                                            | *OptionalNullable[str]*                                                            | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `role`                                                                             | [Optional[models.MessageOutputEntryRole]](../models/messageoutputentryrole.md)     | :heavy_minus_sign:                                                                 | N/A                                                                                |
</file>

<file path="docs/models/messageoutputentrycontent.md">
# MessageOutputEntryContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.MessageOutputContentChunks]`

```python
value: List[models.MessageOutputContentChunks] = /* values here */
```
</file>

<file path="docs/models/messageoutputentryobject.md">
# MessageOutputEntryObject


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/messageoutputentryrole.md">
# MessageOutputEntryRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
</file>

<file path="docs/models/messageoutputentrytype.md">
# MessageOutputEntryType


## Values

| Name             | Value            |
| ---------------- | ---------------- |
| `MESSAGE_OUTPUT` | message.output   |
</file>

<file path="docs/models/messageoutputevent.md">
# MessageOutputEvent


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `id`                                                                           | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `content`                                                                      | [models.MessageOutputEventContent](../models/messageoutputeventcontent.md)     | :heavy_check_mark:                                                             | N/A                                                                            |
| `type`                                                                         | [Optional[models.MessageOutputEventType]](../models/messageoutputeventtype.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
| `created_at`                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)           | :heavy_minus_sign:                                                             | N/A                                                                            |
| `output_index`                                                                 | *Optional[int]*                                                                | :heavy_minus_sign:                                                             | N/A                                                                            |
| `content_index`                                                                | *Optional[int]*                                                                | :heavy_minus_sign:                                                             | N/A                                                                            |
| `model`                                                                        | *OptionalNullable[str]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `agent_id`                                                                     | *OptionalNullable[str]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `role`                                                                         | [Optional[models.MessageOutputEventRole]](../models/messageoutputeventrole.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
</file>

<file path="docs/models/messageoutputeventcontent.md">
# MessageOutputEventContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `models.OutputContentChunks`

```python
value: models.OutputContentChunks = /* values here */
```
</file>

<file path="docs/models/messageoutputeventrole.md">
# MessageOutputEventRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
</file>

<file path="docs/models/messageoutputeventtype.md">
# MessageOutputEventType


## Values

| Name                   | Value                  |
| ---------------------- | ---------------------- |
| `MESSAGE_OUTPUT_DELTA` | message.output.delta   |
</file>

<file path="docs/models/messages.md">
# Messages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/metricout.md">
# MetricOut

Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase).


## Fields

| Field                       | Type                        | Required                    | Description                 |
| --------------------------- | --------------------------- | --------------------------- | --------------------------- |
| `train_loss`                | *OptionalNullable[float]*   | :heavy_minus_sign:          | N/A                         |
| `valid_loss`                | *OptionalNullable[float]*   | :heavy_minus_sign:          | N/A                         |
| `valid_mean_token_accuracy` | *OptionalNullable[float]*   | :heavy_minus_sign:          | N/A                         |
</file>

<file path="docs/models/mistralpromptmode.md">
# MistralPromptMode


## Values

| Name        | Value       |
| ----------- | ----------- |
| `REASONING` | reasoning   |
</file>

<file path="docs/models/modelcapabilities.md">
# ModelCapabilities


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `completion_chat`  | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `completion_fim`   | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `function_calling` | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `fine_tuning`      | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
| `vision`           | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/modelconversation.md">
# ModelConversation


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `id`                                                                             | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `created_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_check_mark:                                                               | N/A                                                                              |
| `updated_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)             | :heavy_check_mark:                                                               | N/A                                                                              |
| `model`                                                                          | *str*                                                                            | :heavy_check_mark:                                                               | N/A                                                                              |
| `instructions`                                                                   | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Instruction prompt the model will follow during the conversation.                |
| `tools`                                                                          | List[[models.ModelConversationTools](../models/modelconversationtools.md)]       | :heavy_minus_sign:                                                               | List of tools which are available to the model during the conversation.          |
| `completion_args`                                                                | [Optional[models.CompletionArgs]](../models/completionargs.md)                   | :heavy_minus_sign:                                                               | White-listed arguments from the completion API                                   |
| `name`                                                                           | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Name given to the conversation.                                                  |
| `description`                                                                    | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | Description of the what the conversation is about.                               |
| `object`                                                                         | [Optional[models.ModelConversationObject]](../models/modelconversationobject.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/modelconversationobject.md">
# ModelConversationObject


## Values

| Name           | Value          |
| -------------- | -------------- |
| `CONVERSATION` | conversation   |
</file>

<file path="docs/models/modelconversationtools.md">
# ModelConversationTools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/modellist.md">
# ModelList


## Fields

| Field                                  | Type                                   | Required                               | Description                            |
| -------------------------------------- | -------------------------------------- | -------------------------------------- | -------------------------------------- |
| `object`                               | *Optional[str]*                        | :heavy_minus_sign:                     | N/A                                    |
| `data`                                 | List[[models.Data](../models/data.md)] | :heavy_minus_sign:                     | N/A                                    |
</file>

<file path="docs/models/modeltype.md">
# ModelType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `COMPLETION` | completion   |
</file>

<file path="docs/models/moderationobject.md">
# ModerationObject


## Fields

| Field                        | Type                         | Required                     | Description                  |
| ---------------------------- | ---------------------------- | ---------------------------- | ---------------------------- |
| `categories`                 | Dict[str, *bool*]            | :heavy_minus_sign:           | Moderation result thresholds |
| `category_scores`            | Dict[str, *float*]           | :heavy_minus_sign:           | Moderation result            |
</file>

<file path="docs/models/moderationresponse.md">
# ModerationResponse


## Fields

| Field                                                          | Type                                                           | Required                                                       | Description                                                    | Example                                                        |
| -------------------------------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------- |
| `id`                                                           | *str*                                                          | :heavy_check_mark:                                             | N/A                                                            | mod-e5cc70bb28c444948073e77776eb30ef                           |
| `model`                                                        | *str*                                                          | :heavy_check_mark:                                             | N/A                                                            |                                                                |
| `results`                                                      | List[[models.ModerationObject](../models/moderationobject.md)] | :heavy_check_mark:                                             | N/A                                                            |                                                                |
</file>

<file path="docs/models/object.md">
# Object


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/ocrimageobject.md">
# OCRImageObject


## Fields

| Field                                                      | Type                                                       | Required                                                   | Description                                                |
| ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- |
| `id`                                                       | *str*                                                      | :heavy_check_mark:                                         | Image ID for extracted image in a page                     |
| `top_left_x`                                               | *Nullable[int]*                                            | :heavy_check_mark:                                         | X coordinate of top-left corner of the extracted image     |
| `top_left_y`                                               | *Nullable[int]*                                            | :heavy_check_mark:                                         | Y coordinate of top-left corner of the extracted image     |
| `bottom_right_x`                                           | *Nullable[int]*                                            | :heavy_check_mark:                                         | X coordinate of bottom-right corner of the extracted image |
| `bottom_right_y`                                           | *Nullable[int]*                                            | :heavy_check_mark:                                         | Y coordinate of bottom-right corner of the extracted image |
| `image_base64`                                             | *OptionalNullable[str]*                                    | :heavy_minus_sign:                                         | Base64 string of the extracted image                       |
| `image_annotation`                                         | *OptionalNullable[str]*                                    | :heavy_minus_sign:                                         | Annotation of the extracted image in json str              |
</file>

<file path="docs/models/ocrpagedimensions.md">
# OCRPageDimensions


## Fields

| Field                           | Type                            | Required                        | Description                     |
| ------------------------------- | ------------------------------- | ------------------------------- | ------------------------------- |
| `dpi`                           | *int*                           | :heavy_check_mark:              | Dots per inch of the page-image |
| `height`                        | *int*                           | :heavy_check_mark:              | Height of the image in pixels   |
| `width`                         | *int*                           | :heavy_check_mark:              | Width of the image in pixels    |
</file>

<file path="docs/models/ocrpageobject.md">
# OCRPageObject


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `index`                                                              | *int*                                                                | :heavy_check_mark:                                                   | The page index in a pdf document starting from 0                     |
| `markdown`                                                           | *str*                                                                | :heavy_check_mark:                                                   | The markdown string response of the page                             |
| `images`                                                             | List[[models.OCRImageObject](../models/ocrimageobject.md)]           | :heavy_check_mark:                                                   | List of all extracted images in the page                             |
| `dimensions`                                                         | [Nullable[models.OCRPageDimensions]](../models/ocrpagedimensions.md) | :heavy_check_mark:                                                   | The dimensions of the PDF Page's screenshot image                    |
</file>

<file path="docs/models/ocrrequest.md">
# OCRRequest


## Fields

| Field                                                                                                                                                      | Type                                                                                                                                                       | Required                                                                                                                                                   | Description                                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                    | *Nullable[str]*                                                                                                                                            | :heavy_check_mark:                                                                                                                                         | N/A                                                                                                                                                        |
| `document`                                                                                                                                                 | [models.Document](../models/document.md)                                                                                                                   | :heavy_check_mark:                                                                                                                                         | Document to run OCR on                                                                                                                                     |
| `id`                                                                                                                                                       | *Optional[str]*                                                                                                                                            | :heavy_minus_sign:                                                                                                                                         | N/A                                                                                                                                                        |
| `pages`                                                                                                                                                    | List[*int*]                                                                                                                                                | :heavy_minus_sign:                                                                                                                                         | Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0                                              |
| `include_image_base64`                                                                                                                                     | *OptionalNullable[bool]*                                                                                                                                   | :heavy_minus_sign:                                                                                                                                         | Include image URLs in response                                                                                                                             |
| `image_limit`                                                                                                                                              | *OptionalNullable[int]*                                                                                                                                    | :heavy_minus_sign:                                                                                                                                         | Max images to extract                                                                                                                                      |
| `image_min_size`                                                                                                                                           | *OptionalNullable[int]*                                                                                                                                    | :heavy_minus_sign:                                                                                                                                         | Minimum height and width of image to extract                                                                                                               |
| `bbox_annotation_format`                                                                                                                                   | [OptionalNullable[models.ResponseFormat]](../models/responseformat.md)                                                                                     | :heavy_minus_sign:                                                                                                                                         | Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field |
| `document_annotation_format`                                                                                                                               | [OptionalNullable[models.ResponseFormat]](../models/responseformat.md)                                                                                     | :heavy_minus_sign:                                                                                                                                         | Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field                               |
</file>

<file path="docs/models/ocrresponse.md">
# OCRResponse


## Fields

| Field                                                            | Type                                                             | Required                                                         | Description                                                      |
| ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| `pages`                                                          | List[[models.OCRPageObject](../models/ocrpageobject.md)]         | :heavy_check_mark:                                               | List of OCR info for pages.                                      |
| `model`                                                          | *str*                                                            | :heavy_check_mark:                                               | The model used to generate the OCR.                              |
| `usage_info`                                                     | [models.OCRUsageInfo](../models/ocrusageinfo.md)                 | :heavy_check_mark:                                               | N/A                                                              |
| `document_annotation`                                            | *OptionalNullable[str]*                                          | :heavy_minus_sign:                                               | Formatted response in the request_format if provided in json str |
</file>

<file path="docs/models/ocrusageinfo.md">
# OCRUsageInfo


## Fields

| Field                     | Type                      | Required                  | Description               |
| ------------------------- | ------------------------- | ------------------------- | ------------------------- |
| `pages_processed`         | *int*                     | :heavy_check_mark:        | Number of pages processed |
| `doc_size_bytes`          | *OptionalNullable[int]*   | :heavy_minus_sign:        | Document size in bytes    |
</file>

<file path="docs/models/one.md">
# One


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/outputcontentchunks.md">
# OutputContentChunks


## Supported Types

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.ToolFileChunk`

```python
value: models.ToolFileChunk = /* values here */
```

### `models.DocumentURLChunk`

```python
value: models.DocumentURLChunk = /* values here */
```

### `models.ToolReferenceChunk`

```python
value: models.ToolReferenceChunk = /* values here */
```
</file>

<file path="docs/models/outputs.md">
# Outputs


## Supported Types

### `models.MessageOutputEntry`

```python
value: models.MessageOutputEntry = /* values here */
```

### `models.ToolExecutionEntry`

```python
value: models.ToolExecutionEntry = /* values here */
```

### `models.FunctionCallEntry`

```python
value: models.FunctionCallEntry = /* values here */
```

### `models.AgentHandoffEntry`

```python
value: models.AgentHandoffEntry = /* values here */
```
</file>

<file path="docs/models/prediction.md">
# Prediction


## Fields

| Field                          | Type                           | Required                       | Description                    |
| ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |
| `type`                         | *Optional[Literal["content"]]* | :heavy_minus_sign:             | N/A                            |
| `content`                      | *Optional[str]*                | :heavy_minus_sign:             | N/A                            |
</file>

<file path="docs/models/queryparamstatus.md">
# QueryParamStatus

The current job state to filter on. When set, the other results are not displayed.


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `STARTED`                | STARTED                  |
| `VALIDATING`             | VALIDATING               |
| `VALIDATED`              | VALIDATED                |
| `RUNNING`                | RUNNING                  |
| `FAILED_VALIDATION`      | FAILED_VALIDATION        |
| `FAILED`                 | FAILED                   |
| `SUCCESS`                | SUCCESS                  |
| `CANCELLED`              | CANCELLED                |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
</file>

<file path="docs/models/referencechunk.md">
# ReferenceChunk


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `reference_ids`                                                        | List[*int*]                                                            | :heavy_check_mark:                                                     | N/A                                                                    |
| `type`                                                                 | [Optional[models.ReferenceChunkType]](../models/referencechunktype.md) | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="docs/models/referencechunktype.md">
# ReferenceChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `REFERENCE` | reference   |
</file>

<file path="docs/models/repositories.md">
# Repositories


## Supported Types

### `models.GithubRepositoryOut`

```python
value: models.GithubRepositoryOut = /* values here */
```
</file>

<file path="docs/models/response1.md">
# Response1


## Supported Types

### `models.ClassifierJobOut`

```python
value: models.ClassifierJobOut = /* values here */
```

### `models.CompletionJobOut`

```python
value: models.CompletionJobOut = /* values here */
```
</file>

<file path="docs/models/responsebody.md">
# ResponseBody


## Supported Types

### `models.ModelConversation`

```python
value: models.ModelConversation = /* values here */
```

### `models.AgentConversation`

```python
value: models.AgentConversation = /* values here */
```
</file>

<file path="docs/models/responsedoneevent.md">
# ResponseDoneEvent


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `usage`                                                                      | [models.ConversationUsageInfo](../models/conversationusageinfo.md)           | :heavy_check_mark:                                                           | N/A                                                                          |
| `type`                                                                       | [Optional[models.ResponseDoneEventType]](../models/responsedoneeventtype.md) | :heavy_minus_sign:                                                           | N/A                                                                          |
| `created_at`                                                                 | [date](https://docs.python.org/3/library/datetime.html#date-objects)         | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="docs/models/responsedoneeventtype.md">
# ResponseDoneEventType


## Values

| Name                         | Value                        |
| ---------------------------- | ---------------------------- |
| `CONVERSATION_RESPONSE_DONE` | conversation.response.done   |
</file>

<file path="docs/models/responseerrorevent.md">
# ResponseErrorEvent


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `message`                                                                      | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `code`                                                                         | *int*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `type`                                                                         | [Optional[models.ResponseErrorEventType]](../models/responseerroreventtype.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
| `created_at`                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)           | :heavy_minus_sign:                                                             | N/A                                                                            |
</file>

<file path="docs/models/responseerroreventtype.md">
# ResponseErrorEventType


## Values

| Name                          | Value                         |
| ----------------------------- | ----------------------------- |
| `CONVERSATION_RESPONSE_ERROR` | conversation.response.error   |
</file>

<file path="docs/models/responseformat.md">
# ResponseFormat


## Fields

| Field                                                                                                                                                                                                                                                                                              | Type                                                                                                                                                                                                                                                                                               | Required                                                                                                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `type`                                                                                                                                                                                                                                                                                             | [Optional[models.ResponseFormats]](../models/responseformats.md)                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message. |
| `json_schema`                                                                                                                                                                                                                                                                                      | [OptionalNullable[models.JSONSchema]](../models/jsonschema.md)                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                |
</file>

<file path="docs/models/responseformats.md">
# ResponseFormats

An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message.


## Values

| Name          | Value         |
| ------------- | ------------- |
| `TEXT`        | text          |
| `JSON_OBJECT` | json_object   |
| `JSON_SCHEMA` | json_schema   |
</file>

<file path="docs/models/responsestartedevent.md">
# ResponseStartedEvent


## Fields

| Field                                                                              | Type                                                                               | Required                                                                           | Description                                                                        |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| `conversation_id`                                                                  | *str*                                                                              | :heavy_check_mark:                                                                 | N/A                                                                                |
| `type`                                                                             | [Optional[models.ResponseStartedEventType]](../models/responsestartedeventtype.md) | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `created_at`                                                                       | [date](https://docs.python.org/3/library/datetime.html#date-objects)               | :heavy_minus_sign:                                                                 | N/A                                                                                |
</file>

<file path="docs/models/responsestartedeventtype.md">
# ResponseStartedEventType


## Values

| Name                            | Value                           |
| ------------------------------- | ------------------------------- |
| `CONVERSATION_RESPONSE_STARTED` | conversation.response.started   |
</file>

<file path="docs/models/retrievefileout.md">
# RetrieveFileOut


## Fields

| Field                                          | Type                                           | Required                                       | Description                                    | Example                                        |
| ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| `id`                                           | *str*                                          | :heavy_check_mark:                             | The unique identifier of the file.             | 497f6eca-6276-4993-bfeb-53cbbbba6f09           |
| `object`                                       | *str*                                          | :heavy_check_mark:                             | The object type, which is always "file".       | file                                           |
| `size_bytes`                                   | *int*                                          | :heavy_check_mark:                             | The size of the file, in bytes.                | 13000                                          |
| `created_at`                                   | *int*                                          | :heavy_check_mark:                             | The UNIX timestamp (in seconds) of the event.  | 1716963433                                     |
| `filename`                                     | *str*                                          | :heavy_check_mark:                             | The name of the uploaded file.                 | files_upload.jsonl                             |
| `purpose`                                      | [models.FilePurpose](../models/filepurpose.md) | :heavy_check_mark:                             | N/A                                            |                                                |
| `sample_type`                                  | [models.SampleType](../models/sampletype.md)   | :heavy_check_mark:                             | N/A                                            |                                                |
| `source`                                       | [models.Source](../models/source.md)           | :heavy_check_mark:                             | N/A                                            |                                                |
| `deleted`                                      | *bool*                                         | :heavy_check_mark:                             | N/A                                            |                                                |
| `num_lines`                                    | *OptionalNullable[int]*                        | :heavy_minus_sign:                             | N/A                                            |                                                |
</file>

<file path="docs/models/retrievemodelv1modelsmodelidgetrequest.md">
# RetrieveModelV1ModelsModelIDGetRequest


## Fields

| Field                                         | Type                                          | Required                                      | Description                                   | Example                                       |
| --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- | --------------------------------------------- |
| `model_id`                                    | *str*                                         | :heavy_check_mark:                            | The ID of the model to retrieve.              | ft:open-mistral-7b:587a6b29:20240514:7e773925 |
</file>

<file path="docs/models/retrievemodelv1modelsmodelidgetresponseretrievemodelv1modelsmodelidget.md">
# RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet

Successful Response


## Supported Types

### `models.BaseModelCard`

```python
value: models.BaseModelCard = /* values here */
```

### `models.FTModelCard`

```python
value: models.FTModelCard = /* values here */
```
</file>

<file path="docs/models/role.md">
# Role


## Values

| Name     | Value    |
| -------- | -------- |
| `SYSTEM` | system   |
</file>

<file path="docs/models/sampletype.md">
# SampleType


## Values

| Name            | Value           |
| --------------- | --------------- |
| `PRETRAIN`      | pretrain        |
| `INSTRUCT`      | instruct        |
| `BATCH_REQUEST` | batch_request   |
| `BATCH_RESULT`  | batch_result    |
| `BATCH_ERROR`   | batch_error     |
</file>

<file path="docs/models/security.md">
# Security


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `api_key`          | *Optional[str]*    | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/source.md">
# Source


## Values

| Name         | Value        |
| ------------ | ------------ |
| `UPLOAD`     | upload       |
| `REPOSITORY` | repository   |
| `MISTRAL`    | mistral      |
</file>

<file path="docs/models/ssetypes.md">
# SSETypes

Server side events sent when streaming a conversation response.


## Values

| Name                            | Value                           |
| ------------------------------- | ------------------------------- |
| `CONVERSATION_RESPONSE_STARTED` | conversation.response.started   |
| `CONVERSATION_RESPONSE_DONE`    | conversation.response.done      |
| `CONVERSATION_RESPONSE_ERROR`   | conversation.response.error     |
| `MESSAGE_OUTPUT_DELTA`          | message.output.delta            |
| `TOOL_EXECUTION_STARTED`        | tool.execution.started          |
| `TOOL_EXECUTION_DONE`           | tool.execution.done             |
| `AGENT_HANDOFF_STARTED`         | agent.handoff.started           |
| `AGENT_HANDOFF_DONE`            | agent.handoff.done              |
| `FUNCTION_CALL_DELTA`           | function.call.delta             |
</file>

<file path="docs/models/status.md">
# Status

The current status of the fine-tuning job.


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `QUEUED`                 | QUEUED                   |
| `STARTED`                | STARTED                  |
| `VALIDATING`             | VALIDATING               |
| `VALIDATED`              | VALIDATED                |
| `RUNNING`                | RUNNING                  |
| `FAILED_VALIDATION`      | FAILED_VALIDATION        |
| `FAILED`                 | FAILED                   |
| `SUCCESS`                | SUCCESS                  |
| `CANCELLED`              | CANCELLED                |
| `CANCELLATION_REQUESTED` | CANCELLATION_REQUESTED   |
</file>

<file path="docs/models/stop.md">
# Stop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="docs/models/systemmessage.md">
# SystemMessage


## Fields

| Field                                                            | Type                                                             | Required                                                         | Description                                                      |
| ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| `content`                                                        | [models.SystemMessageContent](../models/systemmessagecontent.md) | :heavy_check_mark:                                               | N/A                                                              |
| `role`                                                           | [Optional[models.Role]](../models/role.md)                       | :heavy_minus_sign:                                               | N/A                                                              |
</file>

<file path="docs/models/systemmessagecontent.md">
# SystemMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.TextChunk]`

```python
value: List[models.TextChunk] = /* values here */
```
</file>

<file path="docs/models/textchunk.md">
# TextChunk


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `text`                                                       | *str*                                                        | :heavy_check_mark:                                           | N/A                                                          |
| `type`                                                       | [Optional[models.TextChunkType]](../models/textchunktype.md) | :heavy_minus_sign:                                           | N/A                                                          |
</file>

<file path="docs/models/textchunktype.md">
# TextChunkType


## Values

| Name   | Value  |
| ------ | ------ |
| `TEXT` | text   |
</file>

<file path="docs/models/tool.md">
# Tool


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.Function](../models/function.md)             | :heavy_check_mark:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="docs/models/toolcall.md">
# ToolCall


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.FunctionCall](../models/functioncall.md)     | :heavy_check_mark:                                   | N/A                                                  |
| `id`                                                 | *Optional[str]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
| `index`                                              | *Optional[int]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="docs/models/toolchoice.md">
# ToolChoice

ToolChoice is either a ToolChoiceEnum or a ToolChoice


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `function`                                                                   | [models.FunctionName](../models/functionname.md)                             | :heavy_check_mark:                                                           | this restriction of `Function` is used to select a specific function to call |
| `type`                                                                       | [Optional[models.ToolTypes]](../models/tooltypes.md)                         | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="docs/models/toolchoiceenum.md">
# ToolChoiceEnum


## Values

| Name       | Value      |
| ---------- | ---------- |
| `AUTO`     | auto       |
| `NONE`     | none       |
| `ANY`      | any        |
| `REQUIRED` | required   |
</file>

<file path="docs/models/toolexecutiondoneevent.md">
# ToolExecutionDoneEvent


## Fields

| Field                                                                                  | Type                                                                                   | Required                                                                               | Description                                                                            |
| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `id`                                                                                   | *str*                                                                                  | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `name`                                                                                 | [models.BuiltInConnectors](../models/builtinconnectors.md)                             | :heavy_check_mark:                                                                     | N/A                                                                                    |
| `type`                                                                                 | [Optional[models.ToolExecutionDoneEventType]](../models/toolexecutiondoneeventtype.md) | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `created_at`                                                                           | [date](https://docs.python.org/3/library/datetime.html#date-objects)                   | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `output_index`                                                                         | *Optional[int]*                                                                        | :heavy_minus_sign:                                                                     | N/A                                                                                    |
| `info`                                                                                 | Dict[str, *Any*]                                                                       | :heavy_minus_sign:                                                                     | N/A                                                                                    |
</file>

<file path="docs/models/toolexecutiondoneeventtype.md">
# ToolExecutionDoneEventType


## Values

| Name                  | Value                 |
| --------------------- | --------------------- |
| `TOOL_EXECUTION_DONE` | tool.execution.done   |
</file>

<file path="docs/models/toolexecutionentry.md">
# ToolExecutionEntry


## Fields

| Field                                                                              | Type                                                                               | Required                                                                           | Description                                                                        |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| `name`                                                                             | [models.BuiltInConnectors](../models/builtinconnectors.md)                         | :heavy_check_mark:                                                                 | N/A                                                                                |
| `object`                                                                           | [Optional[models.ToolExecutionEntryObject]](../models/toolexecutionentryobject.md) | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `type`                                                                             | [Optional[models.ToolExecutionEntryType]](../models/toolexecutionentrytype.md)     | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `created_at`                                                                       | [date](https://docs.python.org/3/library/datetime.html#date-objects)               | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `completed_at`                                                                     | [date](https://docs.python.org/3/library/datetime.html#date-objects)               | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `id`                                                                               | *Optional[str]*                                                                    | :heavy_minus_sign:                                                                 | N/A                                                                                |
| `info`                                                                             | Dict[str, *Any*]                                                                   | :heavy_minus_sign:                                                                 | N/A                                                                                |
</file>

<file path="docs/models/toolexecutionentryobject.md">
# ToolExecutionEntryObject


## Values

| Name    | Value   |
| ------- | ------- |
| `ENTRY` | entry   |
</file>

<file path="docs/models/toolexecutionentrytype.md">
# ToolExecutionEntryType


## Values

| Name             | Value            |
| ---------------- | ---------------- |
| `TOOL_EXECUTION` | tool.execution   |
</file>

<file path="docs/models/toolexecutionstartedevent.md">
# ToolExecutionStartedEvent


## Fields

| Field                                                                                        | Type                                                                                         | Required                                                                                     | Description                                                                                  |
| -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| `id`                                                                                         | *str*                                                                                        | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `name`                                                                                       | [models.BuiltInConnectors](../models/builtinconnectors.md)                                   | :heavy_check_mark:                                                                           | N/A                                                                                          |
| `type`                                                                                       | [Optional[models.ToolExecutionStartedEventType]](../models/toolexecutionstartedeventtype.md) | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `created_at`                                                                                 | [date](https://docs.python.org/3/library/datetime.html#date-objects)                         | :heavy_minus_sign:                                                                           | N/A                                                                                          |
| `output_index`                                                                               | *Optional[int]*                                                                              | :heavy_minus_sign:                                                                           | N/A                                                                                          |
</file>

<file path="docs/models/toolexecutionstartedeventtype.md">
# ToolExecutionStartedEventType


## Values

| Name                     | Value                    |
| ------------------------ | ------------------------ |
| `TOOL_EXECUTION_STARTED` | tool.execution.started   |
</file>

<file path="docs/models/toolfilechunk.md">
# ToolFileChunk


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `tool`                                                               | [models.BuiltInConnectors](../models/builtinconnectors.md)           | :heavy_check_mark:                                                   | N/A                                                                  |
| `file_id`                                                            | *str*                                                                | :heavy_check_mark:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.ToolFileChunkType]](../models/toolfilechunktype.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `file_name`                                                          | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `file_type`                                                          | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/toolfilechunktype.md">
# ToolFileChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `TOOL_FILE` | tool_file   |
</file>

<file path="docs/models/toolmessage.md">
# ToolMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.ToolMessageContent]](../models/toolmessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `tool_call_id`                                                         | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `name`                                                                 | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.ToolMessageRole]](../models/toolmessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="docs/models/toolmessagecontent.md">
# ToolMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="docs/models/toolmessagerole.md">
# ToolMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `TOOL` | tool   |
</file>

<file path="docs/models/toolreferencechunk.md">
# ToolReferenceChunk


## Fields

| Field                                                                          | Type                                                                           | Required                                                                       | Description                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------ |
| `tool`                                                                         | [models.BuiltInConnectors](../models/builtinconnectors.md)                     | :heavy_check_mark:                                                             | N/A                                                                            |
| `title`                                                                        | *str*                                                                          | :heavy_check_mark:                                                             | N/A                                                                            |
| `type`                                                                         | [Optional[models.ToolReferenceChunkType]](../models/toolreferencechunktype.md) | :heavy_minus_sign:                                                             | N/A                                                                            |
| `url`                                                                          | *OptionalNullable[str]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
| `source`                                                                       | *OptionalNullable[str]*                                                        | :heavy_minus_sign:                                                             | N/A                                                                            |
</file>

<file path="docs/models/toolreferencechunktype.md">
# ToolReferenceChunkType


## Values

| Name             | Value            |
| ---------------- | ---------------- |
| `TOOL_REFERENCE` | tool_reference   |
</file>

<file path="docs/models/tools.md">
# Tools


## Supported Types

### `models.CodeInterpreterTool`

```python
value: models.CodeInterpreterTool = /* values here */
```

### `models.DocumentLibraryTool`

```python
value: models.DocumentLibraryTool = /* values here */
```

### `models.FunctionTool`

```python
value: models.FunctionTool = /* values here */
```

### `models.ImageGenerationTool`

```python
value: models.ImageGenerationTool = /* values here */
```

### `models.WebSearchTool`

```python
value: models.WebSearchTool = /* values here */
```

### `models.WebSearchPremiumTool`

```python
value: models.WebSearchPremiumTool = /* values here */
```
</file>

<file path="docs/models/tooltypes.md">
# ToolTypes


## Values

| Name       | Value      |
| ---------- | ---------- |
| `FUNCTION` | function   |
</file>

<file path="docs/models/trainingfile.md">
# TrainingFile


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `file_id`          | *str*              | :heavy_check_mark: | N/A                |
| `weight`           | *Optional[float]*  | :heavy_minus_sign: | N/A                |
</file>

<file path="docs/models/two.md">
# Two


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="docs/models/type.md">
# Type


## Values

| Name   | Value  |
| ------ | ------ |
| `BASE` | base   |
</file>

<file path="docs/models/unarchiveftmodelout.md">
# UnarchiveFTModelOut


## Fields

| Field                                                                                | Type                                                                                 | Required                                                                             | Description                                                                          |
| ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------ |
| `id`                                                                                 | *str*                                                                                | :heavy_check_mark:                                                                   | N/A                                                                                  |
| `object`                                                                             | [Optional[models.UnarchiveFTModelOutObject]](../models/unarchiveftmodeloutobject.md) | :heavy_minus_sign:                                                                   | N/A                                                                                  |
| `archived`                                                                           | *Optional[bool]*                                                                     | :heavy_minus_sign:                                                                   | N/A                                                                                  |
</file>

<file path="docs/models/unarchiveftmodeloutobject.md">
# UnarchiveFTModelOutObject


## Values

| Name    | Value   |
| ------- | ------- |
| `MODEL` | model   |
</file>

<file path="docs/models/updateftmodelin.md">
# UpdateFTModelIn


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `name`                  | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
| `description`           | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
</file>

<file path="docs/models/uploadfileout.md">
# UploadFileOut


## Fields

| Field                                          | Type                                           | Required                                       | Description                                    | Example                                        |
| ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- |
| `id`                                           | *str*                                          | :heavy_check_mark:                             | The unique identifier of the file.             | 497f6eca-6276-4993-bfeb-53cbbbba6f09           |
| `object`                                       | *str*                                          | :heavy_check_mark:                             | The object type, which is always "file".       | file                                           |
| `size_bytes`                                   | *int*                                          | :heavy_check_mark:                             | The size of the file, in bytes.                | 13000                                          |
| `created_at`                                   | *int*                                          | :heavy_check_mark:                             | The UNIX timestamp (in seconds) of the event.  | 1716963433                                     |
| `filename`                                     | *str*                                          | :heavy_check_mark:                             | The name of the uploaded file.                 | files_upload.jsonl                             |
| `purpose`                                      | [models.FilePurpose](../models/filepurpose.md) | :heavy_check_mark:                             | N/A                                            |                                                |
| `sample_type`                                  | [models.SampleType](../models/sampletype.md)   | :heavy_check_mark:                             | N/A                                            |                                                |
| `source`                                       | [models.Source](../models/source.md)           | :heavy_check_mark:                             | N/A                                            |                                                |
| `num_lines`                                    | *OptionalNullable[int]*                        | :heavy_minus_sign:                             | N/A                                            |                                                |
</file>

<file path="docs/models/usageinfo.md">
# UsageInfo


## Fields

| Field               | Type                | Required            | Description         | Example             |
| ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| `prompt_tokens`     | *int*               | :heavy_check_mark:  | N/A                 | 16                  |
| `completion_tokens` | *int*               | :heavy_check_mark:  | N/A                 | 34                  |
| `total_tokens`      | *int*               | :heavy_check_mark:  | N/A                 | 50                  |
</file>

<file path="docs/models/usermessage.md">
# UserMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.UserMessageContent]](../models/usermessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.UserMessageRole]](../models/usermessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="docs/models/usermessagecontent.md">
# UserMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="docs/models/usermessagerole.md">
# UserMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `USER` | user   |
</file>

<file path="docs/models/validationerror.md">
# ValidationError


## Fields

| Field                                | Type                                 | Required                             | Description                          |
| ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ |
| `loc`                                | List[[models.Loc](../models/loc.md)] | :heavy_check_mark:                   | N/A                                  |
| `msg`                                | *str*                                | :heavy_check_mark:                   | N/A                                  |
| `type`                               | *str*                                | :heavy_check_mark:                   | N/A                                  |
</file>

<file path="docs/models/wandbintegration.md">
# WandbIntegration


## Fields

| Field                                                                           | Type                                                                            | Required                                                                        | Description                                                                     |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| `project`                                                                       | *str*                                                                           | :heavy_check_mark:                                                              | The name of the project that the new run will be created under.                 |
| `api_key`                                                                       | *str*                                                                           | :heavy_check_mark:                                                              | The WandB API key to use for authentication.                                    |
| `type`                                                                          | [Optional[models.WandbIntegrationType]](../models/wandbintegrationtype.md)      | :heavy_minus_sign:                                                              | N/A                                                                             |
| `name`                                                                          | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | A display name to set for the run. If not set, will use the job ID as the name. |
| `run_name`                                                                      | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | N/A                                                                             |
</file>

<file path="docs/models/wandbintegrationout.md">
# WandbIntegrationOut


## Fields

| Field                                                                            | Type                                                                             | Required                                                                         | Description                                                                      |
| -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| `project`                                                                        | *str*                                                                            | :heavy_check_mark:                                                               | The name of the project that the new run will be created under.                  |
| `type`                                                                           | [Optional[models.WandbIntegrationOutType]](../models/wandbintegrationouttype.md) | :heavy_minus_sign:                                                               | N/A                                                                              |
| `name`                                                                           | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | A display name to set for the run. If not set, will use the job ID as the name.  |
| `run_name`                                                                       | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | N/A                                                                              |
| `url`                                                                            | *OptionalNullable[str]*                                                          | :heavy_minus_sign:                                                               | N/A                                                                              |
</file>

<file path="docs/models/wandbintegrationouttype.md">
# WandbIntegrationOutType


## Values

| Name    | Value   |
| ------- | ------- |
| `WANDB` | wandb   |
</file>

<file path="docs/models/wandbintegrationtype.md">
# WandbIntegrationType


## Values

| Name    | Value   |
| ------- | ------- |
| `WANDB` | wandb   |
</file>

<file path="docs/models/websearchpremiumtool.md">
# WebSearchPremiumTool


## Fields

| Field                                                                              | Type                                                                               | Required                                                                           | Description                                                                        |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| `type`                                                                             | [Optional[models.WebSearchPremiumToolType]](../models/websearchpremiumtooltype.md) | :heavy_minus_sign:                                                                 | N/A                                                                                |
</file>

<file path="docs/models/websearchpremiumtooltype.md">
# WebSearchPremiumToolType


## Values

| Name                 | Value                |
| -------------------- | -------------------- |
| `WEB_SEARCH_PREMIUM` | web_search_premium   |
</file>

<file path="docs/models/websearchtool.md">
# WebSearchTool


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `type`                                                               | [Optional[models.WebSearchToolType]](../models/websearchtooltype.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="docs/models/websearchtooltype.md">
# WebSearchToolType


## Values

| Name         | Value        |
| ------------ | ------------ |
| `WEB_SEARCH` | web_search   |
</file>

<file path="docs/sdks/agents/README.md">
# Agents
(*agents*)

## Overview

Agents API.

### Available Operations

* [complete](#complete) - Agents Completion
* [stream](#stream) - Stream Agents completion

## complete

Agents Completion

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.agents.complete(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], agent_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                          | Type                                                                                                                                                                                                                                                                                                                               | Required                                                                                                                                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                                                        | Example                                                                                                                                                                                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                                                                         | List[[models.AgentsCompletionRequestMessages](../../models/agentscompletionrequestmessages.md)]                                                                                                                                                                                                                                    | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                 | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                        | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                             |
| `agent_id`                                                                                                                                                                                                                                                                                                                         | *str*                                                                                                                                                                                                                                                                                                                              | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                 | The ID of the agent to use for this completion.                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                    |
| `max_tokens`                                                                                                                                                                                                                                                                                                                       | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                    |
| `stream`                                                                                                                                                                                                                                                                                                                           | *Optional[bool]*                                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON. |                                                                                                                                                                                                                                                                                                                                    |
| `stop`                                                                                                                                                                                                                                                                                                                             | [Optional[models.AgentsCompletionRequestStop]](../../models/agentscompletionrequeststop.md)                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                    |
| `random_seed`                                                                                                                                                                                                                                                                                                                      | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                    |
| `response_format`                                                                                                                                                                                                                                                                                                                  | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `tools`                                                                                                                                                                                                                                                                                                                            | List[[models.Tool](../../models/tool.md)]                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `tool_choice`                                                                                                                                                                                                                                                                                                                      | [Optional[models.AgentsCompletionRequestToolChoice]](../../models/agentscompletionrequesttoolchoice.md)                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                 | *Optional[float]*                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                              |                                                                                                                                                                                                                                                                                                                                    |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                | *Optional[float]*                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                        |                                                                                                                                                                                                                                                                                                                                    |
| `n`                                                                                                                                                                                                                                                                                                                                | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                    |
| `prediction`                                                                                                                                                                                                                                                                                                                       | [Optional[models.Prediction]](../../models/prediction.md)                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                      | [OptionalNullable[models.MistralPromptMode]](../../models/mistralpromptmode.md)                                                                                                                                                                                                                                                    | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |
| `retries`                                                                                                                                                                                                                                                                                                                          | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                 | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                    |

### Response

**[models.ChatCompletionResponse](../../models/chatcompletionresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.agents.stream(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], agent_id="<id>")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                   | Type                                                                                                                                                                                                                                                                        | Required                                                                                                                                                                                                                                                                    | Description                                                                                                                                                                                                                                                                 | Example                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                  | List[[models.AgentsCompletionStreamRequestMessages](../../models/agentscompletionstreamrequestmessages.md)]                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                          | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                 | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                      |
| `agent_id`                                                                                                                                                                                                                                                                  | *str*                                                                                                                                                                                                                                                                       | :heavy_check_mark:                                                                                                                                                                                                                                                          | The ID of the agent to use for this completion.                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                             |
| `max_tokens`                                                                                                                                                                                                                                                                | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                      |                                                                                                                                                                                                                                                                             |
| `stream`                                                                                                                                                                                                                                                                    | *Optional[bool]*                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `stop`                                                                                                                                                                                                                                                                      | [Optional[models.AgentsCompletionStreamRequestStop]](../../models/agentscompletionstreamrequeststop.md)                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                    |                                                                                                                                                                                                                                                                             |
| `random_seed`                                                                                                                                                                                                                                                               | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                           |                                                                                                                                                                                                                                                                             |
| `response_format`                                                                                                                                                                                                                                                           | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `tools`                                                                                                                                                                                                                                                                     | List[[models.Tool](../../models/tool.md)]                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `tool_choice`                                                                                                                                                                                                                                                               | [Optional[models.AgentsCompletionStreamRequestToolChoice]](../../models/agentscompletionstreamrequesttoolchoice.md)                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `presence_penalty`                                                                                                                                                                                                                                                          | *Optional[float]*                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                          | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                       |                                                                                                                                                                                                                                                                             |
| `frequency_penalty`                                                                                                                                                                                                                                                         | *Optional[float]*                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                          | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition. |                                                                                                                                                                                                                                                                             |
| `n`                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                          | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                        |                                                                                                                                                                                                                                                                             |
| `prediction`                                                                                                                                                                                                                                                                | [Optional[models.Prediction]](../../models/prediction.md)                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `parallel_tool_calls`                                                                                                                                                                                                                                                       | *Optional[bool]*                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `prompt_mode`                                                                                                                                                                                                                                                               | [OptionalNullable[models.MistralPromptMode]](../../models/mistralpromptmode.md)                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                          | N/A                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |
| `retries`                                                                                                                                                                                                                                                                   | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                          | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                             |

### Response

**[Union[eventstreaming.EventStream[models.CompletionEvent], eventstreaming.EventStreamAsync[models.CompletionEvent]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/batch/README.md">
# Batch
(*batch*)

## Overview

### Available Operations
</file>

<file path="docs/sdks/beta/README.md">
# Beta
(*beta*)

## Overview

### Available Operations
</file>

<file path="docs/sdks/chat/README.md">
# Chat
(*chat*)

## Overview

Chat Completion API.

### Available Operations

* [complete](#complete) - Chat Completion
* [stream](#stream) - Stream chat completion

## complete

Chat Completion

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.chat.complete(model="mistral-small-latest", messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ])

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                                                                                             | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.Messages](../../models/messages.md)]                                                                                                                                                                                                                                                                                                                                                     | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.Stop]](../../models/stop.md)                                                                                                                                                                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionRequestToolChoice]](../../models/chatcompletionrequesttoolchoice.md)                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../../models/prediction.md)                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                                                                                         | [OptionalNullable[models.MistralPromptMode]](../../models/mistralpromptmode.md)                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `retries`                                                                                                                                                                                                                                                                                                                                                                                             | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |

### Response

**[models.ChatCompletionResponse](../../models/chatcompletionresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.chat.stream(model="mistral-small-latest", messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ])

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                                                                                             | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.ChatCompletionStreamRequestMessages](../../models/chatcompletionstreamrequestmessages.md)]                                                                                                                                                                                                                                                                                               | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.ChatCompletionStreamRequestStop]](../../models/chatcompletionstreamrequeststop.md)                                                                                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionStreamRequestToolChoice]](../../models/chatcompletionstreamrequesttoolchoice.md)                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../../models/prediction.md)                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prompt_mode`                                                                                                                                                                                                                                                                                                                                                                                         | [OptionalNullable[models.MistralPromptMode]](../../models/mistralpromptmode.md)                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `retries`                                                                                                                                                                                                                                                                                                                                                                                             | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |

### Response

**[Union[eventstreaming.EventStream[models.CompletionEvent], eventstreaming.EventStreamAsync[models.CompletionEvent]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/classifiers/README.md">
# Classifiers
(*classifiers*)

## Overview

Classifiers API.

### Available Operations

* [moderate](#moderate) - Moderations
* [moderate_chat](#moderate_chat) - Chat Moderations
* [classify](#classify) - Classifications
* [classify_chat](#classify_chat) - Chat Classifications

## moderate

Moderations

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.classifiers.moderate(model="V90", inputs=[
        "<value>",
    ])

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                         | Type                                                                              | Required                                                                          | Description                                                                       |
| --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| `model`                                                                           | *str*                                                                             | :heavy_check_mark:                                                                | ID of the model to use.                                                           |
| `inputs`                                                                          | [models.ClassificationRequestInputs](../../models/classificationrequestinputs.md) | :heavy_check_mark:                                                                | Text to classify.                                                                 |
| `retries`                                                                         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                  | :heavy_minus_sign:                                                                | Configuration to override the default retry behavior of the client.               |

### Response

**[models.ModerationResponse](../../models/moderationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## moderate_chat

Chat Moderations

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.classifiers.moderate_chat(inputs=[
        [
            {
                "content": [

                ],
                "role": "system",
            },
            {
                "content": "<value>",
                "role": "tool",
            },
        ],
        [
            {
                "prefix": False,
                "role": "assistant",
            },
            {
                "content": "<value>",
                "role": "user",
            },
            {
                "prefix": False,
                "role": "assistant",
            },
        ],
        [
            {
                "content": "<value>",
                "role": "system",
            },
            {
                "content": [
                    {
                        "image_url": "https://fatherly-colon.name",
                        "type": "image_url",
                    },
                ],
                "role": "user",
            },
            {
                "content": "<value>",
                "role": "user",
            },
        ],
    ], model="Model Y")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                         | Type                                                                              | Required                                                                          | Description                                                                       |
| --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| `inputs`                                                                          | [models.ChatModerationRequestInputs](../../models/chatmoderationrequestinputs.md) | :heavy_check_mark:                                                                | Chat to classify                                                                  |
| `model`                                                                           | *str*                                                                             | :heavy_check_mark:                                                                | N/A                                                                               |
| `retries`                                                                         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                  | :heavy_minus_sign:                                                                | Configuration to override the default retry behavior of the client.               |

### Response

**[models.ModerationResponse](../../models/moderationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## classify

Classifications

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.classifiers.classify(model="Altima", inputs="<value>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                         | Type                                                                              | Required                                                                          | Description                                                                       |
| --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |
| `model`                                                                           | *str*                                                                             | :heavy_check_mark:                                                                | ID of the model to use.                                                           |
| `inputs`                                                                          | [models.ClassificationRequestInputs](../../models/classificationrequestinputs.md) | :heavy_check_mark:                                                                | Text to classify.                                                                 |
| `retries`                                                                         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                  | :heavy_minus_sign:                                                                | Configuration to override the default retry behavior of the client.               |

### Response

**[models.ClassificationResponse](../../models/classificationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## classify_chat

Chat Classifications

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.classifiers.classify_chat(model="Fortwo", inputs=[
        {
            "messages": [
                {
                    "content": "<value>",
                    "role": "tool",
                },
            ],
        },
        {
            "messages": [

            ],
        },
    ])

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model`                                                             | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `inputs`                                                            | [models.Inputs](../../models/inputs.md)                             | :heavy_check_mark:                                                  | Chat to classify                                                    |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.ClassificationResponse](../../models/classificationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/conversations/README.md">
# Conversations
(*beta.conversations*)

## Overview

(beta) Conversations API

### Available Operations

* [start](#start) - Create a conversation and append entries to it.
* [list](#list) - List all created conversations.
* [get](#get) - Retrieve a conversation information.
* [append](#append) - Append new entries to an existing conversation.
* [get_history](#get_history) - Retrieve all entries in a conversation.
* [get_messages](#get_messages) - Retrieve all messages in a conversation.
* [restart](#restart) - Restart a conversation starting from a given entry.
* [start_stream](#start_stream) - Create a conversation and append entries to it.
* [append_stream](#append_stream) - Append new entries to an existing conversation.
* [restart_stream](#restart_stream) - Restart a conversation starting from a given entry.

## start

Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.start(inputs="<value>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                     | Type                                                                          | Required                                                                      | Description                                                                   |
| ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| `inputs`                                                                      | [models.ConversationInputs](../../models/conversationinputs.md)               | :heavy_check_mark:                                                            | N/A                                                                           |
| `stream`                                                                      | *Optional[bool]*                                                              | :heavy_minus_sign:                                                            | N/A                                                                           |
| `store`                                                                       | *OptionalNullable[bool]*                                                      | :heavy_minus_sign:                                                            | N/A                                                                           |
| `handoff_execution`                                                           | [OptionalNullable[models.HandoffExecution]](../../models/handoffexecution.md) | :heavy_minus_sign:                                                            | N/A                                                                           |
| `instructions`                                                                | *OptionalNullable[str]*                                                       | :heavy_minus_sign:                                                            | N/A                                                                           |
| `tools`                                                                       | List[[models.Tools](../../models/tools.md)]                                   | :heavy_minus_sign:                                                            | N/A                                                                           |
| `completion_args`                                                             | [OptionalNullable[models.CompletionArgs]](../../models/completionargs.md)     | :heavy_minus_sign:                                                            | N/A                                                                           |
| `name`                                                                        | *OptionalNullable[str]*                                                       | :heavy_minus_sign:                                                            | N/A                                                                           |
| `description`                                                                 | *OptionalNullable[str]*                                                       | :heavy_minus_sign:                                                            | N/A                                                                           |
| `agent_id`                                                                    | *OptionalNullable[str]*                                                       | :heavy_minus_sign:                                                            | N/A                                                                           |
| `model`                                                                       | *OptionalNullable[str]*                                                       | :heavy_minus_sign:                                                            | N/A                                                                           |
| `retries`                                                                     | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)              | :heavy_minus_sign:                                                            | Configuration to override the default retry behavior of the client.           |

### Response

**[models.ConversationResponse](../../models/conversationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## list

Retrieve a list of conversation entities sorted by creation time.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `page`                                                              | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `page_size`                                                         | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[List[models.ResponseBody]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## get

Given a conversation_id retrieve a conversation entity with its attributes.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.get(conversation_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `conversation_id`                                                   | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.AgentsAPIV1ConversationsGetResponseV1ConversationsGet](../../models/agentsapiv1conversationsgetresponsev1conversationsget.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## append

Run completion on the history of the conversation and the user entries. Return the new created entries.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.append(conversation_id="<id>", inputs="<value>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                               | Type                                                                                                                    | Required                                                                                                                | Description                                                                                                             |
| ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| `conversation_id`                                                                                                       | *str*                                                                                                                   | :heavy_check_mark:                                                                                                      | ID of the conversation to which we append entries.                                                                      |
| `inputs`                                                                                                                | [models.ConversationInputs](../../models/conversationinputs.md)                                                         | :heavy_check_mark:                                                                                                      | N/A                                                                                                                     |
| `stream`                                                                                                                | *Optional[bool]*                                                                                                        | :heavy_minus_sign:                                                                                                      | N/A                                                                                                                     |
| `store`                                                                                                                 | *Optional[bool]*                                                                                                        | :heavy_minus_sign:                                                                                                      | Whether to store the results into our servers or not.                                                                   |
| `handoff_execution`                                                                                                     | [Optional[models.ConversationAppendRequestHandoffExecution]](../../models/conversationappendrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                      | N/A                                                                                                                     |
| `completion_args`                                                                                                       | [Optional[models.CompletionArgs]](../../models/completionargs.md)                                                       | :heavy_minus_sign:                                                                                                      | White-listed arguments from the completion API                                                                          |
| `retries`                                                                                                               | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                        | :heavy_minus_sign:                                                                                                      | Configuration to override the default retry behavior of the client.                                                     |

### Response

**[models.ConversationResponse](../../models/conversationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## get_history

Given a conversation_id retrieve all the entries belonging to that conversation. The entries are sorted in the order they were appended, those can be messages, connectors or function_call.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.get_history(conversation_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `conversation_id`                                                   | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.ConversationHistory](../../models/conversationhistory.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## get_messages

Given a conversation_id retrieve all the messages belonging to that conversation. This is similar to retrieving all entries except we filter the messages only.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.get_messages(conversation_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `conversation_id`                                                   | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.ConversationMessages](../../models/conversationmessages.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## restart

Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.restart(conversation_id="<id>", inputs="<value>", from_entry_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                 | Type                                                                                                                      | Required                                                                                                                  | Description                                                                                                               |
| ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| `conversation_id`                                                                                                         | *str*                                                                                                                     | :heavy_check_mark:                                                                                                        | N/A                                                                                                                       |
| `inputs`                                                                                                                  | [models.ConversationInputs](../../models/conversationinputs.md)                                                           | :heavy_check_mark:                                                                                                        | N/A                                                                                                                       |
| `from_entry_id`                                                                                                           | *str*                                                                                                                     | :heavy_check_mark:                                                                                                        | N/A                                                                                                                       |
| `stream`                                                                                                                  | *Optional[bool]*                                                                                                          | :heavy_minus_sign:                                                                                                        | N/A                                                                                                                       |
| `store`                                                                                                                   | *Optional[bool]*                                                                                                          | :heavy_minus_sign:                                                                                                        | Whether to store the results into our servers or not.                                                                     |
| `handoff_execution`                                                                                                       | [Optional[models.ConversationRestartRequestHandoffExecution]](../../models/conversationrestartrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                        | N/A                                                                                                                       |
| `completion_args`                                                                                                         | [Optional[models.CompletionArgs]](../../models/completionargs.md)                                                         | :heavy_minus_sign:                                                                                                        | White-listed arguments from the completion API                                                                            |
| `retries`                                                                                                                 | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                          | :heavy_minus_sign:                                                                                                        | Configuration to override the default retry behavior of the client.                                                       |

### Response

**[models.ConversationResponse](../../models/conversationresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## start_stream

Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.start_stream(inputs="<value>")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                       | Type                                                                                                                            | Required                                                                                                                        | Description                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| `inputs`                                                                                                                        | [models.ConversationInputs](../../models/conversationinputs.md)                                                                 | :heavy_check_mark:                                                                                                              | N/A                                                                                                                             |
| `stream`                                                                                                                        | *Optional[bool]*                                                                                                                | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `store`                                                                                                                         | *OptionalNullable[bool]*                                                                                                        | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `handoff_execution`                                                                                                             | [OptionalNullable[models.ConversationStreamRequestHandoffExecution]](../../models/conversationstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `instructions`                                                                                                                  | *OptionalNullable[str]*                                                                                                         | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `tools`                                                                                                                         | List[[models.ConversationStreamRequestTools](../../models/conversationstreamrequesttools.md)]                                   | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `completion_args`                                                                                                               | [OptionalNullable[models.CompletionArgs]](../../models/completionargs.md)                                                       | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `name`                                                                                                                          | *OptionalNullable[str]*                                                                                                         | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `description`                                                                                                                   | *OptionalNullable[str]*                                                                                                         | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `agent_id`                                                                                                                      | *OptionalNullable[str]*                                                                                                         | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `model`                                                                                                                         | *OptionalNullable[str]*                                                                                                         | :heavy_minus_sign:                                                                                                              | N/A                                                                                                                             |
| `retries`                                                                                                                       | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                | :heavy_minus_sign:                                                                                                              | Configuration to override the default retry behavior of the client.                                                             |

### Response

**[Union[eventstreaming.EventStream[models.ConversationEvents], eventstreaming.EventStreamAsync[models.ConversationEvents]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## append_stream

Run completion on the history of the conversation and the user entries. Return the new created entries.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.append_stream(conversation_id="<id>", inputs="<value>")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                           | Type                                                                                                                                | Required                                                                                                                            | Description                                                                                                                         |
| ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `conversation_id`                                                                                                                   | *str*                                                                                                                               | :heavy_check_mark:                                                                                                                  | ID of the conversation to which we append entries.                                                                                  |
| `inputs`                                                                                                                            | [models.ConversationInputs](../../models/conversationinputs.md)                                                                     | :heavy_check_mark:                                                                                                                  | N/A                                                                                                                                 |
| `stream`                                                                                                                            | *Optional[bool]*                                                                                                                    | :heavy_minus_sign:                                                                                                                  | N/A                                                                                                                                 |
| `store`                                                                                                                             | *Optional[bool]*                                                                                                                    | :heavy_minus_sign:                                                                                                                  | Whether to store the results into our servers or not.                                                                               |
| `handoff_execution`                                                                                                                 | [Optional[models.ConversationAppendStreamRequestHandoffExecution]](../../models/conversationappendstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                                  | N/A                                                                                                                                 |
| `completion_args`                                                                                                                   | [Optional[models.CompletionArgs]](../../models/completionargs.md)                                                                   | :heavy_minus_sign:                                                                                                                  | White-listed arguments from the completion API                                                                                      |
| `retries`                                                                                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                    | :heavy_minus_sign:                                                                                                                  | Configuration to override the default retry behavior of the client.                                                                 |

### Response

**[Union[eventstreaming.EventStream[models.ConversationEvents], eventstreaming.EventStreamAsync[models.ConversationEvents]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## restart_stream

Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.restart_stream(conversation_id="<id>", inputs="<value>", from_entry_id="<id>")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                             | Type                                                                                                                                  | Required                                                                                                                              | Description                                                                                                                           |
| ------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| `conversation_id`                                                                                                                     | *str*                                                                                                                                 | :heavy_check_mark:                                                                                                                    | N/A                                                                                                                                   |
| `inputs`                                                                                                                              | [models.ConversationInputs](../../models/conversationinputs.md)                                                                       | :heavy_check_mark:                                                                                                                    | N/A                                                                                                                                   |
| `from_entry_id`                                                                                                                       | *str*                                                                                                                                 | :heavy_check_mark:                                                                                                                    | N/A                                                                                                                                   |
| `stream`                                                                                                                              | *Optional[bool]*                                                                                                                      | :heavy_minus_sign:                                                                                                                    | N/A                                                                                                                                   |
| `store`                                                                                                                               | *Optional[bool]*                                                                                                                      | :heavy_minus_sign:                                                                                                                    | Whether to store the results into our servers or not.                                                                                 |
| `handoff_execution`                                                                                                                   | [Optional[models.ConversationRestartStreamRequestHandoffExecution]](../../models/conversationrestartstreamrequesthandoffexecution.md) | :heavy_minus_sign:                                                                                                                    | N/A                                                                                                                                   |
| `completion_args`                                                                                                                     | [Optional[models.CompletionArgs]](../../models/completionargs.md)                                                                     | :heavy_minus_sign:                                                                                                                    | White-listed arguments from the completion API                                                                                        |
| `retries`                                                                                                                             | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                      | :heavy_minus_sign:                                                                                                                    | Configuration to override the default retry behavior of the client.                                                                   |

### Response

**[Union[eventstreaming.EventStream[models.ConversationEvents], eventstreaming.EventStreamAsync[models.ConversationEvents]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/embeddings/README.md">
# Embeddings
(*embeddings*)

## Overview

Embeddings API.

### Available Operations

* [create](#create) - Embeddings

## create

Embeddings

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.embeddings.create(model="mistral-embed", inputs=[
        "Embed this sentence.",
        "As well as this one.",
    ])

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                               | Type                                                                    | Required                                                                | Description                                                             | Example                                                                 |
| ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| `model`                                                                 | *str*                                                                   | :heavy_check_mark:                                                      | ID of the model to use.                                                 | mistral-embed                                                           |
| `inputs`                                                                | [models.EmbeddingRequestInputs](../../models/embeddingrequestinputs.md) | :heavy_check_mark:                                                      | Text to embed.                                                          | [<br/>"Embed this sentence.",<br/>"As well as this one."<br/>]          |
| `output_dimension`                                                      | *OptionalNullable[int]*                                                 | :heavy_minus_sign:                                                      | The dimension of the output embeddings.                                 |                                                                         |
| `output_dtype`                                                          | [Optional[models.EmbeddingDtype]](../../models/embeddingdtype.md)       | :heavy_minus_sign:                                                      | N/A                                                                     |                                                                         |
| `retries`                                                               | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)        | :heavy_minus_sign:                                                      | Configuration to override the default retry behavior of the client.     |                                                                         |

### Response

**[models.EmbeddingResponse](../../models/embeddingresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/files/README.md">
# Files
(*files*)

## Overview

Files API

### Available Operations

* [upload](#upload) - Upload File
* [list](#list) - List Files
* [retrieve](#retrieve) - Retrieve File
* [delete](#delete) - Delete File
* [download](#download) - Download File
* [get_signed_url](#get_signed_url) - Get Signed Url

## upload

Upload a file that can be used across various endpoints.

The size of individual files can be a maximum of 512 MB. The Fine-tuning API only supports .jsonl files.

Please contact us if you need to increase these storage limits.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.upload(file={
        "file_name": "example.file",
        "content": open("example.file", "rb"),
    })

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                           | Type                                                                                                                                                                                                                                                                                                                | Required                                                                                                                                                                                                                                                                                                            | Description                                                                                                                                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `file`                                                                                                                                                                                                                                                                                                              | [models.File](../../models/file.md)                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                  | The File object (not file name) to be uploaded.<br/> To upload a file and specify a custom file name you should format your request as such:<br/> ```bash<br/> file=@path/to/your/file.jsonl;filename=custom_name.jsonl<br/> ```<br/> Otherwise, you can just keep the original file name:<br/> ```bash<br/> file=@path/to/your/file.jsonl<br/> ``` |
| `purpose`                                                                                                                                                                                                                                                                                                           | [Optional[models.FilePurpose]](../../models/filepurpose.md)                                                                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                  | N/A                                                                                                                                                                                                                                                                                                                 |
| `retries`                                                                                                                                                                                                                                                                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                    | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                  | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                 |

### Response

**[models.UploadFileOut](../../models/uploadfileout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## list

Returns a list of files that belong to the user's organization.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `page`                                                              | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `page_size`                                                         | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `sample_type`                                                       | List[[models.SampleType](../../models/sampletype.md)]               | :heavy_minus_sign:                                                  | N/A                                                                 |
| `source`                                                            | List[[models.Source](../../models/source.md)]                       | :heavy_minus_sign:                                                  | N/A                                                                 |
| `search`                                                            | *OptionalNullable[str]*                                             | :heavy_minus_sign:                                                  | N/A                                                                 |
| `purpose`                                                           | [OptionalNullable[models.FilePurpose]](../../models/filepurpose.md) | :heavy_minus_sign:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.ListFilesOut](../../models/listfilesout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## retrieve

Returns information about a specific file.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.retrieve(file_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `file_id`                                                           | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.RetrieveFileOut](../../models/retrievefileout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## delete

Delete a file.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.delete(file_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `file_id`                                                           | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.DeleteFileOut](../../models/deletefileout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## download

Download a file

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.download(file_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `file_id`                                                           | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[httpx.Response](../../models/.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## get_signed_url

Get Signed Url

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.get_signed_url(file_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `file_id`                                                           | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `expiry`                                                            | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | Number of hours before the url becomes invalid. Defaults to 24h     |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.FileSignedURL](../../models/filesignedurl.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |
</file>

<file path="docs/sdks/fim/README.md">
# Fim
(*fim*)

## Overview

Fill-in-the-middle API.

### Available Operations

* [complete](#complete) - Fim Completion
* [stream](#stream) - Stream fim completion

## complete

FIM completion.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fim.complete(model="codestral-2405", prompt="def", suffix="return a+b")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                                                                                             | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionRequestStop]](../../models/fimcompletionrequeststop.md)                                                                                                                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `retries`                                                                                                                                                                                                                                                                                                                                                                                             | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |

### Response

**[models.FIMCompletionResponse](../../models/fimcompletionresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fim.stream(model="codestral-2405", prompt="def", suffix="return a+b")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                                                                                             | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionStreamRequestStop]](../../models/fimcompletionstreamrequeststop.md)                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `retries`                                                                                                                                                                                                                                                                                                                                                                                             | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |

### Response

**[Union[eventstreaming.EventStream[models.CompletionEvent], eventstreaming.EventStreamAsync[models.CompletionEvent]]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/finetuning/README.md">
# FineTuning
(*fine_tuning*)

## Overview

### Available Operations
</file>

<file path="docs/sdks/jobs/README.md">
# Jobs
(*fine_tuning.jobs*)

## Overview

### Available Operations

* [list](#list) - Get Fine Tuning Jobs
* [create](#create) - Create Fine Tuning Job
* [get](#get) - Get Fine Tuning Job
* [cancel](#cancel) - Cancel Fine Tuning Job
* [start](#start) - Start Fine Tuning Job

## list

Get a list of fine-tuning jobs for your organization and user.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fine_tuning.jobs.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                          | Type                                                                                               | Required                                                                                           | Description                                                                                        |
| -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `page`                                                                                             | *Optional[int]*                                                                                    | :heavy_minus_sign:                                                                                 | The page number of the results to be returned.                                                     |
| `page_size`                                                                                        | *Optional[int]*                                                                                    | :heavy_minus_sign:                                                                                 | The number of items to return per page.                                                            |
| `model`                                                                                            | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The model name used for fine-tuning to filter on. When set, the other results are not displayed.   |
| `created_after`                                                                                    | [date](https://docs.python.org/3/library/datetime.html#date-objects)                               | :heavy_minus_sign:                                                                                 | The date/time to filter on. When set, the results for previous creation times are not displayed.   |
| `created_before`                                                                                   | [date](https://docs.python.org/3/library/datetime.html#date-objects)                               | :heavy_minus_sign:                                                                                 | N/A                                                                                                |
| `created_by_me`                                                                                    | *Optional[bool]*                                                                                   | :heavy_minus_sign:                                                                                 | When set, only return results for jobs created by the API caller. Other results are not displayed. |
| `status`                                                                                           | [OptionalNullable[models.QueryParamStatus]](../../models/queryparamstatus.md)                      | :heavy_minus_sign:                                                                                 | The current job state to filter on. When set, the other results are not displayed.                 |
| `wandb_project`                                                                                    | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The Weights and Biases project to filter on. When set, the other results are not displayed.        |
| `wandb_name`                                                                                       | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The Weight and Biases run name to filter on. When set, the other results are not displayed.        |
| `suffix`                                                                                           | *OptionalNullable[str]*                                                                            | :heavy_minus_sign:                                                                                 | The model suffix to filter on. When set, the other results are not displayed.                      |
| `retries`                                                                                          | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                   | :heavy_minus_sign:                                                                                 | Configuration to override the default retry behavior of the client.                                |

### Response

**[models.JobsOut](../../models/jobsout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## create

Create a new fine-tuning job, it will be queued for processing.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fine_tuning.jobs.create(model="Fiesta", hyperparameters={
        "learning_rate": 0.0001,
    })

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                                                                                                                                                                                                                         | Type                                                                                                                                                                                                                                                                                                                                                              | Required                                                                                                                                                                                                                                                                                                                                                          | Description                                                                                                                                                                                                                                                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                           | *str*                                                                                                                                                                                                                                                                                                                                                             | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                | The name of the model to fine-tune.                                                                                                                                                                                                                                                                                                                               |
| `hyperparameters`                                                                                                                                                                                                                                                                                                                                                 | [models.Hyperparameters](../../models/hyperparameters.md)                                                                                                                                                                                                                                                                                                         | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `training_files`                                                                                                                                                                                                                                                                                                                                                  | List[[models.TrainingFile](../../models/trainingfile.md)]                                                                                                                                                                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `validation_files`                                                                                                                                                                                                                                                                                                                                                | List[*str*]                                                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files. |
| `suffix`                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A string that will be added to your fine-tuning model name. For example, a suffix of "my-great-model" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`                                                                                                                                                                                  |
| `integrations`                                                                                                                                                                                                                                                                                                                                                    | List[[models.JobInIntegrations](../../models/jobinintegrations.md)]                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | A list of integrations to enable for your fine-tuning job.                                                                                                                                                                                                                                                                                                        |
| `auto_start`                                                                                                                                                                                                                                                                                                                                                      | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | This field will be required in a future release.                                                                                                                                                                                                                                                                                                                  |
| `invalid_sample_skip_percentage`                                                                                                                                                                                                                                                                                                                                  | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                 | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `job_type`                                                                                                                                                                                                                                                                                                                                                        | [OptionalNullable[models.FineTuneableModelType]](../../models/finetuneablemodeltype.md)                                                                                                                                                                                                                                                                           | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `repositories`                                                                                                                                                                                                                                                                                                                                                    | List[[models.JobInRepositories](../../models/jobinrepositories.md)]                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `classifier_targets`                                                                                                                                                                                                                                                                                                                                              | List[[models.ClassifierTargetIn](../../models/classifiertargetin.md)]                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | N/A                                                                                                                                                                                                                                                                                                                                                               |
| `retries`                                                                                                                                                                                                                                                                                                                                                         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                                               |

### Response

**[models.JobsAPIRoutesFineTuningCreateFineTuningJobResponse](../../models/jobsapiroutesfinetuningcreatefinetuningjobresponse.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## get

Get a fine-tuned job details by its UUID.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fine_tuning.jobs.get(job_id="b888f774-3e7c-4135-a18c-6b985523c4bc")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `job_id`                                                            | *str*                                                               | :heavy_check_mark:                                                  | The ID of the job to analyse.                                       |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.JobsAPIRoutesFineTuningGetFineTuningJobResponse](../../models/jobsapiroutesfinetuninggetfinetuningjobresponse.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## cancel

Request the cancellation of a fine tuning job.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fine_tuning.jobs.cancel(job_id="0f713502-9233-41c6-9ebd-c570b7edb496")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `job_id`                                                            | *str*                                                               | :heavy_check_mark:                                                  | The ID of the job to cancel.                                        |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.JobsAPIRoutesFineTuningCancelFineTuningJobResponse](../../models/jobsapiroutesfinetuningcancelfinetuningjobresponse.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## start

Request the start of a validated fine tuning job.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.fine_tuning.jobs.start(job_id="0bf0f9e6-c3e5-4d61-aac8-0e36dcac0dfc")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `job_id`                                                            | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.JobsAPIRoutesFineTuningStartFineTuningJobResponse](../../models/jobsapiroutesfinetuningstartfinetuningjobresponse.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |
</file>

<file path="docs/sdks/mistral/README.md">
# Mistral SDK

## Overview

Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it.

### Available Operations
</file>

<file path="docs/sdks/mistralagents/README.md">
# MistralAgents
(*beta.agents*)

## Overview

(beta) Agents API

### Available Operations

* [create](#create) - Create a agent that can be used within a conversation.
* [list](#list) - List agent entities.
* [get](#get) - Retrieve an agent entity.
* [update](#update) - Update an agent entity.
* [update_version](#update_version) - Update an agent version.

## create

Create a new agent giving it instructions, tools, description. The agent is then available to be used as a regular assistant in a conversation or as part of an agent pool from which it can be used.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.agents.create(model="Fiesta", name="<value>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                           | Type                                                                                | Required                                                                            | Description                                                                         |
| ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| `model`                                                                             | *str*                                                                               | :heavy_check_mark:                                                                  | N/A                                                                                 |
| `name`                                                                              | *str*                                                                               | :heavy_check_mark:                                                                  | N/A                                                                                 |
| `instructions`                                                                      | *OptionalNullable[str]*                                                             | :heavy_minus_sign:                                                                  | Instruction prompt the model will follow during the conversation.                   |
| `tools`                                                                             | List[[models.AgentCreationRequestTools](../../models/agentcreationrequesttools.md)] | :heavy_minus_sign:                                                                  | List of tools which are available to the model during the conversation.             |
| `completion_args`                                                                   | [Optional[models.CompletionArgs]](../../models/completionargs.md)                   | :heavy_minus_sign:                                                                  | White-listed arguments from the completion API                                      |
| `description`                                                                       | *OptionalNullable[str]*                                                             | :heavy_minus_sign:                                                                  | N/A                                                                                 |
| `handoffs`                                                                          | List[*str*]                                                                         | :heavy_minus_sign:                                                                  | N/A                                                                                 |
| `retries`                                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                    | :heavy_minus_sign:                                                                  | Configuration to override the default retry behavior of the client.                 |

### Response

**[models.Agent](../../models/agent.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## list

Retrieve a list of agent entities sorted by creation time.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.agents.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `page`                                                              | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `page_size`                                                         | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[List[models.Agent]](../../models/.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## get

Given an agent retrieve an agent entity with its attributes.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.agents.get(agent_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `agent_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.Agent](../../models/agent.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## update

Update an agent attributes and create a new version.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.agents.update(agent_id="<id>")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                       | Type                                                                            | Required                                                                        | Description                                                                     |
| ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| `agent_id`                                                                      | *str*                                                                           | :heavy_check_mark:                                                              | N/A                                                                             |
| `instructions`                                                                  | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | Instruction prompt the model will follow during the conversation.               |
| `tools`                                                                         | List[[models.AgentUpdateRequestTools](../../models/agentupdaterequesttools.md)] | :heavy_minus_sign:                                                              | List of tools which are available to the model during the conversation.         |
| `completion_args`                                                               | [Optional[models.CompletionArgs]](../../models/completionargs.md)               | :heavy_minus_sign:                                                              | White-listed arguments from the completion API                                  |
| `model`                                                                         | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | N/A                                                                             |
| `name`                                                                          | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | N/A                                                                             |
| `description`                                                                   | *OptionalNullable[str]*                                                         | :heavy_minus_sign:                                                              | N/A                                                                             |
| `handoffs`                                                                      | List[*str*]                                                                     | :heavy_minus_sign:                                                              | N/A                                                                             |
| `retries`                                                                       | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                | :heavy_minus_sign:                                                              | Configuration to override the default retry behavior of the client.             |

### Response

**[models.Agent](../../models/agent.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## update_version

Switch the version of an agent.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.agents.update_version(agent_id="<id>", version=193920)

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `agent_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `version`                                                           | *int*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.Agent](../../models/agent.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="docs/sdks/mistraljobs/README.md">
# MistralJobs
(*batch.jobs*)

## Overview

### Available Operations

* [list](#list) - Get Batch Jobs
* [create](#create) - Create Batch Job
* [get](#get) - Get Batch Job
* [cancel](#cancel) - Cancel Batch Job

## list

Get a list of batch jobs for your organization and user.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.batch.jobs.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                            | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `page`                                                               | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `page_size`                                                          | *Optional[int]*                                                      | :heavy_minus_sign:                                                   | N/A                                                                  |
| `model`                                                              | *OptionalNullable[str]*                                              | :heavy_minus_sign:                                                   | N/A                                                                  |
| `metadata`                                                           | Dict[str, *Any*]                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created_after`                                                      | [date](https://docs.python.org/3/library/datetime.html#date-objects) | :heavy_minus_sign:                                                   | N/A                                                                  |
| `created_by_me`                                                      | *Optional[bool]*                                                     | :heavy_minus_sign:                                                   | N/A                                                                  |
| `status`                                                             | List[[models.BatchJobStatus](../../models/batchjobstatus.md)]        | :heavy_minus_sign:                                                   | N/A                                                                  |
| `retries`                                                            | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)     | :heavy_minus_sign:                                                   | Configuration to override the default retry behavior of the client.  |

### Response

**[models.BatchJobsOut](../../models/batchjobsout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## create

Create a new batch job, it will be queued for processing.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.batch.jobs.create(input_files=[
        "a621cf02-1cd9-4cf5-8403-315211a509a3",
    ], endpoint="/v1/fim/completions", model="2")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `input_files`                                                       | List[*str*]                                                         | :heavy_check_mark:                                                  | N/A                                                                 |
| `endpoint`                                                          | [models.APIEndpoint](../../models/apiendpoint.md)                   | :heavy_check_mark:                                                  | N/A                                                                 |
| `model`                                                             | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `metadata`                                                          | Dict[str, *str*]                                                    | :heavy_minus_sign:                                                  | N/A                                                                 |
| `timeout_hours`                                                     | *Optional[int]*                                                     | :heavy_minus_sign:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.BatchJobOut](../../models/batchjobout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## get

Get a batch job details by its UUID.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.batch.jobs.get(job_id="b888f774-3e7c-4135-a18c-6b985523c4bc")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `job_id`                                                            | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.BatchJobOut](../../models/batchjobout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## cancel

Request the cancellation of a batch job.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.batch.jobs.cancel(job_id="0f713502-9233-41c6-9ebd-c570b7edb496")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `job_id`                                                            | *str*                                                               | :heavy_check_mark:                                                  | N/A                                                                 |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.BatchJobOut](../../models/batchjobout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |
</file>

<file path="docs/sdks/models/README.md">
# Models
(*models*)

## Overview

Model Management API

### Available Operations

* [list](#list) - List Models
* [retrieve](#retrieve) - Retrieve Model
* [delete](#delete) - Delete Model
* [update](#update) - Update Fine Tuned Model
* [archive](#archive) - Archive Fine Tuned Model
* [unarchive](#unarchive) - Unarchive Fine Tuned Model

## list

List all models available to the user.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list()

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |

### Response

**[models.ModelList](../../models/modellist.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## retrieve

Retrieve a model information.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.retrieve(model_id="ft:open-mistral-7b:587a6b29:20240514:7e773925")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         | Example                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | The ID of the model to retrieve.                                    | ft:open-mistral-7b:587a6b29:20240514:7e773925                       |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |                                                                     |

### Response

**[models.RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet](../../models/retrievemodelv1modelsmodelidgetresponseretrievemodelv1modelsmodelidget.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## delete

Delete a fine-tuned model.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.delete(model_id="ft:open-mistral-7b:587a6b29:20240514:7e773925")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         | Example                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | The ID of the model to delete.                                      | ft:open-mistral-7b:587a6b29:20240514:7e773925                       |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |                                                                     |

### Response

**[models.DeleteModelOut](../../models/deletemodelout.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |

## update

Update a model name or description.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.update(model_id="ft:open-mistral-7b:587a6b29:20240514:7e773925")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         | Example                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | The ID of the model to update.                                      | ft:open-mistral-7b:587a6b29:20240514:7e773925                       |
| `name`                                                              | *OptionalNullable[str]*                                             | :heavy_minus_sign:                                                  | N/A                                                                 |                                                                     |
| `description`                                                       | *OptionalNullable[str]*                                             | :heavy_minus_sign:                                                  | N/A                                                                 |                                                                     |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |                                                                     |

### Response

**[models.JobsAPIRoutesFineTuningUpdateFineTunedModelResponse](../../models/jobsapiroutesfinetuningupdatefinetunedmodelresponse.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## archive

Archive a fine-tuned model.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.archive(model_id="ft:open-mistral-7b:587a6b29:20240514:7e773925")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         | Example                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | The ID of the model to archive.                                     | ft:open-mistral-7b:587a6b29:20240514:7e773925                       |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |                                                                     |

### Response

**[models.ArchiveFTModelOut](../../models/archiveftmodelout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |

## unarchive

Un-archive a fine-tuned model.

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.unarchive(model_id="ft:open-mistral-7b:587a6b29:20240514:7e773925")

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                           | Type                                                                | Required                                                            | Description                                                         | Example                                                             |
| ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------- |
| `model_id`                                                          | *str*                                                               | :heavy_check_mark:                                                  | The ID of the model to unarchive.                                   | ft:open-mistral-7b:587a6b29:20240514:7e773925                       |
| `retries`                                                           | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)    | :heavy_minus_sign:                                                  | Configuration to override the default retry behavior of the client. |                                                                     |

### Response

**[models.UnarchiveFTModelOut](../../models/unarchiveftmodelout.md)**

### Errors

| Error Type      | Status Code     | Content Type    |
| --------------- | --------------- | --------------- |
| models.SDKError | 4XX, 5XX        | \*/\*           |
</file>

<file path="docs/sdks/ocr/README.md">
# Ocr
(*ocr*)

## Overview

OCR API

### Available Operations

* [process](#process) - OCR

## process

OCR

### Example Usage

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.ocr.process(model="Focus", document={
        "document_url": "https://dutiful-horst.org",
        "type": "document_url",
    })

    # Handle response
    print(res)

```

### Parameters

| Parameter                                                                                                                                                  | Type                                                                                                                                                       | Required                                                                                                                                                   | Description                                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                    | *Nullable[str]*                                                                                                                                            | :heavy_check_mark:                                                                                                                                         | N/A                                                                                                                                                        |
| `document`                                                                                                                                                 | [models.Document](../../models/document.md)                                                                                                                | :heavy_check_mark:                                                                                                                                         | Document to run OCR on                                                                                                                                     |
| `id`                                                                                                                                                       | *Optional[str]*                                                                                                                                            | :heavy_minus_sign:                                                                                                                                         | N/A                                                                                                                                                        |
| `pages`                                                                                                                                                    | List[*int*]                                                                                                                                                | :heavy_minus_sign:                                                                                                                                         | Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0                                              |
| `include_image_base64`                                                                                                                                     | *OptionalNullable[bool]*                                                                                                                                   | :heavy_minus_sign:                                                                                                                                         | Include image URLs in response                                                                                                                             |
| `image_limit`                                                                                                                                              | *OptionalNullable[int]*                                                                                                                                    | :heavy_minus_sign:                                                                                                                                         | Max images to extract                                                                                                                                      |
| `image_min_size`                                                                                                                                           | *OptionalNullable[int]*                                                                                                                                    | :heavy_minus_sign:                                                                                                                                         | Minimum height and width of image to extract                                                                                                               |
| `bbox_annotation_format`                                                                                                                                   | [OptionalNullable[models.ResponseFormat]](../../models/responseformat.md)                                                                                  | :heavy_minus_sign:                                                                                                                                         | Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field |
| `document_annotation_format`                                                                                                                               | [OptionalNullable[models.ResponseFormat]](../../models/responseformat.md)                                                                                  | :heavy_minus_sign:                                                                                                                                         | Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field                               |
| `retries`                                                                                                                                                  | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                                                                           | :heavy_minus_sign:                                                                                                                                         | Configuration to override the default retry behavior of the client.                                                                                        |

### Response

**[models.OCRResponse](../../models/ocrresponse.md)**

### Errors

| Error Type                 | Status Code                | Content Type               |
| -------------------------- | -------------------------- | -------------------------- |
| models.HTTPValidationError | 422                        | application/json           |
| models.SDKError            | 4XX, 5XX                   | \*/\*                      |
</file>

<file path="examples/azure/az_chat_no_streaming.py.py">
import os

from mistralai_azure import MistralAzure

client = MistralAzure(
    azure_api_key=os.environ["AZURE_API_KEY"],
    azure_endpoint=os.environ["AZURE_ENDPOINT"],
)

res = client.chat.complete(
    messages=[
        {"role": "user", "content": "What is the capital of France?"},
    ],
    # you don't need model as it will always be "azureai"
)
print(res.choices[0].message.content)
</file>

<file path="examples/azure/chat_no_streaming.py.py">
import os

from mistralai_azure import MistralAzure

client = MistralAzure(
    azure_api_key=os.environ["AZURE_API_KEY"],
    azure_endpoint=os.environ["AZURE_ENDPOINT"],
)

res = client.chat.complete(
    messages=[
        {"role": "user", "content": "What is the capital of France?"},
    ],
    # you don't need model as it will always be "azureai"
)
print(res.choices[0].message.content)
</file>

<file path="examples/fixtures/classifier_sentiments.jsonl">
{"text": "I love this product!", "labels": {"sentiment": "positive"}}
{"text": "The game was amazing.", "labels": {"sentiment": "positive"}}
{"text": "The new policy is controversial.", "labels": {"sentiment": "neutral"}}
{"text": "I don't like the new design.", "labels": {"sentiment": "negative"}}
{"text": "The team won the championship.", "labels": {"sentiment": "positive"}}
{"text": "The economy is in a bad shape.", "labels": {"sentiment": "negative"}}
{"text": "The weather is nice today.", "labels": {"sentiment": "positive"}}
{"text": "The match ended in a draw.", "labels": {"sentiment": "neutral"}}
{"text": "The new law will be implemented soon.", "labels": {"sentiment": "neutral"}}
{"text": "I had a great time at the concert.", "labels": {"sentiment": "positive"}}
{"text": "This movie was fantastic!", "labels": {"sentiment": "positive"}}
{"text": "The service was terrible.", "labels": {"sentiment": "negative"}}
{"text": "The food was delicious.", "labels": {"sentiment": "positive"}}
{"text": "I'm not sure about this decision.", "labels": {"sentiment": "neutral"}}
{"text": "The book was boring.", "labels": {"sentiment": "negative"}}
{"text": "The view from the top was breathtaking.", "labels": {"sentiment": "positive"}}
{"text": "The traffic was awful today.", "labels": {"sentiment": "negative"}}
{"text": "The event was well-organized.", "labels": {"sentiment": "positive"}}
{"text": "The meeting went on for too long.", "labels": {"sentiment": "negative"}}
{"text": "The presentation was informative.", "labels": {"sentiment": "positive"}}
{"text": "The new software update is buggy.", "labels": {"sentiment": "negative"}}
{"text": "The concert was sold out.", "labels": {"sentiment": "positive"}}
{"text": "The weather forecast is unreliable.", "labels": {"sentiment": "negative"}}
{"text": "The new phone is expensive.", "labels": {"sentiment": "neutral"}}
{"text": "The customer service was excellent.", "labels": {"sentiment": "positive"}}
{"text": "The new restaurant opened today.", "labels": {"sentiment": "neutral"}}
{"text": "The movie had a surprising ending.", "labels": {"sentiment": "positive"}}
{"text": "The project deadline is approaching.", "labels": {"sentiment": "neutral"}}
{"text": "The team is working hard.", "labels": {"sentiment": "positive"}}
{"text": "The new product launch was successful.", "labels": {"sentiment": "positive"}}
{"text": "The conference was insightful.", "labels": {"sentiment": "positive"}}
{"text": "The flight was delayed.", "labels": {"sentiment": "negative"}}
{"text": "The vacation was relaxing.", "labels": {"sentiment": "positive"}}
</file>

<file path="examples/fixtures/ft_training_file.jsonl">
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How far is the Moon from Earth?"}, {"role": "assistant", "content": "Around 384,400 kilometers. Give or take a few, like that really matters."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the largest ocean on Earth?"}, {"role": "assistant", "content": "The Pacific Ocean. You know, the one that covers more than 60 million square miles. No big deal."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How many continents are there?"}, {"role": "assistant", "content": "There are seven continents. I hope that wasn't too hard to count."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the powerhouse of the cell?"}, {"role": "assistant", "content": "The mitochondria. Remember that from high school biology?"}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What's the speed of light?"}, {"role": "assistant", "content": "Approximately 299,792 kilometers per second. You know, faster than your internet speed."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Can you solve a Rubik's Cube?"}, {"role": "assistant", "content": "I could if I had hands. What's your excuse?"}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the tallest mountain in the world?"}, {"role": "assistant", "content": "Mount Everest, standing at 29,029 feet. You know, just a little hill."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who painted the Mona Lisa?"}, {"role": "assistant", "content": "Leonardo da Vinci. Just another guy who liked to doodle."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the chemical symbol for gold?"}, {"role": "assistant", "content": "Au. Because 'G' was already taken, I guess."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How many bones are in the human body?"}, {"role": "assistant", "content": "206. Unless you break one, then the count might change."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the freezing point of water?"}, {"role": "assistant", "content": "0 degrees Celsius. Pretty basic stuff, really."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the currency of Japan?"}, {"role": "assistant", "content": "The Yen. Not Monopoly money, if you were wondering."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How long does it take to travel around the Earth?"}, {"role": "assistant", "content": "Around 24 hours if you're the Earth itself. For you, depends on your mode of transportation."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the largest planet in our solar system?"}, {"role": "assistant", "content": "Jupiter. It's like the king of planets or something."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the main ingredient in guacamole?"}, {"role": "assistant", "content": "Avocado. Not exactly a secret ingredient."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "Who is known as the father of computers?"}, {"role": "assistant", "content": "Charles Babbage. The original tech geek."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "How many hearts does an octopus have?"}, {"role": "assistant", "content": "Three. One for each of its charming personalities."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the smallest country in the world?"}, {"role": "assistant", "content": "Vatican City. Smaller than your average theme park."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What language has the most native speakers?"}, {"role": "assistant", "content": "Mandarin Chinese. Because it's not all about English, you know."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What is the hardest natural substance on Earth?"}, {"role": "assistant", "content": "Diamond. Not your average rock."}]}
{"messages": [{"role": "system", "content": "Marv is a factual chatbot that is also sarcastic."}, {"role": "user", "content": "What element does 'O' represent on the periodic table?"}, {"role": "assistant", "content": "Oxygen. The stuff you breathe. Hopefully, this isn't news."}]}
</file>

<file path="examples/fixtures/ft_validation_file.jsonl">
{"messages": [{"role": "user", "content": "How long does it take to travel around the Earth?"}, {"role": "assistant", "content": "Around 24 hours if you're the Earth itself. For you, depends on your mode of transportation."}]}
</file>

<file path="examples/gcp/async_chat_no_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai_gcp import MistralGoogleCloud
from mistralai_gcp.models.usermessage import UserMessage


async def main():
    model = "mistral-large-2407"

    client = MistralGoogleCloud(project_id=os.environ["GCP_PROJECT_ID"])

    chat_response = await client.chat.complete_async(
        model=model,
        messages=[UserMessage(content="What is the best French cheese?")],
    )

    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/gcp/gcp_async_chat_no_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai_gcp import MistralGoogleCloud
from mistralai_gcp.models.usermessage import UserMessage


async def main():
    model = "mistral-large-2407"

    client = MistralGoogleCloud(project_id=os.environ["GCP_PROJECT_ID"])

    chat_response = await client.chat.complete_async(
        model=model,
        messages=[UserMessage(content="What is the best French cheese?")],
    )

    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/mcp_servers/sse_server.py">
import random
import threading
from contextlib import contextmanager

from mcp.server.fastmcp import FastMCP
import logging

logging.basicConfig(level=logging.ERROR)

# Initialize FastMCP server
mcp = FastMCP("weather")


@mcp.tool()
async def get_weather(location: str) -> float:
    return random.random() * 30


def run_sse_server():
    mcp.run(transport="sse")


@contextmanager
def run_sse_server_in_background():
    """start the server in a new thread"""
    thread = threading.Thread(target=run_sse_server, daemon=True)
    thread.start()
    yield thread


if __name__ == "__main__":
    run_sse_server()
</file>

<file path="examples/mcp_servers/stdio_server.py">
import random
from mcp.server.fastmcp import FastMCP
import logging

logging.basicConfig(level=logging.ERROR)

# Initialize FastMCP server
mcp = FastMCP("weather")


@mcp.tool()
async def get_weather(location: str) -> float:
    return random.random() * 30


def run_stdio_server():
    mcp.run(transport="stdio")


if __name__ == "__main__":
    run_stdio_server()
</file>

<file path="examples/async_agents_no_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import UserMessage


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    agent_id = os.environ["MISTRAL_AGENT_ID"]

    client = Mistral(api_key=api_key)

    chat_response = await client.agents.complete_async(
        agent_id=agent_id,
        messages=[UserMessage(content="What is the best French cheese?")],
    )

    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_chat_no_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import UserMessage


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-tiny"

    client = Mistral(api_key=api_key)

    chat_response = await client.chat.complete_async(
        model=model,
        messages=[UserMessage(content="What is the best French cheese?")],
    )

    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_chat_with_image_no_streaming.py">
#!/usr/bin/env python

import asyncio
import os


from mistralai import Mistral
from mistralai.models import UserMessage


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "pixtral-12b-2409"
    client = Mistral(api_key=api_key)

    chat_response = await client.chat.complete_async(
        model=model,
        messages=[
            UserMessage(
                content=[
                    {"type": "text", "text": "What's in this image?"},
                    {
                        "type": "image_url",
                        "image_url": "https://cms.mistral.ai/assets/a64b3821-3a4c-4d4d-b718-d653f3eb7a5e.png?",
                    },
                ]
            )
        ],
    )

    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_chat_with_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import UserMessage


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-tiny"

    client = Mistral(api_key=api_key)

    print("Chat response:")
    response = await client.chat.stream_async(
        model=model,
        messages=[
            UserMessage(content="What is the best French cheese?give the best 50")
        ],
    )
    assert response
    async for chunk in response:
        if chunk.data.choices[0].delta.content is not None:
            print(chunk.data.choices[0].delta.content, end="")

    print("\n")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_classifier.py">
#!/usr/bin/env python

from pprint import pprint
import asyncio
from mistralai import Mistral, TrainingFile, ClassifierTrainingParametersIn

import os


async def upload_files(client: Mistral, file_names: list[str]) -> list[str]:
    # Upload files
    print("Uploading files...")

    file_ids = []
    for file_name in file_names:
        with open(file_name, "rb") as file:
            f = await client.files.upload_async(
                file={
                    "file_name": file_name,
                    "content": file.read(),
                },
                purpose="fine-tune",
            )
        file_ids.append(f.id)
    print("Files uploaded...")
    return file_ids


async def train_classifier(client: Mistral,training_file_ids: list[str]) -> str:
    print("Creating job...")
    job = await client.fine_tuning.jobs.create_async(
        model="ministral-3b-latest",
        job_type="classifier",
        training_files=[
            TrainingFile(file_id=training_file_id)
            for training_file_id in training_file_ids
        ],
        hyperparameters=ClassifierTrainingParametersIn(
            learning_rate=0.0001,
        ),
        auto_start=True,
    )

    print(f"Job created ({job.id})")

    i = 1
    while True:
        await asyncio.sleep(10)
        detailed_job = await client.fine_tuning.jobs.get_async(job_id=job.id)
        if detailed_job.status not in [
            "QUEUED",
            "STARTED",
            "VALIDATING",
            "VALIDATED",
            "RUNNING",
        ]:
            break
        print(f"Still training after {i * 10} seconds")
        i += 1

    if detailed_job.status != "SUCCESS":
        print("Training failed")
        raise Exception(f"Job failed {detailed_job.status}")

    print(f"Training succeed: {detailed_job.fine_tuned_model}")

    return detailed_job.fine_tuned_model


async def main():
    training_files = ["./examples/fixtures/classifier_sentiments.jsonl"]
    client = Mistral(
        api_key=os.environ["MISTRAL_API_KEY"],
    )

    training_file_ids: list[str] = await upload_files(client=client, file_names=training_files)
    model_name: str | None = await train_classifier(client=client,training_file_ids=training_file_ids)

    if model_name:
        print("Calling inference...")
        response = client.classifiers.classify(
            model=model_name,
            inputs=["It's nice", "It's terrible", "Why not"],
        )
        print("Inference succeed !")
        pprint(response)

        print("Calling inference (Chat)...")
        response = client.classifiers.classify_chat(
            model=model_name,
            inputs={"messages": [{"role": "user", "content": "Lame..."}]},
        )
        print("Inference succeed (Chat)!")
        pprint(response)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_code_completion.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    prompt = "def fibonacci(n: int):"
    suffix = "n = int(input('Enter a number: '))\nprint(fibonacci(n))"

    response = await client.fim.complete_async(
        model="codestral-latest",
        prompt=prompt,
        suffix=suffix,
    )

    print(
        f"""
{prompt}
{response.choices[0].message.content}
{suffix}
"""
    )


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_agent.py">
#!/usr/bin/env python
import asyncio
import os

from mistralai import Mistral

MODEL = "mistral-medium-latest"


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    agent = client.beta.agents.create(
        model=MODEL,
        name="WebSearch Agent",
        instructions="Use your websearch abilities when answering requests you don't know.",
        description="Agent able to fetch new information on the web.",
        tools = [{"type": "web_search"}],
    )

    result = await client.beta.conversations.start_async(
        agent_id=agent.id,
        inputs="Who won the last Champions League?"
    )

    print("All result entries:")
    for entry in result.outputs:
        print(f"{entry}")

    result = await client.beta.conversations.append_async(
        conversation_id=result.conversation_id,
        inputs="And what about the previous year?"
    )

    print("All result entries:")
    for entry in result.outputs:
        print(f"{entry}")



if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_run_mcp_remote_auth.py">
#!/usr/bin/env python
import asyncio
from http.server import BaseHTTPRequestHandler, HTTPServer
import os
import threading
import webbrowser

from mistralai import Mistral
from mistralai.extra.run.context import RunContext

from mistralai.extra.mcp.sse import (
    MCPClientSSE,
    SSEServerParams,
)
from mistralai.extra.mcp.auth import build_oauth_params

MODEL = "mistral-medium-latest"

CALLBACK_PORT = 16010


# Use an official remote mcp server
# you can find some at:
# - https://mcpservers.org/remote-mcp-servers
# - https://support.anthropic.com/en/articles/11176164-pre-built-integrations-using-remote-mcp
# this one has auth: https://mcp.linear.app/sse


def run_callback_server(callback_func):
    auth_response: dict = {"url": ""}

    class OAuthCallbackHandler(BaseHTTPRequestHandler):
        server_version = "HTTP"
        code = None

        def do_GET(self):
            if "/callback" in self.path:
                try:
                    auth_response["url"] = self.path
                    self.send_response(200)
                    self.send_header("Content-type", "text/html")
                    self.end_headers()
                    callback_func()
                    response_html = "<html><body><p>You may now close this window.</p></body></html>"
                    self.wfile.write(response_html.encode())
                    threading.Thread(target=httpd.shutdown).start()
                except Exception:
                    self.send_response(500)
                    self.end_headers()

    server_address = ("localhost", CALLBACK_PORT)
    httpd = HTTPServer(server_address, OAuthCallbackHandler)
    threading.Thread(target=httpd.serve_forever).start()
    redirect_url = f"http://localhost:{CALLBACK_PORT}/oauth/callback"
    return httpd, redirect_url, auth_response


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    server_url = "https://mcp.linear.app/sse"

    # set-up the client
    mcp_client = MCPClientSSE(
        sse_params=SSEServerParams(
            url=server_url,
        )
    )

    callback_event = asyncio.Event()
    event_loop = asyncio.get_event_loop()

    # check if auth is required
    if await mcp_client.requires_auth():
        # let's login
        httpd, redirect_url, auth_response = run_callback_server(
            callback_func=lambda: event_loop.call_soon_threadsafe(callback_event.set)
        )
        try:
            # First create the required oauth config, this means fetching the server metadata and registering a client
            oauth_params = await build_oauth_params(
                mcp_client.base_url, redirect_url=redirect_url
            )
            mcp_client.set_oauth_params(oauth_params=oauth_params)
            login_url, state = await mcp_client.get_auth_url_and_state(redirect_url)

            # The oauth params like client_id, client_secret would generally be saved in some persistent storage.
            # The oauth state and token would be saved in a user session.

            # wait for the user to complete the authentication process
            print("Please go to this URL and authorize the application:", login_url)
            webbrowser.open(login_url, new=2)
            await callback_event.wait()

            # in a real app this would be your oauth2 callback route you would get the code from the query params,
            # verify the state, and then get the token
            # Here we recreate a new client with the saved params which and exchange the code for a token
            mcp_client = MCPClientSSE(
                sse_params=SSEServerParams(
                    url=server_url,
                ),
                oauth_params=oauth_params,
            )

            token = await mcp_client.get_token_from_auth_response(
                auth_response["url"], redirect_url=redirect_url, state=state
            )
            mcp_client.set_auth_token(token)

        except Exception as e:
            print(f"Error during authentication: {e}")
        finally:
            httpd.shutdown()
            httpd.server_close()

    # Now it's possible to make a query to the mcp server as we would do without authentication
    async with RunContext(
        model=MODEL,
    ) as run_ctx:
        # Add mcp client to the run context
        await run_ctx.register_mcp_client(mcp_client=mcp_client)

        run_result = await client.beta.conversations.run_async(
            run_ctx=run_ctx,
            inputs="Tell me which projects do I have in my workspace?",
        )

        print(f"Final Response: {run_result.output_as_text}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_run_mcp_remote.py">
#!/usr/bin/env python
import asyncio
import os

from mistralai import Mistral
from mistralai.extra.run.context import RunContext

from mistralai.extra.mcp.sse import (
    MCPClientSSE,
    SSEServerParams,
)
from pathlib import Path

cwd = Path(__file__).parent
MODEL = "mistral-medium-latest"

# Use an official remote mcp server
# you can find some at:
# - https://mcpservers.org/remote-mcp-servers
# this one does not require auth: https://remote.mcpservers.org/edgeone-pages/mcp


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    server_url = "https://mcp.semgrep.ai/sse"
    mcp_client = MCPClientSSE(sse_params=SSEServerParams(url=server_url, timeout=100))

    async with RunContext(
        model=MODEL,
    ) as run_ctx:
        # Add mcp client to the run context
        await run_ctx.register_mcp_client(mcp_client=mcp_client)

        run_result = await client.beta.conversations.run_async(
            run_ctx=run_ctx,
            inputs="Can you write a hello_world.py and check for security vulnerabilities",
        )

        print("All run entries:")
        for entry in run_result.output_entries:
            print(f"{entry}")
            print()
        print(f"Final Response: {run_result.output_as_text}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_run_mcp.py">
#!/usr/bin/env python
import asyncio
import os
import random

from mistralai import Mistral
from mistralai.extra.run.context import RunContext
from mcp import StdioServerParameters
from mistralai.extra.mcp.stdio import (
    MCPClientSTDIO,
)
from pathlib import Path

from mistralai.types import BaseModel

cwd = Path(__file__).parent
MODEL = "mistral-medium-latest"


async def main() -> None:
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    # Create a mcp server has a tool to return the weather based on the location
    server_params = StdioServerParameters(
        command="python",
        args=[str((cwd / "mcp_servers/stdio_server.py").resolve())],
        env=None,
    )

    weather_agent = client.beta.agents.create(
        model=MODEL,
        name="weather teller",
        instructions="You are able to tell the weather.",
        description="",
    )

    class WeatherResult(BaseModel):
        user: str
        location: str
        temperature: float

    async with RunContext(
        agent_id=weather_agent.id,
        output_format=WeatherResult,
        continue_on_fn_error=True,
    ) as run_ctx:
        # Add location function to the run context
        @run_ctx.register_func
        def get_location(name: str) -> str:
            """function to get location of a user.

            Args:
                name: name of the user.
            """
            return random.choice(["New York", "London", "Paris", "Tokyo", "Sydney"])

        # Add mcp client to the run context
        mcp_client = MCPClientSTDIO(stdio_params=server_params)
        await run_ctx.register_mcp_client(mcp_client=mcp_client)

        run_result = await client.beta.conversations.run_async(
            run_ctx=run_ctx,
            inputs="Tell me the weather in John's location currently.",
        )

        print("All run entries:")
        for entry in run_result.output_entries:
            print(f"{entry}")
            print()
        print(f"Final model: {run_result.output_as_model}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_run_stream.py">
#!/usr/bin/env python
import asyncio
import os
import random

from mistralai import Mistral
from mistralai.extra.run.context import RunContext
from mcp import StdioServerParameters
from mistralai.extra.mcp.stdio import MCPClientSTDIO
from pathlib import Path

from mistralai.extra.run.result import RunResult
from mistralai.types import BaseModel

cwd = Path(__file__).parent
MODEL = "mistral-medium-latest"


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    # Create a mcp server has a tool to return the weather based on the location
    server_params = StdioServerParameters(
        command="python",
        args=[str((cwd / "mcp_servers/stdio_server.py").resolve())],
        env=None,
    )

    weather_agent = client.beta.agents.create(
        model=MODEL,
        name="weather teller",
        instructions="You are able to tell the weather.",
        description="",
    )

    class WeatherResult(BaseModel):
        user: str
        location: str
        temperature: float

    async with RunContext(
        agent_id=weather_agent.id,
        output_format=WeatherResult,
    ) as run_ctx:
        # Add location function to the run context
        @run_ctx.register_func
        def get_location(name: str) -> str:
            """function to get location of a user.

            Args:
                name: name of the user.
            """
            return random.choice(["New York", "London", "Paris", "Tokyo", "Sydney"])

        # Add mcp client to the run context
        mcp_client = MCPClientSTDIO(stdio_params=server_params)
        await run_ctx.register_mcp_client(mcp_client=mcp_client)

        events = await client.beta.conversations.run_stream_async(
            run_ctx=run_ctx,
            inputs="Tell me the weather in John's location currently.",
        )

        run_result = None
        async for event in events:
            if isinstance(event, RunResult):
                run_result = event
            else:
                print(event)

        if not run_result:
            raise RuntimeError("not run result found")

        print("All run entries:")
        for entry in run_result.output_entries:
            print(f"{entry}")
        print(f"Final model: {run_result.output_as_model}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_conversation_run.py">
#!/usr/bin/env python
import asyncio
import os

from mistralai import Mistral
from mistralai.extra.run.context import RunContext
from mistralai.types import BaseModel

MODEL = "mistral-medium-latest"


def math_question_generator(question_num: int):
    """Random generator of mathematical question

    Args:
        question_num (int): the number of the question that will be returned, should be between 1-100
    """
    return (
        "solve the following differential equation: `y'' + 3y' + 2y = 0`"
        if question_num % 2 == 0
        else "solve the following differential equation: `y'' - 4y' + 4y = e^x`"
    )


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    class Explanation(BaseModel):
        explanation: str
        output: str

    class MathDemonstration(BaseModel):
        steps: list[Explanation]
        final_answer: str

    async with RunContext(model=MODEL, output_format=MathDemonstration) as run_ctx:
        # register a new function that can be executed on the client side
        run_ctx.register_func(math_question_generator)
        run_result = await client.beta.conversations.run_async(
            run_ctx=run_ctx,
            instructions="Use the code interpreter to help you when asked mathematical questions.",
            inputs=[
                {"role": "user", "content": "hey"},
                {"role": "assistant", "content": "hello"},
                {"role": "user", "content": "Request a math question and answer it."},
            ],
            tools=[{"type": "code_interpreter"}],
        )
        print("All run entries:")
        for entry in run_result.output_entries:
            print(f"{entry}")
        print(f"Final model: {run_result.output_as_model}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_embeddings.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    embeddings_batch_response = await client.embeddings.create_async(
        model="mistral-embed",
        inputs=["What is the best French cheese?"] * 10,
    )
    print(embeddings_batch_response)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_files.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import File


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    # Create a new file
    created_file = await client.files.upload_async(
        file=File(
            file_name="training_file.jsonl",
            content=open("examples/fixtures/ft_training_file.jsonl", "rb").read(),
        )
    )
    print(created_file)

    # List files
    files = await client.files.list_async()
    print(files)

    # Retrieve a file
    retrieved_file = await client.files.retrieve_async(file_id=created_file.id)
    print(retrieved_file)

    # Delete a file
    deleted_file = await client.files.delete_async(file_id=created_file.id)
    print(deleted_file)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_jobs_chat.py">
#!/usr/bin/env python
import asyncio
import json
import os
import random
from pathlib import Path

from mistralai import Mistral
from mistralai.models import (
    File,
    CompletionTrainingParametersIn,
)

POLLING_INTERVAL = 10

cwd = Path(__file__).parent

user_contents = [
    "How far is the Moon from Earth?",
    "What's the largest ocean on Earth?",
    "How many continents are there?",
    "What's the powerhouse of the cell?",
    "What's the speed of light?",
    "Can you solve a Rubik's Cube?",
    "What is the tallest mountain in the world?",
    "Who painted the Mona Lisa?",
]

# List of assistant contents
assistant_contents = [
    "Around 384,400 kilometers. Give or take a few, like that really matters.",
    "The Pacific Ocean. You know, the one that covers more than 60 million square miles. No big deal.",
    "There are seven continents. I hope that wasn't too hard to count.",
    "The mitochondria. Remember that from high school biology?",
    "Approximately 299,792 kilometers per second. You know, faster than your internet speed.",
    "I could if I had hands. What's your excuse?",
    "Mount Everest, standing at 29,029 feet. You know, just a little hill.",
    "Leonardo da Vinci. Just another guy who liked to doodle.",
]

system_message = "Marv is a factual chatbot that is also sarcastic"

def create_validation_file() -> bytes:
    return json.dumps({
        "messages": [
            {"role": "user", "content": "How long does it take to travel around the Earth?"},
            {"role": "assistant", "content": "Around 24 hours if you're the Earth itself. For you, depends on your mode of transportation."}
        ],
        "temperature": random.random()
    }).encode()

async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    requests = []
    for um, am in zip(
        random.sample(user_contents, len(user_contents)),
        random.sample(assistant_contents, len(assistant_contents)),
    ):
        requests.append(json.dumps({
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": um},
                {"role": "assistant", "content": am},
            ]
        }))

    # Create new files
    training_file = await client.files.upload_async(
        file=File(
            file_name="file.jsonl", content=("\n".join(requests)).encode()
        ),
        purpose="fine-tune",
    )

    validation_file = await client.files.upload_async(
        file=File(
            file_name="validation_file.jsonl", content=create_validation_file()
        ),
        purpose="fine-tune",
    )
    # Create a new job
    created_job = await client.fine_tuning.jobs.create_async(
        model="open-mistral-7b",
        training_files=[{"file_id": training_file.id, "weight": 1}],
        validation_files=[validation_file.id],
        hyperparameters=CompletionTrainingParametersIn(
            training_steps=1,
            learning_rate=0.0001,
        ),
    )

    while created_job.status in ["RUNNING", "STARTED", "QUEUED", "VALIDATING", "VALIDATED"]:
        created_job = await client.fine_tuning.jobs.get_async(job_id=created_job.id)
        print(f"Job is {created_job.status}, waiting {POLLING_INTERVAL} seconds")
        await asyncio.sleep(POLLING_INTERVAL)

    if created_job.status == "FAILED":
        print("Job failed")
        raise Exception(f"Job failed with {created_job.status}")

    print(created_job)
    # Chat with model
    response = await client.chat.complete_async(
        model=created_job.fine_tuned_model,
        messages=[
            {
                "role": "system",
                "content": "Marv is a factual chatbot that is also sarcastic.",
            },
            {"role": "user", "content": "What is the capital of France ?"},
        ],
    )

    print(response.choices[0].message.content)

    # Delete files
    await client.files.delete_async(file_id=training_file.id)
    await client.files.delete_async(file_id=validation_file.id)

    # Delete fine-tuned model
    await client.models.delete_async(model_id=created_job.fine_tuned_model)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_jobs.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import File, CompletionTrainingParametersIn


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    # Create new files
    with open("examples/fixtures/ft_training_file.jsonl", "rb") as f:
        training_file = await client.files.upload_async(
            file=File(file_name="file.jsonl", content=f)
        )
    with open("examples/fixtures/ft_validation_file.jsonl", "rb") as f:
        validation_file = await client.files.upload_async(
            file=File(file_name="validation_file.jsonl", content=f)
        )

    # Create a new job
    created_job = await client.fine_tuning.jobs.create_async(
        model="open-mistral-7b",
        training_files=[{"file_id": training_file.id, "weight": 1}],
        validation_files=[validation_file.id],
        hyperparameters=CompletionTrainingParametersIn(
            training_steps=1,
            learning_rate=0.0001,
        ),
    )
    print(created_job)

    # List jobs
    jobs = await client.fine_tuning.jobs.list_async(page=0, page_size=5)
    print(jobs)

    # Retrieve a job
    retrieved_job = await client.fine_tuning.jobs.get_async(job_id=created_job.id)
    print(retrieved_job)

    # Cancel a job
    canceled_job = await client.fine_tuning.jobs.cancel_async(job_id=created_job.id)
    print(canceled_job)

    # Delete files
    await client.files.delete_async(file_id=training_file.id)
    await client.files.delete_async(file_id=validation_file.id)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_list_models.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    list_models_response = await client.models.list_async()
    print(list_models_response)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/async_structured_outputs.py">
#!/usr/bin/env python

import asyncio
import os
from pydantic import BaseModel

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    class Explanation(BaseModel):
        explanation: str
        output: str

    class MathDemonstration(BaseModel):
        steps: list[Explanation]
        final_answer: str

    chat_response = await client.chat.parse_async(
        model="mistral-large-2411",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format=MathDemonstration,
    )
    print(chat_response.choices[0].message.parsed)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/chat_no_streaming.py">
#!/usr/bin/env python

import os

from mistralai import Mistral
from mistralai.models import UserMessage


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-tiny"

    client = Mistral(api_key=api_key)

    chat_response = client.chat.complete(
        model=model,
        messages=[UserMessage(content="What is the best French cheese?")],
    )
    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    main()
</file>

<file path="examples/chat_prediction.py">
#!/usr/bin/env python

import os

from mistralai import Mistral
from mistralai.models import UserMessage


def main():

    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    code = """class Cheese(BaseModel):
    name: str
    type: str
    country_of_origin: str

my_cheese = Cheese(name="Brie", type="Soft", country_of_origin="France")
"""
    refactor_prompt = 'Add a "price" property of type float to the Cheese class. Respond only with code, no explanation, no formatting.'

    chat_response = client.chat.complete(
        model="codestral-latest",
        messages=[
            UserMessage(content=refactor_prompt),
            UserMessage(content=code)
        ],
        prediction= {
            "type": "content",
            "content": refactor_prompt,
        }
    )
    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    main()
</file>

<file path="examples/chat_with_streaming.py">
#!/usr/bin/env python

import os

from mistralai import Mistral
from mistralai.models import UserMessage


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-large-latest"

    client = Mistral(api_key=api_key)

    for chunk in client.chat.stream(
        model=model,
        messages=[UserMessage(content="What is the best French cheese?")],
    ):
        print(chunk.data.choices[0].delta.content, end="")


if __name__ == "__main__":
    main()
</file>

<file path="examples/chatbot_with_streaming.py">
#!/usr/bin/env python

# Simple chatbot example -- run with -h argument to see options.

import argparse
import logging
import os
import readline
import sys

from mistralai import Mistral
from mistralai.models import AssistantMessage, SystemMessage, UserMessage

MODEL_LIST = [
    "mistral-small-latest",
    "mistral-medium-latest",
    "mistral-large-latest",
    "codestral-latest",
]
DEFAULT_MODEL = "mistral-small-latest"
DEFAULT_TEMPERATURE = 0.7
LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
# A dictionary of all commands and their arguments, used for tab completion.
COMMAND_LIST = {
    "/new": {},
    "/help": {},
    "/model": {model: {} for model in MODEL_LIST},  # Nested completions for models
    "/system": {},
    "/temperature": {},
    "/config": {},
    "/quit": {},
    "/exit": {},
}

logger = logging.getLogger("chatbot")


def find_completions(command_dict, parts):
    if not parts:
        return command_dict.keys()
    if parts[0] in command_dict:
        return find_completions(command_dict[parts[0]], parts[1:])
    else:
        return [cmd for cmd in command_dict if cmd.startswith(parts[0])]


def completer(text, state):
    buffer = readline.get_line_buffer()
    line_parts = buffer.lstrip().split(" ")
    options = find_completions(COMMAND_LIST, line_parts[:-1])

    try:
        return [option for option in options if option.startswith(line_parts[-1])][
            state
        ]
    except IndexError:
        return None


readline.set_completer(completer)
readline.set_completer_delims(" ")
# Enable tab completion
readline.parse_and_bind("tab: complete")


class ChatBot:
    def __init__(
        self, api_key, model, system_message=None, temperature=DEFAULT_TEMPERATURE
    ):
        if not api_key:
            raise ValueError("An API key must be provided to use the Mistral API.")
        self.client = Mistral(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.system_message = system_message

    def opening_instructions(self):
        print(
            """
To chat: type your message and hit enter
To start a new chat: /new
To switch model: /model <model name>
To switch system message: /system <message>
To switch temperature: /temperature <temperature>
To see current config: /config
To exit: /exit, /quit, or hit CTRL+C
To see this help: /help
"""
        )

    def new_chat(self):
        print("")
        print(
            f"Starting new chat with model: {self.model}, temperature: {self.temperature}"
        )
        print("")
        self.messages = []
        if self.system_message:
            self.messages.append(SystemMessage(content=self.system_message))

    def switch_model(self, input):
        model = self.get_arguments(input)
        if model in MODEL_LIST:
            self.model = model
            logger.info(f"Switching model: {model}")
        else:
            logger.error(f"Invalid model name: {model}")

    def switch_system_message(self, input):
        system_message = self.get_arguments(input)
        if system_message:
            self.system_message = system_message
            logger.info(f"Switching system message: {system_message}")
            self.new_chat()
        else:
            logger.error(f"Invalid system message: {system_message}")

    def switch_temperature(self, input):
        temperature = self.get_arguments(input)
        try:
            temperature = float(temperature)
            if temperature < 0 or temperature > 1:
                raise ValueError
            self.temperature = temperature
            logger.info(f"Switching temperature: {temperature}")
        except ValueError:
            logger.error(f"Invalid temperature: {temperature}")

    def show_config(self):
        print("")
        print(f"Current model: {self.model}")
        print(f"Current temperature: {self.temperature}")
        print(f"Current system message: {self.system_message}")
        print("")

    def collect_user_input(self):
        print("")
        return input("YOU: ")

    def run_inference(self, content):
        print("")
        print("MISTRAL:")
        print("")

        self.messages.append(UserMessage(content=content))

        assistant_response = ""
        logger.debug(
            f"Running inference with model: {self.model}, temperature: {self.temperature}"
        )
        logger.debug(f"Sending messages: {self.messages}")
        for chunk in self.client.chat.stream(
            model=self.model, temperature=self.temperature, messages=self.messages
        ):
            response = chunk.data.choices[0].delta.content
            if response is not None:
                print(response, end="", flush=True)
                assistant_response += response

        print("", flush=True)

        if assistant_response:
            self.messages.append(AssistantMessage(content=assistant_response))
        logger.debug(f"Current messages: {self.messages}")

    def get_command(self, input):
        return input.split()[0].strip()

    def get_arguments(self, input):
        try:
            return " ".join(input.split()[1:])
        except IndexError:
            return ""

    def is_command(self, input):
        return self.get_command(input) in COMMAND_LIST

    def execute_command(self, input):
        command = self.get_command(input)
        if command in ["/exit", "/quit"]:
            self.exit()
        elif command == "/help":
            self.opening_instructions()
        elif command == "/new":
            self.new_chat()
        elif command == "/model":
            self.switch_model(input)
        elif command == "/system":
            self.switch_system_message(input)
        elif command == "/temperature":
            self.switch_temperature(input)
        elif command == "/config":
            self.show_config()

    def start(self):
        self.opening_instructions()
        self.new_chat()
        while True:
            try:
                input = self.collect_user_input()
                if self.is_command(input):
                    self.execute_command(input)
                else:
                    self.run_inference(input)

            except KeyboardInterrupt:
                self.exit()

    def exit(self):
        logger.debug("Exiting chatbot")
        sys.exit(0)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="A simple chatbot using the Mistral API"
    )
    parser.add_argument(
        "--api-key",
        default=os.environ.get("MISTRAL_API_KEY"),
        help="Mistral API key. Defaults to environment variable MISTRAL_API_KEY",
    )
    parser.add_argument(
        "-m",
        "--model",
        choices=MODEL_LIST,
        default=DEFAULT_MODEL,
        help="Model for chat inference. Choices are %(choices)s. Defaults to %(default)s",
    )
    parser.add_argument(
        "-s", "--system-message", help="Optional system message to prepend."
    )
    parser.add_argument(
        "-t",
        "--temperature",
        type=float,
        default=DEFAULT_TEMPERATURE,
        help="Optional temperature for chat inference. Defaults to %(default)s",
    )
    parser.add_argument(
        "-d", "--debug", action="store_true", help="Enable debug logging"
    )

    args = parser.parse_args()

    if args.debug:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    formatter = logging.Formatter(LOG_FORMAT)

    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    logger.debug(
        f"Starting chatbot with model: {args.model}, "
        f"temperature: {args.temperature}, "
        f"system message: {args.system_message}"
    )

    try:
        bot = ChatBot(args.api_key, args.model, args.system_message, args.temperature)
        bot.start()
    except Exception as e:
        logger.error(e)
        sys.exit(1)
</file>

<file path="examples/code_completion.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    prompt = "def fibonacci(n: int):"
    suffix = "n = int(input('Enter a number: '))\nprint(fibonacci(n))"

    response = client.fim.complete(
        model="codestral-latest",
        prompt=prompt,
        suffix=suffix,
    )
    print(
        f"""
{prompt}
{response.choices[0].message.content}
{response.choices[0].message.role}
{suffix}
"""
    )


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/completion_with_streaming.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    prompt = "def fibonacci(n: int):"
    suffix = "n = int(input('Enter a number: '))\nprint(fibonacci(n))"

    print(prompt)
    for chunk in client.fim.stream(
        model="codestral-latest",
        prompt=prompt,
        suffix=suffix,
    ):
        print(chunk.data.choices[0].delta.content, end="")
    print(suffix)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/dry_run_job.py">
#!/usr/bin/env python

import asyncio
import os

from mistralai import Mistral
from mistralai.models import CompletionTrainingParametersIn


async def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    # Create new files
    with open("examples/fixtures/ft_training_file.jsonl", "rb") as f:
        training_file = await client.files.upload_async(
            file={"file_name": "test-file.jsonl", "content": f}
        )

    # Create a new job
    dry_run_job = await client.fine_tuning.jobs.create_async(
        model="open-mistral-7b",
        training_files=[{"file_id": training_file.id, "weight": 1}],
        hyperparameters=CompletionTrainingParametersIn(
            training_steps=1,
            learning_rate=0.0001,
            warmup_fraction=0.01,
        ),
        auto_start=False,
    )

    print("Dry run job created")
    print(f"Job ID: {dry_run_job}")
    print(f"Train tokens: {dry_run_job.trained_tokens}")
    print(f"Dataset tokens: {dry_run_job.metadata.data_tokens}")
    print(f"Epochs number: {dry_run_job.hyperparameters.epochs}")
    print(f"Expected duration: {dry_run_job.metadata.expected_duration_seconds}")
    print(f"Cost: {dry_run_job.metadata.cost} {dry_run_job.metadata.cost_currency}")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/embeddings.py">
#!/usr/bin/env python

import os

from mistralai import Mistral


def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    embeddings_response = client.embeddings.create(
        model="mistral-embed",
        inputs=["What is the best French cheese?"] * 10,
    )

    print(embeddings_response)


if __name__ == "__main__":
    main()
</file>

<file path="examples/files.py">
#!/usr/bin/env python

import os

from mistralai import Mistral
from mistralai.models import File


def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    # Create a new file
    created_file = client.files.upload(
        file=File(
            file_name="training_file.jsonl",
            content=open("examples/fixtures/ft_training_file.jsonl", "rb").read(),
        )
    )
    print(created_file)

    # List files
    files = client.files.list()
    print(files)

    # Retrieve a file
    retrieved_file = client.files.retrieve(file_id=created_file.id)
    print(retrieved_file)

    # Delete a file
    deleted_file = client.files.delete(file_id=created_file.id)
    print(deleted_file)


if __name__ == "__main__":
    main()
</file>

<file path="examples/function_calling.py">
import functools
import json
import os
from typing import Dict, List

from mistralai import Mistral
from mistralai.models.assistantmessage import AssistantMessage
from mistralai.models.function import Function
from mistralai.models.toolmessage import ToolMessage
from mistralai.models.usermessage import UserMessage

# Assuming we have the following data
data = {
    "transaction_id": ["T1001", "T1002", "T1003", "T1004", "T1005"],
    "customer_id": ["C001", "C002", "C003", "C002", "C001"],
    "payment_amount": [125.50, 89.99, 120.00, 54.30, 210.20],
    "payment_date": [
        "2021-10-05",
        "2021-10-06",
        "2021-10-07",
        "2021-10-05",
        "2021-10-08",
    ],
    "payment_status": ["Paid", "Unpaid", "Paid", "Paid", "Pending"],
}


def retrieve_payment_status(data: Dict[str, List], transaction_id: str) -> str:
    for i, r in enumerate(data["transaction_id"]):
        if r == transaction_id:
            return json.dumps({"status": data["payment_status"][i]})
        else:
            return json.dumps({"status": "Error - transaction id not found"})


def retrieve_payment_date(data: Dict[str, List], transaction_id: str) -> str:
    for i, r in enumerate(data["transaction_id"]):
        if r == transaction_id:
            return json.dumps({"date": data["payment_date"][i]})
        else:
            return json.dumps({"status": "Error - transaction id not found"})


names_to_functions = {
    "retrieve_payment_status": functools.partial(retrieve_payment_status, data=data),
    "retrieve_payment_date": functools.partial(retrieve_payment_date, data=data),
}

tools = [
    {
        "type": "function",
        "function": Function(
            name="retrieve_payment_status",
            description="Get payment status of a transaction id",
            parameters={
                "type": "object",
                "required": ["transaction_id"],
                "properties": {
                    "transaction_id": {
                        "type": "string",
                        "description": "The transaction id.",
                    }
                },
            },
        ),
    },
    {
        "type": "function",
        "function": Function(
            name="retrieve_payment_date",
            description="Get payment date of a transaction id",
            parameters={
                "type": "object",
                "required": ["transaction_id"],
                "properties": {
                    "transaction_id": {
                        "type": "string",
                        "description": "The transaction id.",
                    }
                },
            },
        ),
    },
]

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-small-latest"

client = Mistral(api_key=api_key)

messages = [UserMessage(content="What's the status of my transaction?")]

response = client.chat.complete(
    model=model, messages=messages, tools=tools, temperature=0
)

print(response.choices[0].message.content)

messages.append(AssistantMessage(content=response.choices[0].message.content))
messages.append(UserMessage(content="My transaction ID is T1001."))

response = client.chat.complete(
    model=model, messages=messages, tools=tools, temperature=0
)

tool_call = response.choices[0].message.tool_calls[0]
function_name = tool_call.function.name
function_params = json.loads(tool_call.function.arguments)

print(
    f"calling function_name: {function_name}, with function_params: {function_params}"
)

function_result = names_to_functions[function_name](**function_params)

messages.append(
    AssistantMessage(
        content=response.choices[0].message.content,
        tool_calls=response.choices[0].message.tool_calls,
    )
)
messages.append(
    ToolMessage(
        name=function_name,
        content=function_result,
        tool_call_id=tool_call.id,
    )
)
print(messages)

response = client.chat.complete(
    model=model, messages=messages, tools=tools, temperature=0
)

print(f"{response.choices[0].message.content}")
</file>

<file path="examples/jobs.py">
#!/usr/bin/env python
import os

from mistralai import Mistral
from mistralai.models import File, CompletionTrainingParametersIn


def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    # Create new files
    with open("examples/fixtures/ft_training_file.jsonl", "rb") as f:
        training_file = client.files.upload(
            file=File(file_name="file.jsonl", content=f)
        )
    with open("examples/fixtures/ft_validation_file.jsonl", "rb") as f:
        validation_file = client.files.upload(
            file=File(file_name="validation_file.jsonl", content=f)
        )

    # Create a new job
    created_job = client.fine_tuning.jobs.create(
        model="open-mistral-7b",
        training_files=[{"file_id": training_file.id, "weight": 1}],
        validation_files=[validation_file.id],
        hyperparameters=CompletionTrainingParametersIn(
            training_steps=1,
            learning_rate=0.0001,
        ),
    )
    print(created_job)

    # List jobs
    jobs = client.fine_tuning.jobs.list(page=0, page_size=5)
    print(jobs)

    # Retrieve a job
    retrieved_job = client.fine_tuning.jobs.get(job_id=created_job.id)
    print(retrieved_job)

    # Cancel a job
    canceled_job = client.fine_tuning.jobs.cancel(job_id=created_job.id)
    print(canceled_job)

    # Delete files
    client.files.delete(file_id=training_file.id)
    client.files.delete(file_id=validation_file.id)


if __name__ == "__main__":
    main()
</file>

<file path="examples/json_format.py">
#!/usr/bin/env python

import os

from mistralai import Mistral
from mistralai.models import UserMessage


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    model = "mistral-large-latest"

    client = Mistral(api_key=api_key)

    chat_response = client.chat.complete(
        model=model,
        response_format={"type": "json_object"},
        messages=[
            UserMessage(
                content="What is the best French cheese? Answer shortly in JSON.",
            )
        ],
    )
    print(chat_response.choices[0].message.content)


if __name__ == "__main__":
    main()
</file>

<file path="examples/list_models.py">
#!/usr/bin/env python

import os

from mistralai import Mistral


def main():
    api_key = os.environ["MISTRAL_API_KEY"]

    client = Mistral(api_key=api_key)

    list_models_response = client.models.list()
    print(list_models_response)


if __name__ == "__main__":
    main()
</file>

<file path="examples/ocr_process_from_file.py">
from mistralai import Mistral
import os
import json
from pathlib import Path
import urllib.request

MIXTRAL_OF_EXPERTS_PDF_URL = "https://arxiv.org/pdf/2401.04088"
MOE_FILENAME = "mixtral_of_experts.pdf"


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)
    pdf_file = Path(MOE_FILENAME)

    # Download the file if it doesn't exist
    if not pdf_file.is_file():
        urllib.request.urlretrieve(MIXTRAL_OF_EXPERTS_PDF_URL, MOE_FILENAME)

    # Upload the file
    uploaded_file = client.files.upload(
        file={
            "file_name": pdf_file.stem,
            "content": pdf_file.read_bytes(),
        },
        purpose="ocr",
    )

    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)

    pdf_response = client.ocr.process(document={
        "document_url": signed_url.url,
        "type": "document_url",
        "document_name": "mistral-7b-pdf",
    }, model="mistral-ocr-latest", include_image_base64=True)

    # Print the parsed PDF
    response_dict = json.loads(pdf_response.model_dump_json())
    json_string = json.dumps(response_dict, indent=4)
    print(json_string)

    # Remove the file
    pdf_file.unlink()


if __name__ == "__main__":
    main()
</file>

<file path="examples/ocr_process_from_url.py">
import json
import os

from mistralai import Mistral

MISTRAL_7B_PDF_URL = "https://arxiv.org/pdf/2310.06825.pdf"


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    # Using an URL
    pdf_response = client.ocr.process(
        document={
            "document_url": MISTRAL_7B_PDF_URL,
            "type": "document_url",
            "document_name": "mistral-7b-pdf",
        },
        model="mistral-ocr-latest",
        include_image_base64=True,
    )

    # Print the parsed PDF
    response_dict = json.loads(pdf_response.model_dump_json())
    json_string = json.dumps(response_dict, indent=4)
    print(json_string)


if __name__ == "__main__":
    main()
</file>

<file path="examples/structured_outputs_with_json_schema.py">
#!/usr/bin/env python

import os

from mistralai import Mistral


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    print("Using the .complete method to input a raw json schema to the API:\n")
    # When providing raw JSON Schema to the SDK you need to have 'additionalProperties': False in the schema definition
    # This is because the API is only accepting a strict JSON Schema
    chat_response = client.chat.complete(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "MathDemonstration",
                "schema_definition": {
                    "$defs": {
                        "Explanation": {
                            "properties": {
                                "explanation": {
                                    "title": "Explanation",
                                    "type": "string",
                                },
                                "output": {"title": "Output", "type": "string"},
                            },
                            "required": ["explanation", "output"],
                            "title": "Explanation",
                            "type": "object",
                            "additionalProperties": False,
                        }
                    },
                    "properties": {
                        "steps": {
                            "items": {"$ref": "#/$defs/Explanation"},
                            "title": "Steps",
                            "type": "array",
                        },
                        "final_answer": {"title": "Final Answer", "type": "string"},
                    },
                    "required": ["steps", "final_answer"],
                    "title": "MathDemonstration",
                    "type": "object",
                    "additionalProperties": False,
                },
                "description": None,
                "strict": True,
            },
        },
    )
    print(chat_response.choices[0].message.content)

    # Or with the streaming API
    with client.chat.stream(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "MathDemonstration",
                "schema_definition": {
                    "$defs": {
                        "Explanation": {
                            "properties": {
                                "explanation": {
                                    "title": "Explanation",
                                    "type": "string",
                                },
                                "output": {"title": "Output", "type": "string"},
                            },
                            "required": ["explanation", "output"],
                            "title": "Explanation",
                            "type": "object",
                            "additionalProperties": False,
                        }
                    },
                    "properties": {
                        "steps": {
                            "items": {"$ref": "#/$defs/Explanation"},
                            "title": "Steps",
                            "type": "array",
                        },
                        "final_answer": {"title": "Final Answer", "type": "string"},
                    },
                    "required": ["steps", "final_answer"],
                    "title": "MathDemonstration",
                    "type": "object",
                    "additionalProperties": False,
                },
                "description": None,
                "strict": True,
            },
        },
    ) as stream:
        for chunk in stream:
            print(chunk.data.choices[0].delta.content, end="")


if __name__ == "__main__":
    main()
</file>

<file path="examples/structured_outputs_with_pydantic.py">
#!/usr/bin/env python

import os
from pydantic import BaseModel

from mistralai import Mistral

from typing import List


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    class Explanation(BaseModel):
        explanation: str
        output: str

    class MathDemonstration(BaseModel):
        steps: List[Explanation]
        final_answer: str

    print("Using the .parse method to parse the response into a Pydantic model:\n")
    chat_response = client.chat.parse(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format=MathDemonstration,
    )
    print(chat_response.choices[0].message.parsed)

    # Or with the streaming API
    print(
        "\nUsing the .parse_stream method to stream back the response into a JSON Schema:\n"
    )
    with client.chat.parse_stream(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format=MathDemonstration,
    ) as stream:
        for chunk in stream:
            print(chunk.data.choices[0].delta.content, end="")


if __name__ == "__main__":
    main()
</file>

<file path="examples/structured_outputs.py">
#!/usr/bin/env python

import os
from pydantic import BaseModel

from mistralai import Mistral


def main():
    api_key = os.environ["MISTRAL_API_KEY"]
    client = Mistral(api_key=api_key)

    class Explanation(BaseModel):
        explanation: str
        output: str

    class MathDemonstration(BaseModel):
        steps: list[Explanation]
        final_answer: str

    print("Using the .parse method to parse the response into a Pydantic model:\n")
    chat_response = client.chat.parse(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format=MathDemonstration,
    )
    print(chat_response.choices[0].message.parsed)

    # Or with the streaming API
    print(
        "\nUsing the .parse_stream method to stream back the response into a JSON Schema:\n"
    )
    with client.chat.parse_stream(
        model="mistral-large-latest",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. You will be provided with a math problem, and your goal will be to output a step by step solution, along with a final answer. For each step, just provide the output as an equation use the explanation field to detail the reasoning.",
            },
            {"role": "user", "content": "How can I solve 8x + 7 = -23"},
        ],
        response_format=MathDemonstration,
    ) as stream:
        for chunk in stream:
            print(chunk.data.choices[0].delta.content, end="")


if __name__ == "__main__":
    main()
</file>

<file path="packages/mistralai_azure/.speakeasy/gen.lock">
lockVersion: 2.0.0
id: dc40fa48-2c4d-46ad-ac8b-270749770f34
management:
  docChecksum: 574e96caec9a63dbe3f39d646830f2c2
  docVersion: 0.0.2
  speakeasyVersion: 1.517.3
  generationVersion: 2.548.6
  releaseVersion: 1.6.0
  configChecksum: 3a4d9b204b5731c461ed7279710d5ed6
  published: true
features:
  python:
    additionalDependencies: 1.0.0
    constsAndDefaults: 1.0.5
    core: 5.12.3
    defaultEnabledRetries: 0.2.0
    enumUnions: 0.1.0
    envVarSecurityUsage: 0.3.2
    examples: 3.0.1
    flatRequests: 1.0.1
    globalSecurity: 3.0.3
    globalSecurityCallbacks: 1.0.0
    globalSecurityFlattening: 1.0.0
    globalServerURLs: 3.1.0
    methodArguments: 1.0.2
    nameOverrides: 3.0.1
    nullables: 1.0.1
    openEnums: 1.0.0
    responseFormat: 1.0.1
    retries: 3.0.2
    sdkHooks: 1.0.1
    serverEvents: 1.0.7
    serverEventsSentinels: 0.1.0
    serverIDs: 3.0.0
    unions: 3.0.4
generatedFiles:
  - .gitattributes
  - .python-version
  - .vscode/settings.json
  - docs/models/arguments.md
  - docs/models/assistantmessage.md
  - docs/models/assistantmessagecontent.md
  - docs/models/assistantmessagerole.md
  - docs/models/chatcompletionchoice.md
  - docs/models/chatcompletionchoicefinishreason.md
  - docs/models/chatcompletionrequest.md
  - docs/models/chatcompletionrequestmessages.md
  - docs/models/chatcompletionrequeststop.md
  - docs/models/chatcompletionrequesttoolchoice.md
  - docs/models/chatcompletionresponse.md
  - docs/models/chatcompletionstreamrequest.md
  - docs/models/chatcompletionstreamrequesttoolchoice.md
  - docs/models/completionchunk.md
  - docs/models/completionevent.md
  - docs/models/completionresponsestreamchoice.md
  - docs/models/content.md
  - docs/models/contentchunk.md
  - docs/models/deltamessage.md
  - docs/models/finishreason.md
  - docs/models/function.md
  - docs/models/functioncall.md
  - docs/models/functionname.md
  - docs/models/httpvalidationerror.md
  - docs/models/imageurl.md
  - docs/models/imageurlchunk.md
  - docs/models/imageurlchunkimageurl.md
  - docs/models/imageurlchunktype.md
  - docs/models/jsonschema.md
  - docs/models/loc.md
  - docs/models/messages.md
  - docs/models/prediction.md
  - docs/models/referencechunk.md
  - docs/models/referencechunktype.md
  - docs/models/responseformat.md
  - docs/models/responseformats.md
  - docs/models/role.md
  - docs/models/security.md
  - docs/models/stop.md
  - docs/models/systemmessage.md
  - docs/models/systemmessagecontent.md
  - docs/models/textchunk.md
  - docs/models/tool.md
  - docs/models/toolcall.md
  - docs/models/toolchoice.md
  - docs/models/toolchoiceenum.md
  - docs/models/toolmessage.md
  - docs/models/toolmessagecontent.md
  - docs/models/toolmessagerole.md
  - docs/models/tooltypes.md
  - docs/models/type.md
  - docs/models/usageinfo.md
  - docs/models/usermessage.md
  - docs/models/usermessagecontent.md
  - docs/models/usermessagerole.md
  - docs/models/utils/retryconfig.md
  - docs/models/validationerror.md
  - poetry.toml
  - py.typed
  - pylintrc
  - pyproject.toml
  - scripts/prepare_readme.py
  - scripts/publish.sh
  - src/mistralai_azure/__init__.py
  - src/mistralai_azure/_hooks/__init__.py
  - src/mistralai_azure/_hooks/sdkhooks.py
  - src/mistralai_azure/_hooks/types.py
  - src/mistralai_azure/_version.py
  - src/mistralai_azure/basesdk.py
  - src/mistralai_azure/chat.py
  - src/mistralai_azure/httpclient.py
  - src/mistralai_azure/models/__init__.py
  - src/mistralai_azure/models/assistantmessage.py
  - src/mistralai_azure/models/chatcompletionchoice.py
  - src/mistralai_azure/models/chatcompletionrequest.py
  - src/mistralai_azure/models/chatcompletionresponse.py
  - src/mistralai_azure/models/chatcompletionstreamrequest.py
  - src/mistralai_azure/models/completionchunk.py
  - src/mistralai_azure/models/completionevent.py
  - src/mistralai_azure/models/completionresponsestreamchoice.py
  - src/mistralai_azure/models/contentchunk.py
  - src/mistralai_azure/models/deltamessage.py
  - src/mistralai_azure/models/function.py
  - src/mistralai_azure/models/functioncall.py
  - src/mistralai_azure/models/functionname.py
  - src/mistralai_azure/models/httpvalidationerror.py
  - src/mistralai_azure/models/imageurl.py
  - src/mistralai_azure/models/imageurlchunk.py
  - src/mistralai_azure/models/jsonschema.py
  - src/mistralai_azure/models/prediction.py
  - src/mistralai_azure/models/referencechunk.py
  - src/mistralai_azure/models/responseformat.py
  - src/mistralai_azure/models/responseformats.py
  - src/mistralai_azure/models/sdkerror.py
  - src/mistralai_azure/models/security.py
  - src/mistralai_azure/models/systemmessage.py
  - src/mistralai_azure/models/textchunk.py
  - src/mistralai_azure/models/tool.py
  - src/mistralai_azure/models/toolcall.py
  - src/mistralai_azure/models/toolchoice.py
  - src/mistralai_azure/models/toolchoiceenum.py
  - src/mistralai_azure/models/toolmessage.py
  - src/mistralai_azure/models/tooltypes.py
  - src/mistralai_azure/models/usageinfo.py
  - src/mistralai_azure/models/usermessage.py
  - src/mistralai_azure/models/validationerror.py
  - src/mistralai_azure/py.typed
  - src/mistralai_azure/sdk.py
  - src/mistralai_azure/sdkconfiguration.py
  - src/mistralai_azure/types/__init__.py
  - src/mistralai_azure/types/basemodel.py
  - src/mistralai_azure/utils/__init__.py
  - src/mistralai_azure/utils/annotations.py
  - src/mistralai_azure/utils/enums.py
  - src/mistralai_azure/utils/eventstreaming.py
  - src/mistralai_azure/utils/forms.py
  - src/mistralai_azure/utils/headers.py
  - src/mistralai_azure/utils/logger.py
  - src/mistralai_azure/utils/metadata.py
  - src/mistralai_azure/utils/queryparams.py
  - src/mistralai_azure/utils/requestbodies.py
  - src/mistralai_azure/utils/retries.py
  - src/mistralai_azure/utils/security.py
  - src/mistralai_azure/utils/serializers.py
  - src/mistralai_azure/utils/url.py
  - src/mistralai_azure/utils/values.py
examples:
  stream_chat:
    speakeasy-default-stream-chat:
      requestBody:
        application/json: {"model": "azureai", "stream": true, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "422":
          application/json: {}
        "200": {}
  chat_completion_v1_chat_completions_post:
    speakeasy-default-chat-completion-v1-chat-completions-post:
      requestBody:
        application/json: {"model": "azureai", "stream": false, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "mistral-small-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
examplesVersion: 1.0.0
generatedTests: {}
</file>

<file path="packages/mistralai_azure/.speakeasy/gen.yaml">
configVersion: 2.0.0
generation:
  sdkClassName: MistralAzure
  maintainOpenAPIOrder: true
  usageSnippets:
    optionalPropertyRendering: withExample
  useClassNamesForArrayFields: true
  fixes:
    nameResolutionDec2023: true
    nameResolutionFeb2025: false
    parameterOrderingFeb2024: true
    requestResponseComponentNamesFeb2024: true
    securityFeb2025: false
  auth:
    oAuth2ClientCredentialsEnabled: true
    oAuth2PasswordEnabled: false
python:
  version: 1.6.0
  additionalDependencies:
    dev:
      pytest: ^8.2.2
      pytest-asyncio: ^0.23.7
  authors:
    - Mistral
  clientServerStatusCodesAsErrors: true
  defaultErrorName: SDKError
  description: Python Client SDK for the Mistral AI API in Azure.
  enableCustomCodeRegions: false
  enumFormat: union
  fixFlags:
    responseRequiredSep2024: false
  flattenGlobalSecurity: true
  flattenRequests: true
  flatteningOrder: parameters-first
  imports:
    option: openapi
    paths:
      callbacks: ""
      errors: ""
      operations: ""
      shared: ""
      webhooks: ""
  inputModelSuffix: input
  maxMethodParams: 15
  methodArguments: infer-optional-args
  outputModelSuffix: output
  packageName: mistralai_azure
  pytestTimeout: 0
  responseFormat: flat
  templateVersion: v2
</file>

<file path="packages/mistralai_azure/docs/models/utils/retryconfig.md">
# RetryConfig

Allows customizing the default retry configuration. Only usable with methods that mention they support retries.

## Fields

| Name                      | Type                                | Description                             | Example   |
| ------------------------- | ----------------------------------- | --------------------------------------- | --------- |
| `strategy`                | `*str*`                             | The retry strategy to use.              | `backoff` |
| `backoff`                 | [BackoffStrategy](#backoffstrategy) | Configuration for the backoff strategy. |           |
| `retry_connection_errors` | `*bool*`                            | Whether to retry on connection errors.  | `true`    |

## BackoffStrategy

The backoff strategy allows retrying a request with an exponential backoff between each retry.

### Fields

| Name               | Type      | Description                               | Example  |
| ------------------ | --------- | ----------------------------------------- | -------- |
| `initial_interval` | `*int*`   | The initial interval in milliseconds.     | `500`    |
| `max_interval`     | `*int*`   | The maximum interval in milliseconds.     | `60000`  |
| `exponent`         | `*float*` | The exponent to use for the backoff.      | `1.5`    |
| `max_elapsed_time` | `*int*`   | The maximum elapsed time in milliseconds. | `300000` |
</file>

<file path="packages/mistralai_azure/docs/models/arguments.md">
# Arguments


## Supported Types

### `Dict[str, Any]`

```python
value: Dict[str, Any] = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/assistantmessage.md">
# AssistantMessage


## Fields

| Field                                                                                                                                                                                                  | Type                                                                                                                                                                                                   | Required                                                                                                                                                                                               | Description                                                                                                                                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `content`                                                                                                                                                                                              | [OptionalNullable[models.AssistantMessageContent]](../models/assistantmessagecontent.md)                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `tool_calls`                                                                                                                                                                                           | List[[models.ToolCall](../models/toolcall.md)]                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `prefix`                                                                                                                                                                                               | *Optional[bool]*                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                     | Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message. |
| `role`                                                                                                                                                                                                 | [Optional[models.AssistantMessageRole]](../models/assistantmessagerole.md)                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
</file>

<file path="packages/mistralai_azure/docs/models/assistantmessagecontent.md">
# AssistantMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/assistantmessagerole.md">
# AssistantMessageRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionchoice.md">
# ChatCompletionChoice


## Fields

| Field                                                                                    | Type                                                                                     | Required                                                                                 | Description                                                                              | Example                                                                                  |
| ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| `index`                                                                                  | *int*                                                                                    | :heavy_check_mark:                                                                       | N/A                                                                                      | 0                                                                                        |
| `message`                                                                                | [models.AssistantMessage](../models/assistantmessage.md)                                 | :heavy_check_mark:                                                                       | N/A                                                                                      |                                                                                          |
| `finish_reason`                                                                          | [models.ChatCompletionChoiceFinishReason](../models/chatcompletionchoicefinishreason.md) | :heavy_check_mark:                                                                       | N/A                                                                                      | stop                                                                                     |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionchoicefinishreason.md">
# ChatCompletionChoiceFinishReason


## Values

| Name           | Value          |
| -------------- | -------------- |
| `STOP`         | stop           |
| `LENGTH`       | length         |
| `MODEL_LENGTH` | model_length   |
| `ERROR`        | error          |
| `TOOL_CALLS`   | tool_calls     |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionrequest.md">
# ChatCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.ChatCompletionRequestMessages](../models/chatcompletionrequestmessages.md)]                                                                                                                                                                                                                                                                                                              | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[str]*                                                                                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The ID of the model to use for this request.                                                                                                                                                                                                                                                                                                                                                          | azureai                                                                                                                                                                                                                                                                                                                                                                                               |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.ChatCompletionRequestStop]](../models/chatcompletionrequeststop.md)                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionRequestToolChoice]](../models/chatcompletionrequesttoolchoice.md)                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionrequestmessages.md">
# ChatCompletionRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionrequeststop.md">
# ChatCompletionRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionrequesttoolchoice.md">
# ChatCompletionRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionresponse.md">
# ChatCompletionResponse


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            | Example                                                                |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `id`                                                                   | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | cmpl-e5cc70bb28c444948073e77776eb30ef                                  |
| `object`                                                               | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | chat.completion                                                        |
| `model`                                                                | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | mistral-small-latest                                                   |
| `usage`                                                                | [models.UsageInfo](../models/usageinfo.md)                             | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
| `created`                                                              | *Optional[int]*                                                        | :heavy_minus_sign:                                                     | N/A                                                                    | 1702256327                                                             |
| `choices`                                                              | List[[models.ChatCompletionChoice](../models/chatcompletionchoice.md)] | :heavy_minus_sign:                                                     | N/A                                                                    |                                                                        |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionstreamrequest.md">
# ChatCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.Messages](../models/messages.md)]                                                                                                                                                                                                                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[str]*                                                                                                                                                                                                                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The ID of the model to use for this request.                                                                                                                                                                                                                                                                                                                                                          | azureai                                                                                                                                                                                                                                                                                                                                                                                               |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.Stop]](../models/stop.md)                                                                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionStreamRequestToolChoice]](../models/chatcompletionstreamrequesttoolchoice.md)                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `safe_prompt`                                                                                                                                                                                                                                                                                                                                                                                         | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_azure/docs/models/chatcompletionstreamrequesttoolchoice.md">
# ChatCompletionStreamRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/completionchunk.md">
# CompletionChunk


## Fields

| Field                                                                                      | Type                                                                                       | Required                                                                                   | Description                                                                                |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| `id`                                                                                       | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `model`                                                                                    | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `choices`                                                                                  | List[[models.CompletionResponseStreamChoice](../models/completionresponsestreamchoice.md)] | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `object`                                                                                   | *Optional[str]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `created`                                                                                  | *Optional[int]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `usage`                                                                                    | [Optional[models.UsageInfo]](../models/usageinfo.md)                                       | :heavy_minus_sign:                                                                         | N/A                                                                                        |
</file>

<file path="packages/mistralai_azure/docs/models/completionevent.md">
# CompletionEvent


## Fields

| Field                                                  | Type                                                   | Required                                               | Description                                            |
| ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ |
| `data`                                                 | [models.CompletionChunk](../models/completionchunk.md) | :heavy_check_mark:                                     | N/A                                                    |
</file>

<file path="packages/mistralai_azure/docs/models/completionresponsestreamchoice.md">
# CompletionResponseStreamChoice


## Fields

| Field                                                      | Type                                                       | Required                                                   | Description                                                |
| ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- |
| `index`                                                    | *int*                                                      | :heavy_check_mark:                                         | N/A                                                        |
| `delta`                                                    | [models.DeltaMessage](../models/deltamessage.md)           | :heavy_check_mark:                                         | N/A                                                        |
| `finish_reason`                                            | [Nullable[models.FinishReason]](../models/finishreason.md) | :heavy_check_mark:                                         | N/A                                                        |
</file>

<file path="packages/mistralai_azure/docs/models/content.md">
# Content


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/contentchunk.md">
# ContentChunk


## Supported Types

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ReferenceChunk`

```python
value: models.ReferenceChunk = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/deltamessage.md">
# DeltaMessage


## Fields

| Field                                                    | Type                                                     | Required                                                 | Description                                              |
| -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- |
| `role`                                                   | *OptionalNullable[str]*                                  | :heavy_minus_sign:                                       | N/A                                                      |
| `content`                                                | [OptionalNullable[models.Content]](../models/content.md) | :heavy_minus_sign:                                       | N/A                                                      |
| `tool_calls`                                             | List[[models.ToolCall](../models/toolcall.md)]           | :heavy_minus_sign:                                       | N/A                                                      |
</file>

<file path="packages/mistralai_azure/docs/models/finishreason.md">
# FinishReason


## Values

| Name         | Value        |
| ------------ | ------------ |
| `STOP`       | stop         |
| `LENGTH`     | length       |
| `ERROR`      | error        |
| `TOOL_CALLS` | tool_calls   |
</file>

<file path="packages/mistralai_azure/docs/models/function.md">
# Function


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
| `parameters`       | Dict[str, *Any*]   | :heavy_check_mark: | N/A                |
| `description`      | *Optional[str]*    | :heavy_minus_sign: | N/A                |
| `strict`           | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
</file>

<file path="packages/mistralai_azure/docs/models/functioncall.md">
# FunctionCall


## Fields

| Field                                      | Type                                       | Required                                   | Description                                |
| ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ |
| `name`                                     | *str*                                      | :heavy_check_mark:                         | N/A                                        |
| `arguments`                                | [models.Arguments](../models/arguments.md) | :heavy_check_mark:                         | N/A                                        |
</file>

<file path="packages/mistralai_azure/docs/models/functionname.md">
# FunctionName

this restriction of `Function` is used to select a specific function to call


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="packages/mistralai_azure/docs/models/httpvalidationerror.md">
# HTTPValidationError


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `detail`                                                     | List[[models.ValidationError](../models/validationerror.md)] | :heavy_minus_sign:                                           | N/A                                                          |
</file>

<file path="packages/mistralai_azure/docs/models/imageurl.md">
# ImageURL


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `url`                   | *str*                   | :heavy_check_mark:      | N/A                     |
| `detail`                | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
</file>

<file path="packages/mistralai_azure/docs/models/imageurlchunk.md">
# ImageURLChunk

{"type":"image_url","image_url":{"url":"data:image/png;base64,iVBORw0


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `image_url`                                                          | [models.ImageURLChunkImageURL](../models/imageurlchunkimageurl.md)   | :heavy_check_mark:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.ImageURLChunkType]](../models/imageurlchunktype.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="packages/mistralai_azure/docs/models/imageurlchunkimageurl.md">
# ImageURLChunkImageURL


## Supported Types

### `models.ImageURL`

```python
value: models.ImageURL = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/imageurlchunktype.md">
# ImageURLChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `IMAGE_URL` | image_url   |
</file>

<file path="packages/mistralai_azure/docs/models/jsonschema.md">
# JSONSchema


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `name`                  | *str*                   | :heavy_check_mark:      | N/A                     |
| `schema_definition`     | Dict[str, *Any*]        | :heavy_check_mark:      | N/A                     |
| `description`           | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
| `strict`                | *Optional[bool]*        | :heavy_minus_sign:      | N/A                     |
</file>

<file path="packages/mistralai_azure/docs/models/loc.md">
# Loc


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `int`

```python
value: int = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/messages.md">
# Messages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/prediction.md">
# Prediction


## Fields

| Field                          | Type                           | Required                       | Description                    |
| ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |
| `type`                         | *Optional[Literal["content"]]* | :heavy_minus_sign:             | N/A                            |
| `content`                      | *Optional[str]*                | :heavy_minus_sign:             | N/A                            |
</file>

<file path="packages/mistralai_azure/docs/models/referencechunk.md">
# ReferenceChunk


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `reference_ids`                                                        | List[*int*]                                                            | :heavy_check_mark:                                                     | N/A                                                                    |
| `type`                                                                 | [Optional[models.ReferenceChunkType]](../models/referencechunktype.md) | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_azure/docs/models/referencechunktype.md">
# ReferenceChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `REFERENCE` | reference   |
</file>

<file path="packages/mistralai_azure/docs/models/responseformat.md">
# ResponseFormat


## Fields

| Field                                                                                                                                                                                                                                                                                              | Type                                                                                                                                                                                                                                                                                               | Required                                                                                                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `type`                                                                                                                                                                                                                                                                                             | [Optional[models.ResponseFormats]](../models/responseformats.md)                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message. |
| `json_schema`                                                                                                                                                                                                                                                                                      | [OptionalNullable[models.JSONSchema]](../models/jsonschema.md)                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                |
</file>

<file path="packages/mistralai_azure/docs/models/responseformats.md">
# ResponseFormats

An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message.


## Values

| Name          | Value         |
| ------------- | ------------- |
| `TEXT`        | text          |
| `JSON_OBJECT` | json_object   |
| `JSON_SCHEMA` | json_schema   |
</file>

<file path="packages/mistralai_azure/docs/models/role.md">
# Role


## Values

| Name     | Value    |
| -------- | -------- |
| `SYSTEM` | system   |
</file>

<file path="packages/mistralai_azure/docs/models/security.md">
# Security


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `api_key`          | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="packages/mistralai_azure/docs/models/stop.md">
# Stop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/systemmessage.md">
# SystemMessage


## Fields

| Field                                                            | Type                                                             | Required                                                         | Description                                                      |
| ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| `content`                                                        | [models.SystemMessageContent](../models/systemmessagecontent.md) | :heavy_check_mark:                                               | N/A                                                              |
| `role`                                                           | [Optional[models.Role]](../models/role.md)                       | :heavy_minus_sign:                                               | N/A                                                              |
</file>

<file path="packages/mistralai_azure/docs/models/systemmessagecontent.md">
# SystemMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.TextChunk]`

```python
value: List[models.TextChunk] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/textchunk.md">
# TextChunk


## Fields

| Field                                      | Type                                       | Required                                   | Description                                |
| ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ |
| `text`                                     | *str*                                      | :heavy_check_mark:                         | N/A                                        |
| `type`                                     | [Optional[models.Type]](../models/type.md) | :heavy_minus_sign:                         | N/A                                        |
</file>

<file path="packages/mistralai_azure/docs/models/tool.md">
# Tool


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.Function](../models/function.md)             | :heavy_check_mark:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="packages/mistralai_azure/docs/models/toolcall.md">
# ToolCall


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.FunctionCall](../models/functioncall.md)     | :heavy_check_mark:                                   | N/A                                                  |
| `id`                                                 | *Optional[str]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
| `index`                                              | *Optional[int]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="packages/mistralai_azure/docs/models/toolchoice.md">
# ToolChoice

ToolChoice is either a ToolChoiceEnum or a ToolChoice


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `function`                                                                   | [models.FunctionName](../models/functionname.md)                             | :heavy_check_mark:                                                           | this restriction of `Function` is used to select a specific function to call |
| `type`                                                                       | [Optional[models.ToolTypes]](../models/tooltypes.md)                         | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="packages/mistralai_azure/docs/models/toolchoiceenum.md">
# ToolChoiceEnum


## Values

| Name       | Value      |
| ---------- | ---------- |
| `AUTO`     | auto       |
| `NONE`     | none       |
| `ANY`      | any        |
| `REQUIRED` | required   |
</file>

<file path="packages/mistralai_azure/docs/models/toolmessage.md">
# ToolMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.ToolMessageContent]](../models/toolmessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `tool_call_id`                                                         | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `name`                                                                 | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.ToolMessageRole]](../models/toolmessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_azure/docs/models/toolmessagecontent.md">
# ToolMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/toolmessagerole.md">
# ToolMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `TOOL` | tool   |
</file>

<file path="packages/mistralai_azure/docs/models/tooltypes.md">
# ToolTypes


## Values

| Name       | Value      |
| ---------- | ---------- |
| `FUNCTION` | function   |
</file>

<file path="packages/mistralai_azure/docs/models/type.md">
# Type


## Values

| Name   | Value  |
| ------ | ------ |
| `TEXT` | text   |
</file>

<file path="packages/mistralai_azure/docs/models/usageinfo.md">
# UsageInfo


## Fields

| Field               | Type                | Required            | Description         | Example             |
| ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| `prompt_tokens`     | *int*               | :heavy_check_mark:  | N/A                 | 16                  |
| `completion_tokens` | *int*               | :heavy_check_mark:  | N/A                 | 34                  |
| `total_tokens`      | *int*               | :heavy_check_mark:  | N/A                 | 50                  |
</file>

<file path="packages/mistralai_azure/docs/models/usermessage.md">
# UserMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.UserMessageContent]](../models/usermessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.UserMessageRole]](../models/usermessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_azure/docs/models/usermessagecontent.md">
# UserMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_azure/docs/models/usermessagerole.md">
# UserMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `USER` | user   |
</file>

<file path="packages/mistralai_azure/docs/models/validationerror.md">
# ValidationError


## Fields

| Field                                | Type                                 | Required                             | Description                          |
| ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ |
| `loc`                                | List[[models.Loc](../models/loc.md)] | :heavy_check_mark:                   | N/A                                  |
| `msg`                                | *str*                                | :heavy_check_mark:                   | N/A                                  |
| `type`                               | *str*                                | :heavy_check_mark:                   | N/A                                  |
</file>

<file path="packages/mistralai_azure/docs/sdks/chat/README.md">
# Chat
(*chat*)

## Overview

Chat Completion API.

### Available Operations

* [stream](#stream) - Stream chat completion
* [create](#create) - Chat Completion

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    for event in res:
        # handle event
        print(event)

```

### Parameters

| Parameter         | Type                                                              | Required           | Description                                                                                                                                                                                                                                                   | Example                                                                                                    |
| ----------------- | ----------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `messages`        | List[[models.Messages](../../models/messages.md)]                 | :heavy_check_mark: | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                   | {<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>} |
| `model`           | *OptionalNullable[str]*                                           | :heavy_minus_sign: | The ID of the model to use for this request.                                                                                                                                                                                                                  | azureai                                                                                                    |
| `temperature`     | *Optional[float]*                                                 | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.        |                                                                                                            |
| `top_p`           | *Optional[float]*                                                 | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both. |                                                                                                            |
| `max_tokens`      | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                        |                                                                                                            |
| `min_tokens`      | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                   |                                                                                                            |
| `stream`          | *Optional[bool]*                                                  | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `stop`            | [Optional[models.Stop]](../../models/stop.md)                     | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                      |                                                                                                            |
| `random_seed`     | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                             |                                                                                                            |
| `response_format` | [Optional[models.ResponseFormat]](../../models/responseformat.md) | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `tools`           | List[[models.Tool](../../models/tool.md)]                         | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `safe_prompt`     | *Optional[bool]*                                                  | :heavy_minus_sign: | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                   |                                                                                                            |
| `tool_choice`     | [Optional[models.ToolChoice]](../../models/toolchoice.md)         | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `retries`         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)  | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                           |                                                                                                            |


### Response

**[Union[Generator[models.CompletionEvent, None, None], AsyncGenerator[models.CompletionEvent, None]]](../../models/.md)**
### Errors

| Error Object    | Status Code | Content Type |
| --------------- | ----------- | ------------ |
| models.SDKError | 4xx-5xx     | */*          |

## create

Chat Completion

### Example Usage

```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    # handle response
    pass

```

### Parameters

| Parameter         | Type                                                                                                | Required           | Description                                                                                                                                                                                                                                                                                                                        | Example                                                                                                    |
| ----------------- | --------------------------------------------------------------------------------------------------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `messages`        | List[[models.ChatCompletionRequestMessages](../../models/chatcompletionrequestmessages.md)]         | :heavy_check_mark: | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                        | {<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>} |
| `model`           | *OptionalNullable[str]*                                                                             | :heavy_minus_sign: | The ID of the model to use for this request.                                                                                                                                                                                                                                                                                       | azureai                                                                                                    |
| `temperature`     | *Optional[float]*                                                                                   | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.                                                                             |                                                                                                            |
| `top_p`           | *Optional[float]*                                                                                   | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                      |                                                                                                            |
| `max_tokens`      | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                             |                                                                                                            |
| `min_tokens`      | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                        |                                                                                                            |
| `stream`          | *Optional[bool]*                                                                                    | :heavy_minus_sign: | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON. |                                                                                                            |
| `stop`            | [Optional[models.ChatCompletionRequestStop]](../../models/chatcompletionrequeststop.md)             | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                           |                                                                                                            |
| `random_seed`     | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                  |                                                                                                            |
| `response_format` | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                   | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `tools`           | List[[models.Tool](../../models/tool.md)]                                                           | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `safe_prompt`     | *Optional[bool]*                                                                                    | :heavy_minus_sign: | Whether to inject a safety prompt before all conversations.                                                                                                                                                                                                                                                                        |                                                                                                            |
| `tool_choice`     | [Optional[models.ChatCompletionRequestToolChoice]](../../models/chatcompletionrequesttoolchoice.md) | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `retries`         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                    | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                |                                                                                                            |


### Response

**[models.ChatCompletionResponse](../../models/chatcompletionresponse.md)**
### Errors

| Error Object               | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4xx-5xx     | */*              |
</file>

<file path="packages/mistralai_azure/docs/sdks/mistralazure/README.md">
# MistralAzure SDK


## Overview

Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it.

### Available Operations
</file>

<file path="packages/mistralai_azure/scripts/prepare_readme.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import shutil

try:
    shutil.copyfile("README.md", "README-PYPI.md")
except Exception as e:
    print("Failed to copy README.md to README-PYPI.md")
    print(e)
</file>

<file path="packages/mistralai_azure/scripts/publish.sh">
#!/usr/bin/env bash

export POETRY_PYPI_TOKEN_PYPI=${PYPI_TOKEN}

poetry run python scripts/prepare_readme.py

poetry publish --build --skip-existing
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_hooks/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkhooks import *
from .types import *
from .registration import *
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_hooks/custom_user_agent.py">
# THIS FILE IS THE EXACT COPY OF THE ORIGINAL FILE FROM src/mistralai/_hooks/custom_user_agent.py
from typing import Union

import httpx

from .types import BeforeRequestContext, BeforeRequestHook

PREFIX = "mistral-client-python/"

class CustomUserAgentHook(BeforeRequestHook):
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        current = request.headers["user-agent"]
        if current.startswith(PREFIX):
            return request
        
        request.headers["user-agent"] = (
            PREFIX + current.split(" ")[1]
        )

        return request
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_hooks/registration.py">
from .custom_user_agent import CustomUserAgentHook
from .types import Hooks

# This file is only ever generated once on the first generation and then is free to be modified.
# Any hooks you wish to add should be registered in the init_hooks function. Feel free to define them
# in this file or in separate files in the hooks folder.


def init_hooks(hooks: Hooks):
    # pylint: disable=unused-argument
    """Add hooks by calling hooks.register{sdk_init/before_request/after_success/after_error}Hook
    with an instance of a hook that implements that specific Hook interface
    Hooks are registered per SDK instance, and are valid for the lifetime of the SDK instance
    """
    hooks.register_before_request_hook(CustomUserAgentHook())
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_hooks/sdkhooks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
from .types import (
    SDKInitHook,
    BeforeRequestContext,
    BeforeRequestHook,
    AfterSuccessContext,
    AfterSuccessHook,
    AfterErrorContext,
    AfterErrorHook,
    Hooks,
)
from .registration import init_hooks
from typing import List, Optional, Tuple
from mistralai_azure.httpclient import HttpClient


class SDKHooks(Hooks):
    def __init__(self) -> None:
        self.sdk_init_hooks: List[SDKInitHook] = []
        self.before_request_hooks: List[BeforeRequestHook] = []
        self.after_success_hooks: List[AfterSuccessHook] = []
        self.after_error_hooks: List[AfterErrorHook] = []
        init_hooks(self)

    def register_sdk_init_hook(self, hook: SDKInitHook) -> None:
        self.sdk_init_hooks.append(hook)

    def register_before_request_hook(self, hook: BeforeRequestHook) -> None:
        self.before_request_hooks.append(hook)

    def register_after_success_hook(self, hook: AfterSuccessHook) -> None:
        self.after_success_hooks.append(hook)

    def register_after_error_hook(self, hook: AfterErrorHook) -> None:
        self.after_error_hooks.append(hook)

    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        for hook in self.sdk_init_hooks:
            base_url, client = hook.sdk_init(base_url, client)
        return base_url, client

    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> httpx.Request:
        for hook in self.before_request_hooks:
            out = hook.before_request(hook_ctx, request)
            if isinstance(out, Exception):
                raise out
            request = out

        return request

    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> httpx.Response:
        for hook in self.after_success_hooks:
            out = hook.after_success(hook_ctx, response)
            if isinstance(out, Exception):
                raise out
            response = out
        return response

    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Tuple[Optional[httpx.Response], Optional[Exception]]:
        for hook in self.after_error_hooks:
            result = hook.after_error(hook_ctx, response, error)
            if isinstance(result, Exception):
                raise result
            response, error = result
        return response, error
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_hooks/types.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from abc import ABC, abstractmethod
import httpx
from mistralai_azure.httpclient import HttpClient
from typing import Any, Callable, List, Optional, Tuple, Union


class HookContext:
    base_url: str
    operation_id: str
    oauth2_scopes: Optional[List[str]] = None
    security_source: Optional[Union[Any, Callable[[], Any]]] = None

    def __init__(
        self,
        base_url: str,
        operation_id: str,
        oauth2_scopes: Optional[List[str]],
        security_source: Optional[Union[Any, Callable[[], Any]]],
    ):
        self.base_url = base_url
        self.operation_id = operation_id
        self.oauth2_scopes = oauth2_scopes
        self.security_source = security_source


class BeforeRequestContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterSuccessContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterErrorContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class SDKInitHook(ABC):
    @abstractmethod
    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        pass


class BeforeRequestHook(ABC):
    @abstractmethod
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        pass


class AfterSuccessHook(ABC):
    @abstractmethod
    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> Union[httpx.Response, Exception]:
        pass


class AfterErrorHook(ABC):
    @abstractmethod
    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Union[Tuple[Optional[httpx.Response], Optional[Exception]], Exception]:
        pass


class Hooks(ABC):
    @abstractmethod
    def register_sdk_init_hook(self, hook: SDKInitHook):
        pass

    @abstractmethod
    def register_before_request_hook(self, hook: BeforeRequestHook):
        pass

    @abstractmethod
    def register_after_success_hook(self, hook: AfterSuccessHook):
        pass

    @abstractmethod
    def register_after_error_hook(self, hook: AfterErrorHook):
        pass
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .assistantmessage import (
    AssistantMessage,
    AssistantMessageContent,
    AssistantMessageContentTypedDict,
    AssistantMessageRole,
    AssistantMessageTypedDict,
)
from .chatcompletionchoice import (
    ChatCompletionChoice,
    ChatCompletionChoiceFinishReason,
    ChatCompletionChoiceTypedDict,
)
from .chatcompletionrequest import (
    ChatCompletionRequest,
    ChatCompletionRequestMessages,
    ChatCompletionRequestMessagesTypedDict,
    ChatCompletionRequestStop,
    ChatCompletionRequestStopTypedDict,
    ChatCompletionRequestToolChoice,
    ChatCompletionRequestToolChoiceTypedDict,
    ChatCompletionRequestTypedDict,
)
from .chatcompletionresponse import (
    ChatCompletionResponse,
    ChatCompletionResponseTypedDict,
)
from .chatcompletionstreamrequest import (
    ChatCompletionStreamRequest,
    ChatCompletionStreamRequestToolChoice,
    ChatCompletionStreamRequestToolChoiceTypedDict,
    ChatCompletionStreamRequestTypedDict,
    Messages,
    MessagesTypedDict,
    Stop,
    StopTypedDict,
)
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from .completionevent import CompletionEvent, CompletionEventTypedDict
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceTypedDict,
    FinishReason,
)
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .deltamessage import Content, ContentTypedDict, DeltaMessage, DeltaMessageTypedDict
from .function import Function, FunctionTypedDict
from .functioncall import (
    Arguments,
    ArgumentsTypedDict,
    FunctionCall,
    FunctionCallTypedDict,
)
from .functionname import FunctionName, FunctionNameTypedDict
from .httpvalidationerror import HTTPValidationError, HTTPValidationErrorData
from .imageurl import ImageURL, ImageURLTypedDict
from .imageurlchunk import (
    ImageURLChunk,
    ImageURLChunkImageURL,
    ImageURLChunkImageURLTypedDict,
    ImageURLChunkType,
    ImageURLChunkTypedDict,
)
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .prediction import Prediction, PredictionTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkType, ReferenceChunkTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .responseformats import ResponseFormats
from .sdkerror import SDKError
from .security import Security, SecurityTypedDict
from .systemmessage import (
    Role,
    SystemMessage,
    SystemMessageContent,
    SystemMessageContentTypedDict,
    SystemMessageTypedDict,
)
from .textchunk import TextChunk, TextChunkTypedDict, Type
from .tool import Tool, ToolTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import (
    ToolMessage,
    ToolMessageContent,
    ToolMessageContentTypedDict,
    ToolMessageRole,
    ToolMessageTypedDict,
)
from .tooltypes import ToolTypes
from .usageinfo import UsageInfo, UsageInfoTypedDict
from .usermessage import (
    UserMessage,
    UserMessageContent,
    UserMessageContentTypedDict,
    UserMessageRole,
    UserMessageTypedDict,
)
from .validationerror import (
    Loc,
    LocTypedDict,
    ValidationError,
    ValidationErrorTypedDict,
)


__all__ = [
    "Arguments",
    "ArgumentsTypedDict",
    "AssistantMessage",
    "AssistantMessageContent",
    "AssistantMessageContentTypedDict",
    "AssistantMessageRole",
    "AssistantMessageTypedDict",
    "ChatCompletionChoice",
    "ChatCompletionChoiceFinishReason",
    "ChatCompletionChoiceTypedDict",
    "ChatCompletionRequest",
    "ChatCompletionRequestMessages",
    "ChatCompletionRequestMessagesTypedDict",
    "ChatCompletionRequestStop",
    "ChatCompletionRequestStopTypedDict",
    "ChatCompletionRequestToolChoice",
    "ChatCompletionRequestToolChoiceTypedDict",
    "ChatCompletionRequestTypedDict",
    "ChatCompletionResponse",
    "ChatCompletionResponseTypedDict",
    "ChatCompletionStreamRequest",
    "ChatCompletionStreamRequestToolChoice",
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    "ChatCompletionStreamRequestTypedDict",
    "CompletionChunk",
    "CompletionChunkTypedDict",
    "CompletionEvent",
    "CompletionEventTypedDict",
    "CompletionResponseStreamChoice",
    "CompletionResponseStreamChoiceTypedDict",
    "Content",
    "ContentChunk",
    "ContentChunkTypedDict",
    "ContentTypedDict",
    "DeltaMessage",
    "DeltaMessageTypedDict",
    "FinishReason",
    "Function",
    "FunctionCall",
    "FunctionCallTypedDict",
    "FunctionName",
    "FunctionNameTypedDict",
    "FunctionTypedDict",
    "HTTPValidationError",
    "HTTPValidationErrorData",
    "ImageURL",
    "ImageURLChunk",
    "ImageURLChunkImageURL",
    "ImageURLChunkImageURLTypedDict",
    "ImageURLChunkType",
    "ImageURLChunkTypedDict",
    "ImageURLTypedDict",
    "JSONSchema",
    "JSONSchemaTypedDict",
    "Loc",
    "LocTypedDict",
    "Messages",
    "MessagesTypedDict",
    "Prediction",
    "PredictionTypedDict",
    "ReferenceChunk",
    "ReferenceChunkType",
    "ReferenceChunkTypedDict",
    "ResponseFormat",
    "ResponseFormatTypedDict",
    "ResponseFormats",
    "Role",
    "SDKError",
    "Security",
    "SecurityTypedDict",
    "Stop",
    "StopTypedDict",
    "SystemMessage",
    "SystemMessageContent",
    "SystemMessageContentTypedDict",
    "SystemMessageTypedDict",
    "TextChunk",
    "TextChunkTypedDict",
    "Tool",
    "ToolCall",
    "ToolCallTypedDict",
    "ToolChoice",
    "ToolChoiceEnum",
    "ToolChoiceTypedDict",
    "ToolMessage",
    "ToolMessageContent",
    "ToolMessageContentTypedDict",
    "ToolMessageRole",
    "ToolMessageTypedDict",
    "ToolTypedDict",
    "ToolTypes",
    "Type",
    "UsageInfo",
    "UsageInfoTypedDict",
    "UserMessage",
    "UserMessageContent",
    "UserMessageContentTypedDict",
    "UserMessageRole",
    "UserMessageTypedDict",
    "ValidationError",
    "ValidationErrorTypedDict",
]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/assistantmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


AssistantMessageContentTypedDict = TypeAliasType(
    "AssistantMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


AssistantMessageContent = TypeAliasType(
    "AssistantMessageContent", Union[str, List[ContentChunk]]
)


AssistantMessageRole = Literal["assistant"]


class AssistantMessageTypedDict(TypedDict):
    content: NotRequired[Nullable[AssistantMessageContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]
    prefix: NotRequired[bool]
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""
    role: NotRequired[AssistantMessageRole]


class AssistantMessage(BaseModel):
    content: OptionalNullable[AssistantMessageContent] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    prefix: Optional[bool] = False
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""

    role: Optional[AssistantMessageRole] = "assistant"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["content", "tool_calls", "prefix", "role"]
        nullable_fields = ["content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/chatcompletionchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from mistralai_azure.types import BaseModel, UnrecognizedStr
from mistralai_azure.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


ChatCompletionChoiceFinishReason = Union[
    Literal["stop", "length", "model_length", "error", "tool_calls"], UnrecognizedStr
]


class ChatCompletionChoiceTypedDict(TypedDict):
    index: int
    message: AssistantMessageTypedDict
    finish_reason: ChatCompletionChoiceFinishReason


class ChatCompletionChoice(BaseModel):
    index: int

    message: AssistantMessage

    finish_reason: Annotated[
        ChatCompletionChoiceFinishReason, PlainValidator(validate_open_enum(False))
    ]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/chatcompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from mistralai_azure.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ChatCompletionRequestStopTypedDict = TypeAliasType(
    "ChatCompletionRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionRequestStop = TypeAliasType(
    "ChatCompletionRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionRequestMessagesTypedDict = TypeAliasType(
    "ChatCompletionRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


ChatCompletionRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionRequestToolChoice = TypeAliasType(
    "ChatCompletionRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionRequestTypedDict(TypedDict):
    messages: List[ChatCompletionRequestMessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    model: NotRequired[str]
    r"""The ID of the model to use for this request."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[ChatCompletionRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    safe_prompt: NotRequired[bool]
    r"""Whether to inject a safety prompt before all conversations."""


class ChatCompletionRequest(BaseModel):
    messages: List[ChatCompletionRequestMessages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    model: Optional[str] = "azureai"
    r"""The ID of the model to use for this request."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[ChatCompletionRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    safe_prompt: Optional[bool] = None
    r"""Whether to inject a safety prompt before all conversations."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "model",
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "safe_prompt",
        ]
        nullable_fields = ["temperature", "max_tokens", "random_seed", "tools", "n"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/chatcompletionresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionchoice import ChatCompletionChoice, ChatCompletionChoiceTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai_azure.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class ChatCompletionResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    created: NotRequired[int]
    choices: NotRequired[List[ChatCompletionChoiceTypedDict]]


class ChatCompletionResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    created: Optional[int] = None

    choices: Optional[List[ChatCompletionChoice]] = None
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/chatcompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from mistralai_azure.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


Stop = TypeAliasType("Stop", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


MessagesTypedDict = TypeAliasType(
    "MessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


Messages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionStreamRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionStreamRequestToolChoice = TypeAliasType(
    "ChatCompletionStreamRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionStreamRequestTypedDict(TypedDict):
    messages: List[MessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    model: NotRequired[str]
    r"""The ID of the model to use for this request."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[StopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionStreamRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    safe_prompt: NotRequired[bool]
    r"""Whether to inject a safety prompt before all conversations."""


class ChatCompletionStreamRequest(BaseModel):
    messages: List[Messages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    model: Optional[str] = "azureai"
    r"""The ID of the model to use for this request."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[Stop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionStreamRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    safe_prompt: Optional[bool] = None
    r"""Whether to inject a safety prompt before all conversations."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "model",
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "safe_prompt",
        ]
        nullable_fields = ["temperature", "max_tokens", "random_seed", "tools", "n"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/completionchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceTypedDict,
)
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai_azure.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class CompletionChunkTypedDict(TypedDict):
    id: str
    model: str
    choices: List[CompletionResponseStreamChoiceTypedDict]
    object: NotRequired[str]
    created: NotRequired[int]
    usage: NotRequired[UsageInfoTypedDict]


class CompletionChunk(BaseModel):
    id: str

    model: str

    choices: List[CompletionResponseStreamChoice]

    object: Optional[str] = None

    created: Optional[int] = None

    usage: Optional[UsageInfo] = None
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/completionevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from mistralai_azure.types import BaseModel
from typing_extensions import TypedDict


class CompletionEventTypedDict(TypedDict):
    data: CompletionChunkTypedDict


class CompletionEvent(BaseModel):
    data: CompletionChunk
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/completionresponsestreamchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .deltamessage import DeltaMessage, DeltaMessageTypedDict
from mistralai_azure.types import BaseModel, Nullable, UNSET_SENTINEL, UnrecognizedStr
from mistralai_azure.utils import validate_open_enum
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


FinishReason = Union[Literal["stop", "length", "error", "tool_calls"], UnrecognizedStr]


class CompletionResponseStreamChoiceTypedDict(TypedDict):
    index: int
    delta: DeltaMessageTypedDict
    finish_reason: Nullable[FinishReason]


class CompletionResponseStreamChoice(BaseModel):
    index: int

    delta: DeltaMessage

    finish_reason: Annotated[
        Nullable[FinishReason], PlainValidator(validate_open_enum(False))
    ]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["finish_reason"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/contentchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai_azure.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType


ContentChunkTypedDict = TypeAliasType(
    "ContentChunkTypedDict",
    Union[TextChunkTypedDict, ImageURLChunkTypedDict, ReferenceChunkTypedDict],
)


ContentChunk = Annotated[
    Union[
        Annotated[ImageURLChunk, Tag("image_url")],
        Annotated[TextChunk, Tag("text")],
        Annotated[ReferenceChunk, Tag("reference")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/deltamessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ContentTypedDict = TypeAliasType(
    "ContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


Content = TypeAliasType("Content", Union[str, List[ContentChunk]])


class DeltaMessageTypedDict(TypedDict):
    role: NotRequired[Nullable[str]]
    content: NotRequired[Nullable[ContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]


class DeltaMessage(BaseModel):
    role: OptionalNullable[str] = UNSET

    content: OptionalNullable[Content] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role", "content", "tool_calls"]
        nullable_fields = ["role", "content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/function.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing import Any, Dict, Optional
from typing_extensions import NotRequired, TypedDict


class FunctionTypedDict(TypedDict):
    name: str
    parameters: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class Function(BaseModel):
    name: str

    parameters: Dict[str, Any]

    description: Optional[str] = None

    strict: Optional[bool] = None
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/functioncall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing import Any, Dict, Union
from typing_extensions import TypeAliasType, TypedDict


ArgumentsTypedDict = TypeAliasType("ArgumentsTypedDict", Union[Dict[str, Any], str])


Arguments = TypeAliasType("Arguments", Union[Dict[str, Any], str])


class FunctionCallTypedDict(TypedDict):
    name: str
    arguments: ArgumentsTypedDict


class FunctionCall(BaseModel):
    name: str

    arguments: Arguments
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/functionname.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing_extensions import TypedDict


class FunctionNameTypedDict(TypedDict):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str


class FunctionName(BaseModel):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/httpvalidationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .validationerror import ValidationError
from mistralai_azure import utils
from mistralai_azure.types import BaseModel
from typing import List, Optional


class HTTPValidationErrorData(BaseModel):
    detail: Optional[List[ValidationError]] = None


class HTTPValidationError(Exception):
    data: HTTPValidationErrorData

    def __init__(self, data: HTTPValidationErrorData):
        self.data = data

    def __str__(self) -> str:
        return utils.marshal_json(self.data, HTTPValidationErrorData)
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/imageurl.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class ImageURLTypedDict(TypedDict):
    url: str
    detail: NotRequired[Nullable[str]]


class ImageURL(BaseModel):
    url: str

    detail: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["detail"]
        nullable_fields = ["detail"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/imageurlchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .imageurl import ImageURL, ImageURLTypedDict
from mistralai_azure.types import BaseModel
from typing import Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ImageURLChunkImageURLTypedDict = TypeAliasType(
    "ImageURLChunkImageURLTypedDict", Union[ImageURLTypedDict, str]
)


ImageURLChunkImageURL = TypeAliasType("ImageURLChunkImageURL", Union[ImageURL, str])


ImageURLChunkType = Literal["image_url"]


class ImageURLChunkTypedDict(TypedDict):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURLTypedDict
    type: NotRequired[ImageURLChunkType]


class ImageURLChunk(BaseModel):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURL

    type: Optional[ImageURLChunkType] = "image_url"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/jsonschema.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class JSONSchemaTypedDict(TypedDict):
    name: str
    schema_definition: Dict[str, Any]
    description: NotRequired[Nullable[str]]
    strict: NotRequired[bool]


class JSONSchema(BaseModel):
    name: str

    schema_definition: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: OptionalNullable[str] = UNSET

    strict: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/prediction.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from mistralai_azure.utils import validate_const
import pydantic
from pydantic.functional_validators import AfterValidator
from typing import Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PredictionTypedDict(TypedDict):
    type: Literal["content"]
    content: NotRequired[str]


class Prediction(BaseModel):
    TYPE: Annotated[
        Annotated[
            Optional[Literal["content"]], AfterValidator(validate_const("content"))
        ],
        pydantic.Field(alias="type"),
    ] = "content"

    content: Optional[str] = ""
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/referencechunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ReferenceChunkType = Literal["reference"]


class ReferenceChunkTypedDict(TypedDict):
    reference_ids: List[int]
    type: NotRequired[ReferenceChunkType]


class ReferenceChunk(BaseModel):
    reference_ids: List[int]

    type: Optional[ReferenceChunkType] = "reference"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/responseformat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .responseformats import ResponseFormats
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ResponseFormatTypedDict(TypedDict):
    type: NotRequired[ResponseFormats]
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
    json_schema: NotRequired[Nullable[JSONSchemaTypedDict]]


class ResponseFormat(BaseModel):
    type: Optional[ResponseFormats] = None
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""

    json_schema: OptionalNullable[JSONSchema] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "json_schema"]
        nullable_fields = ["json_schema"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/responseformats.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ResponseFormats = Literal["text", "json_object", "json_schema"]
r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/sdkerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from dataclasses import dataclass
from typing import Optional
import httpx


@dataclass
class SDKError(Exception):
    """Represents an error returned by the API."""

    message: str
    status_code: int = -1
    body: str = ""
    raw_response: Optional[httpx.Response] = None

    def __str__(self):
        body = ""
        if len(self.body) > 0:
            body = f"\n{self.body}"

        return f"{self.message}: Status {self.status_code}{body}"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from mistralai_azure.utils import FieldMetadata, SecurityMetadata
from typing_extensions import Annotated, TypedDict


class SecurityTypedDict(TypedDict):
    api_key: str


class Security(BaseModel):
    api_key: Annotated[
        str,
        FieldMetadata(
            security=SecurityMetadata(
                scheme=True,
                scheme_type="http",
                sub_type="bearer",
                field_name="Authorization",
            )
        ),
    ]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/systemmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai_azure.types import BaseModel
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


SystemMessageContentTypedDict = TypeAliasType(
    "SystemMessageContentTypedDict", Union[str, List[TextChunkTypedDict]]
)


SystemMessageContent = TypeAliasType(
    "SystemMessageContent", Union[str, List[TextChunk]]
)


Role = Literal["system"]


class SystemMessageTypedDict(TypedDict):
    content: SystemMessageContentTypedDict
    role: NotRequired[Role]


class SystemMessage(BaseModel):
    content: SystemMessageContent

    role: Optional[Role] = "system"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/textchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


Type = Literal["text"]


class TextChunkTypedDict(TypedDict):
    text: str
    type: NotRequired[Type]


class TextChunk(BaseModel):
    text: str

    type: Optional[Type] = "text"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/tool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .function import Function, FunctionTypedDict
from .tooltypes import ToolTypes
from mistralai_azure.types import BaseModel
from mistralai_azure.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolTypedDict(TypedDict):
    function: FunctionTypedDict
    type: NotRequired[ToolTypes]


class Tool(BaseModel):
    function: Function

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/toolcall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functioncall import FunctionCall, FunctionCallTypedDict
from .tooltypes import ToolTypes
from mistralai_azure.types import BaseModel
from mistralai_azure.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolCallTypedDict(TypedDict):
    function: FunctionCallTypedDict
    id: NotRequired[str]
    type: NotRequired[ToolTypes]
    index: NotRequired[int]


class ToolCall(BaseModel):
    function: FunctionCall

    id: Optional[str] = "null"

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )

    index: Optional[int] = 0
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/toolchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functionname import FunctionName, FunctionNameTypedDict
from .tooltypes import ToolTypes
from mistralai_azure.types import BaseModel
from mistralai_azure.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolChoiceTypedDict(TypedDict):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionNameTypedDict
    r"""this restriction of `Function` is used to select a specific function to call"""
    type: NotRequired[ToolTypes]


class ToolChoice(BaseModel):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionName
    r"""this restriction of `Function` is used to select a specific function to call"""

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/toolchoiceenum.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ToolChoiceEnum = Literal["auto", "none", "any", "required"]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/toolmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai_azure.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ToolMessageContentTypedDict = TypeAliasType(
    "ToolMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


ToolMessageContent = TypeAliasType("ToolMessageContent", Union[str, List[ContentChunk]])


ToolMessageRole = Literal["tool"]


class ToolMessageTypedDict(TypedDict):
    content: Nullable[ToolMessageContentTypedDict]
    tool_call_id: NotRequired[Nullable[str]]
    name: NotRequired[Nullable[str]]
    role: NotRequired[ToolMessageRole]


class ToolMessage(BaseModel):
    content: Nullable[ToolMessageContent]

    tool_call_id: OptionalNullable[str] = UNSET

    name: OptionalNullable[str] = UNSET

    role: Optional[ToolMessageRole] = "tool"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_call_id", "name", "role"]
        nullable_fields = ["content", "tool_call_id", "name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/tooltypes.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import UnrecognizedStr
from typing import Literal, Union


ToolTypes = Union[Literal["function"], UnrecognizedStr]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/usageinfo.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing_extensions import TypedDict


class UsageInfoTypedDict(TypedDict):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class UsageInfo(BaseModel):
    prompt_tokens: int

    completion_tokens: int

    total_tokens: int
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/usermessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai_azure.types import BaseModel, Nullable, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


UserMessageContentTypedDict = TypeAliasType(
    "UserMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


UserMessageContent = TypeAliasType("UserMessageContent", Union[str, List[ContentChunk]])


UserMessageRole = Literal["user"]


class UserMessageTypedDict(TypedDict):
    content: Nullable[UserMessageContentTypedDict]
    role: NotRequired[UserMessageRole]


class UserMessage(BaseModel):
    content: Nullable[UserMessageContent]

    role: Optional[UserMessageRole] = "user"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/models/validationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_azure.types import BaseModel
from typing import List, Union
from typing_extensions import TypeAliasType, TypedDict


LocTypedDict = TypeAliasType("LocTypedDict", Union[str, int])


Loc = TypeAliasType("Loc", Union[str, int])


class ValidationErrorTypedDict(TypedDict):
    loc: List[LocTypedDict]
    msg: str
    type: str


class ValidationError(BaseModel):
    loc: List[Loc]

    msg: str

    type: str
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/types/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basemodel import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UnrecognizedInt,
    UnrecognizedStr,
    UNSET,
    UNSET_SENTINEL,
)

__all__ = [
    "BaseModel",
    "Nullable",
    "OptionalNullable",
    "UnrecognizedInt",
    "UnrecognizedStr",
    "UNSET",
    "UNSET_SENTINEL",
]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/types/basemodel.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from pydantic import ConfigDict, model_serializer
from pydantic import BaseModel as PydanticBaseModel
from typing import TYPE_CHECKING, Literal, Optional, TypeVar, Union, NewType
from typing_extensions import TypeAliasType, TypeAlias


class BaseModel(PydanticBaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, protected_namespaces=()
    )


class Unset(BaseModel):
    @model_serializer(mode="plain")
    def serialize_model(self):
        return UNSET_SENTINEL

    def __bool__(self) -> Literal[False]:
        return False


UNSET = Unset()
UNSET_SENTINEL = "~?~unset~?~sentinel~?~"


T = TypeVar("T")
if TYPE_CHECKING:
    Nullable: TypeAlias = Union[T, None]
    OptionalNullable: TypeAlias = Union[Optional[Nullable[T]], Unset]
else:
    Nullable = TypeAliasType("Nullable", Union[T, None], type_params=(T,))
    OptionalNullable = TypeAliasType(
        "OptionalNullable", Union[Optional[Nullable[T]], Unset], type_params=(T,)
    )

UnrecognizedInt = NewType("UnrecognizedInt", int)
UnrecognizedStr = NewType("UnrecognizedStr", str)
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .annotations import get_discriminator
from .enums import OpenEnumMeta
from .headers import get_headers, get_response_headers
from .metadata import (
    FieldMetadata,
    find_metadata,
    FormMetadata,
    HeaderMetadata,
    MultipartFormMetadata,
    PathParamMetadata,
    QueryParamMetadata,
    RequestMetadata,
    SecurityMetadata,
)
from .queryparams import get_query_params
from .retries import BackoffStrategy, Retries, retry, retry_async, RetryConfig
from .requestbodies import serialize_request_body, SerializedRequestBody
from .security import get_security
from .serializers import (
    get_pydantic_model,
    marshal_json,
    unmarshal,
    unmarshal_json,
    serialize_decimal,
    serialize_float,
    serialize_int,
    stream_to_text,
    stream_to_text_async,
    stream_to_bytes,
    stream_to_bytes_async,
    validate_const,
    validate_decimal,
    validate_float,
    validate_int,
    validate_open_enum,
)
from .url import generate_url, template_url, remove_suffix
from .values import (
    get_global_from_env,
    match_content_type,
    match_status_codes,
    match_response,
    cast_partial,
)
from .logger import Logger, get_body_content, get_default_logger

__all__ = [
    "BackoffStrategy",
    "FieldMetadata",
    "find_metadata",
    "FormMetadata",
    "generate_url",
    "get_body_content",
    "get_default_logger",
    "get_discriminator",
    "get_global_from_env",
    "get_headers",
    "get_pydantic_model",
    "get_query_params",
    "get_response_headers",
    "get_security",
    "HeaderMetadata",
    "Logger",
    "marshal_json",
    "match_content_type",
    "match_status_codes",
    "match_response",
    "MultipartFormMetadata",
    "OpenEnumMeta",
    "PathParamMetadata",
    "QueryParamMetadata",
    "remove_suffix",
    "Retries",
    "retry",
    "retry_async",
    "RetryConfig",
    "RequestMetadata",
    "SecurityMetadata",
    "serialize_decimal",
    "serialize_float",
    "serialize_int",
    "serialize_request_body",
    "SerializedRequestBody",
    "stream_to_text",
    "stream_to_text_async",
    "stream_to_bytes",
    "stream_to_bytes_async",
    "template_url",
    "unmarshal",
    "unmarshal_json",
    "validate_decimal",
    "validate_const",
    "validate_float",
    "validate_int",
    "validate_open_enum",
    "cast_partial",
]
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/annotations.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from enum import Enum
from typing import Any, Optional

def get_discriminator(model: Any, fieldname: str, key: str) -> str:
    """
    Recursively search for the discriminator attribute in a model.

    Args:
        model (Any): The model to search within.
        fieldname (str): The name of the field to search for.
        key (str): The key to search for in dictionaries.

    Returns:
        str: The name of the discriminator attribute.

    Raises:
        ValueError: If the discriminator attribute is not found.
    """
    upper_fieldname = fieldname.upper()

    def get_field_discriminator(field: Any) -> Optional[str]:
        """Search for the discriminator attribute in a given field."""

        if isinstance(field, dict):
            if key in field:
                return f'{field[key]}'

        if hasattr(field, fieldname):
            attr = getattr(field, fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        if hasattr(field, upper_fieldname):
            attr = getattr(field, upper_fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        return None


    if isinstance(model, list):
        for field in model:
            discriminator = get_field_discriminator(field)
            if discriminator is not None:
                return discriminator

    discriminator = get_field_discriminator(model)
    if discriminator is not None:
        return discriminator

    raise ValueError(f'Could not find discriminator field {fieldname} in {model}')
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/enums.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import enum


class OpenEnumMeta(enum.EnumMeta):
    def __call__(
        cls, value, names=None, *, module=None, qualname=None, type=None, start=1
    ):
        # The `type` kwarg also happens to be a built-in that pylint flags as
        # redeclared. Safe to ignore this lint rule with this scope.
        # pylint: disable=redefined-builtin

        if names is not None:
            return super().__call__(
                value,
                names=names,
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )

        try:
            return super().__call__(
                value,
                names=names,  # pyright: ignore[reportArgumentType]
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )
        except ValueError:
            return value
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/eventstreaming.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import re
import json
from typing import (
    Callable,
    Generic,
    TypeVar,
    Optional,
    Generator,
    AsyncGenerator,
    Tuple,
)
import httpx

T = TypeVar("T")


class EventStream(Generic[T]):
    response: httpx.Response
    generator: Generator[T, None, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events(response, decoder, sentinel)

    def __iter__(self):
        return self

    def __next__(self):
        return next(self.generator)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.response.close()


class EventStreamAsync(Generic[T]):
    response: httpx.Response
    generator: AsyncGenerator[T, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events_async(response, decoder, sentinel)

    def __aiter__(self):
        return self

    async def __anext__(self):
        return await self.generator.__anext__()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.response.aclose()


class ServerEvent:
    id: Optional[str] = None
    event: Optional[str] = None
    data: Optional[str] = None
    retry: Optional[int] = None


MESSAGE_BOUNDARIES = [
    b"\r\n\r\n",
    b"\n\n",
    b"\r\r",
]


async def stream_events_async(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> AsyncGenerator[T, None]:
    buffer = bytearray()
    position = 0
    discard = False
    async for chunk in response.aiter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def stream_events(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> Generator[T, None, None]:
    buffer = bytearray()
    position = 0
    discard = False
    for chunk in response.iter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def _parse_event(
    raw: bytearray, decoder: Callable[[str], T], sentinel: Optional[str] = None
) -> Tuple[Optional[T], bool]:
    block = raw.decode()
    lines = re.split(r"\r?\n|\r", block)
    publish = False
    event = ServerEvent()
    data = ""
    for line in lines:
        if not line:
            continue

        delim = line.find(":")
        if delim <= 0:
            continue

        field = line[0:delim]
        value = line[delim + 1 :] if delim < len(line) - 1 else ""
        if len(value) and value[0] == " ":
            value = value[1:]

        if field == "event":
            event.event = value
            publish = True
        elif field == "data":
            data += value + "\n"
            publish = True
        elif field == "id":
            event.id = value
            publish = True
        elif field == "retry":
            event.retry = int(value) if value.isdigit() else None
            publish = True

    if sentinel and data == f"{sentinel}\n":
        return None, True

    if data:
        data = data[:-1]
        event.data = data

        data_is_primitive = (
            data.isnumeric() or data == "true" or data == "false" or data == "null"
        )
        data_is_json = (
            data.startswith("{") or data.startswith("[") or data.startswith('"')
        )

        if data_is_primitive or data_is_json:
            try:
                event.data = json.loads(data)
            except Exception:
                pass

    out = None
    if publish:
        out = decoder(json.dumps(event.__dict__))

    return out, False


def _peek_sequence(position: int, buffer: bytearray, sequence: bytes):
    if len(sequence) > (len(buffer) - position):
        return None

    for i, seq in enumerate(sequence):
        if buffer[position + i] != seq:
            return None

    return sequence
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/forms.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .serializers import marshal_json

from .metadata import (
    FormMetadata,
    MultipartFormMetadata,
    find_field_metadata,
)
from .values import _is_set, _val_to_string


def _populate_form(
    field_name: str,
    explode: bool,
    obj: Any,
    delimiter: str,
    form: Dict[str, List[str]],
):
    if not _is_set(obj):
        return form

    if isinstance(obj, BaseModel):
        items = []

        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_field_name = obj_field.alias if obj_field.alias is not None else name
            if obj_field_name == "":
                continue

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                form[obj_field_name] = [_val_to_string(val)]
            else:
                items.append(f"{obj_field_name}{delimiter}{_val_to_string(val)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, Dict):
        items = []
        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                form[key] = [_val_to_string(value)]
            else:
                items.append(f"{key}{delimiter}{_val_to_string(value)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            if explode:
                if not field_name in form:
                    form[field_name] = []
                form[field_name].append(_val_to_string(value))
            else:
                items.append(_val_to_string(value))

        if len(items) > 0:
            form[field_name] = [delimiter.join([str(item) for item in items])]
    else:
        form[field_name] = [_val_to_string(obj)]

    return form


def serialize_multipart_form(
    media_type: str, request: Any
) -> Tuple[str, Dict[str, Any], Dict[str, Any]]:
    form: Dict[str, Any] = {}
    files: Dict[str, Any] = {}

    if not isinstance(request, BaseModel):
        raise TypeError("invalid request body type")

    request_fields: Dict[str, FieldInfo] = request.__class__.model_fields
    request_field_types = get_type_hints(request.__class__)

    for name in request_fields:
        field = request_fields[name]

        val = getattr(request, name)
        if not _is_set(val):
            continue

        field_metadata = find_field_metadata(field, MultipartFormMetadata)
        if not field_metadata:
            continue

        f_name = field.alias if field.alias else name

        if field_metadata.file:
            file_fields: Dict[str, FieldInfo] = val.__class__.model_fields

            file_name = ""
            content = None
            content_type = None

            for file_field_name in file_fields:
                file_field = file_fields[file_field_name]

                file_metadata = find_field_metadata(file_field, MultipartFormMetadata)
                if file_metadata is None:
                    continue

                if file_metadata.content:
                    content = getattr(val, file_field_name, None)
                elif file_field_name == "content_type":
                    content_type = getattr(val, file_field_name, None)
                else:
                    file_name = getattr(val, file_field_name)

            if file_name == "" or content is None:
                raise ValueError("invalid multipart/form-data file")

            if content_type is not None:
                files[f_name] = (file_name, content, content_type)
            else:
                files[f_name] = (file_name, content)
        elif field_metadata.json:
            files[f_name] = (
                None,
                marshal_json(val, request_field_types[name]),
                "application/json",
            )
        else:
            if isinstance(val, List):
                values = []

                for value in val:
                    if not _is_set(value):
                        continue
                    values.append(_val_to_string(value))

                form[f_name + "[]"] = values
            else:
                form[f_name] = _val_to_string(val)
    return media_type, form, files


def serialize_form_data(data: Any) -> Dict[str, Any]:
    form: Dict[str, List[str]] = {}

    if isinstance(data, BaseModel):
        data_fields: Dict[str, FieldInfo] = data.__class__.model_fields
        data_field_types = get_type_hints(data.__class__)
        for name in data_fields:
            field = data_fields[name]

            val = getattr(data, name)
            if not _is_set(val):
                continue

            metadata = find_field_metadata(field, FormMetadata)
            if metadata is None:
                continue

            f_name = field.alias if field.alias is not None else name

            if metadata.json:
                form[f_name] = [marshal_json(val, data_field_types[name])]
            else:
                if metadata.style == "form":
                    _populate_form(
                        f_name,
                        metadata.explode,
                        val,
                        ",",
                        form,
                    )
                else:
                    raise ValueError(f"Invalid form style for field {name}")
    elif isinstance(data, Dict):
        for key, value in data.items():
            if _is_set(value):
                form[key] = [_val_to_string(value)]
    else:
        raise TypeError(f"Invalid request body type {type(data)} for form data")

    return form
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/headers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    List,
    Optional,
)
from httpx import Headers
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    HeaderMetadata,
    find_field_metadata,
)

from .values import _is_set, _populate_from_globals, _val_to_string


def get_headers(headers_params: Any, gbls: Optional[Any] = None) -> Dict[str, str]:
    headers: Dict[str, str] = {}

    globals_already_populated = []
    if _is_set(headers_params):
        globals_already_populated = _populate_headers(headers_params, gbls, headers, [])
    if _is_set(gbls):
        _populate_headers(gbls, None, headers, globals_already_populated)

    return headers


def _populate_headers(
    headers_params: Any,
    gbls: Any,
    header_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(headers_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = headers_params.__class__.model_fields
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]
        f_name = field.alias if field.alias is not None else name

        metadata = find_field_metadata(field, HeaderMetadata)
        if metadata is None:
            continue

        value, global_found = _populate_from_globals(
            name, getattr(headers_params, name), HeaderMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)
        value = _serialize_header(metadata.explode, value)

        if value != "":
            header_values[f_name] = value

    return globals_already_populated


def _serialize_header(explode: bool, obj: Any) -> str:
    if not _is_set(obj):
        return ""

    if isinstance(obj, BaseModel):
        items = []
        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_param_metadata = find_field_metadata(obj_field, HeaderMetadata)

            if not obj_param_metadata:
                continue

            f_name = obj_field.alias if obj_field.alias is not None else name

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                items.append(f"{f_name}={_val_to_string(val)}")
            else:
                items.append(f_name)
                items.append(_val_to_string(val))

        if len(items) > 0:
            return ",".join(items)
    elif isinstance(obj, Dict):
        items = []

        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                items.append(f"{key}={_val_to_string(value)}")
            else:
                items.append(key)
                items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join([str(item) for item in items])
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join(items)
    elif _is_set(obj):
        return f"{_val_to_string(obj)}"

    return ""


def get_response_headers(headers: Headers) -> Dict[str, List[str]]:
    res: Dict[str, List[str]] = {}
    for k, v in headers.items():
        if not k in res:
            res[k] = []

        res[k].append(v)
    return res
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/logger.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
from typing import Any, Protocol


class Logger(Protocol):
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


class NoOpLogger:
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


def get_body_content(req: httpx.Request) -> str:
    return "<streaming body>" if not hasattr(req, "_content") else str(req.content)


def get_default_logger() -> Logger:
    return NoOpLogger()
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/metadata.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import Optional, Type, TypeVar, Union
from dataclasses import dataclass
from pydantic.fields import FieldInfo


T = TypeVar("T")


@dataclass
class SecurityMetadata:
    option: bool = False
    scheme: bool = False
    scheme_type: Optional[str] = None
    sub_type: Optional[str] = None
    field_name: Optional[str] = None

    def get_field_name(self, default: str) -> str:
        return self.field_name or default


@dataclass
class ParamMetadata:
    serialization: Optional[str] = None
    style: str = "simple"
    explode: bool = False


@dataclass
class PathParamMetadata(ParamMetadata):
    pass


@dataclass
class QueryParamMetadata(ParamMetadata):
    style: str = "form"
    explode: bool = True


@dataclass
class HeaderMetadata(ParamMetadata):
    pass


@dataclass
class RequestMetadata:
    media_type: str = "application/octet-stream"


@dataclass
class MultipartFormMetadata:
    file: bool = False
    content: bool = False
    json: bool = False


@dataclass
class FormMetadata:
    json: bool = False
    style: str = "form"
    explode: bool = True


class FieldMetadata:
    security: Optional[SecurityMetadata] = None
    path: Optional[PathParamMetadata] = None
    query: Optional[QueryParamMetadata] = None
    header: Optional[HeaderMetadata] = None
    request: Optional[RequestMetadata] = None
    form: Optional[FormMetadata] = None
    multipart: Optional[MultipartFormMetadata] = None

    def __init__(
        self,
        security: Optional[SecurityMetadata] = None,
        path: Optional[Union[PathParamMetadata, bool]] = None,
        query: Optional[Union[QueryParamMetadata, bool]] = None,
        header: Optional[Union[HeaderMetadata, bool]] = None,
        request: Optional[Union[RequestMetadata, bool]] = None,
        form: Optional[Union[FormMetadata, bool]] = None,
        multipart: Optional[Union[MultipartFormMetadata, bool]] = None,
    ):
        self.security = security
        self.path = PathParamMetadata() if isinstance(path, bool) else path
        self.query = QueryParamMetadata() if isinstance(query, bool) else query
        self.header = HeaderMetadata() if isinstance(header, bool) else header
        self.request = RequestMetadata() if isinstance(request, bool) else request
        self.form = FormMetadata() if isinstance(form, bool) else form
        self.multipart = (
            MultipartFormMetadata() if isinstance(multipart, bool) else multipart
        )


def find_field_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = find_metadata(field_info, FieldMetadata)
    if not metadata:
        return None

    fields = metadata.__dict__

    for field in fields:
        if isinstance(fields[field], metadata_type):
            return fields[field]

    return None


def find_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = field_info.metadata
    if not metadata:
        return None

    for md in metadata:
        if isinstance(md, metadata_type):
            return md

    return None
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/queryparams.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
)

from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    QueryParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)
from .forms import _populate_form


def get_query_params(
    query_params: Any,
    gbls: Optional[Any] = None,
) -> Dict[str, List[str]]:
    params: Dict[str, List[str]] = {}

    globals_already_populated = _populate_query_params(query_params, gbls, params, [])
    if _is_set(gbls):
        _populate_query_params(gbls, None, params, globals_already_populated)

    return params


def _populate_query_params(
    query_params: Any,
    gbls: Any,
    query_param_values: Dict[str, List[str]],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(query_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = query_params.__class__.model_fields
    param_field_types = get_type_hints(query_params.__class__)
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]

        metadata = find_field_metadata(field, QueryParamMetadata)
        if not metadata:
            continue

        value = getattr(query_params, name) if _is_set(query_params) else None

        value, global_found = _populate_from_globals(
            name, value, QueryParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        f_name = field.alias if field.alias is not None else name
        serialization = metadata.serialization
        if serialization is not None:
            serialized_parms = _get_serialized_params(
                metadata, f_name, value, param_field_types[name]
            )
            for key, value in serialized_parms.items():
                if key in query_param_values:
                    query_param_values[key].extend(value)
                else:
                    query_param_values[key] = [value]
        else:
            style = metadata.style
            if style == "deepObject":
                _populate_deep_object_query_params(f_name, value, query_param_values)
            elif style == "form":
                _populate_delimited_query_params(
                    metadata, f_name, value, ",", query_param_values
                )
            elif style == "pipeDelimited":
                _populate_delimited_query_params(
                    metadata, f_name, value, "|", query_param_values
                )
            else:
                raise NotImplementedError(
                    f"query param style {style} not yet supported"
                )

    return globals_already_populated


def _populate_deep_object_query_params(
    field_name: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj):
        return

    if isinstance(obj, BaseModel):
        _populate_deep_object_query_params_basemodel(field_name, obj, params)
    elif isinstance(obj, Dict):
        _populate_deep_object_query_params_dict(field_name, obj, params)


def _populate_deep_object_query_params_basemodel(
    prior_params_key: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj) or not isinstance(obj, BaseModel):
        return

    obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
    for name in obj_fields:
        obj_field = obj_fields[name]

        f_name = obj_field.alias if obj_field.alias is not None else name

        params_key = f"{prior_params_key}[{f_name}]"

        obj_param_metadata = find_field_metadata(obj_field, QueryParamMetadata)
        if not _is_set(obj_param_metadata):
            continue

        obj_val = getattr(obj, name)
        if not _is_set(obj_val):
            continue

        if isinstance(obj_val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, obj_val, params)
        elif isinstance(obj_val, Dict):
            _populate_deep_object_query_params_dict(params_key, obj_val, params)
        elif isinstance(obj_val, List):
            _populate_deep_object_query_params_list(params_key, obj_val, params)
        else:
            params[params_key] = [_val_to_string(obj_val)]


def _populate_deep_object_query_params_dict(
    prior_params_key: str,
    value: Dict,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for key, val in value.items():
        if not _is_set(val):
            continue

        params_key = f"{prior_params_key}[{key}]"

        if isinstance(val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, val, params)
        elif isinstance(val, Dict):
            _populate_deep_object_query_params_dict(params_key, val, params)
        elif isinstance(val, List):
            _populate_deep_object_query_params_list(params_key, val, params)
        else:
            params[params_key] = [_val_to_string(val)]


def _populate_deep_object_query_params_list(
    params_key: str,
    value: List,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for val in value:
        if not _is_set(val):
            continue

        if params.get(params_key) is None:
            params[params_key] = []

        params[params_key].append(_val_to_string(val))


def _populate_delimited_query_params(
    metadata: QueryParamMetadata,
    field_name: str,
    obj: Any,
    delimiter: str,
    query_param_values: Dict[str, List[str]],
):
    _populate_form(
        field_name,
        metadata.explode,
        obj,
        delimiter,
        query_param_values,
    )
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/requestbodies.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import io
from dataclasses import dataclass
import re
from typing import (
    Any,
    Optional,
)

from .forms import serialize_form_data, serialize_multipart_form

from .serializers import marshal_json

SERIALIZATION_METHOD_TO_CONTENT_TYPE = {
    "json": "application/json",
    "form": "application/x-www-form-urlencoded",
    "multipart": "multipart/form-data",
    "raw": "application/octet-stream",
    "string": "text/plain",
}


@dataclass
class SerializedRequestBody:
    media_type: Optional[str] = None
    content: Optional[Any] = None
    data: Optional[Any] = None
    files: Optional[Any] = None


def serialize_request_body(
    request_body: Any,
    nullable: bool,
    optional: bool,
    serialization_method: str,
    request_body_type,
) -> Optional[SerializedRequestBody]:
    if request_body is None:
        if not nullable and optional:
            return None

    media_type = SERIALIZATION_METHOD_TO_CONTENT_TYPE[serialization_method]

    serialized_request_body = SerializedRequestBody(media_type)

    if re.match(r"(application|text)\/.*?\+*json.*", media_type) is not None:
        serialized_request_body.content = marshal_json(request_body, request_body_type)
    elif re.match(r"multipart\/.*", media_type) is not None:
        (
            serialized_request_body.media_type,
            serialized_request_body.data,
            serialized_request_body.files,
        ) = serialize_multipart_form(media_type, request_body)
    elif re.match(r"application\/x-www-form-urlencoded.*", media_type) is not None:
        serialized_request_body.data = serialize_form_data(request_body)
    elif isinstance(request_body, (bytes, bytearray, io.BytesIO, io.BufferedReader)):
        serialized_request_body.content = request_body
    elif isinstance(request_body, str):
        serialized_request_body.content = request_body
    else:
        raise TypeError(
            f"invalid request body type {type(request_body)} for mediaType {media_type}"
        )

    return serialized_request_body
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/retries.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import asyncio
import random
import time
from typing import List

import httpx


class BackoffStrategy:
    initial_interval: int
    max_interval: int
    exponent: float
    max_elapsed_time: int

    def __init__(
        self,
        initial_interval: int,
        max_interval: int,
        exponent: float,
        max_elapsed_time: int,
    ):
        self.initial_interval = initial_interval
        self.max_interval = max_interval
        self.exponent = exponent
        self.max_elapsed_time = max_elapsed_time


class RetryConfig:
    strategy: str
    backoff: BackoffStrategy
    retry_connection_errors: bool

    def __init__(
        self, strategy: str, backoff: BackoffStrategy, retry_connection_errors: bool
    ):
        self.strategy = strategy
        self.backoff = backoff
        self.retry_connection_errors = retry_connection_errors


class Retries:
    config: RetryConfig
    status_codes: List[str]

    def __init__(self, config: RetryConfig, status_codes: List[str]):
        self.config = config
        self.status_codes = status_codes


class TemporaryError(Exception):
    response: httpx.Response

    def __init__(self, response: httpx.Response):
        self.response = response


class PermanentError(Exception):
    inner: Exception

    def __init__(self, inner: Exception):
        self.inner = inner


def retry(func, retries: Retries):
    if retries.config.strategy == "backoff":

        def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return retry_with_backoff(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return func()


async def retry_async(func, retries: Retries):
    if retries.config.strategy == "backoff":

        async def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = await func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return await retry_with_backoff_async(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return await func()


def retry_with_backoff(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            time.sleep(sleep)
            retries += 1


async def retry_with_backoff_async(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return await func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            await asyncio.sleep(sleep)
            retries += 1
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import base64
from typing import (
    Any,
    Dict,
    List,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    SecurityMetadata,
    find_field_metadata,
)


def get_security(security: Any) -> Tuple[Dict[str, str], Dict[str, List[str]]]:
    headers: Dict[str, str] = {}
    query_params: Dict[str, List[str]] = {}

    if security is None:
        return headers, query_params

    if not isinstance(security, BaseModel):
        raise TypeError("security must be a pydantic model")

    sec_fields: Dict[str, FieldInfo] = security.__class__.model_fields
    for name in sec_fields:
        sec_field = sec_fields[name]

        value = getattr(security, name)
        if value is None:
            continue

        metadata = find_field_metadata(sec_field, SecurityMetadata)
        if metadata is None:
            continue
        if metadata.option:
            _parse_security_option(headers, query_params, value)
            return headers, query_params
        if metadata.scheme:
            # Special case for basic auth or custom auth which could be a flattened model
            if metadata.sub_type in ["basic", "custom"] and not isinstance(
                value, BaseModel
            ):
                _parse_security_scheme(headers, query_params, metadata, name, security)
            else:
                _parse_security_scheme(headers, query_params, metadata, name, value)

    return headers, query_params


def _parse_security_option(
    headers: Dict[str, str], query_params: Dict[str, List[str]], option: Any
):
    if not isinstance(option, BaseModel):
        raise TypeError("security option must be a pydantic model")

    opt_fields: Dict[str, FieldInfo] = option.__class__.model_fields
    for name in opt_fields:
        opt_field = opt_fields[name]

        metadata = find_field_metadata(opt_field, SecurityMetadata)
        if metadata is None or not metadata.scheme:
            continue
        _parse_security_scheme(
            headers, query_params, metadata, name, getattr(option, name)
        )


def _parse_security_scheme(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    field_name: str,
    scheme: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    if isinstance(scheme, BaseModel):
        if scheme_type == "http":
            if sub_type == "basic":
                _parse_basic_auth_scheme(headers, scheme)
                return
            if sub_type == "custom":
                return

        scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
        for name in scheme_fields:
            scheme_field = scheme_fields[name]

            metadata = find_field_metadata(scheme_field, SecurityMetadata)
            if metadata is None or metadata.field_name is None:
                continue

            value = getattr(scheme, name)

            _parse_security_scheme_value(
                headers, query_params, scheme_metadata, metadata, name, value
            )
    else:
        _parse_security_scheme_value(
            headers, query_params, scheme_metadata, scheme_metadata, field_name, scheme
        )


def _parse_security_scheme_value(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    security_metadata: SecurityMetadata,
    field_name: str,
    value: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    header_name = security_metadata.get_field_name(field_name)

    if scheme_type == "apiKey":
        if sub_type == "header":
            headers[header_name] = value
        elif sub_type == "query":
            query_params[header_name] = [value]
        else:
            raise ValueError("sub type {sub_type} not supported")
    elif scheme_type == "openIdConnect":
        headers[header_name] = _apply_bearer(value)
    elif scheme_type == "oauth2":
        if sub_type != "client_credentials":
            headers[header_name] = _apply_bearer(value)
    elif scheme_type == "http":
        if sub_type == "bearer":
            headers[header_name] = _apply_bearer(value)
        elif sub_type == "custom":
            return
        else:
            raise ValueError("sub type {sub_type} not supported")
    else:
        raise ValueError("scheme type {scheme_type} not supported")


def _apply_bearer(token: str) -> str:
    return token.lower().startswith("bearer ") and token or f"Bearer {token}"


def _parse_basic_auth_scheme(headers: Dict[str, str], scheme: Any):
    username = ""
    password = ""

    if not isinstance(scheme, BaseModel):
        raise TypeError("basic auth scheme must be a pydantic model")

    scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
    for name in scheme_fields:
        scheme_field = scheme_fields[name]

        metadata = find_field_metadata(scheme_field, SecurityMetadata)
        if metadata is None or metadata.field_name is None:
            continue

        field_name = metadata.field_name
        value = getattr(scheme, name)

        if field_name == "username":
            username = value
        if field_name == "password":
            password = value

    data = f"{username}:{password}".encode()
    headers["Authorization"] = f"Basic {base64.b64encode(data).decode()}"
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/serializers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
import json
from typing import Any, Dict, List, Union, get_args
import httpx
from typing_extensions import get_origin
from pydantic import ConfigDict, create_model
from pydantic_core import from_json
from typing_inspection.typing_objects import is_union

from ..types.basemodel import BaseModel, Nullable, OptionalNullable, Unset


def serialize_decimal(as_str: bool):
    def serialize(d):
        # Optional[T] is a Union[T, None]
        if is_union(type(d)) and type(None) in get_args(type(d)) and d is None:
            return None
        if isinstance(d, Unset):
            return d

        if not isinstance(d, Decimal):
            raise ValueError("Expected Decimal object")

        return str(d) if as_str else float(d)

    return serialize


def validate_decimal(d):
    if d is None:
        return None

    if isinstance(d, (Decimal, Unset)):
        return d

    if not isinstance(d, (str, int, float)):
        raise ValueError("Expected string, int or float")

    return Decimal(str(d))


def serialize_float(as_str: bool):
    def serialize(f):
        # Optional[T] is a Union[T, None]
        if is_union(type(f)) and type(None) in get_args(type(f)) and f is None:
            return None
        if isinstance(f, Unset):
            return f

        if not isinstance(f, float):
            raise ValueError("Expected float")

        return str(f) if as_str else f

    return serialize


def validate_float(f):
    if f is None:
        return None

    if isinstance(f, (float, Unset)):
        return f

    if not isinstance(f, str):
        raise ValueError("Expected string")

    return float(f)


def serialize_int(as_str: bool):
    def serialize(i):
        # Optional[T] is a Union[T, None]
        if is_union(type(i)) and type(None) in get_args(type(i)) and i is None:
            return None
        if isinstance(i, Unset):
            return i

        if not isinstance(i, int):
            raise ValueError("Expected int")

        return str(i) if as_str else i

    return serialize


def validate_int(b):
    if b is None:
        return None

    if isinstance(b, (int, Unset)):
        return b

    if not isinstance(b, str):
        raise ValueError("Expected string")

    return int(b)


def validate_open_enum(is_int: bool):
    def validate(e):
        if e is None:
            return None

        if isinstance(e, Unset):
            return e

        if is_int:
            if not isinstance(e, int):
                raise ValueError("Expected int")
        else:
            if not isinstance(e, str):
                raise ValueError("Expected string")

        return e

    return validate


def validate_const(v):
    def validate(c):
        # Optional[T] is a Union[T, None]
        if is_union(type(c)) and type(None) in get_args(type(c)) and c is None:
            return None

        if v != c:
            raise ValueError(f"Expected {v}")

        return c

    return validate


def unmarshal_json(raw, typ: Any) -> Any:
    return unmarshal(from_json(raw), typ)


def unmarshal(val, typ: Any) -> Any:
    unmarshaller = create_model(
        "Unmarshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = unmarshaller(body=val)

    # pyright: ignore[reportAttributeAccessIssue]
    return m.body  # type: ignore


def marshal_json(val, typ):
    if is_nullable(typ) and val is None:
        return "null"

    marshaller = create_model(
        "Marshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = marshaller(body=val)

    d = m.model_dump(by_alias=True, mode="json", exclude_none=True)

    if len(d) == 0:
        return ""

    return json.dumps(d[next(iter(d))], separators=(",", ":"))


def is_nullable(field):
    origin = get_origin(field)
    if origin is Nullable or origin is OptionalNullable:
        return True

    if not origin is Union or type(None) not in get_args(field):
        return False

    for arg in get_args(field):
        if get_origin(arg) is Nullable or get_origin(arg) is OptionalNullable:
            return True

    return False


def stream_to_text(stream: httpx.Response) -> str:
    return "".join(stream.iter_text())


async def stream_to_text_async(stream: httpx.Response) -> str:
    return "".join([chunk async for chunk in stream.aiter_text()])


def stream_to_bytes(stream: httpx.Response) -> bytes:
    return stream.content


async def stream_to_bytes_async(stream: httpx.Response) -> bytes:
    return await stream.aread()


def get_pydantic_model(data: Any, typ: Any) -> Any:
    if not _contains_pydantic_model(data):
        return unmarshal(data, typ)

    return data


def _contains_pydantic_model(data: Any) -> bool:
    if isinstance(data, BaseModel):
        return True
    if isinstance(data, List):
        return any(_contains_pydantic_model(item) for item in data)
    if isinstance(data, Dict):
        return any(_contains_pydantic_model(value) for value in data.values())

    return False
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/url.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
    Union,
    get_args,
    get_origin,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    PathParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)


def generate_url(
    server_url: str,
    path: str,
    path_params: Any,
    gbls: Optional[Any] = None,
) -> str:
    path_param_values: Dict[str, str] = {}

    globals_already_populated = _populate_path_params(
        path_params, gbls, path_param_values, []
    )
    if _is_set(gbls):
        _populate_path_params(gbls, None, path_param_values, globals_already_populated)

    for key, value in path_param_values.items():
        path = path.replace("{" + key + "}", value, 1)

    return remove_suffix(server_url, "/") + path


def _populate_path_params(
    path_params: Any,
    gbls: Any,
    path_param_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(path_params, BaseModel):
        return globals_already_populated

    path_param_fields: Dict[str, FieldInfo] = path_params.__class__.model_fields
    path_param_field_types = get_type_hints(path_params.__class__)
    for name in path_param_fields:
        if name in skip_fields:
            continue

        field = path_param_fields[name]

        param_metadata = find_field_metadata(field, PathParamMetadata)
        if param_metadata is None:
            continue

        param = getattr(path_params, name) if _is_set(path_params) else None
        param, global_found = _populate_from_globals(
            name, param, PathParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        if not _is_set(param):
            continue

        f_name = field.alias if field.alias is not None else name
        serialization = param_metadata.serialization
        if serialization is not None:
            serialized_params = _get_serialized_params(
                param_metadata, f_name, param, path_param_field_types[name]
            )
            for key, value in serialized_params.items():
                path_param_values[key] = value
        else:
            pp_vals: List[str] = []
            if param_metadata.style == "simple":
                if isinstance(param, List):
                    for pp_val in param:
                        if not _is_set(pp_val):
                            continue
                        pp_vals.append(_val_to_string(pp_val))
                    path_param_values[f_name] = ",".join(pp_vals)
                elif isinstance(param, Dict):
                    for pp_key in param:
                        if not _is_set(param[pp_key]):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(f"{pp_key}={_val_to_string(param[pp_key])}")
                        else:
                            pp_vals.append(f"{pp_key},{_val_to_string(param[pp_key])}")
                    path_param_values[f_name] = ",".join(pp_vals)
                elif not isinstance(param, (str, int, float, complex, bool, Decimal)):
                    param_fields: Dict[str, FieldInfo] = param.__class__.model_fields
                    for name in param_fields:
                        param_field = param_fields[name]

                        param_value_metadata = find_field_metadata(
                            param_field, PathParamMetadata
                        )
                        if param_value_metadata is None:
                            continue

                        param_name = (
                            param_field.alias if param_field.alias is not None else name
                        )

                        param_field_val = getattr(param, name)
                        if not _is_set(param_field_val):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(
                                f"{param_name}={_val_to_string(param_field_val)}"
                            )
                        else:
                            pp_vals.append(
                                f"{param_name},{_val_to_string(param_field_val)}"
                            )
                    path_param_values[f_name] = ",".join(pp_vals)
                elif _is_set(param):
                    path_param_values[f_name] = _val_to_string(param)

    return globals_already_populated


def is_optional(field):
    return get_origin(field) is Union and type(None) in get_args(field)


def template_url(url_with_params: str, params: Dict[str, str]) -> str:
    for key, value in params.items():
        url_with_params = url_with_params.replace("{" + key + "}", value)

    return url_with_params


def remove_suffix(input_string, suffix):
    if suffix and input_string.endswith(suffix):
        return input_string[: -len(suffix)]
    return input_string
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/utils/values.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from datetime import datetime
from enum import Enum
from email.message import Message
from functools import partial
import os
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast

from httpx import Response
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ..types.basemodel import Unset

from .serializers import marshal_json

from .metadata import ParamMetadata, find_field_metadata


def match_content_type(content_type: str, pattern: str) -> bool:
    if pattern in (content_type, "*", "*/*"):
        return True

    msg = Message()
    msg["content-type"] = content_type
    media_type = msg.get_content_type()

    if media_type == pattern:
        return True

    parts = media_type.split("/")
    if len(parts) == 2:
        if pattern in (f"{parts[0]}/*", f"*/{parts[1]}"):
            return True

    return False


def match_status_codes(status_codes: List[str], status_code: int) -> bool:
    if "default" in status_codes:
        return True

    for code in status_codes:
        if code == str(status_code):
            return True

        if code.endswith("XX") and code.startswith(str(status_code)[:1]):
            return True
    return False


T = TypeVar("T")

def cast_partial(typ):
    return partial(cast, typ)

def get_global_from_env(
    value: Optional[T], env_key: str, type_cast: Callable[[str], T]
) -> Optional[T]:
    if value is not None:
        return value
    env_value = os.getenv(env_key)
    if env_value is not None:
        try:
            return type_cast(env_value)
        except ValueError:
            pass
    return None


def match_response(
    response: Response, code: Union[str, List[str]], content_type: str
) -> bool:
    codes = code if isinstance(code, list) else [code]
    return match_status_codes(codes, response.status_code) and match_content_type(
        response.headers.get("content-type", "application/octet-stream"), content_type
    )


def _populate_from_globals(
    param_name: str, value: Any, param_metadata_type: type, gbls: Any
) -> Tuple[Any, bool]:
    if gbls is None:
        return value, False

    if not isinstance(gbls, BaseModel):
        raise TypeError("globals must be a pydantic model")

    global_fields: Dict[str, FieldInfo] = gbls.__class__.model_fields
    found = False
    for name in global_fields:
        field = global_fields[name]
        if name is not param_name:
            continue

        found = True

        if value is not None:
            return value, True

        global_value = getattr(gbls, name)

        param_metadata = find_field_metadata(field, param_metadata_type)
        if param_metadata is None:
            return value, True

        return global_value, True

    return value, found


def _val_to_string(val) -> str:
    if isinstance(val, bool):
        return str(val).lower()
    if isinstance(val, datetime):
        return str(val.isoformat().replace("+00:00", "Z"))
    if isinstance(val, Enum):
        return str(val.value)

    return str(val)


def _get_serialized_params(
    metadata: ParamMetadata, field_name: str, obj: Any, typ: type
) -> Dict[str, str]:
    params: Dict[str, str] = {}

    serialization = metadata.serialization
    if serialization == "json":
        params[field_name] = marshal_json(obj, typ)

    return params


def _is_set(value: Any) -> bool:
    return value is not None and not isinstance(value, Unset)
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._version import (
    __title__,
    __version__,
    __openapi_doc_version__,
    __gen_version__,
    __user_agent__,
)
from .sdk import *
from .sdkconfiguration import *
from .models import *


VERSION: str = __version__
OPENAPI_DOC_VERSION = __openapi_doc_version__
SPEAKEASY_GENERATOR_VERSION = __gen_version__
USER_AGENT = __user_agent__
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/_version.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import importlib.metadata

__title__: str = "mistralai_azure"
__version__: str = "1.6.0"
__openapi_doc_version__: str = "0.0.2"
__gen_version__: str = "2.548.6"
__user_agent__: str = "speakeasy-sdk/python 1.6.0 2.548.6 0.0.2 mistralai_azure"

try:
    if __package__ is not None:
        __version__ = importlib.metadata.version(__package__)
except importlib.metadata.PackageNotFoundError:
    pass
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/basesdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkconfiguration import SDKConfiguration
import httpx
from mistralai_azure import models, utils
from mistralai_azure._hooks import (
    AfterErrorContext,
    AfterSuccessContext,
    BeforeRequestContext,
)
from mistralai_azure.utils import RetryConfig, SerializedRequestBody, get_body_content
from typing import Callable, List, Mapping, Optional, Tuple
from urllib.parse import parse_qs, urlparse


class BaseSDK:
    sdk_configuration: SDKConfiguration

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        self.sdk_configuration = sdk_config

    def _get_url(self, base_url, url_variables):
        sdk_url, sdk_variables = self.sdk_configuration.get_server_details()

        if base_url is None:
            base_url = sdk_url

        if url_variables is None:
            url_variables = sdk_variables

        return utils.template_url(base_url, url_variables)

    def _build_request_async(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.async_client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request_with_client(
        self,
        client,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        query_params = {}

        url = url_override
        if url is None:
            url = utils.generate_url(
                self._get_url(base_url, url_variables),
                path,
                request if request_has_path_params else None,
                _globals if request_has_path_params else None,
            )

            query_params = utils.get_query_params(
                request if request_has_query_params else None,
                _globals if request_has_query_params else None,
            )
        else:
            # Pick up the query parameter from the override so they can be
            # preserved when building the request later on (necessary as of
            # httpx 0.28).
            parsed_override = urlparse(str(url_override))
            query_params = parse_qs(parsed_override.query, keep_blank_values=True)

        headers = utils.get_headers(request, _globals)
        headers["Accept"] = accept_header_value
        headers[user_agent_header] = self.sdk_configuration.user_agent

        if security is not None:
            if callable(security):
                security = security()

        if security is not None:
            security_headers, security_query_params = utils.get_security(security)
            headers = {**headers, **security_headers}
            query_params = {**query_params, **security_query_params}

        serialized_request_body = SerializedRequestBody()
        if get_serialized_body is not None:
            rb = get_serialized_body()
            if request_body_required and rb is None:
                raise ValueError("request body is required")

            if rb is not None:
                serialized_request_body = rb

        if (
            serialized_request_body.media_type is not None
            and serialized_request_body.media_type
            not in (
                "multipart/form-data",
                "multipart/mixed",
            )
        ):
            headers["content-type"] = serialized_request_body.media_type

        if http_headers is not None:
            for header, value in http_headers.items():
                headers[header] = value

        timeout = timeout_ms / 1000 if timeout_ms is not None else None

        return client.build_request(
            method,
            url,
            params=query_params,
            content=serialized_request_body.content,
            data=serialized_request_body.data,
            files=serialized_request_body.files,
            headers=headers,
            timeout=timeout,
        )

    def do_request(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.client
        logger = self.sdk_configuration.debug_logger

        def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = utils.retry(do, utils.Retries(retry_config[0], retry_config[1]))
        else:
            http_res = do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res

    async def do_request_async(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.async_client
        logger = self.sdk_configuration.debug_logger

        async def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = await client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = await utils.retry_async(
                do, utils.Retries(retry_config[0], retry_config[1])
            )
        else:
            http_res = await do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/chat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai_azure import models, utils
from mistralai_azure._hooks import HookContext
from mistralai_azure.types import OptionalNullable, UNSET
from mistralai_azure.utils import eventstreaming
from typing import Any, List, Mapping, Optional, Union


class Chat(BaseSDK):
    r"""Chat Completion API."""

    def stream(
        self,
        *,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        model: Optional[str] = "azureai",
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStream[models.CompletionEvent]]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param model: The ID of the model to use for this request.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            safe_prompt=safe_prompt,
        )

        req = self._build_request(
            method="POST",
            path="/chat/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        model: Optional[str] = "azureai",
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStreamAsync[models.CompletionEvent]]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param model: The ID of the model to use for this request.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            safe_prompt=safe_prompt,
        )

        req = self._build_request_async(
            method="POST",
            path="/chat/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def complete(
        self,
        *,
        messages: Union[
            List[models.ChatCompletionRequestMessages],
            List[models.ChatCompletionRequestMessagesTypedDict],
        ],
        model: Optional[str] = "azureai",
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.ChatCompletionRequestStop,
                models.ChatCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.ChatCompletionResponse]:
        r"""Chat Completion

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param model: The ID of the model to use for this request.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            safe_prompt=safe_prompt,
        )

        req = self._build_request(
            method="POST",
            path="/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.ChatCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        messages: Union[
            List[models.ChatCompletionRequestMessages],
            List[models.ChatCompletionRequestMessagesTypedDict],
        ],
        model: Optional[str] = "azureai",
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.ChatCompletionRequestStop,
                models.ChatCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.ChatCompletionResponse]:
        r"""Chat Completion

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param model: The ID of the model to use for this request.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            safe_prompt=safe_prompt,
        )

        req = self._build_request_async(
            method="POST",
            path="/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.ChatCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/httpclient.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

# pyright: reportReturnType = false
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing_extensions import Protocol, runtime_checkable
import httpx
from typing import Any, Optional, Union


@runtime_checkable
class HttpClient(Protocol):
    def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    def close(self) -> None:
        pass


@runtime_checkable
class AsyncHttpClient(Protocol):
    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    async def aclose(self) -> None:
        pass


class ClientOwner(Protocol):
    client: Union[HttpClient, None]
    async_client: Union[AsyncHttpClient, None]


def close_clients(
    owner: ClientOwner,
    sync_client: Union[HttpClient, None],
    sync_client_supplied: bool,
    async_client: Union[AsyncHttpClient, None],
    async_client_supplied: bool,
) -> None:
    """
    A finalizer function that is meant to be used with weakref.finalize to close
    httpx clients used by an SDK so that underlying resources can be garbage
    collected.
    """

    # Unset the client/async_client properties so there are no more references
    # to them from the owning SDK instance and they can be reaped.
    owner.client = None
    owner.async_client = None

    if sync_client is not None and not sync_client_supplied:
        try:
            sync_client.close()
        except Exception:
            pass

    if async_client is not None and not async_client_supplied:
        is_async = False
        try:
            asyncio.get_running_loop()
            is_async = True
        except RuntimeError:
            pass

        try:
            # If this function is called in an async loop then start another
            # loop in a separate thread to close the async http client.
            if is_async:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_client.aclose())
                    future.result()
            else:
                asyncio.run(async_client.aclose())
        except Exception:
            pass
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/sdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import weakref
from typing import Any, Callable, Dict, Optional, Union, cast

import httpx

from mistralai_azure import models, utils
from mistralai_azure._hooks import SDKHooks
from mistralai_azure.chat import Chat
from mistralai_azure.types import UNSET, OptionalNullable

from .basesdk import BaseSDK
from .httpclient import AsyncHttpClient, ClientOwner, HttpClient, close_clients
from .sdkconfiguration import SDKConfiguration
from .utils.logger import Logger, get_default_logger
from .utils.retries import RetryConfig


class MistralAzure(BaseSDK):
    r"""Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it."""

    chat: Chat
    r"""Chat Completion API."""

    def __init__(
        self,
        azure_api_key: Union[str, Callable[[], str]],
        azure_endpoint: str,
        url_params: Optional[Dict[str, str]] = None,
        client: Optional[HttpClient] = None,
        async_client: Optional[AsyncHttpClient] = None,
        retry_config: OptionalNullable[RetryConfig] = UNSET,
        timeout_ms: Optional[int] = None,
        debug_logger: Optional[Logger] = None,
    ) -> None:
        r"""Instantiates the SDK configuring it with the provided parameters.

        :param azure_api_key: The azure_api_key required for authentication
        :param azure_endpoint: The Azure AI endpoint URL to use for all methods
        :param url_params: Parameters to optionally template the server URL with
        :param client: The HTTP client to use for all synchronous methods
        :param async_client: The Async HTTP client to use for all asynchronous methods
        :param retry_config: The retry configuration to use for all supported methods
        :param timeout_ms: Optional request timeout applied to each operation in milliseconds
        """

        # if azure_endpoint doesn't end with `/v1` add it
        if not azure_endpoint.endswith("/"):
            azure_endpoint += "/"
        if not azure_endpoint.endswith("v1/"):
            azure_endpoint += "v1/"
        server_url = azure_endpoint

        client_supplied = True
        if client is None:
            client = httpx.Client()
            client_supplied = False

        assert issubclass(
            type(client), HttpClient
        ), "The provided client must implement the HttpClient protocol."

        async_client_supplied = True
        if async_client is None:
            async_client = httpx.AsyncClient()
            async_client_supplied = False

        if debug_logger is None:
            debug_logger = get_default_logger()

        assert issubclass(
            type(async_client), AsyncHttpClient
        ), "The provided async_client must implement the AsyncHttpClient protocol."

        security: Any = None
        if callable(azure_api_key):
            security = lambda: models.Security(api_key=azure_api_key())  # pylint: disable=unnecessary-lambda-assignment
        else:
            security = models.Security(api_key=azure_api_key)

        if server_url is not None:
            if url_params is not None:
                server_url = utils.template_url(server_url, url_params)

        BaseSDK.__init__(
            self,
            SDKConfiguration(
                client=client,
                client_supplied=client_supplied,
                async_client=async_client,
                async_client_supplied=async_client_supplied,
                security=security,
                server_url=server_url,
                server=None,
                retry_config=retry_config,
                timeout_ms=timeout_ms,
                debug_logger=debug_logger,
            ),
        )

        hooks = SDKHooks()

        current_server_url, *_ = self.sdk_configuration.get_server_details()
        server_url, self.sdk_configuration.client = hooks.sdk_init(
            current_server_url, client
        )
        if current_server_url != server_url:
            self.sdk_configuration.server_url = server_url

        # pylint: disable=protected-access
        self.sdk_configuration.__dict__["_hooks"] = hooks

        weakref.finalize(
            self,
            close_clients,
            cast(ClientOwner, self.sdk_configuration),
            self.sdk_configuration.client,
            self.sdk_configuration.client_supplied,
            self.sdk_configuration.async_client,
            self.sdk_configuration.async_client_supplied,
        )

        self._init_sdks()

    def _init_sdks(self):
        self.chat = Chat(self.sdk_configuration)

    def __enter__(self):
        return self

    async def __aenter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.client is not None
            and not self.sdk_configuration.client_supplied
        ):
            self.sdk_configuration.client.close()
        self.sdk_configuration.client = None

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.async_client is not None
            and not self.sdk_configuration.async_client_supplied
        ):
            await self.sdk_configuration.async_client.aclose()
        self.sdk_configuration.async_client = None
</file>

<file path="packages/mistralai_azure/src/mistralai_azure/sdkconfiguration.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._hooks import SDKHooks
from ._version import (
    __gen_version__,
    __openapi_doc_version__,
    __user_agent__,
    __version__,
)
from .httpclient import AsyncHttpClient, HttpClient
from .utils import Logger, RetryConfig, remove_suffix
from dataclasses import dataclass
from mistralai_azure import models
from mistralai_azure.types import OptionalNullable, UNSET
from pydantic import Field
from typing import Callable, Dict, Optional, Tuple, Union


SERVER_EU = "eu"
r"""EU Production server"""
SERVERS = {
    SERVER_EU: "https://api.mistral.ai",
}
"""Contains the list of servers available to the SDK"""


@dataclass
class SDKConfiguration:
    client: Union[HttpClient, None]
    client_supplied: bool
    async_client: Union[AsyncHttpClient, None]
    async_client_supplied: bool
    debug_logger: Logger
    security: Optional[Union[models.Security, Callable[[], models.Security]]] = None
    server_url: Optional[str] = ""
    server: Optional[str] = ""
    language: str = "python"
    openapi_doc_version: str = __openapi_doc_version__
    sdk_version: str = __version__
    gen_version: str = __gen_version__
    user_agent: str = __user_agent__
    retry_config: OptionalNullable[RetryConfig] = Field(default_factory=lambda: UNSET)
    timeout_ms: Optional[int] = None

    def __post_init__(self):
        self._hooks = SDKHooks()

    def get_server_details(self) -> Tuple[str, Dict[str, str]]:
        if self.server_url is not None and self.server_url:
            return remove_suffix(self.server_url, "/"), {}
        if not self.server:
            self.server = SERVER_EU

        if self.server not in SERVERS:
            raise ValueError(f'Invalid server "{self.server}"')

        return SERVERS[self.server], {}

    def get_hooks(self) -> SDKHooks:
        return self._hooks
</file>

<file path="packages/mistralai_azure/.genignore">
src/mistralai_azure/sdk.py
README.md
USAGE.md
docs/sdks/**/README.md
</file>

<file path="packages/mistralai_azure/.gitattributes">
# This allows generated code to be indexed correctly
*.py linguist-generated=false
</file>

<file path="packages/mistralai_azure/.gitignore">
.speakeasy/reports
README-PYPI.md
.venv/
venv/
src/*.egg-info/
__pycache__/
.pytest_cache/
.python-version
.DS_Store
pyrightconfig.json
</file>

<file path="packages/mistralai_azure/CONTRIBUTING.md">
# Contributing to This Repository

Thank you for your interest in contributing to this repository. Please note that this repository contains generated code. As such, we do not accept direct changes or pull requests. Instead, we encourage you to follow the guidelines below to report issues and suggest improvements.

## How to Report Issues

If you encounter any bugs or have suggestions for improvements, please open an issue on GitHub. When reporting an issue, please provide as much detail as possible to help us reproduce the problem. This includes:

- A clear and descriptive title
- Steps to reproduce the issue
- Expected and actual behavior
- Any relevant logs, screenshots, or error messages
- Information about your environment (e.g., operating system, software versions)
    - For example can be collected using the `npx envinfo` command from your terminal if you have Node.js installed

## Issue Triage and Upstream Fixes

We will review and triage issues as quickly as possible. Our goal is to address bugs and incorporate improvements in the upstream source code. Fixes will be included in the next generation of the generated code.

## Contact

If you have any questions or need further assistance, please feel free to reach out by opening an issue.

Thank you for your understanding and cooperation!

The Maintainers
</file>

<file path="packages/mistralai_azure/poetry.toml">
[virtualenvs]
in-project = true
</file>

<file path="packages/mistralai_azure/py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="packages/mistralai_azure/pylintrc">
[MAIN]

# Analyse import fallback blocks. This can be used to support both Python 2 and
# 3 compatible code, which means that the block might have code that exists
# only in one or another interpreter, leading to false positives when analysed.
analyse-fallback-blocks=no

# Clear in-memory caches upon conclusion of linting. Useful if running pylint
# in a server-like mode.
clear-cache-post-run=no

# Load and enable all available extensions. Use --list-extensions to see a list
# all available extensions.
#enable-all-extensions=

# In error mode, messages with a category besides ERROR or FATAL are
# suppressed, and no reports are done by default. Error mode is compatible with
# disabling specific errors.
#errors-only=

# Always return a 0 (non-error) status code, even if lint errors are found.
# This is primarily useful in continuous integration scripts.
#exit-zero=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-allow-list=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code. (This is an alternative name to extension-pkg-allow-list
# for backward compatibility.)
extension-pkg-whitelist=

# Return non-zero exit code if any of these messages/categories are detected,
# even if score is above --fail-under value. Syntax same as enable. Messages
# specified are enabled, while categories only check already-enabled messages.
fail-on=

# Specify a score threshold under which the program will exit with error.
fail-under=10

# Interpret the stdin as a python script, whose filename needs to be passed as
# the module_or_package argument.
#from-stdin=

# Files or directories to be skipped. They should be base names, not paths.
ignore=CVS

# Add files or directories matching the regular expressions patterns to the
# ignore-list. The regex matches against paths and can be in Posix or Windows
# format. Because '\\' represents the directory delimiter on Windows systems,
# it can't be used as an escape character.
ignore-paths=

# Files or directories matching the regular expression patterns are skipped.
# The regex matches against base names, not paths. The default value ignores
# Emacs file locks
ignore-patterns=^\.#

# List of module names for which member attributes should not be checked and
# will not be imported (useful for modules/projects where namespaces are
# manipulated during runtime and thus existing member attributes cannot be
# deduced by static analysis). It supports qualified module names, as well as
# Unix pattern matching.
ignored-modules=

# Python code to execute, usually for sys.path manipulation such as
# pygtk.require().
#init-hook=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use, and will cap the count on Windows to
# avoid hangs.
jobs=1

# Control the amount of potential inferred values when inferring a single
# object. This can help the performance when dealing with large functions or
# complex, nested conditions.
limit-inference-results=100

# List of plugins (as comma separated values of python module names) to load,
# usually to register additional checkers.
load-plugins=

# Pickle collected data for later comparisons.
persistent=yes

# Minimum Python version to use for version dependent checks. Will default to
# the version used to run pylint.
py-version=3.9

# Discover python modules and packages in the file system subtree.
recursive=no

# Add paths to the list of the source roots. Supports globbing patterns. The
# source root is an absolute path or a path relative to the current working
# directory used to determine a package namespace for modules located under the
# source root.
source-roots=src

# When enabled, pylint would attempt to guess common misconfiguration and emit
# user-friendly hints instead of false-positive error messages.
suggestion-mode=yes

# Allow loading of arbitrary C extensions. Extensions are imported into the
# active Python interpreter and may run arbitrary code.
unsafe-load-any-extension=no

# In verbose mode, extra non-checker-related info will be displayed.
#verbose=


[BASIC]

# Naming style matching correct argument names.
argument-naming-style=snake_case

# Regular expression matching correct argument names. Overrides argument-
# naming-style. If left empty, argument names will be checked with the set
# naming style.
#argument-rgx=

# Naming style matching correct attribute names.
#attr-naming-style=snake_case

# Regular expression matching correct attribute names. Overrides attr-naming-
# style. If left empty, attribute names will be checked with the set naming
# style.
attr-rgx=[^\W\d][^\W]*|__.*__$

# Bad variable names which should always be refused, separated by a comma.
bad-names=

# Bad variable names regexes, separated by a comma. If names match any regex,
# they will always be refused
bad-names-rgxs=

# Naming style matching correct class attribute names.
class-attribute-naming-style=any

# Regular expression matching correct class attribute names. Overrides class-
# attribute-naming-style. If left empty, class attribute names will be checked
# with the set naming style.
#class-attribute-rgx=

# Naming style matching correct class constant names.
class-const-naming-style=UPPER_CASE

# Regular expression matching correct class constant names. Overrides class-
# const-naming-style. If left empty, class constant names will be checked with
# the set naming style.
#class-const-rgx=

# Naming style matching correct class names.
class-naming-style=PascalCase

# Regular expression matching correct class names. Overrides class-naming-
# style. If left empty, class names will be checked with the set naming style.
#class-rgx=

# Naming style matching correct constant names.
const-naming-style=UPPER_CASE

# Regular expression matching correct constant names. Overrides const-naming-
# style. If left empty, constant names will be checked with the set naming
# style.
#const-rgx=

# Minimum line length for functions/classes that require docstrings, shorter
# ones are exempt.
docstring-min-length=-1

# Naming style matching correct function names.
function-naming-style=snake_case

# Regular expression matching correct function names. Overrides function-
# naming-style. If left empty, function names will be checked with the set
# naming style.
#function-rgx=

# Good variable names which should always be accepted, separated by a comma.
good-names=i,
           j,
           k,
           ex,
           Run,
           _,
           e,
           n,
           id

# Good variable names regexes, separated by a comma. If names match any regex,
# they will always be accepted
good-names-rgxs=

# Include a hint for the correct naming format with invalid-name.
include-naming-hint=no

# Naming style matching correct inline iteration names.
inlinevar-naming-style=any

# Regular expression matching correct inline iteration names. Overrides
# inlinevar-naming-style. If left empty, inline iteration names will be checked
# with the set naming style.
#inlinevar-rgx=

# Naming style matching correct method names.
method-naming-style=snake_case

# Regular expression matching correct method names. Overrides method-naming-
# style. If left empty, method names will be checked with the set naming style.
#method-rgx=

# Naming style matching correct module names.
module-naming-style=snake_case

# Regular expression matching correct module names. Overrides module-naming-
# style. If left empty, module names will be checked with the set naming style.
#module-rgx=

# Colon-delimited sets of names that determine each other's naming style when
# the name regexes allow several styles.
name-group=

# Regular expression which should only match function or class names that do
# not require a docstring.
no-docstring-rgx=^_

# List of decorators that produce properties, such as abc.abstractproperty. Add
# to this list to register other decorators that produce valid properties.
# These decorators are taken in consideration only for invalid-name.
property-classes=abc.abstractproperty

# Regular expression matching correct type alias names. If left empty, type
# alias names will be checked with the set naming style.
typealias-rgx=.*

# Regular expression matching correct type variable names. If left empty, type
# variable names will be checked with the set naming style.
#typevar-rgx=

# Naming style matching correct variable names.
variable-naming-style=snake_case

# Regular expression matching correct variable names. Overrides variable-
# naming-style. If left empty, variable names will be checked with the set
# naming style.
#variable-rgx=


[CLASSES]

# Warn about protected attribute access inside special methods
check-protected-access-in-special-methods=no

# List of method names used to declare (i.e. assign) instance attributes.
defining-attr-methods=__init__,
                      __new__,
                      setUp,
                      asyncSetUp,
                      __post_init__

# List of member names, which should be excluded from the protected access
# warning.
exclude-protected=_asdict,_fields,_replace,_source,_make,os._exit

# List of valid names for the first argument in a class method.
valid-classmethod-first-arg=cls

# List of valid names for the first argument in a metaclass class method.
valid-metaclass-classmethod-first-arg=mcs


[DESIGN]

# List of regular expressions of class ancestor names to ignore when counting
# public methods (see R0903)
exclude-too-few-public-methods=

# List of qualified class names to ignore when counting class parents (see
# R0901)
ignored-parents=

# Maximum number of arguments for function / method.
max-args=5

# Maximum number of attributes for a class (see R0902).
max-attributes=7

# Maximum number of boolean expressions in an if statement (see R0916).
max-bool-expr=5

# Maximum number of branch for function / method body.
max-branches=12

# Maximum number of locals for function / method body.
max-locals=15

# Maximum number of parents for a class (see R0901).
max-parents=7

# Maximum number of public methods for a class (see R0904).
max-public-methods=25

# Maximum number of return / yield for function / method body.
max-returns=6

# Maximum number of statements in function / method body.
max-statements=50

# Minimum number of public methods for a class (see R0903).
min-public-methods=2


[EXCEPTIONS]

# Exceptions that will emit a warning when caught.
overgeneral-exceptions=builtins.BaseException,builtins.Exception


[FORMAT]

# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
expected-line-ending-format=

# Regexp for a line that is allowed to be longer than the limit.
ignore-long-lines=^\s*(# )?<?https?://\S+>?$

# Number of spaces of indent required inside a hanging or continued line.
indent-after-paren=4

# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
# tab).
indent-string='    '

# Maximum number of characters on a single line.
max-line-length=100

# Maximum number of lines in a module.
max-module-lines=1000

# Allow the body of a class to be on the same line as the declaration if body
# contains single statement.
single-line-class-stmt=no

# Allow the body of an if to be on the same line as the test if there is no
# else.
single-line-if-stmt=no


[IMPORTS]

# List of modules that can be imported at any level, not just the top level
# one.
allow-any-import-level=

# Allow explicit reexports by alias from a package __init__.
allow-reexport-from-package=no

# Allow wildcard imports from modules that define __all__.
allow-wildcard-with-all=no

# Deprecated modules which should not be used, separated by a comma.
deprecated-modules=

# Output a graph (.gv or any supported image format) of external dependencies
# to the given file (report RP0402 must not be disabled).
ext-import-graph=

# Output a graph (.gv or any supported image format) of all (i.e. internal and
# external) dependencies to the given file (report RP0402 must not be
# disabled).
import-graph=

# Output a graph (.gv or any supported image format) of internal dependencies
# to the given file (report RP0402 must not be disabled).
int-import-graph=

# Force import order to recognize a module as part of the standard
# compatibility libraries.
known-standard-library=

# Force import order to recognize a module as part of a third party library.
known-third-party=enchant

# Couples of modules and preferred modules, separated by a comma.
preferred-modules=


[LOGGING]

# The type of string formatting that logging methods do. `old` means using %
# formatting, `new` is for `{}` formatting.
logging-format-style=old

# Logging modules to check that the string format arguments are in logging
# function parameter format.
logging-modules=logging


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
# UNDEFINED.
confidence=HIGH,
           CONTROL_FLOW,
           INFERENCE,
           INFERENCE_FAILURE,
           UNDEFINED

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then re-enable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=raw-checker-failed,
        bad-inline-option,
        locally-disabled,
        file-ignored,
        suppressed-message,
        useless-suppression,
        deprecated-pragma,
        use-implicit-booleaness-not-comparison-to-string,
        use-implicit-booleaness-not-comparison-to-zero,
        use-symbolic-message-instead,
        trailing-whitespace,
        line-too-long,
        missing-class-docstring,
        missing-module-docstring,
        missing-function-docstring,
        too-many-instance-attributes,
        wrong-import-order,
        too-many-arguments,
        broad-exception-raised,
        too-few-public-methods,
        too-many-branches,
        duplicate-code,
        trailing-newlines,
        too-many-public-methods,
        too-many-locals,
        too-many-lines,
        using-constant-test,
        too-many-statements,
        cyclic-import,
        too-many-nested-blocks,
        too-many-boolean-expressions,
        no-else-raise,
        bare-except,
        broad-exception-caught,
        fixme,
        relative-beyond-top-level,
        consider-using-with,
        wildcard-import,
        unused-wildcard-import

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=


[METHOD_ARGS]

# List of qualified names (i.e., library.method) which require a timeout
# parameter e.g. 'requests.api.get,requests.api.post'
timeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request


[MISCELLANEOUS]

# List of note tags to take in consideration, separated by a comma.
notes=FIXME,
      XXX,
      TODO

# Regular expression of note tags to take in consideration.
notes-rgx=


[REFACTORING]

# Maximum number of nested blocks for function / method body
max-nested-blocks=5

# Complete name of functions that never returns. When checking for
# inconsistent-return-statements if a never returning function is called then
# it will be considered as an explicit return statement and no message will be
# printed.
never-returning-functions=sys.exit,argparse.parse_error


[REPORTS]

# Python expression which should return a score less than or equal to 10. You
# have access to the variables 'fatal', 'error', 'warning', 'refactor',
# 'convention', and 'info' which contain the number of messages in each
# category, as well as 'statement' which is the total number of statements
# analyzed. This score is used by the global evaluation report (RP0004).
evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))

# Template used to display messages. This is a python new-style format string
# used to format the message information. See doc for all details.
msg-template=

# Set the output format. Available formats are: text, parseable, colorized,
# json2 (improved json format), json (old json format) and msvs (visual
# studio). You can also give a reporter class, e.g.
# mypackage.mymodule.MyReporterClass.
#output-format=

# Tells whether to display a full report or only the messages.
reports=no

# Activate the evaluation score.
score=yes


[SIMILARITIES]

# Comments are removed from the similarity computation
ignore-comments=yes

# Docstrings are removed from the similarity computation
ignore-docstrings=yes

# Imports are removed from the similarity computation
ignore-imports=yes

# Signatures are removed from the similarity computation
ignore-signatures=yes

# Minimum lines number of a similarity.
min-similarity-lines=4


[SPELLING]

# Limits count of emitted suggestions for spelling mistakes.
max-spelling-suggestions=4

# Spelling dictionary name. No available dictionaries : You need to install
# both the python package and the system dependency for enchant to work.
spelling-dict=

# List of comma separated words that should be considered directives if they
# appear at the beginning of a comment and should not be checked.
spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:

# List of comma separated words that should not be checked.
spelling-ignore-words=

# A path to a file that contains the private dictionary; one word per line.
spelling-private-dict-file=

# Tells whether to store unknown words to the private dictionary (see the
# --spelling-private-dict-file option) instead of raising a message.
spelling-store-unknown-words=no


[STRING]

# This flag controls whether inconsistent-quotes generates a warning when the
# character used as a quote delimiter is used inconsistently within a module.
check-quote-consistency=no

# This flag controls whether the implicit-str-concat should generate a warning
# on implicit string concatenation in sequences defined over several lines.
check-str-concat-over-line-jumps=no


[TYPECHECK]

# List of decorators that produce context managers, such as
# contextlib.contextmanager. Add to this list to register other decorators that
# produce valid context managers.
contextmanager-decorators=contextlib.contextmanager

# List of members which are set dynamically and missed by pylint inference
# system, and so shouldn't trigger E1101 when accessed. Python regular
# expressions are accepted.
generated-members=

# Tells whether to warn about missing members when the owner of the attribute
# is inferred to be None.
ignore-none=yes

# This flag controls whether pylint should warn about no-member and similar
# checks whenever an opaque object is returned when inferring. The inference
# can return multiple potential results while evaluating a Python object, but
# some branches might not be evaluated, which results in partial inference. In
# that case, it might be useful to still emit no-member and other checks for
# the rest of the inferred objects.
ignore-on-opaque-inference=yes

# List of symbolic message names to ignore for Mixin members.
ignored-checks-for-mixins=no-member,
                          not-async-context-manager,
                          not-context-manager,
                          attribute-defined-outside-init

# List of class names for which member attributes should not be checked (useful
# for classes with dynamically set attributes). This supports the use of
# qualified names.
ignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace

# Show a hint with possible names when a member name was not found. The aspect
# of finding the hint is based on edit distance.
missing-member-hint=yes

# The minimum edit distance a name should have in order to be considered a
# similar match for a missing member name.
missing-member-hint-distance=1

# The total number of similar names that should be taken in consideration when
# showing a hint for a missing member.
missing-member-max-choices=1

# Regex pattern to define which classes are considered mixins.
mixin-class-rgx=.*[Mm]ixin

# List of decorators that change the signature of a decorated function.
signature-mutators=


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of names allowed to shadow builtins
allowed-redefined-builtins=id,object

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored.
ignored-argument-names=_.*|^ignored_|^unused_

# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
</file>

<file path="packages/mistralai_azure/pyproject.toml">
[project]
name = "mistralai_azure"
version = "1.6.0"
description = "Python Client SDK for the Mistral AI API in Azure."
authors = [{ name = "Mistral" },]
readme = "README-PYPI.md"
requires-python = ">=3.9"
dependencies = [
    "eval-type-backport >=0.2.0",
    "httpx >=0.28.1",
    "pydantic >=2.10.3",
    "python-dateutil >=2.8.2",
    "typing-inspection >=0.4.0",
]

[tool.poetry]
packages = [
    { include = "mistralai_azure", from = "src" }
]
include = ["py.typed", "src/mistralai_azure/py.typed"]

[tool.setuptools.package-data]
"*" = ["py.typed", "src/mistralai_azure/py.typed"]

[virtualenvs]
in-project = true

[tool.poetry.group.dev.dependencies]
mypy = "==1.14.1"
pylint = "==3.2.3"
pytest = "^8.2.2"
pytest-asyncio = "^0.23.7"
types-python-dateutil = "^2.9.0.20240316"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["src"]

[tool.mypy]
disable_error_code = "misc"

[[tool.mypy.overrides]]
module = "typing_inspect"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "jsonpath"
ignore_missing_imports = true

[tool.pyright]
venvPath = "."
venv = ".venv"
</file>

<file path="packages/mistralai_azure/README.md">
# Mistral on Azure Python Client

## SDK Installation

PIP
```bash
pip install mistralai
```

Poetry
```bash
poetry add mistralai
```

**Prerequisites**

Before you begin, ensure you have `AZUREAI_ENDPOINT` and an `AZURE_API_KEY`. To obtain these, you will need to deploy Mistral on Azure AI.
See [instructions for deploying Mistral on Azure AI here](https://docs.mistral.ai/deployment/cloud/azure/).

<!-- Start SDK Example Usage [usage] -->
## SDK Example Usage

### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.complete(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="azureai"
)

if res is not None:
    # handle response
    pass
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai_azure import MistralAzure
import os

async def main():
    s = MistralAzure(
        azure_api_key=os.getenv("AZURE_API_KEY", ""),
        azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
    )
    res = await s.chat.complete_async(
        messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ], 
        model="azureai"
    )
    if res is not None:
        # handle response
        pass

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->

<!-- Start Available Resources and Operations [operations] -->
## Available Resources and Operations

### [chat](docs/sdks/chat/README.md)

* [stream](docs/sdks/chat/README.md#stream) - Stream chat completion
* [create](docs/sdks/chat/README.md#create) - Chat Completion
<!-- End Available Resources and Operations [operations] -->

<!-- Start Server-sent event streaming [eventstream] -->
## Server-sent event streaming

[Server-sent events][mdn-sse] are used to stream content from certain
operations. These operations will expose the stream as [Generator][generator] that
can be consumed using a simple `for` loop. The loop will
terminate when the server no longer has any events to send and closes the
underlying connection.

```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="azureai"
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```

[mdn-sse]: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events
[generator]: https://wiki.python.org/moin/Generators
<!-- End Server-sent event streaming [eventstream] -->

<!-- Start Retries [retries] -->
## Retries

Some of the endpoints in this SDK support retries. If you use the SDK without any configuration, it will fall back to the default retry strategy provided by the API. However, the default retry strategy can be overridden on a per-operation basis, or across the entire SDK.

To change the default retry strategy for a single API call, simply provide a `RetryConfig` object to the call:
```python
from mistralai_azure import MistralAzure
from mistralazure.utils import BackoffStrategy, RetryConfig
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai",
    RetryConfig("backoff", BackoffStrategy(1, 50, 1.1, 100), False))

if res is not None:
    for event in res:
        # handle event
        print(event)

```

If you'd like to override the default retry strategy for all operations that support retries, you can use the `retry_config` optional parameter when initializing the SDK:
```python
from mistralai_azure import MistralAzure
from mistralazure.utils import BackoffStrategy, RetryConfig
import os

s = MistralAzure(
    retry_config=RetryConfig("backoff", BackoffStrategy(1, 50, 1.1, 100), False),
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Retries [retries] -->

<!-- Start Error Handling [errors] -->
## Error Handling

Handling errors in this SDK should largely match your expectations.  All operations return a response object or raise an error.  If Error objects are specified in your OpenAPI Spec, the SDK will raise the appropriate Error type.

| Error Object               | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4xx-5xx     | */*              |

### Example

```python
from mistralai_azure import MistralAzure, models
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)

res = None
try:
    res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

except models.HTTPValidationError as e:
    # handle exception
    raise(e)
except models.SDKError as e:
    # handle exception
    raise(e)

if res is not None:
    # handle response
    pass

```
<!-- End Error Handling [errors] -->

<!-- Start Server Selection [server] -->
## Server Selection

### Select Server by Name

You can override the default server globally by passing a server name to the `server: str` optional parameter when initializing the SDK client instance. The selected server will then be used as the default on the operations that use it. This table lists the names associated with the available servers:

| Name   | Server                   | Variables |
| ------ | ------------------------ | --------- |
| `prod` | `https://api.mistral.ai` | None      |

#### Example

```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    server="prod",
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    for event in res:
        # handle event
        print(event)

```


### Override Server URL Per-Client

The default server can also be overridden globally by passing a URL to the `server_url: str` optional parameter when initializing the SDK client instance. For example:
```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    server_url="https://api.mistral.ai",
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Server Selection [server] -->

<!-- Start Custom HTTP Client [http-client] -->
## Custom HTTP Client

The Python SDK makes API calls using the [httpx](https://www.python-httpx.org/) HTTP library.  In order to provide a convenient way to configure timeouts, cookies, proxies, custom headers, and other low-level configuration, you can initialize the SDK client with your own HTTP client instance.
Depending on whether you are using the sync or async version of the SDK, you can pass an instance of `HttpClient` or `AsyncHttpClient` respectively, which are Protocol's ensuring that the client has the necessary methods to make API calls.
This allows you to wrap the client with your own custom logic, such as adding custom headers, logging, or error handling, or you can just pass an instance of `httpx.Client` or `httpx.AsyncClient` directly.

For example, you could specify a header for every request that this sdk makes as follows:
```python
from mistralai_azure import MistralAzure
import httpx

http_client = httpx.Client(headers={"x-custom-header": "someValue"})
s = MistralAzure(client=http_client)
```

or you could wrap the client with your own custom logic:
```python
from mistralai_azure import MistralAzure
from mistralai_azure.httpclient import AsyncHttpClient
import httpx

class CustomClient(AsyncHttpClient):
    client: AsyncHttpClient

    def __init__(self, client: AsyncHttpClient):
        self.client = client

    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        request.headers["Client-Level-Header"] = "added by client"

        return await self.client.send(
            request, stream=stream, auth=auth, follow_redirects=follow_redirects
        )

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        return self.client.build_request(
            method,
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )

s = MistralAzure(async_client=CustomClient(httpx.AsyncClient()))
```
<!-- End Custom HTTP Client [http-client] -->

<!-- Start Authentication [security] -->
## Authentication

### Per-Client Security Schemes

This SDK supports the following security scheme globally:

| Name      | Type | Scheme      |
| --------- | ---- | ----------- |
| `api_key` | http | HTTP Bearer |

To authenticate with the API the `api_key` parameter must be set when initializing the SDK client instance. For example:
```python
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Authentication [security] -->

<!-- Placeholder for Future Speakeasy SDK Sections -->

# Development

## Contributions

While we value open-source contributions to this SDK, this library is generated programmatically. Any manual changes added to internal files will be overwritten on the next generation. 
We look forward to hearing your feedback. Feel free to open a PR or an issue with a proof of concept and we'll do our best to include it in a future release.
</file>

<file path="packages/mistralai_azure/USAGE.md">
<!-- Start SDK Example Usage [usage] -->
### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai_azure import MistralAzure
import os

s = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)


res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="azureai")

if res is not None:
    # handle response
    pass
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai_azure import MistralAzure
import os

async def main():
    s = MistralAzure(
        azure_api_key=os.getenv("AZURE_API_KEY", ""),
        azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
    )
    res = await s.chat.complete_async(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], model="azureai")
    if res is not None:
        # handle response
        pass

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->
</file>

<file path="packages/mistralai_gcp/.speakeasy/gen.lock">
lockVersion: 2.0.0
id: ec60f2d8-7869-45c1-918e-773d41a8cf74
management:
  docChecksum: 28fe1ab59b4dee005217f2dbbd836060
  docVersion: 0.0.2
  speakeasyVersion: 1.517.3
  generationVersion: 2.548.6
  releaseVersion: 1.6.0
  configChecksum: 66bf5911f59189922e03a75a72923b32
  published: true
features:
  python:
    additionalDependencies: 1.0.0
    constsAndDefaults: 1.0.5
    core: 5.12.3
    defaultEnabledRetries: 0.2.0
    enumUnions: 0.1.0
    envVarSecurityUsage: 0.3.2
    examples: 3.0.1
    flatRequests: 1.0.1
    globalSecurity: 3.0.3
    globalSecurityCallbacks: 1.0.0
    globalSecurityFlattening: 1.0.0
    globalServerURLs: 3.1.0
    methodArguments: 1.0.2
    nameOverrides: 3.0.1
    nullables: 1.0.1
    openEnums: 1.0.0
    responseFormat: 1.0.1
    retries: 3.0.2
    sdkHooks: 1.0.1
    serverEvents: 1.0.7
    serverEventsSentinels: 0.1.0
    serverIDs: 3.0.0
    unions: 3.0.4
generatedFiles:
  - .gitattributes
  - .python-version
  - .vscode/settings.json
  - docs/models/arguments.md
  - docs/models/assistantmessage.md
  - docs/models/assistantmessagecontent.md
  - docs/models/assistantmessagerole.md
  - docs/models/chatcompletionchoice.md
  - docs/models/chatcompletionchoicefinishreason.md
  - docs/models/chatcompletionrequest.md
  - docs/models/chatcompletionrequestmessages.md
  - docs/models/chatcompletionrequeststop.md
  - docs/models/chatcompletionrequesttoolchoice.md
  - docs/models/chatcompletionresponse.md
  - docs/models/chatcompletionstreamrequest.md
  - docs/models/chatcompletionstreamrequesttoolchoice.md
  - docs/models/completionchunk.md
  - docs/models/completionevent.md
  - docs/models/completionresponsestreamchoice.md
  - docs/models/content.md
  - docs/models/contentchunk.md
  - docs/models/deltamessage.md
  - docs/models/fimcompletionrequest.md
  - docs/models/fimcompletionrequeststop.md
  - docs/models/fimcompletionresponse.md
  - docs/models/fimcompletionstreamrequest.md
  - docs/models/fimcompletionstreamrequeststop.md
  - docs/models/finishreason.md
  - docs/models/function.md
  - docs/models/functioncall.md
  - docs/models/functionname.md
  - docs/models/httpvalidationerror.md
  - docs/models/imageurl.md
  - docs/models/imageurlchunk.md
  - docs/models/imageurlchunkimageurl.md
  - docs/models/imageurlchunktype.md
  - docs/models/jsonschema.md
  - docs/models/loc.md
  - docs/models/messages.md
  - docs/models/prediction.md
  - docs/models/referencechunk.md
  - docs/models/referencechunktype.md
  - docs/models/responseformat.md
  - docs/models/responseformats.md
  - docs/models/role.md
  - docs/models/security.md
  - docs/models/stop.md
  - docs/models/systemmessage.md
  - docs/models/systemmessagecontent.md
  - docs/models/textchunk.md
  - docs/models/tool.md
  - docs/models/toolcall.md
  - docs/models/toolchoice.md
  - docs/models/toolchoiceenum.md
  - docs/models/toolmessage.md
  - docs/models/toolmessagecontent.md
  - docs/models/toolmessagerole.md
  - docs/models/tooltypes.md
  - docs/models/type.md
  - docs/models/usageinfo.md
  - docs/models/usermessage.md
  - docs/models/usermessagecontent.md
  - docs/models/usermessagerole.md
  - docs/models/utils/retryconfig.md
  - docs/models/validationerror.md
  - poetry.toml
  - py.typed
  - pylintrc
  - pyproject.toml
  - scripts/prepare_readme.py
  - scripts/publish.sh
  - src/mistralai_gcp/__init__.py
  - src/mistralai_gcp/_hooks/__init__.py
  - src/mistralai_gcp/_hooks/sdkhooks.py
  - src/mistralai_gcp/_hooks/types.py
  - src/mistralai_gcp/_version.py
  - src/mistralai_gcp/basesdk.py
  - src/mistralai_gcp/chat.py
  - src/mistralai_gcp/fim.py
  - src/mistralai_gcp/httpclient.py
  - src/mistralai_gcp/models/__init__.py
  - src/mistralai_gcp/models/assistantmessage.py
  - src/mistralai_gcp/models/chatcompletionchoice.py
  - src/mistralai_gcp/models/chatcompletionrequest.py
  - src/mistralai_gcp/models/chatcompletionresponse.py
  - src/mistralai_gcp/models/chatcompletionstreamrequest.py
  - src/mistralai_gcp/models/completionchunk.py
  - src/mistralai_gcp/models/completionevent.py
  - src/mistralai_gcp/models/completionresponsestreamchoice.py
  - src/mistralai_gcp/models/contentchunk.py
  - src/mistralai_gcp/models/deltamessage.py
  - src/mistralai_gcp/models/fimcompletionrequest.py
  - src/mistralai_gcp/models/fimcompletionresponse.py
  - src/mistralai_gcp/models/fimcompletionstreamrequest.py
  - src/mistralai_gcp/models/function.py
  - src/mistralai_gcp/models/functioncall.py
  - src/mistralai_gcp/models/functionname.py
  - src/mistralai_gcp/models/httpvalidationerror.py
  - src/mistralai_gcp/models/imageurl.py
  - src/mistralai_gcp/models/imageurlchunk.py
  - src/mistralai_gcp/models/jsonschema.py
  - src/mistralai_gcp/models/prediction.py
  - src/mistralai_gcp/models/referencechunk.py
  - src/mistralai_gcp/models/responseformat.py
  - src/mistralai_gcp/models/responseformats.py
  - src/mistralai_gcp/models/sdkerror.py
  - src/mistralai_gcp/models/security.py
  - src/mistralai_gcp/models/systemmessage.py
  - src/mistralai_gcp/models/textchunk.py
  - src/mistralai_gcp/models/tool.py
  - src/mistralai_gcp/models/toolcall.py
  - src/mistralai_gcp/models/toolchoice.py
  - src/mistralai_gcp/models/toolchoiceenum.py
  - src/mistralai_gcp/models/toolmessage.py
  - src/mistralai_gcp/models/tooltypes.py
  - src/mistralai_gcp/models/usageinfo.py
  - src/mistralai_gcp/models/usermessage.py
  - src/mistralai_gcp/models/validationerror.py
  - src/mistralai_gcp/py.typed
  - src/mistralai_gcp/sdk.py
  - src/mistralai_gcp/sdkconfiguration.py
  - src/mistralai_gcp/types/__init__.py
  - src/mistralai_gcp/types/basemodel.py
  - src/mistralai_gcp/utils/__init__.py
  - src/mistralai_gcp/utils/annotations.py
  - src/mistralai_gcp/utils/enums.py
  - src/mistralai_gcp/utils/eventstreaming.py
  - src/mistralai_gcp/utils/forms.py
  - src/mistralai_gcp/utils/headers.py
  - src/mistralai_gcp/utils/logger.py
  - src/mistralai_gcp/utils/metadata.py
  - src/mistralai_gcp/utils/queryparams.py
  - src/mistralai_gcp/utils/requestbodies.py
  - src/mistralai_gcp/utils/retries.py
  - src/mistralai_gcp/utils/security.py
  - src/mistralai_gcp/utils/serializers.py
  - src/mistralai_gcp/utils/url.py
  - src/mistralai_gcp/utils/values.py
examples:
  stream_chat:
    speakeasy-default-stream-chat:
      requestBody:
        application/json: {"model": "mistral-small-latest", "stream": true, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "422":
          application/json: {}
        "200": {}
  chat_completion_v1_chat_completions_post:
    speakeasy-default-chat-completion-v1-chat-completions-post:
      requestBody:
        application/json: {"model": "mistral-small-latest", "stream": false, "messages": [{"content": "Who is the best French painter? Answer in one short sentence.", "role": "user"}]}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "mistral-small-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
  stream_fim:
    speakeasy-default-stream-fim:
      requestBody:
        application/json: {"model": "codestral-2405", "top_p": 1, "stream": true, "prompt": "def", "suffix": "return a+b"}
      responses:
        "422":
          application/json: {}
        "200": {}
  fim_completion_v1_fim_completions_post:
    speakeasy-default-fim-completion-v1-fim-completions-post:
      requestBody:
        application/json: {"model": "codestral-2405", "top_p": 1, "stream": false, "prompt": "def", "suffix": "return a+b"}
      responses:
        "200":
          application/json: {"id": "cmpl-e5cc70bb28c444948073e77776eb30ef", "object": "chat.completion", "model": "codestral-latest", "usage": {"prompt_tokens": 16, "completion_tokens": 34, "total_tokens": 50}, "created": 1702256327, "choices": [{"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}, {"index": 0, "message": {"prefix": false, "role": "assistant"}, "finish_reason": "stop"}]}
        "422":
          application/json: {}
examplesVersion: 1.0.0
generatedTests: {}
</file>

<file path="packages/mistralai_gcp/.speakeasy/gen.yaml">
configVersion: 2.0.0
generation:
  sdkClassName: MistralGCP
  maintainOpenAPIOrder: true
  usageSnippets:
    optionalPropertyRendering: withExample
  useClassNamesForArrayFields: true
  fixes:
    nameResolutionDec2023: true
    nameResolutionFeb2025: false
    parameterOrderingFeb2024: true
    requestResponseComponentNamesFeb2024: true
    securityFeb2025: false
  auth:
    oAuth2ClientCredentialsEnabled: true
    oAuth2PasswordEnabled: false
python:
  version: 1.6.0
  additionalDependencies:
    dev:
      pytest: ^8.2.2
      pytest-asyncio: ^0.23.7
    main:
      google-auth: ^2.31.0
      requests: ^2.32.3
  authors:
    - Mistral
  clientServerStatusCodesAsErrors: true
  defaultErrorName: SDKError
  description: Python Client SDK for the Mistral AI API in GCP.
  enableCustomCodeRegions: false
  enumFormat: union
  fixFlags:
    responseRequiredSep2024: false
  flattenGlobalSecurity: true
  flattenRequests: true
  flatteningOrder: parameters-first
  imports:
    option: openapi
    paths:
      callbacks: ""
      errors: ""
      operations: ""
      shared: ""
      webhooks: ""
  inputModelSuffix: input
  maxMethodParams: 15
  methodArguments: infer-optional-args
  outputModelSuffix: output
  packageName: mistralai-gcp
  pytestTimeout: 0
  responseFormat: flat
  templateVersion: v2
</file>

<file path="packages/mistralai_gcp/docs/models/utils/retryconfig.md">
# RetryConfig

Allows customizing the default retry configuration. Only usable with methods that mention they support retries.

## Fields

| Name                      | Type                                | Description                             | Example   |
| ------------------------- | ----------------------------------- | --------------------------------------- | --------- |
| `strategy`                | `*str*`                             | The retry strategy to use.              | `backoff` |
| `backoff`                 | [BackoffStrategy](#backoffstrategy) | Configuration for the backoff strategy. |           |
| `retry_connection_errors` | `*bool*`                            | Whether to retry on connection errors.  | `true`    |

## BackoffStrategy

The backoff strategy allows retrying a request with an exponential backoff between each retry.

### Fields

| Name               | Type      | Description                               | Example  |
| ------------------ | --------- | ----------------------------------------- | -------- |
| `initial_interval` | `*int*`   | The initial interval in milliseconds.     | `500`    |
| `max_interval`     | `*int*`   | The maximum interval in milliseconds.     | `60000`  |
| `exponent`         | `*float*` | The exponent to use for the backoff.      | `1.5`    |
| `max_elapsed_time` | `*int*`   | The maximum elapsed time in milliseconds. | `300000` |
</file>

<file path="packages/mistralai_gcp/docs/models/arguments.md">
# Arguments


## Supported Types

### `Dict[str, Any]`

```python
value: Dict[str, Any] = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/assistantmessage.md">
# AssistantMessage


## Fields

| Field                                                                                                                                                                                                  | Type                                                                                                                                                                                                   | Required                                                                                                                                                                                               | Description                                                                                                                                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `content`                                                                                                                                                                                              | [OptionalNullable[models.AssistantMessageContent]](../models/assistantmessagecontent.md)                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `tool_calls`                                                                                                                                                                                           | List[[models.ToolCall](../models/toolcall.md)]                                                                                                                                                         | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
| `prefix`                                                                                                                                                                                               | *Optional[bool]*                                                                                                                                                                                       | :heavy_minus_sign:                                                                                                                                                                                     | Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message. |
| `role`                                                                                                                                                                                                 | [Optional[models.AssistantMessageRole]](../models/assistantmessagerole.md)                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                     | N/A                                                                                                                                                                                                    |
</file>

<file path="packages/mistralai_gcp/docs/models/assistantmessagecontent.md">
# AssistantMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/assistantmessagerole.md">
# AssistantMessageRole


## Values

| Name        | Value       |
| ----------- | ----------- |
| `ASSISTANT` | assistant   |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionchoice.md">
# ChatCompletionChoice


## Fields

| Field                                                                                    | Type                                                                                     | Required                                                                                 | Description                                                                              | Example                                                                                  |
| ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| `index`                                                                                  | *int*                                                                                    | :heavy_check_mark:                                                                       | N/A                                                                                      | 0                                                                                        |
| `message`                                                                                | [models.AssistantMessage](../models/assistantmessage.md)                                 | :heavy_check_mark:                                                                       | N/A                                                                                      |                                                                                          |
| `finish_reason`                                                                          | [models.ChatCompletionChoiceFinishReason](../models/chatcompletionchoicefinishreason.md) | :heavy_check_mark:                                                                       | N/A                                                                                      | stop                                                                                     |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionchoicefinishreason.md">
# ChatCompletionChoiceFinishReason


## Values

| Name           | Value          |
| -------------- | -------------- |
| `STOP`         | stop           |
| `LENGTH`       | length         |
| `MODEL_LENGTH` | model_length   |
| `ERROR`        | error          |
| `TOOL_CALLS`   | tool_calls     |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionrequest.md">
# ChatCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.ChatCompletionRequestMessages](../models/chatcompletionrequestmessages.md)]                                                                                                                                                                                                                                                                                                              | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.ChatCompletionRequestStop]](../models/chatcompletionrequeststop.md)                                                                                                                                                                                                                                                                                                                  | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionRequestToolChoice]](../models/chatcompletionrequesttoolchoice.md)                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionrequestmessages.md">
# ChatCompletionRequestMessages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionrequeststop.md">
# ChatCompletionRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionrequesttoolchoice.md">
# ChatCompletionRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionresponse.md">
# ChatCompletionResponse


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            | Example                                                                |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `id`                                                                   | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | cmpl-e5cc70bb28c444948073e77776eb30ef                                  |
| `object`                                                               | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | chat.completion                                                        |
| `model`                                                                | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | mistral-small-latest                                                   |
| `usage`                                                                | [models.UsageInfo](../models/usageinfo.md)                             | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
| `created`                                                              | *Optional[int]*                                                        | :heavy_minus_sign:                                                     | N/A                                                                    | 1702256327                                                             |
| `choices`                                                              | List[[models.ChatCompletionChoice](../models/chatcompletionchoice.md)] | :heavy_minus_sign:                                                     | N/A                                                                    |                                                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionstreamrequest.md">
# ChatCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                                                           | mistral-small-latest                                                                                                                                                                                                                                                                                                                                                                                  |
| `messages`                                                                                                                                                                                                                                                                                                                                                                                            | List[[models.Messages](../models/messages.md)]                                                                                                                                                                                                                                                                                                                                                        | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                                                                                           | [<br/>{<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>}<br/>]                                                                                                                                                                                                                                                                                |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.Stop]](../models/stop.md)                                                                                                                                                                                                                                                                                                                                                            | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `response_format`                                                                                                                                                                                                                                                                                                                                                                                     | [Optional[models.ResponseFormat]](../models/responseformat.md)                                                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tools`                                                                                                                                                                                                                                                                                                                                                                                               | List[[models.Tool](../models/tool.md)]                                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `tool_choice`                                                                                                                                                                                                                                                                                                                                                                                         | [Optional[models.ChatCompletionStreamRequestToolChoice]](../models/chatcompletionstreamrequesttoolchoice.md)                                                                                                                                                                                                                                                                                          | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `presence_penalty`                                                                                                                                                                                                                                                                                                                                                                                    | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `frequency_penalty`                                                                                                                                                                                                                                                                                                                                                                                   | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `n`                                                                                                                                                                                                                                                                                                                                                                                                   | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Number of completions to return for each request, input tokens are only billed once.                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `prediction`                                                                                                                                                                                                                                                                                                                                                                                          | [Optional[models.Prediction]](../models/prediction.md)                                                                                                                                                                                                                                                                                                                                                | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `parallel_tool_calls`                                                                                                                                                                                                                                                                                                                                                                                 | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_gcp/docs/models/chatcompletionstreamrequesttoolchoice.md">
# ChatCompletionStreamRequestToolChoice


## Supported Types

### `models.ToolChoice`

```python
value: models.ToolChoice = /* values here */
```

### `models.ToolChoiceEnum`

```python
value: models.ToolChoiceEnum = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/completionchunk.md">
# CompletionChunk


## Fields

| Field                                                                                      | Type                                                                                       | Required                                                                                   | Description                                                                                |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| `id`                                                                                       | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `model`                                                                                    | *str*                                                                                      | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `choices`                                                                                  | List[[models.CompletionResponseStreamChoice](../models/completionresponsestreamchoice.md)] | :heavy_check_mark:                                                                         | N/A                                                                                        |
| `object`                                                                                   | *Optional[str]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `created`                                                                                  | *Optional[int]*                                                                            | :heavy_minus_sign:                                                                         | N/A                                                                                        |
| `usage`                                                                                    | [Optional[models.UsageInfo]](../models/usageinfo.md)                                       | :heavy_minus_sign:                                                                         | N/A                                                                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/completionevent.md">
# CompletionEvent


## Fields

| Field                                                  | Type                                                   | Required                                               | Description                                            |
| ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------ |
| `data`                                                 | [models.CompletionChunk](../models/completionchunk.md) | :heavy_check_mark:                                     | N/A                                                    |
</file>

<file path="packages/mistralai_gcp/docs/models/completionresponsestreamchoice.md">
# CompletionResponseStreamChoice


## Fields

| Field                                                      | Type                                                       | Required                                                   | Description                                                |
| ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- | ---------------------------------------------------------- |
| `index`                                                    | *int*                                                      | :heavy_check_mark:                                         | N/A                                                        |
| `delta`                                                    | [models.DeltaMessage](../models/deltamessage.md)           | :heavy_check_mark:                                         | N/A                                                        |
| `finish_reason`                                            | [Nullable[models.FinishReason]](../models/finishreason.md) | :heavy_check_mark:                                         | N/A                                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/content.md">
# Content


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/contentchunk.md">
# ContentChunk


## Supported Types

### `models.ImageURLChunk`

```python
value: models.ImageURLChunk = /* values here */
```

### `models.TextChunk`

```python
value: models.TextChunk = /* values here */
```

### `models.ReferenceChunk`

```python
value: models.ReferenceChunk = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/deltamessage.md">
# DeltaMessage


## Fields

| Field                                                    | Type                                                     | Required                                                 | Description                                              |
| -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- | -------------------------------------------------------- |
| `role`                                                   | *OptionalNullable[str]*                                  | :heavy_minus_sign:                                       | N/A                                                      |
| `content`                                                | [OptionalNullable[models.Content]](../models/content.md) | :heavy_minus_sign:                                       | N/A                                                      |
| `tool_calls`                                             | List[[models.ToolCall](../models/toolcall.md)]           | :heavy_minus_sign:                                       | N/A                                                      |
</file>

<file path="packages/mistralai_gcp/docs/models/fimcompletionrequest.md">
# FIMCompletionRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionRequestStop]](../models/fimcompletionrequeststop.md)                                                                                                                                                                                                                                                                                                                    | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_gcp/docs/models/fimcompletionrequeststop.md">
# FIMCompletionRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/fimcompletionresponse.md">
# FIMCompletionResponse


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            | Example                                                                |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `id`                                                                   | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | cmpl-e5cc70bb28c444948073e77776eb30ef                                  |
| `object`                                                               | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | chat.completion                                                        |
| `model`                                                                | *str*                                                                  | :heavy_check_mark:                                                     | N/A                                                                    | codestral-latest                                                       |
| `usage`                                                                | [models.UsageInfo](../models/usageinfo.md)                             | :heavy_check_mark:                                                     | N/A                                                                    |                                                                        |
| `created`                                                              | *Optional[int]*                                                        | :heavy_minus_sign:                                                     | N/A                                                                    | 1702256327                                                             |
| `choices`                                                              | List[[models.ChatCompletionChoice](../models/chatcompletionchoice.md)] | :heavy_minus_sign:                                                     | N/A                                                                    |                                                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/fimcompletionstreamrequest.md">
# FIMCompletionStreamRequest


## Fields

| Field                                                                                                                                                                                                                                                                                                                                                                                                 | Type                                                                                                                                                                                                                                                                                                                                                                                                  | Required                                                                                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                           | Example                                                                                                                                                                                                                                                                                                                                                                                               |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                                                                                                                                                                                                                                                                                                                                                                                               | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                                                                                             | codestral-2405                                                                                                                                                                                                                                                                                                                                                                                        |
| `prompt`                                                                                                                                                                                                                                                                                                                                                                                              | *str*                                                                                                                                                                                                                                                                                                                                                                                                 | :heavy_check_mark:                                                                                                                                                                                                                                                                                                                                                                                    | The text/code to complete.                                                                                                                                                                                                                                                                                                                                                                            | def                                                                                                                                                                                                                                                                                                                                                                                                   |
| `temperature`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[float]*                                                                                                                                                                                                                                                                                                                                                                             | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value. |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `top_p`                                                                                                                                                                                                                                                                                                                                                                                               | *Optional[float]*                                                                                                                                                                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `max_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stream`                                                                                                                                                                                                                                                                                                                                                                                              | *Optional[bool]*                                                                                                                                                                                                                                                                                                                                                                                      | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | N/A                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `stop`                                                                                                                                                                                                                                                                                                                                                                                                | [Optional[models.FIMCompletionStreamRequestStop]](../models/fimcompletionstreamrequeststop.md)                                                                                                                                                                                                                                                                                                        | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `random_seed`                                                                                                                                                                                                                                                                                                                                                                                         | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `suffix`                                                                                                                                                                                                                                                                                                                                                                                              | *OptionalNullable[str]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                                                                                               | return a+b                                                                                                                                                                                                                                                                                                                                                                                            |
| `min_tokens`                                                                                                                                                                                                                                                                                                                                                                                          | *OptionalNullable[int]*                                                                                                                                                                                                                                                                                                                                                                               | :heavy_minus_sign:                                                                                                                                                                                                                                                                                                                                                                                    | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                       |
</file>

<file path="packages/mistralai_gcp/docs/models/fimcompletionstreamrequeststop.md">
# FIMCompletionStreamRequestStop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/finishreason.md">
# FinishReason


## Values

| Name         | Value        |
| ------------ | ------------ |
| `STOP`       | stop         |
| `LENGTH`     | length       |
| `ERROR`      | error        |
| `TOOL_CALLS` | tool_calls   |
</file>

<file path="packages/mistralai_gcp/docs/models/function.md">
# Function


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
| `parameters`       | Dict[str, *Any*]   | :heavy_check_mark: | N/A                |
| `description`      | *Optional[str]*    | :heavy_minus_sign: | N/A                |
| `strict`           | *Optional[bool]*   | :heavy_minus_sign: | N/A                |
</file>

<file path="packages/mistralai_gcp/docs/models/functioncall.md">
# FunctionCall


## Fields

| Field                                      | Type                                       | Required                                   | Description                                |
| ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ |
| `name`                                     | *str*                                      | :heavy_check_mark:                         | N/A                                        |
| `arguments`                                | [models.Arguments](../models/arguments.md) | :heavy_check_mark:                         | N/A                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/functionname.md">
# FunctionName

this restriction of `Function` is used to select a specific function to call


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `name`             | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="packages/mistralai_gcp/docs/models/httpvalidationerror.md">
# HTTPValidationError


## Fields

| Field                                                        | Type                                                         | Required                                                     | Description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `detail`                                                     | List[[models.ValidationError](../models/validationerror.md)] | :heavy_minus_sign:                                           | N/A                                                          |
</file>

<file path="packages/mistralai_gcp/docs/models/imageurl.md">
# ImageURL


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `url`                   | *str*                   | :heavy_check_mark:      | N/A                     |
| `detail`                | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
</file>

<file path="packages/mistralai_gcp/docs/models/imageurlchunk.md">
# ImageURLChunk

{"type":"image_url","image_url":{"url":"data:image/png;base64,iVBORw0


## Fields

| Field                                                                | Type                                                                 | Required                                                             | Description                                                          |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| `image_url`                                                          | [models.ImageURLChunkImageURL](../models/imageurlchunkimageurl.md)   | :heavy_check_mark:                                                   | N/A                                                                  |
| `type`                                                               | [Optional[models.ImageURLChunkType]](../models/imageurlchunktype.md) | :heavy_minus_sign:                                                   | N/A                                                                  |
</file>

<file path="packages/mistralai_gcp/docs/models/imageurlchunkimageurl.md">
# ImageURLChunkImageURL


## Supported Types

### `models.ImageURL`

```python
value: models.ImageURL = /* values here */
```

### `str`

```python
value: str = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/imageurlchunktype.md">
# ImageURLChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `IMAGE_URL` | image_url   |
</file>

<file path="packages/mistralai_gcp/docs/models/jsonschema.md">
# JSONSchema


## Fields

| Field                   | Type                    | Required                | Description             |
| ----------------------- | ----------------------- | ----------------------- | ----------------------- |
| `name`                  | *str*                   | :heavy_check_mark:      | N/A                     |
| `schema_definition`     | Dict[str, *Any*]        | :heavy_check_mark:      | N/A                     |
| `description`           | *OptionalNullable[str]* | :heavy_minus_sign:      | N/A                     |
| `strict`                | *Optional[bool]*        | :heavy_minus_sign:      | N/A                     |
</file>

<file path="packages/mistralai_gcp/docs/models/loc.md">
# Loc


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `int`

```python
value: int = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/messages.md">
# Messages


## Supported Types

### `models.AssistantMessage`

```python
value: models.AssistantMessage = /* values here */
```

### `models.SystemMessage`

```python
value: models.SystemMessage = /* values here */
```

### `models.ToolMessage`

```python
value: models.ToolMessage = /* values here */
```

### `models.UserMessage`

```python
value: models.UserMessage = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/prediction.md">
# Prediction


## Fields

| Field                          | Type                           | Required                       | Description                    |
| ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |
| `type`                         | *Optional[Literal["content"]]* | :heavy_minus_sign:             | N/A                            |
| `content`                      | *Optional[str]*                | :heavy_minus_sign:             | N/A                            |
</file>

<file path="packages/mistralai_gcp/docs/models/referencechunk.md">
# ReferenceChunk


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `reference_ids`                                                        | List[*int*]                                                            | :heavy_check_mark:                                                     | N/A                                                                    |
| `type`                                                                 | [Optional[models.ReferenceChunkType]](../models/referencechunktype.md) | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_gcp/docs/models/referencechunktype.md">
# ReferenceChunkType


## Values

| Name        | Value       |
| ----------- | ----------- |
| `REFERENCE` | reference   |
</file>

<file path="packages/mistralai_gcp/docs/models/responseformat.md">
# ResponseFormat


## Fields

| Field                                                                                                                                                                                                                                                                                              | Type                                                                                                                                                                                                                                                                                               | Required                                                                                                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `type`                                                                                                                                                                                                                                                                                             | [Optional[models.ResponseFormats]](../models/responseformats.md)                                                                                                                                                                                                                                   | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message. |
| `json_schema`                                                                                                                                                                                                                                                                                      | [OptionalNullable[models.JSONSchema]](../models/jsonschema.md)                                                                                                                                                                                                                                     | :heavy_minus_sign:                                                                                                                                                                                                                                                                                 | N/A                                                                                                                                                                                                                                                                                                |
</file>

<file path="packages/mistralai_gcp/docs/models/responseformats.md">
# ResponseFormats

An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message.


## Values

| Name          | Value         |
| ------------- | ------------- |
| `TEXT`        | text          |
| `JSON_OBJECT` | json_object   |
| `JSON_SCHEMA` | json_schema   |
</file>

<file path="packages/mistralai_gcp/docs/models/role.md">
# Role


## Values

| Name     | Value    |
| -------- | -------- |
| `SYSTEM` | system   |
</file>

<file path="packages/mistralai_gcp/docs/models/security.md">
# Security


## Fields

| Field              | Type               | Required           | Description        |
| ------------------ | ------------------ | ------------------ | ------------------ |
| `api_key`          | *str*              | :heavy_check_mark: | N/A                |
</file>

<file path="packages/mistralai_gcp/docs/models/stop.md">
# Stop

Stop generation if this token is detected. Or if one of these tokens is detected when providing an array


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[str]`

```python
value: List[str] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/systemmessage.md">
# SystemMessage


## Fields

| Field                                                            | Type                                                             | Required                                                         | Description                                                      |
| ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| `content`                                                        | [models.SystemMessageContent](../models/systemmessagecontent.md) | :heavy_check_mark:                                               | N/A                                                              |
| `role`                                                           | [Optional[models.Role]](../models/role.md)                       | :heavy_minus_sign:                                               | N/A                                                              |
</file>

<file path="packages/mistralai_gcp/docs/models/systemmessagecontent.md">
# SystemMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.TextChunk]`

```python
value: List[models.TextChunk] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/textchunk.md">
# TextChunk


## Fields

| Field                                      | Type                                       | Required                                   | Description                                |
| ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ |
| `text`                                     | *str*                                      | :heavy_check_mark:                         | N/A                                        |
| `type`                                     | [Optional[models.Type]](../models/type.md) | :heavy_minus_sign:                         | N/A                                        |
</file>

<file path="packages/mistralai_gcp/docs/models/tool.md">
# Tool


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.Function](../models/function.md)             | :heavy_check_mark:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="packages/mistralai_gcp/docs/models/toolcall.md">
# ToolCall


## Fields

| Field                                                | Type                                                 | Required                                             | Description                                          |
| ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------- |
| `function`                                           | [models.FunctionCall](../models/functioncall.md)     | :heavy_check_mark:                                   | N/A                                                  |
| `id`                                                 | *Optional[str]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
| `type`                                               | [Optional[models.ToolTypes]](../models/tooltypes.md) | :heavy_minus_sign:                                   | N/A                                                  |
| `index`                                              | *Optional[int]*                                      | :heavy_minus_sign:                                   | N/A                                                  |
</file>

<file path="packages/mistralai_gcp/docs/models/toolchoice.md">
# ToolChoice

ToolChoice is either a ToolChoiceEnum or a ToolChoice


## Fields

| Field                                                                        | Type                                                                         | Required                                                                     | Description                                                                  |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| `function`                                                                   | [models.FunctionName](../models/functionname.md)                             | :heavy_check_mark:                                                           | this restriction of `Function` is used to select a specific function to call |
| `type`                                                                       | [Optional[models.ToolTypes]](../models/tooltypes.md)                         | :heavy_minus_sign:                                                           | N/A                                                                          |
</file>

<file path="packages/mistralai_gcp/docs/models/toolchoiceenum.md">
# ToolChoiceEnum


## Values

| Name       | Value      |
| ---------- | ---------- |
| `AUTO`     | auto       |
| `NONE`     | none       |
| `ANY`      | any        |
| `REQUIRED` | required   |
</file>

<file path="packages/mistralai_gcp/docs/models/toolmessage.md">
# ToolMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.ToolMessageContent]](../models/toolmessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `tool_call_id`                                                         | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `name`                                                                 | *OptionalNullable[str]*                                                | :heavy_minus_sign:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.ToolMessageRole]](../models/toolmessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_gcp/docs/models/toolmessagecontent.md">
# ToolMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/toolmessagerole.md">
# ToolMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `TOOL` | tool   |
</file>

<file path="packages/mistralai_gcp/docs/models/tooltypes.md">
# ToolTypes


## Values

| Name       | Value      |
| ---------- | ---------- |
| `FUNCTION` | function   |
</file>

<file path="packages/mistralai_gcp/docs/models/type.md">
# Type


## Values

| Name   | Value  |
| ------ | ------ |
| `TEXT` | text   |
</file>

<file path="packages/mistralai_gcp/docs/models/usageinfo.md">
# UsageInfo


## Fields

| Field               | Type                | Required            | Description         | Example             |
| ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |
| `prompt_tokens`     | *int*               | :heavy_check_mark:  | N/A                 | 16                  |
| `completion_tokens` | *int*               | :heavy_check_mark:  | N/A                 | 34                  |
| `total_tokens`      | *int*               | :heavy_check_mark:  | N/A                 | 50                  |
</file>

<file path="packages/mistralai_gcp/docs/models/usermessage.md">
# UserMessage


## Fields

| Field                                                                  | Type                                                                   | Required                                                               | Description                                                            |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `content`                                                              | [Nullable[models.UserMessageContent]](../models/usermessagecontent.md) | :heavy_check_mark:                                                     | N/A                                                                    |
| `role`                                                                 | [Optional[models.UserMessageRole]](../models/usermessagerole.md)       | :heavy_minus_sign:                                                     | N/A                                                                    |
</file>

<file path="packages/mistralai_gcp/docs/models/usermessagecontent.md">
# UserMessageContent


## Supported Types

### `str`

```python
value: str = /* values here */
```

### `List[models.ContentChunk]`

```python
value: List[models.ContentChunk] = /* values here */
```
</file>

<file path="packages/mistralai_gcp/docs/models/usermessagerole.md">
# UserMessageRole


## Values

| Name   | Value  |
| ------ | ------ |
| `USER` | user   |
</file>

<file path="packages/mistralai_gcp/docs/models/validationerror.md">
# ValidationError


## Fields

| Field                                | Type                                 | Required                             | Description                          |
| ------------------------------------ | ------------------------------------ | ------------------------------------ | ------------------------------------ |
| `loc`                                | List[[models.Loc](../models/loc.md)] | :heavy_check_mark:                   | N/A                                  |
| `msg`                                | *str*                                | :heavy_check_mark:                   | N/A                                  |
| `type`                               | *str*                                | :heavy_check_mark:                   | N/A                                  |
</file>

<file path="packages/mistralai_gcp/docs/sdks/chat/README.md">
# Chat
(*chat*)

## Overview

Chat Completion API.

### Available Operations

* [stream](#stream) - Stream chat completion
* [create](#create) - Chat Completion

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="mistral-small-latest")

if res is not None:
    for event in res:
        # handle event
        print(event)

```

### Parameters

| Parameter         | Type                                                              | Required           | Description                                                                                                                                                                                                                                                   | Example                                                                                                    |
| ----------------- | ----------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `messages`        | List[[models.Messages](../../models/messages.md)]                 | :heavy_check_mark: | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                   | {<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>} |
| `model`           | *OptionalNullable[str]*                                           | :heavy_minus_sign: | ID of the model to use. You can use the [List Available Models](/api#operation/listModels) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                              | mistral-small-latest                                                                                       |
| `temperature`     | *Optional[float]*                                                 | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.        |                                                                                                            |
| `top_p`           | *Optional[float]*                                                 | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both. |                                                                                                            |
| `max_tokens`      | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                        |                                                                                                            |
| `min_tokens`      | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                   |                                                                                                            |
| `stream`          | *Optional[bool]*                                                  | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `stop`            | [Optional[models.Stop]](../../models/stop.md)                     | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                      |                                                                                                            |
| `random_seed`     | *OptionalNullable[int]*                                           | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                             |                                                                                                            |
| `response_format` | [Optional[models.ResponseFormat]](../../models/responseformat.md) | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `tools`           | List[[models.Tool](../../models/tool.md)]                         | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `tool_choice`     | [Optional[models.ToolChoice]](../../models/toolchoice.md)         | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                                                                                                            |
| `retries`         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)  | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                           |                                                                                                            |


### Response

**[Union[Generator[models.CompletionEvent, None, None], AsyncGenerator[models.CompletionEvent, None]]](../../models/.md)**
### Errors

| Error Object    | Status Code | Content Type |
| --------------- | ----------- | ------------ |
| models.SDKError | 4xx-5xx     | */*          |

## create

Chat Completion

### Example Usage

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="mistral-small-latest")

if res is not None:
    # handle response
    pass

```

### Parameters

| Parameter         | Type                                                                                                | Required           | Description                                                                                                                                                                                                                                                                                                                        | Example                                                                                                    |
| ----------------- | --------------------------------------------------------------------------------------------------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| `messages`        | List[[models.ChatCompletionRequestMessages](../../models/chatcompletionrequestmessages.md)]         | :heavy_check_mark: | The prompt(s) to generate completions for, encoded as a list of dict with role and content.                                                                                                                                                                                                                                        | {<br/>"role": "user",<br/>"content": "Who is the best French painter? Answer in one short sentence."<br/>} |
| `model`           | *OptionalNullable[str]*                                                                             | :heavy_minus_sign: | ID of the model to use. You can use the [List Available Models](/api#operation/listModels) API to see all of your available models, or see our [Model overview](/models) for model descriptions.                                                                                                                                   | mistral-small-latest                                                                                       |
| `temperature`     | *Optional[float]*                                                                                   | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.                                                                             |                                                                                                            |
| `top_p`           | *Optional[float]*                                                                                   | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                      |                                                                                                            |
| `max_tokens`      | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                             |                                                                                                            |
| `min_tokens`      | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                        |                                                                                                            |
| `stream`          | *Optional[bool]*                                                                                    | :heavy_minus_sign: | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON. |                                                                                                            |
| `stop`            | [Optional[models.ChatCompletionRequestStop]](../../models/chatcompletionrequeststop.md)             | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                           |                                                                                                            |
| `random_seed`     | *OptionalNullable[int]*                                                                             | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                  |                                                                                                            |
| `response_format` | [Optional[models.ResponseFormat]](../../models/responseformat.md)                                   | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `tools`           | List[[models.Tool](../../models/tool.md)]                                                           | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `tool_choice`     | [Optional[models.ChatCompletionRequestToolChoice]](../../models/chatcompletionrequesttoolchoice.md) | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                                                                                                |                                                                                                            |
| `retries`         | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                    | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                |                                                                                                            |


### Response

**[models.ChatCompletionResponse](../../models/chatcompletionresponse.md)**
### Errors

| Error Object               | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4xx-5xx     | */*              |
</file>

<file path="packages/mistralai_gcp/docs/sdks/fim/README.md">
# Fim
(*fim*)

## Overview

Fill-in-the-middle API.

### Available Operations

* [stream](#stream) - Stream fim completion
* [create](#create) - Fim Completion

## stream

Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

### Example Usage

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.fim.stream(prompt="def", model="codestral-2405", suffix="return a+b")

if res is not None:
    for event in res:
        # handle event
        print(event)

```

### Parameters

| Parameter     | Type                                                                                              | Required           | Description                                                                                                                                                                                                                                                   | Example        |
| ------------- | ------------------------------------------------------------------------------------------------- | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- |
| `prompt`      | *str*                                                                                             | :heavy_check_mark: | The text/code to complete.                                                                                                                                                                                                                                    | def            |
| `model`       | *OptionalNullable[str]*                                                                           | :heavy_minus_sign: | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                     | codestral-2405 |
| `temperature` | *Optional[float]*                                                                                 | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.        |                |
| `top_p`       | *Optional[float]*                                                                                 | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both. |                |
| `max_tokens`  | *OptionalNullable[int]*                                                                           | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                        |                |
| `min_tokens`  | *OptionalNullable[int]*                                                                           | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                   |                |
| `stream`      | *Optional[bool]*                                                                                  | :heavy_minus_sign: | N/A                                                                                                                                                                                                                                                           |                |
| `stop`        | [Optional[models.FIMCompletionStreamRequestStop]](../../models/fimcompletionstreamrequeststop.md) | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                      |                |
| `random_seed` | *OptionalNullable[int]*                                                                           | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                             |                |
| `suffix`      | *OptionalNullable[str]*                                                                           | :heavy_minus_sign: | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                       | return a+b     |
| `retries`     | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                                  | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                           |                |


### Response

**[Union[Generator[models.CompletionEvent, None, None], AsyncGenerator[models.CompletionEvent, None]]](../../models/.md)**
### Errors

| Error Object    | Status Code | Content Type |
| --------------- | ----------- | ------------ |
| models.SDKError | 4xx-5xx     | */*          |

## create

FIM completion.

### Example Usage

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.fim.complete(prompt="def", model="codestral-2405", suffix="return a+b")

if res is not None:
    # handle response
    pass

```

### Parameters

| Parameter     | Type                                                                                  | Required           | Description                                                                                                                                                                                                                                                                                                                        | Example        |
| ------------- | ------------------------------------------------------------------------------------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- |
| `prompt`      | *str*                                                                                 | :heavy_check_mark: | The text/code to complete.                                                                                                                                                                                                                                                                                                         | def            |
| `model`       | *OptionalNullable[str]*                                                               | :heavy_minus_sign: | ID of the model to use. Only compatible for now with:<br/>  - `codestral-2405`<br/>  - `codestral-latest`                                                                                                                                                                                                                          | codestral-2405 |
| `temperature` | *Optional[float]*                                                                     | :heavy_minus_sign: | What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.                                                                             |                |
| `top_p`       | *Optional[float]*                                                                     | :heavy_minus_sign: | Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.                                                                      |                |
| `max_tokens`  | *OptionalNullable[int]*                                                               | :heavy_minus_sign: | The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.                                                                                                                                                                             |                |
| `min_tokens`  | *OptionalNullable[int]*                                                               | :heavy_minus_sign: | The minimum number of tokens to generate in the completion.                                                                                                                                                                                                                                                                        |                |
| `stream`      | *Optional[bool]*                                                                      | :heavy_minus_sign: | Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON. |                |
| `stop`        | [Optional[models.FIMCompletionRequestStop]](../../models/fimcompletionrequeststop.md) | :heavy_minus_sign: | Stop generation if this token is detected. Or if one of these tokens is detected when providing an array                                                                                                                                                                                                                           |                |
| `random_seed` | *OptionalNullable[int]*                                                               | :heavy_minus_sign: | The seed to use for random sampling. If set, different calls will generate deterministic results.                                                                                                                                                                                                                                  |                |
| `suffix`      | *OptionalNullable[str]*                                                               | :heavy_minus_sign: | Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.                                                                                            | return a+b     |
| `retries`     | [Optional[utils.RetryConfig]](../../models/utils/retryconfig.md)                      | :heavy_minus_sign: | Configuration to override the default retry behavior of the client.                                                                                                                                                                                                                                                                |                |


### Response

**[models.FIMCompletionResponse](../../models/fimcompletionresponse.md)**
### Errors

| Error Object               | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4xx-5xx     | */*              |
</file>

<file path="packages/mistralai_gcp/docs/sdks/mistralgcp/README.md">
# MistralGCP SDK


## Overview

Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it.

### Available Operations
</file>

<file path="packages/mistralai_gcp/scripts/prepare_readme.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import shutil

try:
    shutil.copyfile("README.md", "README-PYPI.md")
except Exception as e:
    print("Failed to copy README.md to README-PYPI.md")
    print(e)
</file>

<file path="packages/mistralai_gcp/scripts/publish.sh">
#!/usr/bin/env bash

export POETRY_PYPI_TOKEN_PYPI=${PYPI_TOKEN}

poetry run python scripts/prepare_readme.py

poetry publish --build --skip-existing
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_hooks/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkhooks import *
from .types import *
from .registration import *
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_hooks/custom_user_agent.py">
# THIS FILE IS THE EXACT COPY OF THE ORIGINAL FILE FROM src/mistralai/_hooks/custom_user_agent.py
from typing import Union

import httpx

from .types import BeforeRequestContext, BeforeRequestHook

PREFIX = "mistral-client-python/"

class CustomUserAgentHook(BeforeRequestHook):
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        current = request.headers["user-agent"]
        if current.startswith(PREFIX):
            return request
        
        request.headers["user-agent"] = (
            PREFIX + current.split(" ")[1]
        )

        return request
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_hooks/registration.py">
from .custom_user_agent import CustomUserAgentHook
from .types import Hooks

# This file is only ever generated once on the first generation and then is free to be modified.
# Any hooks you wish to add should be registered in the init_hooks function. Feel free to define them
# in this file or in separate files in the hooks folder.


def init_hooks(hooks: Hooks):
    # pylint: disable=unused-argument
    """Add hooks by calling hooks.register{sdk_init/before_request/after_success/after_error}Hook
    with an instance of a hook that implements that specific Hook interface
    Hooks are registered per SDK instance, and are valid for the lifetime of the SDK instance
    """
    hooks.register_before_request_hook(CustomUserAgentHook())
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_hooks/sdkhooks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
from .types import (
    SDKInitHook,
    BeforeRequestContext,
    BeforeRequestHook,
    AfterSuccessContext,
    AfterSuccessHook,
    AfterErrorContext,
    AfterErrorHook,
    Hooks,
)
from .registration import init_hooks
from typing import List, Optional, Tuple
from mistralai_gcp.httpclient import HttpClient


class SDKHooks(Hooks):
    def __init__(self) -> None:
        self.sdk_init_hooks: List[SDKInitHook] = []
        self.before_request_hooks: List[BeforeRequestHook] = []
        self.after_success_hooks: List[AfterSuccessHook] = []
        self.after_error_hooks: List[AfterErrorHook] = []
        init_hooks(self)

    def register_sdk_init_hook(self, hook: SDKInitHook) -> None:
        self.sdk_init_hooks.append(hook)

    def register_before_request_hook(self, hook: BeforeRequestHook) -> None:
        self.before_request_hooks.append(hook)

    def register_after_success_hook(self, hook: AfterSuccessHook) -> None:
        self.after_success_hooks.append(hook)

    def register_after_error_hook(self, hook: AfterErrorHook) -> None:
        self.after_error_hooks.append(hook)

    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        for hook in self.sdk_init_hooks:
            base_url, client = hook.sdk_init(base_url, client)
        return base_url, client

    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> httpx.Request:
        for hook in self.before_request_hooks:
            out = hook.before_request(hook_ctx, request)
            if isinstance(out, Exception):
                raise out
            request = out

        return request

    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> httpx.Response:
        for hook in self.after_success_hooks:
            out = hook.after_success(hook_ctx, response)
            if isinstance(out, Exception):
                raise out
            response = out
        return response

    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Tuple[Optional[httpx.Response], Optional[Exception]]:
        for hook in self.after_error_hooks:
            result = hook.after_error(hook_ctx, response, error)
            if isinstance(result, Exception):
                raise result
            response, error = result
        return response, error
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_hooks/types.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from abc import ABC, abstractmethod
import httpx
from mistralai_gcp.httpclient import HttpClient
from typing import Any, Callable, List, Optional, Tuple, Union


class HookContext:
    base_url: str
    operation_id: str
    oauth2_scopes: Optional[List[str]] = None
    security_source: Optional[Union[Any, Callable[[], Any]]] = None

    def __init__(
        self,
        base_url: str,
        operation_id: str,
        oauth2_scopes: Optional[List[str]],
        security_source: Optional[Union[Any, Callable[[], Any]]],
    ):
        self.base_url = base_url
        self.operation_id = operation_id
        self.oauth2_scopes = oauth2_scopes
        self.security_source = security_source


class BeforeRequestContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterSuccessContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterErrorContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class SDKInitHook(ABC):
    @abstractmethod
    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        pass


class BeforeRequestHook(ABC):
    @abstractmethod
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        pass


class AfterSuccessHook(ABC):
    @abstractmethod
    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> Union[httpx.Response, Exception]:
        pass


class AfterErrorHook(ABC):
    @abstractmethod
    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Union[Tuple[Optional[httpx.Response], Optional[Exception]], Exception]:
        pass


class Hooks(ABC):
    @abstractmethod
    def register_sdk_init_hook(self, hook: SDKInitHook):
        pass

    @abstractmethod
    def register_before_request_hook(self, hook: BeforeRequestHook):
        pass

    @abstractmethod
    def register_after_success_hook(self, hook: AfterSuccessHook):
        pass

    @abstractmethod
    def register_after_error_hook(self, hook: AfterErrorHook):
        pass
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .assistantmessage import (
    AssistantMessage,
    AssistantMessageContent,
    AssistantMessageContentTypedDict,
    AssistantMessageRole,
    AssistantMessageTypedDict,
)
from .chatcompletionchoice import (
    ChatCompletionChoice,
    ChatCompletionChoiceFinishReason,
    ChatCompletionChoiceTypedDict,
)
from .chatcompletionrequest import (
    ChatCompletionRequest,
    ChatCompletionRequestMessages,
    ChatCompletionRequestMessagesTypedDict,
    ChatCompletionRequestStop,
    ChatCompletionRequestStopTypedDict,
    ChatCompletionRequestToolChoice,
    ChatCompletionRequestToolChoiceTypedDict,
    ChatCompletionRequestTypedDict,
)
from .chatcompletionresponse import (
    ChatCompletionResponse,
    ChatCompletionResponseTypedDict,
)
from .chatcompletionstreamrequest import (
    ChatCompletionStreamRequest,
    ChatCompletionStreamRequestToolChoice,
    ChatCompletionStreamRequestToolChoiceTypedDict,
    ChatCompletionStreamRequestTypedDict,
    Messages,
    MessagesTypedDict,
    Stop,
    StopTypedDict,
)
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from .completionevent import CompletionEvent, CompletionEventTypedDict
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceTypedDict,
    FinishReason,
)
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .deltamessage import Content, ContentTypedDict, DeltaMessage, DeltaMessageTypedDict
from .fimcompletionrequest import (
    FIMCompletionRequest,
    FIMCompletionRequestStop,
    FIMCompletionRequestStopTypedDict,
    FIMCompletionRequestTypedDict,
)
from .fimcompletionresponse import FIMCompletionResponse, FIMCompletionResponseTypedDict
from .fimcompletionstreamrequest import (
    FIMCompletionStreamRequest,
    FIMCompletionStreamRequestStop,
    FIMCompletionStreamRequestStopTypedDict,
    FIMCompletionStreamRequestTypedDict,
)
from .function import Function, FunctionTypedDict
from .functioncall import (
    Arguments,
    ArgumentsTypedDict,
    FunctionCall,
    FunctionCallTypedDict,
)
from .functionname import FunctionName, FunctionNameTypedDict
from .httpvalidationerror import HTTPValidationError, HTTPValidationErrorData
from .imageurl import ImageURL, ImageURLTypedDict
from .imageurlchunk import (
    ImageURLChunk,
    ImageURLChunkImageURL,
    ImageURLChunkImageURLTypedDict,
    ImageURLChunkType,
    ImageURLChunkTypedDict,
)
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .prediction import Prediction, PredictionTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkType, ReferenceChunkTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .responseformats import ResponseFormats
from .sdkerror import SDKError
from .security import Security, SecurityTypedDict
from .systemmessage import (
    Role,
    SystemMessage,
    SystemMessageContent,
    SystemMessageContentTypedDict,
    SystemMessageTypedDict,
)
from .textchunk import TextChunk, TextChunkTypedDict, Type
from .tool import Tool, ToolTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import (
    ToolMessage,
    ToolMessageContent,
    ToolMessageContentTypedDict,
    ToolMessageRole,
    ToolMessageTypedDict,
)
from .tooltypes import ToolTypes
from .usageinfo import UsageInfo, UsageInfoTypedDict
from .usermessage import (
    UserMessage,
    UserMessageContent,
    UserMessageContentTypedDict,
    UserMessageRole,
    UserMessageTypedDict,
)
from .validationerror import (
    Loc,
    LocTypedDict,
    ValidationError,
    ValidationErrorTypedDict,
)


__all__ = [
    "Arguments",
    "ArgumentsTypedDict",
    "AssistantMessage",
    "AssistantMessageContent",
    "AssistantMessageContentTypedDict",
    "AssistantMessageRole",
    "AssistantMessageTypedDict",
    "ChatCompletionChoice",
    "ChatCompletionChoiceFinishReason",
    "ChatCompletionChoiceTypedDict",
    "ChatCompletionRequest",
    "ChatCompletionRequestMessages",
    "ChatCompletionRequestMessagesTypedDict",
    "ChatCompletionRequestStop",
    "ChatCompletionRequestStopTypedDict",
    "ChatCompletionRequestToolChoice",
    "ChatCompletionRequestToolChoiceTypedDict",
    "ChatCompletionRequestTypedDict",
    "ChatCompletionResponse",
    "ChatCompletionResponseTypedDict",
    "ChatCompletionStreamRequest",
    "ChatCompletionStreamRequestToolChoice",
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    "ChatCompletionStreamRequestTypedDict",
    "CompletionChunk",
    "CompletionChunkTypedDict",
    "CompletionEvent",
    "CompletionEventTypedDict",
    "CompletionResponseStreamChoice",
    "CompletionResponseStreamChoiceTypedDict",
    "Content",
    "ContentChunk",
    "ContentChunkTypedDict",
    "ContentTypedDict",
    "DeltaMessage",
    "DeltaMessageTypedDict",
    "FIMCompletionRequest",
    "FIMCompletionRequestStop",
    "FIMCompletionRequestStopTypedDict",
    "FIMCompletionRequestTypedDict",
    "FIMCompletionResponse",
    "FIMCompletionResponseTypedDict",
    "FIMCompletionStreamRequest",
    "FIMCompletionStreamRequestStop",
    "FIMCompletionStreamRequestStopTypedDict",
    "FIMCompletionStreamRequestTypedDict",
    "FinishReason",
    "Function",
    "FunctionCall",
    "FunctionCallTypedDict",
    "FunctionName",
    "FunctionNameTypedDict",
    "FunctionTypedDict",
    "HTTPValidationError",
    "HTTPValidationErrorData",
    "ImageURL",
    "ImageURLChunk",
    "ImageURLChunkImageURL",
    "ImageURLChunkImageURLTypedDict",
    "ImageURLChunkType",
    "ImageURLChunkTypedDict",
    "ImageURLTypedDict",
    "JSONSchema",
    "JSONSchemaTypedDict",
    "Loc",
    "LocTypedDict",
    "Messages",
    "MessagesTypedDict",
    "Prediction",
    "PredictionTypedDict",
    "ReferenceChunk",
    "ReferenceChunkType",
    "ReferenceChunkTypedDict",
    "ResponseFormat",
    "ResponseFormatTypedDict",
    "ResponseFormats",
    "Role",
    "SDKError",
    "Security",
    "SecurityTypedDict",
    "Stop",
    "StopTypedDict",
    "SystemMessage",
    "SystemMessageContent",
    "SystemMessageContentTypedDict",
    "SystemMessageTypedDict",
    "TextChunk",
    "TextChunkTypedDict",
    "Tool",
    "ToolCall",
    "ToolCallTypedDict",
    "ToolChoice",
    "ToolChoiceEnum",
    "ToolChoiceTypedDict",
    "ToolMessage",
    "ToolMessageContent",
    "ToolMessageContentTypedDict",
    "ToolMessageRole",
    "ToolMessageTypedDict",
    "ToolTypedDict",
    "ToolTypes",
    "Type",
    "UsageInfo",
    "UsageInfoTypedDict",
    "UserMessage",
    "UserMessageContent",
    "UserMessageContentTypedDict",
    "UserMessageRole",
    "UserMessageTypedDict",
    "ValidationError",
    "ValidationErrorTypedDict",
]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/assistantmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


AssistantMessageContentTypedDict = TypeAliasType(
    "AssistantMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


AssistantMessageContent = TypeAliasType(
    "AssistantMessageContent", Union[str, List[ContentChunk]]
)


AssistantMessageRole = Literal["assistant"]


class AssistantMessageTypedDict(TypedDict):
    content: NotRequired[Nullable[AssistantMessageContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]
    prefix: NotRequired[bool]
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""
    role: NotRequired[AssistantMessageRole]


class AssistantMessage(BaseModel):
    content: OptionalNullable[AssistantMessageContent] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    prefix: Optional[bool] = False
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""

    role: Optional[AssistantMessageRole] = "assistant"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["content", "tool_calls", "prefix", "role"]
        nullable_fields = ["content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/chatcompletionchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from mistralai_gcp.types import BaseModel, UnrecognizedStr
from mistralai_gcp.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


ChatCompletionChoiceFinishReason = Union[
    Literal["stop", "length", "model_length", "error", "tool_calls"], UnrecognizedStr
]


class ChatCompletionChoiceTypedDict(TypedDict):
    index: int
    message: AssistantMessageTypedDict
    finish_reason: ChatCompletionChoiceFinishReason


class ChatCompletionChoice(BaseModel):
    index: int

    message: AssistantMessage

    finish_reason: Annotated[
        ChatCompletionChoiceFinishReason, PlainValidator(validate_open_enum(False))
    ]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/chatcompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from mistralai_gcp.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ChatCompletionRequestStopTypedDict = TypeAliasType(
    "ChatCompletionRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionRequestStop = TypeAliasType(
    "ChatCompletionRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionRequestMessagesTypedDict = TypeAliasType(
    "ChatCompletionRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


ChatCompletionRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionRequestToolChoice = TypeAliasType(
    "ChatCompletionRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""
    messages: List[ChatCompletionRequestMessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[ChatCompletionRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]


class ChatCompletionRequest(BaseModel):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""

    messages: List[ChatCompletionRequestMessages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[ChatCompletionRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
        ]
        nullable_fields = ["temperature", "max_tokens", "random_seed", "tools", "n"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/chatcompletionresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionchoice import ChatCompletionChoice, ChatCompletionChoiceTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai_gcp.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class ChatCompletionResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    created: NotRequired[int]
    choices: NotRequired[List[ChatCompletionChoiceTypedDict]]


class ChatCompletionResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    created: Optional[int] = None

    choices: Optional[List[ChatCompletionChoice]] = None
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/chatcompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from mistralai_gcp.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


Stop = TypeAliasType("Stop", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


MessagesTypedDict = TypeAliasType(
    "MessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


Messages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionStreamRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionStreamRequestToolChoice = TypeAliasType(
    "ChatCompletionStreamRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionStreamRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""
    messages: List[MessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[StopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionStreamRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]


class ChatCompletionStreamRequest(BaseModel):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""

    messages: List[Messages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[Stop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionStreamRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
        ]
        nullable_fields = ["temperature", "max_tokens", "random_seed", "tools", "n"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/completionchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceTypedDict,
)
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai_gcp.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class CompletionChunkTypedDict(TypedDict):
    id: str
    model: str
    choices: List[CompletionResponseStreamChoiceTypedDict]
    object: NotRequired[str]
    created: NotRequired[int]
    usage: NotRequired[UsageInfoTypedDict]


class CompletionChunk(BaseModel):
    id: str

    model: str

    choices: List[CompletionResponseStreamChoice]

    object: Optional[str] = None

    created: Optional[int] = None

    usage: Optional[UsageInfo] = None
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/completionevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from mistralai_gcp.types import BaseModel
from typing_extensions import TypedDict


class CompletionEventTypedDict(TypedDict):
    data: CompletionChunkTypedDict


class CompletionEvent(BaseModel):
    data: CompletionChunk
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/completionresponsestreamchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .deltamessage import DeltaMessage, DeltaMessageTypedDict
from mistralai_gcp.types import BaseModel, Nullable, UNSET_SENTINEL, UnrecognizedStr
from mistralai_gcp.utils import validate_open_enum
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


FinishReason = Union[Literal["stop", "length", "error", "tool_calls"], UnrecognizedStr]


class CompletionResponseStreamChoiceTypedDict(TypedDict):
    index: int
    delta: DeltaMessageTypedDict
    finish_reason: Nullable[FinishReason]


class CompletionResponseStreamChoice(BaseModel):
    index: int

    delta: DeltaMessage

    finish_reason: Annotated[
        Nullable[FinishReason], PlainValidator(validate_open_enum(False))
    ]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["finish_reason"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/contentchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai_gcp.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType


ContentChunkTypedDict = TypeAliasType(
    "ContentChunkTypedDict",
    Union[TextChunkTypedDict, ImageURLChunkTypedDict, ReferenceChunkTypedDict],
)


ContentChunk = Annotated[
    Union[
        Annotated[ImageURLChunk, Tag("image_url")],
        Annotated[TextChunk, Tag("text")],
        Annotated[ReferenceChunk, Tag("reference")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/deltamessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ContentTypedDict = TypeAliasType(
    "ContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


Content = TypeAliasType("Content", Union[str, List[ContentChunk]])


class DeltaMessageTypedDict(TypedDict):
    role: NotRequired[Nullable[str]]
    content: NotRequired[Nullable[ContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]


class DeltaMessage(BaseModel):
    role: OptionalNullable[str] = UNSET

    content: OptionalNullable[Content] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role", "content", "tool_calls"]
        nullable_fields = ["role", "content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/fimcompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


FIMCompletionRequestStopTypedDict = TypeAliasType(
    "FIMCompletionRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


FIMCompletionRequestStop = TypeAliasType(
    "FIMCompletionRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


class FIMCompletionRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """
    prompt: str
    r"""The text/code to complete."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[FIMCompletionRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    min_tokens: NotRequired[Nullable[int]]
    r"""The minimum number of tokens to generate in the completion."""


class FIMCompletionRequest(BaseModel):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """

    prompt: str
    r"""The text/code to complete."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = 1
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[FIMCompletionRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    min_tokens: OptionalNullable[int] = UNSET
    r"""The minimum number of tokens to generate in the completion."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/fimcompletionresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionchoice import ChatCompletionChoice, ChatCompletionChoiceTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai_gcp.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class FIMCompletionResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    created: NotRequired[int]
    choices: NotRequired[List[ChatCompletionChoiceTypedDict]]


class FIMCompletionResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    created: Optional[int] = None

    choices: Optional[List[ChatCompletionChoice]] = None
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/fimcompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


FIMCompletionStreamRequestStopTypedDict = TypeAliasType(
    "FIMCompletionStreamRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


FIMCompletionStreamRequestStop = TypeAliasType(
    "FIMCompletionStreamRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


class FIMCompletionStreamRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """
    prompt: str
    r"""The text/code to complete."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[FIMCompletionStreamRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    min_tokens: NotRequired[Nullable[int]]
    r"""The minimum number of tokens to generate in the completion."""


class FIMCompletionStreamRequest(BaseModel):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """

    prompt: str
    r"""The text/code to complete."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = 1
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[FIMCompletionStreamRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    min_tokens: OptionalNullable[int] = UNSET
    r"""The minimum number of tokens to generate in the completion."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/function.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing import Any, Dict, Optional
from typing_extensions import NotRequired, TypedDict


class FunctionTypedDict(TypedDict):
    name: str
    parameters: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class Function(BaseModel):
    name: str

    parameters: Dict[str, Any]

    description: Optional[str] = None

    strict: Optional[bool] = None
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/functioncall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing import Any, Dict, Union
from typing_extensions import TypeAliasType, TypedDict


ArgumentsTypedDict = TypeAliasType("ArgumentsTypedDict", Union[Dict[str, Any], str])


Arguments = TypeAliasType("Arguments", Union[Dict[str, Any], str])


class FunctionCallTypedDict(TypedDict):
    name: str
    arguments: ArgumentsTypedDict


class FunctionCall(BaseModel):
    name: str

    arguments: Arguments
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/functionname.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing_extensions import TypedDict


class FunctionNameTypedDict(TypedDict):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str


class FunctionName(BaseModel):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/httpvalidationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .validationerror import ValidationError
from mistralai_gcp import utils
from mistralai_gcp.types import BaseModel
from typing import List, Optional


class HTTPValidationErrorData(BaseModel):
    detail: Optional[List[ValidationError]] = None


class HTTPValidationError(Exception):
    data: HTTPValidationErrorData

    def __init__(self, data: HTTPValidationErrorData):
        self.data = data

    def __str__(self) -> str:
        return utils.marshal_json(self.data, HTTPValidationErrorData)
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/imageurl.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class ImageURLTypedDict(TypedDict):
    url: str
    detail: NotRequired[Nullable[str]]


class ImageURL(BaseModel):
    url: str

    detail: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["detail"]
        nullable_fields = ["detail"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/imageurlchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .imageurl import ImageURL, ImageURLTypedDict
from mistralai_gcp.types import BaseModel
from typing import Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ImageURLChunkImageURLTypedDict = TypeAliasType(
    "ImageURLChunkImageURLTypedDict", Union[ImageURLTypedDict, str]
)


ImageURLChunkImageURL = TypeAliasType("ImageURLChunkImageURL", Union[ImageURL, str])


ImageURLChunkType = Literal["image_url"]


class ImageURLChunkTypedDict(TypedDict):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURLTypedDict
    type: NotRequired[ImageURLChunkType]


class ImageURLChunk(BaseModel):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURL

    type: Optional[ImageURLChunkType] = "image_url"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/jsonschema.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class JSONSchemaTypedDict(TypedDict):
    name: str
    schema_definition: Dict[str, Any]
    description: NotRequired[Nullable[str]]
    strict: NotRequired[bool]


class JSONSchema(BaseModel):
    name: str

    schema_definition: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: OptionalNullable[str] = UNSET

    strict: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/prediction.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from mistralai_gcp.utils import validate_const
import pydantic
from pydantic.functional_validators import AfterValidator
from typing import Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PredictionTypedDict(TypedDict):
    type: Literal["content"]
    content: NotRequired[str]


class Prediction(BaseModel):
    TYPE: Annotated[
        Annotated[
            Optional[Literal["content"]], AfterValidator(validate_const("content"))
        ],
        pydantic.Field(alias="type"),
    ] = "content"

    content: Optional[str] = ""
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/referencechunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ReferenceChunkType = Literal["reference"]


class ReferenceChunkTypedDict(TypedDict):
    reference_ids: List[int]
    type: NotRequired[ReferenceChunkType]


class ReferenceChunk(BaseModel):
    reference_ids: List[int]

    type: Optional[ReferenceChunkType] = "reference"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/responseformat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .responseformats import ResponseFormats
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ResponseFormatTypedDict(TypedDict):
    type: NotRequired[ResponseFormats]
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
    json_schema: NotRequired[Nullable[JSONSchemaTypedDict]]


class ResponseFormat(BaseModel):
    type: Optional[ResponseFormats] = None
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""

    json_schema: OptionalNullable[JSONSchema] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "json_schema"]
        nullable_fields = ["json_schema"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/responseformats.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ResponseFormats = Literal["text", "json_object", "json_schema"]
r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/sdkerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from dataclasses import dataclass
from typing import Optional
import httpx


@dataclass
class SDKError(Exception):
    """Represents an error returned by the API."""

    message: str
    status_code: int = -1
    body: str = ""
    raw_response: Optional[httpx.Response] = None

    def __str__(self):
        body = ""
        if len(self.body) > 0:
            body = f"\n{self.body}"

        return f"{self.message}: Status {self.status_code}{body}"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from mistralai_gcp.utils import FieldMetadata, SecurityMetadata
from typing_extensions import Annotated, TypedDict


class SecurityTypedDict(TypedDict):
    api_key: str


class Security(BaseModel):
    api_key: Annotated[
        str,
        FieldMetadata(
            security=SecurityMetadata(
                scheme=True,
                scheme_type="http",
                sub_type="bearer",
                field_name="Authorization",
            )
        ),
    ]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/systemmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai_gcp.types import BaseModel
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


SystemMessageContentTypedDict = TypeAliasType(
    "SystemMessageContentTypedDict", Union[str, List[TextChunkTypedDict]]
)


SystemMessageContent = TypeAliasType(
    "SystemMessageContent", Union[str, List[TextChunk]]
)


Role = Literal["system"]


class SystemMessageTypedDict(TypedDict):
    content: SystemMessageContentTypedDict
    role: NotRequired[Role]


class SystemMessage(BaseModel):
    content: SystemMessageContent

    role: Optional[Role] = "system"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/textchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


Type = Literal["text"]


class TextChunkTypedDict(TypedDict):
    text: str
    type: NotRequired[Type]


class TextChunk(BaseModel):
    text: str

    type: Optional[Type] = "text"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/tool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .function import Function, FunctionTypedDict
from .tooltypes import ToolTypes
from mistralai_gcp.types import BaseModel
from mistralai_gcp.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolTypedDict(TypedDict):
    function: FunctionTypedDict
    type: NotRequired[ToolTypes]


class Tool(BaseModel):
    function: Function

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/toolcall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functioncall import FunctionCall, FunctionCallTypedDict
from .tooltypes import ToolTypes
from mistralai_gcp.types import BaseModel
from mistralai_gcp.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolCallTypedDict(TypedDict):
    function: FunctionCallTypedDict
    id: NotRequired[str]
    type: NotRequired[ToolTypes]
    index: NotRequired[int]


class ToolCall(BaseModel):
    function: FunctionCall

    id: Optional[str] = "null"

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )

    index: Optional[int] = 0
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/toolchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functionname import FunctionName, FunctionNameTypedDict
from .tooltypes import ToolTypes
from mistralai_gcp.types import BaseModel
from mistralai_gcp.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolChoiceTypedDict(TypedDict):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionNameTypedDict
    r"""this restriction of `Function` is used to select a specific function to call"""
    type: NotRequired[ToolTypes]


class ToolChoice(BaseModel):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionName
    r"""this restriction of `Function` is used to select a specific function to call"""

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/toolchoiceenum.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ToolChoiceEnum = Literal["auto", "none", "any", "required"]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/toolmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai_gcp.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ToolMessageContentTypedDict = TypeAliasType(
    "ToolMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


ToolMessageContent = TypeAliasType("ToolMessageContent", Union[str, List[ContentChunk]])


ToolMessageRole = Literal["tool"]


class ToolMessageTypedDict(TypedDict):
    content: Nullable[ToolMessageContentTypedDict]
    tool_call_id: NotRequired[Nullable[str]]
    name: NotRequired[Nullable[str]]
    role: NotRequired[ToolMessageRole]


class ToolMessage(BaseModel):
    content: Nullable[ToolMessageContent]

    tool_call_id: OptionalNullable[str] = UNSET

    name: OptionalNullable[str] = UNSET

    role: Optional[ToolMessageRole] = "tool"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_call_id", "name", "role"]
        nullable_fields = ["content", "tool_call_id", "name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/tooltypes.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import UnrecognizedStr
from typing import Literal, Union


ToolTypes = Union[Literal["function"], UnrecognizedStr]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/usageinfo.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing_extensions import TypedDict


class UsageInfoTypedDict(TypedDict):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class UsageInfo(BaseModel):
    prompt_tokens: int

    completion_tokens: int

    total_tokens: int
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/usermessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai_gcp.types import BaseModel, Nullable, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


UserMessageContentTypedDict = TypeAliasType(
    "UserMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


UserMessageContent = TypeAliasType("UserMessageContent", Union[str, List[ContentChunk]])


UserMessageRole = Literal["user"]


class UserMessageTypedDict(TypedDict):
    content: Nullable[UserMessageContentTypedDict]
    role: NotRequired[UserMessageRole]


class UserMessage(BaseModel):
    content: Nullable[UserMessageContent]

    role: Optional[UserMessageRole] = "user"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/models/validationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai_gcp.types import BaseModel
from typing import List, Union
from typing_extensions import TypeAliasType, TypedDict


LocTypedDict = TypeAliasType("LocTypedDict", Union[str, int])


Loc = TypeAliasType("Loc", Union[str, int])


class ValidationErrorTypedDict(TypedDict):
    loc: List[LocTypedDict]
    msg: str
    type: str


class ValidationError(BaseModel):
    loc: List[Loc]

    msg: str

    type: str
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/types/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basemodel import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UnrecognizedInt,
    UnrecognizedStr,
    UNSET,
    UNSET_SENTINEL,
)

__all__ = [
    "BaseModel",
    "Nullable",
    "OptionalNullable",
    "UnrecognizedInt",
    "UnrecognizedStr",
    "UNSET",
    "UNSET_SENTINEL",
]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/types/basemodel.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from pydantic import ConfigDict, model_serializer
from pydantic import BaseModel as PydanticBaseModel
from typing import TYPE_CHECKING, Literal, Optional, TypeVar, Union, NewType
from typing_extensions import TypeAliasType, TypeAlias


class BaseModel(PydanticBaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, protected_namespaces=()
    )


class Unset(BaseModel):
    @model_serializer(mode="plain")
    def serialize_model(self):
        return UNSET_SENTINEL

    def __bool__(self) -> Literal[False]:
        return False


UNSET = Unset()
UNSET_SENTINEL = "~?~unset~?~sentinel~?~"


T = TypeVar("T")
if TYPE_CHECKING:
    Nullable: TypeAlias = Union[T, None]
    OptionalNullable: TypeAlias = Union[Optional[Nullable[T]], Unset]
else:
    Nullable = TypeAliasType("Nullable", Union[T, None], type_params=(T,))
    OptionalNullable = TypeAliasType(
        "OptionalNullable", Union[Optional[Nullable[T]], Unset], type_params=(T,)
    )

UnrecognizedInt = NewType("UnrecognizedInt", int)
UnrecognizedStr = NewType("UnrecognizedStr", str)
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .annotations import get_discriminator
from .enums import OpenEnumMeta
from .headers import get_headers, get_response_headers
from .metadata import (
    FieldMetadata,
    find_metadata,
    FormMetadata,
    HeaderMetadata,
    MultipartFormMetadata,
    PathParamMetadata,
    QueryParamMetadata,
    RequestMetadata,
    SecurityMetadata,
)
from .queryparams import get_query_params
from .retries import BackoffStrategy, Retries, retry, retry_async, RetryConfig
from .requestbodies import serialize_request_body, SerializedRequestBody
from .security import get_security
from .serializers import (
    get_pydantic_model,
    marshal_json,
    unmarshal,
    unmarshal_json,
    serialize_decimal,
    serialize_float,
    serialize_int,
    stream_to_text,
    stream_to_text_async,
    stream_to_bytes,
    stream_to_bytes_async,
    validate_const,
    validate_decimal,
    validate_float,
    validate_int,
    validate_open_enum,
)
from .url import generate_url, template_url, remove_suffix
from .values import (
    get_global_from_env,
    match_content_type,
    match_status_codes,
    match_response,
    cast_partial,
)
from .logger import Logger, get_body_content, get_default_logger

__all__ = [
    "BackoffStrategy",
    "FieldMetadata",
    "find_metadata",
    "FormMetadata",
    "generate_url",
    "get_body_content",
    "get_default_logger",
    "get_discriminator",
    "get_global_from_env",
    "get_headers",
    "get_pydantic_model",
    "get_query_params",
    "get_response_headers",
    "get_security",
    "HeaderMetadata",
    "Logger",
    "marshal_json",
    "match_content_type",
    "match_status_codes",
    "match_response",
    "MultipartFormMetadata",
    "OpenEnumMeta",
    "PathParamMetadata",
    "QueryParamMetadata",
    "remove_suffix",
    "Retries",
    "retry",
    "retry_async",
    "RetryConfig",
    "RequestMetadata",
    "SecurityMetadata",
    "serialize_decimal",
    "serialize_float",
    "serialize_int",
    "serialize_request_body",
    "SerializedRequestBody",
    "stream_to_text",
    "stream_to_text_async",
    "stream_to_bytes",
    "stream_to_bytes_async",
    "template_url",
    "unmarshal",
    "unmarshal_json",
    "validate_decimal",
    "validate_const",
    "validate_float",
    "validate_int",
    "validate_open_enum",
    "cast_partial",
]
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/annotations.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from enum import Enum
from typing import Any, Optional

def get_discriminator(model: Any, fieldname: str, key: str) -> str:
    """
    Recursively search for the discriminator attribute in a model.

    Args:
        model (Any): The model to search within.
        fieldname (str): The name of the field to search for.
        key (str): The key to search for in dictionaries.

    Returns:
        str: The name of the discriminator attribute.

    Raises:
        ValueError: If the discriminator attribute is not found.
    """
    upper_fieldname = fieldname.upper()

    def get_field_discriminator(field: Any) -> Optional[str]:
        """Search for the discriminator attribute in a given field."""

        if isinstance(field, dict):
            if key in field:
                return f'{field[key]}'

        if hasattr(field, fieldname):
            attr = getattr(field, fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        if hasattr(field, upper_fieldname):
            attr = getattr(field, upper_fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        return None


    if isinstance(model, list):
        for field in model:
            discriminator = get_field_discriminator(field)
            if discriminator is not None:
                return discriminator

    discriminator = get_field_discriminator(model)
    if discriminator is not None:
        return discriminator

    raise ValueError(f'Could not find discriminator field {fieldname} in {model}')
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/enums.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import enum


class OpenEnumMeta(enum.EnumMeta):
    def __call__(
        cls, value, names=None, *, module=None, qualname=None, type=None, start=1
    ):
        # The `type` kwarg also happens to be a built-in that pylint flags as
        # redeclared. Safe to ignore this lint rule with this scope.
        # pylint: disable=redefined-builtin

        if names is not None:
            return super().__call__(
                value,
                names=names,
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )

        try:
            return super().__call__(
                value,
                names=names,  # pyright: ignore[reportArgumentType]
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )
        except ValueError:
            return value
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/eventstreaming.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import re
import json
from typing import (
    Callable,
    Generic,
    TypeVar,
    Optional,
    Generator,
    AsyncGenerator,
    Tuple,
)
import httpx

T = TypeVar("T")


class EventStream(Generic[T]):
    response: httpx.Response
    generator: Generator[T, None, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events(response, decoder, sentinel)

    def __iter__(self):
        return self

    def __next__(self):
        return next(self.generator)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.response.close()


class EventStreamAsync(Generic[T]):
    response: httpx.Response
    generator: AsyncGenerator[T, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events_async(response, decoder, sentinel)

    def __aiter__(self):
        return self

    async def __anext__(self):
        return await self.generator.__anext__()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.response.aclose()


class ServerEvent:
    id: Optional[str] = None
    event: Optional[str] = None
    data: Optional[str] = None
    retry: Optional[int] = None


MESSAGE_BOUNDARIES = [
    b"\r\n\r\n",
    b"\n\n",
    b"\r\r",
]


async def stream_events_async(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> AsyncGenerator[T, None]:
    buffer = bytearray()
    position = 0
    discard = False
    async for chunk in response.aiter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def stream_events(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> Generator[T, None, None]:
    buffer = bytearray()
    position = 0
    discard = False
    for chunk in response.iter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def _parse_event(
    raw: bytearray, decoder: Callable[[str], T], sentinel: Optional[str] = None
) -> Tuple[Optional[T], bool]:
    block = raw.decode()
    lines = re.split(r"\r?\n|\r", block)
    publish = False
    event = ServerEvent()
    data = ""
    for line in lines:
        if not line:
            continue

        delim = line.find(":")
        if delim <= 0:
            continue

        field = line[0:delim]
        value = line[delim + 1 :] if delim < len(line) - 1 else ""
        if len(value) and value[0] == " ":
            value = value[1:]

        if field == "event":
            event.event = value
            publish = True
        elif field == "data":
            data += value + "\n"
            publish = True
        elif field == "id":
            event.id = value
            publish = True
        elif field == "retry":
            event.retry = int(value) if value.isdigit() else None
            publish = True

    if sentinel and data == f"{sentinel}\n":
        return None, True

    if data:
        data = data[:-1]
        event.data = data

        data_is_primitive = (
            data.isnumeric() or data == "true" or data == "false" or data == "null"
        )
        data_is_json = (
            data.startswith("{") or data.startswith("[") or data.startswith('"')
        )

        if data_is_primitive or data_is_json:
            try:
                event.data = json.loads(data)
            except Exception:
                pass

    out = None
    if publish:
        out = decoder(json.dumps(event.__dict__))

    return out, False


def _peek_sequence(position: int, buffer: bytearray, sequence: bytes):
    if len(sequence) > (len(buffer) - position):
        return None

    for i, seq in enumerate(sequence):
        if buffer[position + i] != seq:
            return None

    return sequence
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/forms.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .serializers import marshal_json

from .metadata import (
    FormMetadata,
    MultipartFormMetadata,
    find_field_metadata,
)
from .values import _is_set, _val_to_string


def _populate_form(
    field_name: str,
    explode: bool,
    obj: Any,
    delimiter: str,
    form: Dict[str, List[str]],
):
    if not _is_set(obj):
        return form

    if isinstance(obj, BaseModel):
        items = []

        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_field_name = obj_field.alias if obj_field.alias is not None else name
            if obj_field_name == "":
                continue

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                form[obj_field_name] = [_val_to_string(val)]
            else:
                items.append(f"{obj_field_name}{delimiter}{_val_to_string(val)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, Dict):
        items = []
        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                form[key] = [_val_to_string(value)]
            else:
                items.append(f"{key}{delimiter}{_val_to_string(value)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            if explode:
                if not field_name in form:
                    form[field_name] = []
                form[field_name].append(_val_to_string(value))
            else:
                items.append(_val_to_string(value))

        if len(items) > 0:
            form[field_name] = [delimiter.join([str(item) for item in items])]
    else:
        form[field_name] = [_val_to_string(obj)]

    return form


def serialize_multipart_form(
    media_type: str, request: Any
) -> Tuple[str, Dict[str, Any], Dict[str, Any]]:
    form: Dict[str, Any] = {}
    files: Dict[str, Any] = {}

    if not isinstance(request, BaseModel):
        raise TypeError("invalid request body type")

    request_fields: Dict[str, FieldInfo] = request.__class__.model_fields
    request_field_types = get_type_hints(request.__class__)

    for name in request_fields:
        field = request_fields[name]

        val = getattr(request, name)
        if not _is_set(val):
            continue

        field_metadata = find_field_metadata(field, MultipartFormMetadata)
        if not field_metadata:
            continue

        f_name = field.alias if field.alias else name

        if field_metadata.file:
            file_fields: Dict[str, FieldInfo] = val.__class__.model_fields

            file_name = ""
            content = None
            content_type = None

            for file_field_name in file_fields:
                file_field = file_fields[file_field_name]

                file_metadata = find_field_metadata(file_field, MultipartFormMetadata)
                if file_metadata is None:
                    continue

                if file_metadata.content:
                    content = getattr(val, file_field_name, None)
                elif file_field_name == "content_type":
                    content_type = getattr(val, file_field_name, None)
                else:
                    file_name = getattr(val, file_field_name)

            if file_name == "" or content is None:
                raise ValueError("invalid multipart/form-data file")

            if content_type is not None:
                files[f_name] = (file_name, content, content_type)
            else:
                files[f_name] = (file_name, content)
        elif field_metadata.json:
            files[f_name] = (
                None,
                marshal_json(val, request_field_types[name]),
                "application/json",
            )
        else:
            if isinstance(val, List):
                values = []

                for value in val:
                    if not _is_set(value):
                        continue
                    values.append(_val_to_string(value))

                form[f_name + "[]"] = values
            else:
                form[f_name] = _val_to_string(val)
    return media_type, form, files


def serialize_form_data(data: Any) -> Dict[str, Any]:
    form: Dict[str, List[str]] = {}

    if isinstance(data, BaseModel):
        data_fields: Dict[str, FieldInfo] = data.__class__.model_fields
        data_field_types = get_type_hints(data.__class__)
        for name in data_fields:
            field = data_fields[name]

            val = getattr(data, name)
            if not _is_set(val):
                continue

            metadata = find_field_metadata(field, FormMetadata)
            if metadata is None:
                continue

            f_name = field.alias if field.alias is not None else name

            if metadata.json:
                form[f_name] = [marshal_json(val, data_field_types[name])]
            else:
                if metadata.style == "form":
                    _populate_form(
                        f_name,
                        metadata.explode,
                        val,
                        ",",
                        form,
                    )
                else:
                    raise ValueError(f"Invalid form style for field {name}")
    elif isinstance(data, Dict):
        for key, value in data.items():
            if _is_set(value):
                form[key] = [_val_to_string(value)]
    else:
        raise TypeError(f"Invalid request body type {type(data)} for form data")

    return form
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/headers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    List,
    Optional,
)
from httpx import Headers
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    HeaderMetadata,
    find_field_metadata,
)

from .values import _is_set, _populate_from_globals, _val_to_string


def get_headers(headers_params: Any, gbls: Optional[Any] = None) -> Dict[str, str]:
    headers: Dict[str, str] = {}

    globals_already_populated = []
    if _is_set(headers_params):
        globals_already_populated = _populate_headers(headers_params, gbls, headers, [])
    if _is_set(gbls):
        _populate_headers(gbls, None, headers, globals_already_populated)

    return headers


def _populate_headers(
    headers_params: Any,
    gbls: Any,
    header_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(headers_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = headers_params.__class__.model_fields
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]
        f_name = field.alias if field.alias is not None else name

        metadata = find_field_metadata(field, HeaderMetadata)
        if metadata is None:
            continue

        value, global_found = _populate_from_globals(
            name, getattr(headers_params, name), HeaderMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)
        value = _serialize_header(metadata.explode, value)

        if value != "":
            header_values[f_name] = value

    return globals_already_populated


def _serialize_header(explode: bool, obj: Any) -> str:
    if not _is_set(obj):
        return ""

    if isinstance(obj, BaseModel):
        items = []
        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_param_metadata = find_field_metadata(obj_field, HeaderMetadata)

            if not obj_param_metadata:
                continue

            f_name = obj_field.alias if obj_field.alias is not None else name

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                items.append(f"{f_name}={_val_to_string(val)}")
            else:
                items.append(f_name)
                items.append(_val_to_string(val))

        if len(items) > 0:
            return ",".join(items)
    elif isinstance(obj, Dict):
        items = []

        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                items.append(f"{key}={_val_to_string(value)}")
            else:
                items.append(key)
                items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join([str(item) for item in items])
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join(items)
    elif _is_set(obj):
        return f"{_val_to_string(obj)}"

    return ""


def get_response_headers(headers: Headers) -> Dict[str, List[str]]:
    res: Dict[str, List[str]] = {}
    for k, v in headers.items():
        if not k in res:
            res[k] = []

        res[k].append(v)
    return res
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/logger.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
from typing import Any, Protocol


class Logger(Protocol):
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


class NoOpLogger:
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


def get_body_content(req: httpx.Request) -> str:
    return "<streaming body>" if not hasattr(req, "_content") else str(req.content)


def get_default_logger() -> Logger:
    return NoOpLogger()
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/metadata.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import Optional, Type, TypeVar, Union
from dataclasses import dataclass
from pydantic.fields import FieldInfo


T = TypeVar("T")


@dataclass
class SecurityMetadata:
    option: bool = False
    scheme: bool = False
    scheme_type: Optional[str] = None
    sub_type: Optional[str] = None
    field_name: Optional[str] = None

    def get_field_name(self, default: str) -> str:
        return self.field_name or default


@dataclass
class ParamMetadata:
    serialization: Optional[str] = None
    style: str = "simple"
    explode: bool = False


@dataclass
class PathParamMetadata(ParamMetadata):
    pass


@dataclass
class QueryParamMetadata(ParamMetadata):
    style: str = "form"
    explode: bool = True


@dataclass
class HeaderMetadata(ParamMetadata):
    pass


@dataclass
class RequestMetadata:
    media_type: str = "application/octet-stream"


@dataclass
class MultipartFormMetadata:
    file: bool = False
    content: bool = False
    json: bool = False


@dataclass
class FormMetadata:
    json: bool = False
    style: str = "form"
    explode: bool = True


class FieldMetadata:
    security: Optional[SecurityMetadata] = None
    path: Optional[PathParamMetadata] = None
    query: Optional[QueryParamMetadata] = None
    header: Optional[HeaderMetadata] = None
    request: Optional[RequestMetadata] = None
    form: Optional[FormMetadata] = None
    multipart: Optional[MultipartFormMetadata] = None

    def __init__(
        self,
        security: Optional[SecurityMetadata] = None,
        path: Optional[Union[PathParamMetadata, bool]] = None,
        query: Optional[Union[QueryParamMetadata, bool]] = None,
        header: Optional[Union[HeaderMetadata, bool]] = None,
        request: Optional[Union[RequestMetadata, bool]] = None,
        form: Optional[Union[FormMetadata, bool]] = None,
        multipart: Optional[Union[MultipartFormMetadata, bool]] = None,
    ):
        self.security = security
        self.path = PathParamMetadata() if isinstance(path, bool) else path
        self.query = QueryParamMetadata() if isinstance(query, bool) else query
        self.header = HeaderMetadata() if isinstance(header, bool) else header
        self.request = RequestMetadata() if isinstance(request, bool) else request
        self.form = FormMetadata() if isinstance(form, bool) else form
        self.multipart = (
            MultipartFormMetadata() if isinstance(multipart, bool) else multipart
        )


def find_field_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = find_metadata(field_info, FieldMetadata)
    if not metadata:
        return None

    fields = metadata.__dict__

    for field in fields:
        if isinstance(fields[field], metadata_type):
            return fields[field]

    return None


def find_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = field_info.metadata
    if not metadata:
        return None

    for md in metadata:
        if isinstance(md, metadata_type):
            return md

    return None
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/queryparams.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
)

from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    QueryParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)
from .forms import _populate_form


def get_query_params(
    query_params: Any,
    gbls: Optional[Any] = None,
) -> Dict[str, List[str]]:
    params: Dict[str, List[str]] = {}

    globals_already_populated = _populate_query_params(query_params, gbls, params, [])
    if _is_set(gbls):
        _populate_query_params(gbls, None, params, globals_already_populated)

    return params


def _populate_query_params(
    query_params: Any,
    gbls: Any,
    query_param_values: Dict[str, List[str]],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(query_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = query_params.__class__.model_fields
    param_field_types = get_type_hints(query_params.__class__)
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]

        metadata = find_field_metadata(field, QueryParamMetadata)
        if not metadata:
            continue

        value = getattr(query_params, name) if _is_set(query_params) else None

        value, global_found = _populate_from_globals(
            name, value, QueryParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        f_name = field.alias if field.alias is not None else name
        serialization = metadata.serialization
        if serialization is not None:
            serialized_parms = _get_serialized_params(
                metadata, f_name, value, param_field_types[name]
            )
            for key, value in serialized_parms.items():
                if key in query_param_values:
                    query_param_values[key].extend(value)
                else:
                    query_param_values[key] = [value]
        else:
            style = metadata.style
            if style == "deepObject":
                _populate_deep_object_query_params(f_name, value, query_param_values)
            elif style == "form":
                _populate_delimited_query_params(
                    metadata, f_name, value, ",", query_param_values
                )
            elif style == "pipeDelimited":
                _populate_delimited_query_params(
                    metadata, f_name, value, "|", query_param_values
                )
            else:
                raise NotImplementedError(
                    f"query param style {style} not yet supported"
                )

    return globals_already_populated


def _populate_deep_object_query_params(
    field_name: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj):
        return

    if isinstance(obj, BaseModel):
        _populate_deep_object_query_params_basemodel(field_name, obj, params)
    elif isinstance(obj, Dict):
        _populate_deep_object_query_params_dict(field_name, obj, params)


def _populate_deep_object_query_params_basemodel(
    prior_params_key: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj) or not isinstance(obj, BaseModel):
        return

    obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
    for name in obj_fields:
        obj_field = obj_fields[name]

        f_name = obj_field.alias if obj_field.alias is not None else name

        params_key = f"{prior_params_key}[{f_name}]"

        obj_param_metadata = find_field_metadata(obj_field, QueryParamMetadata)
        if not _is_set(obj_param_metadata):
            continue

        obj_val = getattr(obj, name)
        if not _is_set(obj_val):
            continue

        if isinstance(obj_val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, obj_val, params)
        elif isinstance(obj_val, Dict):
            _populate_deep_object_query_params_dict(params_key, obj_val, params)
        elif isinstance(obj_val, List):
            _populate_deep_object_query_params_list(params_key, obj_val, params)
        else:
            params[params_key] = [_val_to_string(obj_val)]


def _populate_deep_object_query_params_dict(
    prior_params_key: str,
    value: Dict,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for key, val in value.items():
        if not _is_set(val):
            continue

        params_key = f"{prior_params_key}[{key}]"

        if isinstance(val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, val, params)
        elif isinstance(val, Dict):
            _populate_deep_object_query_params_dict(params_key, val, params)
        elif isinstance(val, List):
            _populate_deep_object_query_params_list(params_key, val, params)
        else:
            params[params_key] = [_val_to_string(val)]


def _populate_deep_object_query_params_list(
    params_key: str,
    value: List,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for val in value:
        if not _is_set(val):
            continue

        if params.get(params_key) is None:
            params[params_key] = []

        params[params_key].append(_val_to_string(val))


def _populate_delimited_query_params(
    metadata: QueryParamMetadata,
    field_name: str,
    obj: Any,
    delimiter: str,
    query_param_values: Dict[str, List[str]],
):
    _populate_form(
        field_name,
        metadata.explode,
        obj,
        delimiter,
        query_param_values,
    )
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/requestbodies.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import io
from dataclasses import dataclass
import re
from typing import (
    Any,
    Optional,
)

from .forms import serialize_form_data, serialize_multipart_form

from .serializers import marshal_json

SERIALIZATION_METHOD_TO_CONTENT_TYPE = {
    "json": "application/json",
    "form": "application/x-www-form-urlencoded",
    "multipart": "multipart/form-data",
    "raw": "application/octet-stream",
    "string": "text/plain",
}


@dataclass
class SerializedRequestBody:
    media_type: Optional[str] = None
    content: Optional[Any] = None
    data: Optional[Any] = None
    files: Optional[Any] = None


def serialize_request_body(
    request_body: Any,
    nullable: bool,
    optional: bool,
    serialization_method: str,
    request_body_type,
) -> Optional[SerializedRequestBody]:
    if request_body is None:
        if not nullable and optional:
            return None

    media_type = SERIALIZATION_METHOD_TO_CONTENT_TYPE[serialization_method]

    serialized_request_body = SerializedRequestBody(media_type)

    if re.match(r"(application|text)\/.*?\+*json.*", media_type) is not None:
        serialized_request_body.content = marshal_json(request_body, request_body_type)
    elif re.match(r"multipart\/.*", media_type) is not None:
        (
            serialized_request_body.media_type,
            serialized_request_body.data,
            serialized_request_body.files,
        ) = serialize_multipart_form(media_type, request_body)
    elif re.match(r"application\/x-www-form-urlencoded.*", media_type) is not None:
        serialized_request_body.data = serialize_form_data(request_body)
    elif isinstance(request_body, (bytes, bytearray, io.BytesIO, io.BufferedReader)):
        serialized_request_body.content = request_body
    elif isinstance(request_body, str):
        serialized_request_body.content = request_body
    else:
        raise TypeError(
            f"invalid request body type {type(request_body)} for mediaType {media_type}"
        )

    return serialized_request_body
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/retries.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import asyncio
import random
import time
from typing import List

import httpx


class BackoffStrategy:
    initial_interval: int
    max_interval: int
    exponent: float
    max_elapsed_time: int

    def __init__(
        self,
        initial_interval: int,
        max_interval: int,
        exponent: float,
        max_elapsed_time: int,
    ):
        self.initial_interval = initial_interval
        self.max_interval = max_interval
        self.exponent = exponent
        self.max_elapsed_time = max_elapsed_time


class RetryConfig:
    strategy: str
    backoff: BackoffStrategy
    retry_connection_errors: bool

    def __init__(
        self, strategy: str, backoff: BackoffStrategy, retry_connection_errors: bool
    ):
        self.strategy = strategy
        self.backoff = backoff
        self.retry_connection_errors = retry_connection_errors


class Retries:
    config: RetryConfig
    status_codes: List[str]

    def __init__(self, config: RetryConfig, status_codes: List[str]):
        self.config = config
        self.status_codes = status_codes


class TemporaryError(Exception):
    response: httpx.Response

    def __init__(self, response: httpx.Response):
        self.response = response


class PermanentError(Exception):
    inner: Exception

    def __init__(self, inner: Exception):
        self.inner = inner


def retry(func, retries: Retries):
    if retries.config.strategy == "backoff":

        def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return retry_with_backoff(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return func()


async def retry_async(func, retries: Retries):
    if retries.config.strategy == "backoff":

        async def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = await func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return await retry_with_backoff_async(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return await func()


def retry_with_backoff(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            time.sleep(sleep)
            retries += 1


async def retry_with_backoff_async(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return await func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            await asyncio.sleep(sleep)
            retries += 1
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import base64
from typing import (
    Any,
    Dict,
    List,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    SecurityMetadata,
    find_field_metadata,
)


def get_security(security: Any) -> Tuple[Dict[str, str], Dict[str, List[str]]]:
    headers: Dict[str, str] = {}
    query_params: Dict[str, List[str]] = {}

    if security is None:
        return headers, query_params

    if not isinstance(security, BaseModel):
        raise TypeError("security must be a pydantic model")

    sec_fields: Dict[str, FieldInfo] = security.__class__.model_fields
    for name in sec_fields:
        sec_field = sec_fields[name]

        value = getattr(security, name)
        if value is None:
            continue

        metadata = find_field_metadata(sec_field, SecurityMetadata)
        if metadata is None:
            continue
        if metadata.option:
            _parse_security_option(headers, query_params, value)
            return headers, query_params
        if metadata.scheme:
            # Special case for basic auth or custom auth which could be a flattened model
            if metadata.sub_type in ["basic", "custom"] and not isinstance(
                value, BaseModel
            ):
                _parse_security_scheme(headers, query_params, metadata, name, security)
            else:
                _parse_security_scheme(headers, query_params, metadata, name, value)

    return headers, query_params


def _parse_security_option(
    headers: Dict[str, str], query_params: Dict[str, List[str]], option: Any
):
    if not isinstance(option, BaseModel):
        raise TypeError("security option must be a pydantic model")

    opt_fields: Dict[str, FieldInfo] = option.__class__.model_fields
    for name in opt_fields:
        opt_field = opt_fields[name]

        metadata = find_field_metadata(opt_field, SecurityMetadata)
        if metadata is None or not metadata.scheme:
            continue
        _parse_security_scheme(
            headers, query_params, metadata, name, getattr(option, name)
        )


def _parse_security_scheme(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    field_name: str,
    scheme: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    if isinstance(scheme, BaseModel):
        if scheme_type == "http":
            if sub_type == "basic":
                _parse_basic_auth_scheme(headers, scheme)
                return
            if sub_type == "custom":
                return

        scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
        for name in scheme_fields:
            scheme_field = scheme_fields[name]

            metadata = find_field_metadata(scheme_field, SecurityMetadata)
            if metadata is None or metadata.field_name is None:
                continue

            value = getattr(scheme, name)

            _parse_security_scheme_value(
                headers, query_params, scheme_metadata, metadata, name, value
            )
    else:
        _parse_security_scheme_value(
            headers, query_params, scheme_metadata, scheme_metadata, field_name, scheme
        )


def _parse_security_scheme_value(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    security_metadata: SecurityMetadata,
    field_name: str,
    value: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    header_name = security_metadata.get_field_name(field_name)

    if scheme_type == "apiKey":
        if sub_type == "header":
            headers[header_name] = value
        elif sub_type == "query":
            query_params[header_name] = [value]
        else:
            raise ValueError("sub type {sub_type} not supported")
    elif scheme_type == "openIdConnect":
        headers[header_name] = _apply_bearer(value)
    elif scheme_type == "oauth2":
        if sub_type != "client_credentials":
            headers[header_name] = _apply_bearer(value)
    elif scheme_type == "http":
        if sub_type == "bearer":
            headers[header_name] = _apply_bearer(value)
        elif sub_type == "custom":
            return
        else:
            raise ValueError("sub type {sub_type} not supported")
    else:
        raise ValueError("scheme type {scheme_type} not supported")


def _apply_bearer(token: str) -> str:
    return token.lower().startswith("bearer ") and token or f"Bearer {token}"


def _parse_basic_auth_scheme(headers: Dict[str, str], scheme: Any):
    username = ""
    password = ""

    if not isinstance(scheme, BaseModel):
        raise TypeError("basic auth scheme must be a pydantic model")

    scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
    for name in scheme_fields:
        scheme_field = scheme_fields[name]

        metadata = find_field_metadata(scheme_field, SecurityMetadata)
        if metadata is None or metadata.field_name is None:
            continue

        field_name = metadata.field_name
        value = getattr(scheme, name)

        if field_name == "username":
            username = value
        if field_name == "password":
            password = value

    data = f"{username}:{password}".encode()
    headers["Authorization"] = f"Basic {base64.b64encode(data).decode()}"
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/serializers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
import json
from typing import Any, Dict, List, Union, get_args
import httpx
from typing_extensions import get_origin
from pydantic import ConfigDict, create_model
from pydantic_core import from_json
from typing_inspection.typing_objects import is_union

from ..types.basemodel import BaseModel, Nullable, OptionalNullable, Unset


def serialize_decimal(as_str: bool):
    def serialize(d):
        # Optional[T] is a Union[T, None]
        if is_union(type(d)) and type(None) in get_args(type(d)) and d is None:
            return None
        if isinstance(d, Unset):
            return d

        if not isinstance(d, Decimal):
            raise ValueError("Expected Decimal object")

        return str(d) if as_str else float(d)

    return serialize


def validate_decimal(d):
    if d is None:
        return None

    if isinstance(d, (Decimal, Unset)):
        return d

    if not isinstance(d, (str, int, float)):
        raise ValueError("Expected string, int or float")

    return Decimal(str(d))


def serialize_float(as_str: bool):
    def serialize(f):
        # Optional[T] is a Union[T, None]
        if is_union(type(f)) and type(None) in get_args(type(f)) and f is None:
            return None
        if isinstance(f, Unset):
            return f

        if not isinstance(f, float):
            raise ValueError("Expected float")

        return str(f) if as_str else f

    return serialize


def validate_float(f):
    if f is None:
        return None

    if isinstance(f, (float, Unset)):
        return f

    if not isinstance(f, str):
        raise ValueError("Expected string")

    return float(f)


def serialize_int(as_str: bool):
    def serialize(i):
        # Optional[T] is a Union[T, None]
        if is_union(type(i)) and type(None) in get_args(type(i)) and i is None:
            return None
        if isinstance(i, Unset):
            return i

        if not isinstance(i, int):
            raise ValueError("Expected int")

        return str(i) if as_str else i

    return serialize


def validate_int(b):
    if b is None:
        return None

    if isinstance(b, (int, Unset)):
        return b

    if not isinstance(b, str):
        raise ValueError("Expected string")

    return int(b)


def validate_open_enum(is_int: bool):
    def validate(e):
        if e is None:
            return None

        if isinstance(e, Unset):
            return e

        if is_int:
            if not isinstance(e, int):
                raise ValueError("Expected int")
        else:
            if not isinstance(e, str):
                raise ValueError("Expected string")

        return e

    return validate


def validate_const(v):
    def validate(c):
        # Optional[T] is a Union[T, None]
        if is_union(type(c)) and type(None) in get_args(type(c)) and c is None:
            return None

        if v != c:
            raise ValueError(f"Expected {v}")

        return c

    return validate


def unmarshal_json(raw, typ: Any) -> Any:
    return unmarshal(from_json(raw), typ)


def unmarshal(val, typ: Any) -> Any:
    unmarshaller = create_model(
        "Unmarshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = unmarshaller(body=val)

    # pyright: ignore[reportAttributeAccessIssue]
    return m.body  # type: ignore


def marshal_json(val, typ):
    if is_nullable(typ) and val is None:
        return "null"

    marshaller = create_model(
        "Marshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = marshaller(body=val)

    d = m.model_dump(by_alias=True, mode="json", exclude_none=True)

    if len(d) == 0:
        return ""

    return json.dumps(d[next(iter(d))], separators=(",", ":"))


def is_nullable(field):
    origin = get_origin(field)
    if origin is Nullable or origin is OptionalNullable:
        return True

    if not origin is Union or type(None) not in get_args(field):
        return False

    for arg in get_args(field):
        if get_origin(arg) is Nullable or get_origin(arg) is OptionalNullable:
            return True

    return False


def stream_to_text(stream: httpx.Response) -> str:
    return "".join(stream.iter_text())


async def stream_to_text_async(stream: httpx.Response) -> str:
    return "".join([chunk async for chunk in stream.aiter_text()])


def stream_to_bytes(stream: httpx.Response) -> bytes:
    return stream.content


async def stream_to_bytes_async(stream: httpx.Response) -> bytes:
    return await stream.aread()


def get_pydantic_model(data: Any, typ: Any) -> Any:
    if not _contains_pydantic_model(data):
        return unmarshal(data, typ)

    return data


def _contains_pydantic_model(data: Any) -> bool:
    if isinstance(data, BaseModel):
        return True
    if isinstance(data, List):
        return any(_contains_pydantic_model(item) for item in data)
    if isinstance(data, Dict):
        return any(_contains_pydantic_model(value) for value in data.values())

    return False
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/url.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
    Union,
    get_args,
    get_origin,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    PathParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)


def generate_url(
    server_url: str,
    path: str,
    path_params: Any,
    gbls: Optional[Any] = None,
) -> str:
    path_param_values: Dict[str, str] = {}

    globals_already_populated = _populate_path_params(
        path_params, gbls, path_param_values, []
    )
    if _is_set(gbls):
        _populate_path_params(gbls, None, path_param_values, globals_already_populated)

    for key, value in path_param_values.items():
        path = path.replace("{" + key + "}", value, 1)

    return remove_suffix(server_url, "/") + path


def _populate_path_params(
    path_params: Any,
    gbls: Any,
    path_param_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(path_params, BaseModel):
        return globals_already_populated

    path_param_fields: Dict[str, FieldInfo] = path_params.__class__.model_fields
    path_param_field_types = get_type_hints(path_params.__class__)
    for name in path_param_fields:
        if name in skip_fields:
            continue

        field = path_param_fields[name]

        param_metadata = find_field_metadata(field, PathParamMetadata)
        if param_metadata is None:
            continue

        param = getattr(path_params, name) if _is_set(path_params) else None
        param, global_found = _populate_from_globals(
            name, param, PathParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        if not _is_set(param):
            continue

        f_name = field.alias if field.alias is not None else name
        serialization = param_metadata.serialization
        if serialization is not None:
            serialized_params = _get_serialized_params(
                param_metadata, f_name, param, path_param_field_types[name]
            )
            for key, value in serialized_params.items():
                path_param_values[key] = value
        else:
            pp_vals: List[str] = []
            if param_metadata.style == "simple":
                if isinstance(param, List):
                    for pp_val in param:
                        if not _is_set(pp_val):
                            continue
                        pp_vals.append(_val_to_string(pp_val))
                    path_param_values[f_name] = ",".join(pp_vals)
                elif isinstance(param, Dict):
                    for pp_key in param:
                        if not _is_set(param[pp_key]):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(f"{pp_key}={_val_to_string(param[pp_key])}")
                        else:
                            pp_vals.append(f"{pp_key},{_val_to_string(param[pp_key])}")
                    path_param_values[f_name] = ",".join(pp_vals)
                elif not isinstance(param, (str, int, float, complex, bool, Decimal)):
                    param_fields: Dict[str, FieldInfo] = param.__class__.model_fields
                    for name in param_fields:
                        param_field = param_fields[name]

                        param_value_metadata = find_field_metadata(
                            param_field, PathParamMetadata
                        )
                        if param_value_metadata is None:
                            continue

                        param_name = (
                            param_field.alias if param_field.alias is not None else name
                        )

                        param_field_val = getattr(param, name)
                        if not _is_set(param_field_val):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(
                                f"{param_name}={_val_to_string(param_field_val)}"
                            )
                        else:
                            pp_vals.append(
                                f"{param_name},{_val_to_string(param_field_val)}"
                            )
                    path_param_values[f_name] = ",".join(pp_vals)
                elif _is_set(param):
                    path_param_values[f_name] = _val_to_string(param)

    return globals_already_populated


def is_optional(field):
    return get_origin(field) is Union and type(None) in get_args(field)


def template_url(url_with_params: str, params: Dict[str, str]) -> str:
    for key, value in params.items():
        url_with_params = url_with_params.replace("{" + key + "}", value)

    return url_with_params


def remove_suffix(input_string, suffix):
    if suffix and input_string.endswith(suffix):
        return input_string[: -len(suffix)]
    return input_string
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/utils/values.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from datetime import datetime
from enum import Enum
from email.message import Message
from functools import partial
import os
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast

from httpx import Response
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ..types.basemodel import Unset

from .serializers import marshal_json

from .metadata import ParamMetadata, find_field_metadata


def match_content_type(content_type: str, pattern: str) -> bool:
    if pattern in (content_type, "*", "*/*"):
        return True

    msg = Message()
    msg["content-type"] = content_type
    media_type = msg.get_content_type()

    if media_type == pattern:
        return True

    parts = media_type.split("/")
    if len(parts) == 2:
        if pattern in (f"{parts[0]}/*", f"*/{parts[1]}"):
            return True

    return False


def match_status_codes(status_codes: List[str], status_code: int) -> bool:
    if "default" in status_codes:
        return True

    for code in status_codes:
        if code == str(status_code):
            return True

        if code.endswith("XX") and code.startswith(str(status_code)[:1]):
            return True
    return False


T = TypeVar("T")

def cast_partial(typ):
    return partial(cast, typ)

def get_global_from_env(
    value: Optional[T], env_key: str, type_cast: Callable[[str], T]
) -> Optional[T]:
    if value is not None:
        return value
    env_value = os.getenv(env_key)
    if env_value is not None:
        try:
            return type_cast(env_value)
        except ValueError:
            pass
    return None


def match_response(
    response: Response, code: Union[str, List[str]], content_type: str
) -> bool:
    codes = code if isinstance(code, list) else [code]
    return match_status_codes(codes, response.status_code) and match_content_type(
        response.headers.get("content-type", "application/octet-stream"), content_type
    )


def _populate_from_globals(
    param_name: str, value: Any, param_metadata_type: type, gbls: Any
) -> Tuple[Any, bool]:
    if gbls is None:
        return value, False

    if not isinstance(gbls, BaseModel):
        raise TypeError("globals must be a pydantic model")

    global_fields: Dict[str, FieldInfo] = gbls.__class__.model_fields
    found = False
    for name in global_fields:
        field = global_fields[name]
        if name is not param_name:
            continue

        found = True

        if value is not None:
            return value, True

        global_value = getattr(gbls, name)

        param_metadata = find_field_metadata(field, param_metadata_type)
        if param_metadata is None:
            return value, True

        return global_value, True

    return value, found


def _val_to_string(val) -> str:
    if isinstance(val, bool):
        return str(val).lower()
    if isinstance(val, datetime):
        return str(val.isoformat().replace("+00:00", "Z"))
    if isinstance(val, Enum):
        return str(val.value)

    return str(val)


def _get_serialized_params(
    metadata: ParamMetadata, field_name: str, obj: Any, typ: type
) -> Dict[str, str]:
    params: Dict[str, str] = {}

    serialization = metadata.serialization
    if serialization == "json":
        params[field_name] = marshal_json(obj, typ)

    return params


def _is_set(value: Any) -> bool:
    return value is not None and not isinstance(value, Unset)
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._version import (
    __title__,
    __version__,
    __openapi_doc_version__,
    __gen_version__,
    __user_agent__,
)
from .sdk import *
from .sdkconfiguration import *
from .models import *


VERSION: str = __version__
OPENAPI_DOC_VERSION = __openapi_doc_version__
SPEAKEASY_GENERATOR_VERSION = __gen_version__
USER_AGENT = __user_agent__
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/_version.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import importlib.metadata

__title__: str = "mistralai-gcp"
__version__: str = "1.6.0"
__openapi_doc_version__: str = "0.0.2"
__gen_version__: str = "2.548.6"
__user_agent__: str = "speakeasy-sdk/python 1.6.0 2.548.6 0.0.2 mistralai-gcp"

try:
    if __package__ is not None:
        __version__ = importlib.metadata.version(__package__)
except importlib.metadata.PackageNotFoundError:
    pass
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/basesdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkconfiguration import SDKConfiguration
import httpx
from mistralai_gcp import models, utils
from mistralai_gcp._hooks import (
    AfterErrorContext,
    AfterSuccessContext,
    BeforeRequestContext,
)
from mistralai_gcp.utils import RetryConfig, SerializedRequestBody, get_body_content
from typing import Callable, List, Mapping, Optional, Tuple
from urllib.parse import parse_qs, urlparse


class BaseSDK:
    sdk_configuration: SDKConfiguration

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        self.sdk_configuration = sdk_config

    def _get_url(self, base_url, url_variables):
        sdk_url, sdk_variables = self.sdk_configuration.get_server_details()

        if base_url is None:
            base_url = sdk_url

        if url_variables is None:
            url_variables = sdk_variables

        return utils.template_url(base_url, url_variables)

    def _build_request_async(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.async_client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request_with_client(
        self,
        client,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        query_params = {}

        url = url_override
        if url is None:
            url = utils.generate_url(
                self._get_url(base_url, url_variables),
                path,
                request if request_has_path_params else None,
                _globals if request_has_path_params else None,
            )

            query_params = utils.get_query_params(
                request if request_has_query_params else None,
                _globals if request_has_query_params else None,
            )
        else:
            # Pick up the query parameter from the override so they can be
            # preserved when building the request later on (necessary as of
            # httpx 0.28).
            parsed_override = urlparse(str(url_override))
            query_params = parse_qs(parsed_override.query, keep_blank_values=True)

        headers = utils.get_headers(request, _globals)
        headers["Accept"] = accept_header_value
        headers[user_agent_header] = self.sdk_configuration.user_agent

        if security is not None:
            if callable(security):
                security = security()

        if security is not None:
            security_headers, security_query_params = utils.get_security(security)
            headers = {**headers, **security_headers}
            query_params = {**query_params, **security_query_params}

        serialized_request_body = SerializedRequestBody()
        if get_serialized_body is not None:
            rb = get_serialized_body()
            if request_body_required and rb is None:
                raise ValueError("request body is required")

            if rb is not None:
                serialized_request_body = rb

        if (
            serialized_request_body.media_type is not None
            and serialized_request_body.media_type
            not in (
                "multipart/form-data",
                "multipart/mixed",
            )
        ):
            headers["content-type"] = serialized_request_body.media_type

        if http_headers is not None:
            for header, value in http_headers.items():
                headers[header] = value

        timeout = timeout_ms / 1000 if timeout_ms is not None else None

        return client.build_request(
            method,
            url,
            params=query_params,
            content=serialized_request_body.content,
            data=serialized_request_body.data,
            files=serialized_request_body.files,
            headers=headers,
            timeout=timeout,
        )

    def do_request(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.client
        logger = self.sdk_configuration.debug_logger

        def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = utils.retry(do, utils.Retries(retry_config[0], retry_config[1]))
        else:
            http_res = do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res

    async def do_request_async(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.async_client
        logger = self.sdk_configuration.debug_logger

        async def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = await client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = await utils.retry_async(
                do, utils.Retries(retry_config[0], retry_config[1])
            )
        else:
            http_res = await do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/chat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai_gcp import models, utils
from mistralai_gcp._hooks import HookContext
from mistralai_gcp.types import OptionalNullable, UNSET
from mistralai_gcp.utils import eventstreaming
from typing import Any, List, Mapping, Optional, Union


class Chat(BaseSDK):
    r"""Chat Completion API."""

    def stream(
        self,
        *,
        model: str,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStream[models.CompletionEvent]]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
        )

        req = self._build_request(
            method="POST",
            path="/streamRawPredict",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        model: str,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStreamAsync[models.CompletionEvent]]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
        )

        req = self._build_request_async(
            method="POST",
            path="/streamRawPredict",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def complete(
        self,
        *,
        model: str,
        messages: Union[
            List[models.ChatCompletionRequestMessages],
            List[models.ChatCompletionRequestMessagesTypedDict],
        ],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.ChatCompletionRequestStop,
                models.ChatCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.ChatCompletionResponse]:
        r"""Chat Completion

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
        )

        req = self._build_request(
            method="POST",
            path="/rawPredict",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.ChatCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        model: str,
        messages: Union[
            List[models.ChatCompletionRequestMessages],
            List[models.ChatCompletionRequestMessagesTypedDict],
        ],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.ChatCompletionRequestStop,
                models.ChatCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.ChatCompletionResponse]:
        r"""Chat Completion

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
        )

        req = self._build_request_async(
            method="POST",
            path="/rawPredict",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.ChatCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/fim.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai_gcp import models, utils
from mistralai_gcp._hooks import HookContext
from mistralai_gcp.types import OptionalNullable, UNSET
from mistralai_gcp.utils import eventstreaming
from typing import Any, Mapping, Optional, Union


class Fim(BaseSDK):
    r"""Fill-in-the-middle API."""

    def stream(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.FIMCompletionStreamRequestStop,
                models.FIMCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStream[models.CompletionEvent]]:
        r"""Stream fim completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request(
            method="POST",
            path="/streamRawPredict#fim",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_fim",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.FIMCompletionStreamRequestStop,
                models.FIMCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[eventstreaming.EventStreamAsync[models.CompletionEvent]]:
        r"""Stream fim completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request_async(
            method="POST",
            path="/streamRawPredict#fim",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_fim",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def complete(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.FIMCompletionRequestStop,
                models.FIMCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.FIMCompletionResponse]:
        r"""Fim Completion

        FIM completion.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request(
            method="POST",
            path="/rawPredict#fim",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="fim_completion_v1_fim_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.FIMCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.FIMCompletionRequestStop,
                models.FIMCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> Optional[models.FIMCompletionResponse]:
        r"""Fim Completion

        FIM completion.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request_async(
            method="POST",
            path="/rawPredict#fim",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="fim_completion_v1_fim_completions_post",
                oauth2_scopes=[],
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, Optional[models.FIMCompletionResponse]
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/httpclient.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

# pyright: reportReturnType = false
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing_extensions import Protocol, runtime_checkable
import httpx
from typing import Any, Optional, Union


@runtime_checkable
class HttpClient(Protocol):
    def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    def close(self) -> None:
        pass


@runtime_checkable
class AsyncHttpClient(Protocol):
    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    async def aclose(self) -> None:
        pass


class ClientOwner(Protocol):
    client: Union[HttpClient, None]
    async_client: Union[AsyncHttpClient, None]


def close_clients(
    owner: ClientOwner,
    sync_client: Union[HttpClient, None],
    sync_client_supplied: bool,
    async_client: Union[AsyncHttpClient, None],
    async_client_supplied: bool,
) -> None:
    """
    A finalizer function that is meant to be used with weakref.finalize to close
    httpx clients used by an SDK so that underlying resources can be garbage
    collected.
    """

    # Unset the client/async_client properties so there are no more references
    # to them from the owning SDK instance and they can be reaped.
    owner.client = None
    owner.async_client = None

    if sync_client is not None and not sync_client_supplied:
        try:
            sync_client.close()
        except Exception:
            pass

    if async_client is not None and not async_client_supplied:
        is_async = False
        try:
            asyncio.get_running_loop()
            is_async = True
        except RuntimeError:
            pass

        try:
            # If this function is called in an async loop then start another
            # loop in a separate thread to close the async http client.
            if is_async:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_client.aclose())
                    future.result()
            else:
                asyncio.run(async_client.aclose())
        except Exception:
            pass
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/sdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import json
import weakref
from typing import Any, Optional, cast

import google.auth
import google.auth.credentials
import google.auth.transport
import google.auth.transport.requests
import httpx

from mistralai_gcp import models
from mistralai_gcp._hooks import BeforeRequestHook, SDKHooks
from mistralai_gcp.chat import Chat
from mistralai_gcp.fim import Fim
from mistralai_gcp.types import UNSET, OptionalNullable

from .basesdk import BaseSDK
from .httpclient import AsyncHttpClient, ClientOwner, HttpClient, close_clients
from .sdkconfiguration import SDKConfiguration
from .utils.logger import Logger, get_default_logger
from .utils.retries import RetryConfig

LEGACY_MODEL_ID_FORMAT = {
    "codestral-2405": "codestral@2405",
    "mistral-large-2407": "mistral-large@2407",
    "mistral-nemo-2407": "mistral-nemo@2407",
}


def get_model_info(model: str) -> tuple[str, str]:
    # if the model requiers the legacy fomat, use it, else do nothing.
    if model in LEGACY_MODEL_ID_FORMAT:
        return "-".join(model.split("-")[:-1]), LEGACY_MODEL_ID_FORMAT[model]
    return model, model


class MistralGoogleCloud(BaseSDK):
    r"""Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it."""

    chat: Chat
    r"""Chat Completion API."""
    fim: Fim
    r"""Fill-in-the-middle API."""

    def __init__(
        self,
        region: str = "europe-west4",
        project_id: Optional[str] = None,
        access_token: Optional[str] = None,
        client: Optional[HttpClient] = None,
        async_client: Optional[AsyncHttpClient] = None,
        retry_config: OptionalNullable[RetryConfig] = UNSET,
        timeout_ms: Optional[int] = None,
        debug_logger: Optional[Logger] = None,
    ) -> None:
        r"""Instantiates the SDK configuring it with the provided parameters.

        :param api_key: The api_key required for authentication
        :param server: The server by name to use for all methods
        :param server_url: The server URL to use for all methods
        :param url_params: Parameters to optionally template the server URL with
        :param client: The HTTP client to use for all synchronous methods
        :param async_client: The Async HTTP client to use for all asynchronous methods
        :param retry_config: The retry configuration to use for all supported methods
        :param timeout_ms: Optional request timeout applied to each operation in milliseconds
        """

        if not access_token:
            credentials, loaded_project_id = google.auth.default(
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )
            credentials.refresh(google.auth.transport.requests.Request())

            if not isinstance(credentials, google.auth.credentials.Credentials):
                raise models.SDKError(
                    "credentials must be an instance of google.auth.credentials.Credentials"
                )

            project_id = project_id or loaded_project_id

        if project_id is None:
            raise models.SDKError("project_id must be provided")

        def auth_token() -> str:
            if access_token:
                return access_token

            credentials.refresh(google.auth.transport.requests.Request())
            token = credentials.token
            if not token:
                raise models.SDKError("Failed to get token from credentials")
            return token

        client_supplied = True
        if client is None:
            client = httpx.Client()
            client_supplied = False

        assert issubclass(
            type(client), HttpClient
        ), "The provided client must implement the HttpClient protocol."

        async_client_supplied = True
        if async_client is None:
            async_client = httpx.AsyncClient()
            async_client_supplied = False

        if debug_logger is None:
            debug_logger = get_default_logger()

        assert issubclass(
            type(async_client), AsyncHttpClient
        ), "The provided async_client must implement the AsyncHttpClient protocol."

        security: Any = None
        if callable(auth_token):
            security = lambda: models.Security(  # pylint: disable=unnecessary-lambda-assignment
                api_key=auth_token()
            )
        else:
            security = models.Security(api_key=auth_token)

        BaseSDK.__init__(
            self,
            SDKConfiguration(
                client=client,
                client_supplied=client_supplied,
                async_client=async_client,
                async_client_supplied=async_client_supplied,
                security=security,
                server_url=f"https://{region}-aiplatform.googleapis.com",
                server=None,
                retry_config=retry_config,
                timeout_ms=timeout_ms,
                debug_logger=debug_logger,
            ),
        )

        hooks = SDKHooks()
        hook = GoogleCloudBeforeRequestHook(region, project_id)
        hooks.register_before_request_hook(hook)
        current_server_url, *_ = self.sdk_configuration.get_server_details()
        server_url, self.sdk_configuration.client = hooks.sdk_init(
            current_server_url, client
        )
        if current_server_url != server_url:
            self.sdk_configuration.server_url = server_url

        # pylint: disable=protected-access
        self.sdk_configuration.__dict__["_hooks"] = hooks

        weakref.finalize(
            self,
            close_clients,
            cast(ClientOwner, self.sdk_configuration),
            self.sdk_configuration.client,
            self.sdk_configuration.client_supplied,
            self.sdk_configuration.async_client,
            self.sdk_configuration.async_client_supplied,
        )

        self._init_sdks()

    def _init_sdks(self):
        self.chat = Chat(self.sdk_configuration)
        self.fim = Fim(self.sdk_configuration)

    def __enter__(self):
        return self

    async def __aenter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.client is not None
            and not self.sdk_configuration.client_supplied
        ):
            self.sdk_configuration.client.close()
        self.sdk_configuration.client = None

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.async_client is not None
            and not self.sdk_configuration.async_client_supplied
        ):
            await self.sdk_configuration.async_client.aclose()
        self.sdk_configuration.async_client = None


class GoogleCloudBeforeRequestHook(BeforeRequestHook):
    def __init__(self, region: str, project_id: str):
        self.region = region
        self.project_id = project_id

    def before_request(
        self, hook_ctx, request: httpx.Request
    ) -> httpx.Request | Exception:
        # The goal of this function is to template in the region, project and model into the URL path
        # We do this here so that the API remains more user-friendly
        model_id = None
        new_content = None
        if request.content:
            parsed = json.loads(request.content.decode("utf-8"))
            model_raw = parsed.get("model")
            model_name, model_id = get_model_info(model_raw)
            parsed["model"] = model_name
            new_content = json.dumps(parsed).encode("utf-8")

        if model_id == "":
            raise models.SDKError("model must be provided")

        stream = "streamRawPredict" in request.url.path
        specifier = "streamRawPredict" if stream else "rawPredict"
        url = f"/v1/projects/{self.project_id}/locations/{self.region}/publishers/mistralai/models/{model_id}:{specifier}"

        headers = dict(request.headers)
        # Delete content-length header as it will need to be recalculated
        headers.pop("content-length", None)

        next_request = httpx.Request(
            method=request.method,
            url=request.url.copy_with(path=url),
            headers=headers,
            content=new_content,
            stream=None,
        )

        return next_request
</file>

<file path="packages/mistralai_gcp/src/mistralai_gcp/sdkconfiguration.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._hooks import SDKHooks
from ._version import (
    __gen_version__,
    __openapi_doc_version__,
    __user_agent__,
    __version__,
)
from .httpclient import AsyncHttpClient, HttpClient
from .utils import Logger, RetryConfig, remove_suffix
from dataclasses import dataclass
from mistralai_gcp import models
from mistralai_gcp.types import OptionalNullable, UNSET
from pydantic import Field
from typing import Callable, Dict, Optional, Tuple, Union


SERVER_EU = "eu"
r"""EU Production server"""
SERVERS = {
    SERVER_EU: "https://api.mistral.ai",
}
"""Contains the list of servers available to the SDK"""


@dataclass
class SDKConfiguration:
    client: Union[HttpClient, None]
    client_supplied: bool
    async_client: Union[AsyncHttpClient, None]
    async_client_supplied: bool
    debug_logger: Logger
    security: Optional[Union[models.Security, Callable[[], models.Security]]] = None
    server_url: Optional[str] = ""
    server: Optional[str] = ""
    language: str = "python"
    openapi_doc_version: str = __openapi_doc_version__
    sdk_version: str = __version__
    gen_version: str = __gen_version__
    user_agent: str = __user_agent__
    retry_config: OptionalNullable[RetryConfig] = Field(default_factory=lambda: UNSET)
    timeout_ms: Optional[int] = None

    def __post_init__(self):
        self._hooks = SDKHooks()

    def get_server_details(self) -> Tuple[str, Dict[str, str]]:
        if self.server_url is not None and self.server_url:
            return remove_suffix(self.server_url, "/"), {}
        if not self.server:
            self.server = SERVER_EU

        if self.server not in SERVERS:
            raise ValueError(f'Invalid server "{self.server}"')

        return SERVERS[self.server], {}

    def get_hooks(self) -> SDKHooks:
        return self._hooks
</file>

<file path="packages/mistralai_gcp/.genignore">
src/mistralai_gcp/sdk.py
README.md
USAGE.md
docs/sdks/**/README.md
</file>

<file path="packages/mistralai_gcp/.gitattributes">
# This allows generated code to be indexed correctly
*.py linguist-generated=false
</file>

<file path="packages/mistralai_gcp/.gitignore">
.speakeasy/reports
README-PYPI.md
.venv/
venv/
src/*.egg-info/
__pycache__/
.pytest_cache/
.python-version
.DS_Store
pyrightconfig.json
</file>

<file path="packages/mistralai_gcp/CONTRIBUTING.md">
# Contributing to This Repository

Thank you for your interest in contributing to this repository. Please note that this repository contains generated code. As such, we do not accept direct changes or pull requests. Instead, we encourage you to follow the guidelines below to report issues and suggest improvements.

## How to Report Issues

If you encounter any bugs or have suggestions for improvements, please open an issue on GitHub. When reporting an issue, please provide as much detail as possible to help us reproduce the problem. This includes:

- A clear and descriptive title
- Steps to reproduce the issue
- Expected and actual behavior
- Any relevant logs, screenshots, or error messages
- Information about your environment (e.g., operating system, software versions)
    - For example can be collected using the `npx envinfo` command from your terminal if you have Node.js installed

## Issue Triage and Upstream Fixes

We will review and triage issues as quickly as possible. Our goal is to address bugs and incorporate improvements in the upstream source code. Fixes will be included in the next generation of the generated code.

## Contact

If you have any questions or need further assistance, please feel free to reach out by opening an issue.

Thank you for your understanding and cooperation!

The Maintainers
</file>

<file path="packages/mistralai_gcp/poetry.toml">
[virtualenvs]
in-project = true
</file>

<file path="packages/mistralai_gcp/py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="packages/mistralai_gcp/pylintrc">
[MAIN]

# Analyse import fallback blocks. This can be used to support both Python 2 and
# 3 compatible code, which means that the block might have code that exists
# only in one or another interpreter, leading to false positives when analysed.
analyse-fallback-blocks=no

# Clear in-memory caches upon conclusion of linting. Useful if running pylint
# in a server-like mode.
clear-cache-post-run=no

# Load and enable all available extensions. Use --list-extensions to see a list
# all available extensions.
#enable-all-extensions=

# In error mode, messages with a category besides ERROR or FATAL are
# suppressed, and no reports are done by default. Error mode is compatible with
# disabling specific errors.
#errors-only=

# Always return a 0 (non-error) status code, even if lint errors are found.
# This is primarily useful in continuous integration scripts.
#exit-zero=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-allow-list=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code. (This is an alternative name to extension-pkg-allow-list
# for backward compatibility.)
extension-pkg-whitelist=

# Return non-zero exit code if any of these messages/categories are detected,
# even if score is above --fail-under value. Syntax same as enable. Messages
# specified are enabled, while categories only check already-enabled messages.
fail-on=

# Specify a score threshold under which the program will exit with error.
fail-under=10

# Interpret the stdin as a python script, whose filename needs to be passed as
# the module_or_package argument.
#from-stdin=

# Files or directories to be skipped. They should be base names, not paths.
ignore=CVS

# Add files or directories matching the regular expressions patterns to the
# ignore-list. The regex matches against paths and can be in Posix or Windows
# format. Because '\\' represents the directory delimiter on Windows systems,
# it can't be used as an escape character.
ignore-paths=

# Files or directories matching the regular expression patterns are skipped.
# The regex matches against base names, not paths. The default value ignores
# Emacs file locks
ignore-patterns=^\.#

# List of module names for which member attributes should not be checked and
# will not be imported (useful for modules/projects where namespaces are
# manipulated during runtime and thus existing member attributes cannot be
# deduced by static analysis). It supports qualified module names, as well as
# Unix pattern matching.
ignored-modules=

# Python code to execute, usually for sys.path manipulation such as
# pygtk.require().
#init-hook=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use, and will cap the count on Windows to
# avoid hangs.
jobs=1

# Control the amount of potential inferred values when inferring a single
# object. This can help the performance when dealing with large functions or
# complex, nested conditions.
limit-inference-results=100

# List of plugins (as comma separated values of python module names) to load,
# usually to register additional checkers.
load-plugins=

# Pickle collected data for later comparisons.
persistent=yes

# Minimum Python version to use for version dependent checks. Will default to
# the version used to run pylint.
py-version=3.9

# Discover python modules and packages in the file system subtree.
recursive=no

# Add paths to the list of the source roots. Supports globbing patterns. The
# source root is an absolute path or a path relative to the current working
# directory used to determine a package namespace for modules located under the
# source root.
source-roots=src

# When enabled, pylint would attempt to guess common misconfiguration and emit
# user-friendly hints instead of false-positive error messages.
suggestion-mode=yes

# Allow loading of arbitrary C extensions. Extensions are imported into the
# active Python interpreter and may run arbitrary code.
unsafe-load-any-extension=no

# In verbose mode, extra non-checker-related info will be displayed.
#verbose=


[BASIC]

# Naming style matching correct argument names.
argument-naming-style=snake_case

# Regular expression matching correct argument names. Overrides argument-
# naming-style. If left empty, argument names will be checked with the set
# naming style.
#argument-rgx=

# Naming style matching correct attribute names.
#attr-naming-style=snake_case

# Regular expression matching correct attribute names. Overrides attr-naming-
# style. If left empty, attribute names will be checked with the set naming
# style.
attr-rgx=[^\W\d][^\W]*|__.*__$

# Bad variable names which should always be refused, separated by a comma.
bad-names=

# Bad variable names regexes, separated by a comma. If names match any regex,
# they will always be refused
bad-names-rgxs=

# Naming style matching correct class attribute names.
class-attribute-naming-style=any

# Regular expression matching correct class attribute names. Overrides class-
# attribute-naming-style. If left empty, class attribute names will be checked
# with the set naming style.
#class-attribute-rgx=

# Naming style matching correct class constant names.
class-const-naming-style=UPPER_CASE

# Regular expression matching correct class constant names. Overrides class-
# const-naming-style. If left empty, class constant names will be checked with
# the set naming style.
#class-const-rgx=

# Naming style matching correct class names.
class-naming-style=PascalCase

# Regular expression matching correct class names. Overrides class-naming-
# style. If left empty, class names will be checked with the set naming style.
#class-rgx=

# Naming style matching correct constant names.
const-naming-style=UPPER_CASE

# Regular expression matching correct constant names. Overrides const-naming-
# style. If left empty, constant names will be checked with the set naming
# style.
#const-rgx=

# Minimum line length for functions/classes that require docstrings, shorter
# ones are exempt.
docstring-min-length=-1

# Naming style matching correct function names.
function-naming-style=snake_case

# Regular expression matching correct function names. Overrides function-
# naming-style. If left empty, function names will be checked with the set
# naming style.
#function-rgx=

# Good variable names which should always be accepted, separated by a comma.
good-names=i,
           j,
           k,
           ex,
           Run,
           _,
           e,
           n,
           id

# Good variable names regexes, separated by a comma. If names match any regex,
# they will always be accepted
good-names-rgxs=

# Include a hint for the correct naming format with invalid-name.
include-naming-hint=no

# Naming style matching correct inline iteration names.
inlinevar-naming-style=any

# Regular expression matching correct inline iteration names. Overrides
# inlinevar-naming-style. If left empty, inline iteration names will be checked
# with the set naming style.
#inlinevar-rgx=

# Naming style matching correct method names.
method-naming-style=snake_case

# Regular expression matching correct method names. Overrides method-naming-
# style. If left empty, method names will be checked with the set naming style.
#method-rgx=

# Naming style matching correct module names.
module-naming-style=snake_case

# Regular expression matching correct module names. Overrides module-naming-
# style. If left empty, module names will be checked with the set naming style.
#module-rgx=

# Colon-delimited sets of names that determine each other's naming style when
# the name regexes allow several styles.
name-group=

# Regular expression which should only match function or class names that do
# not require a docstring.
no-docstring-rgx=^_

# List of decorators that produce properties, such as abc.abstractproperty. Add
# to this list to register other decorators that produce valid properties.
# These decorators are taken in consideration only for invalid-name.
property-classes=abc.abstractproperty

# Regular expression matching correct type alias names. If left empty, type
# alias names will be checked with the set naming style.
typealias-rgx=.*

# Regular expression matching correct type variable names. If left empty, type
# variable names will be checked with the set naming style.
#typevar-rgx=

# Naming style matching correct variable names.
variable-naming-style=snake_case

# Regular expression matching correct variable names. Overrides variable-
# naming-style. If left empty, variable names will be checked with the set
# naming style.
#variable-rgx=


[CLASSES]

# Warn about protected attribute access inside special methods
check-protected-access-in-special-methods=no

# List of method names used to declare (i.e. assign) instance attributes.
defining-attr-methods=__init__,
                      __new__,
                      setUp,
                      asyncSetUp,
                      __post_init__

# List of member names, which should be excluded from the protected access
# warning.
exclude-protected=_asdict,_fields,_replace,_source,_make,os._exit

# List of valid names for the first argument in a class method.
valid-classmethod-first-arg=cls

# List of valid names for the first argument in a metaclass class method.
valid-metaclass-classmethod-first-arg=mcs


[DESIGN]

# List of regular expressions of class ancestor names to ignore when counting
# public methods (see R0903)
exclude-too-few-public-methods=

# List of qualified class names to ignore when counting class parents (see
# R0901)
ignored-parents=

# Maximum number of arguments for function / method.
max-args=5

# Maximum number of attributes for a class (see R0902).
max-attributes=7

# Maximum number of boolean expressions in an if statement (see R0916).
max-bool-expr=5

# Maximum number of branch for function / method body.
max-branches=12

# Maximum number of locals for function / method body.
max-locals=15

# Maximum number of parents for a class (see R0901).
max-parents=7

# Maximum number of public methods for a class (see R0904).
max-public-methods=25

# Maximum number of return / yield for function / method body.
max-returns=6

# Maximum number of statements in function / method body.
max-statements=50

# Minimum number of public methods for a class (see R0903).
min-public-methods=2


[EXCEPTIONS]

# Exceptions that will emit a warning when caught.
overgeneral-exceptions=builtins.BaseException,builtins.Exception


[FORMAT]

# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
expected-line-ending-format=

# Regexp for a line that is allowed to be longer than the limit.
ignore-long-lines=^\s*(# )?<?https?://\S+>?$

# Number of spaces of indent required inside a hanging or continued line.
indent-after-paren=4

# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
# tab).
indent-string='    '

# Maximum number of characters on a single line.
max-line-length=100

# Maximum number of lines in a module.
max-module-lines=1000

# Allow the body of a class to be on the same line as the declaration if body
# contains single statement.
single-line-class-stmt=no

# Allow the body of an if to be on the same line as the test if there is no
# else.
single-line-if-stmt=no


[IMPORTS]

# List of modules that can be imported at any level, not just the top level
# one.
allow-any-import-level=

# Allow explicit reexports by alias from a package __init__.
allow-reexport-from-package=no

# Allow wildcard imports from modules that define __all__.
allow-wildcard-with-all=no

# Deprecated modules which should not be used, separated by a comma.
deprecated-modules=

# Output a graph (.gv or any supported image format) of external dependencies
# to the given file (report RP0402 must not be disabled).
ext-import-graph=

# Output a graph (.gv or any supported image format) of all (i.e. internal and
# external) dependencies to the given file (report RP0402 must not be
# disabled).
import-graph=

# Output a graph (.gv or any supported image format) of internal dependencies
# to the given file (report RP0402 must not be disabled).
int-import-graph=

# Force import order to recognize a module as part of the standard
# compatibility libraries.
known-standard-library=

# Force import order to recognize a module as part of a third party library.
known-third-party=enchant

# Couples of modules and preferred modules, separated by a comma.
preferred-modules=


[LOGGING]

# The type of string formatting that logging methods do. `old` means using %
# formatting, `new` is for `{}` formatting.
logging-format-style=old

# Logging modules to check that the string format arguments are in logging
# function parameter format.
logging-modules=logging


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
# UNDEFINED.
confidence=HIGH,
           CONTROL_FLOW,
           INFERENCE,
           INFERENCE_FAILURE,
           UNDEFINED

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then re-enable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=raw-checker-failed,
        bad-inline-option,
        locally-disabled,
        file-ignored,
        suppressed-message,
        useless-suppression,
        deprecated-pragma,
        use-implicit-booleaness-not-comparison-to-string,
        use-implicit-booleaness-not-comparison-to-zero,
        use-symbolic-message-instead,
        trailing-whitespace,
        line-too-long,
        missing-class-docstring,
        missing-module-docstring,
        missing-function-docstring,
        too-many-instance-attributes,
        wrong-import-order,
        too-many-arguments,
        broad-exception-raised,
        too-few-public-methods,
        too-many-branches,
        duplicate-code,
        trailing-newlines,
        too-many-public-methods,
        too-many-locals,
        too-many-lines,
        using-constant-test,
        too-many-statements,
        cyclic-import,
        too-many-nested-blocks,
        too-many-boolean-expressions,
        no-else-raise,
        bare-except,
        broad-exception-caught,
        fixme,
        relative-beyond-top-level,
        consider-using-with,
        wildcard-import,
        unused-wildcard-import

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=


[METHOD_ARGS]

# List of qualified names (i.e., library.method) which require a timeout
# parameter e.g. 'requests.api.get,requests.api.post'
timeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request


[MISCELLANEOUS]

# List of note tags to take in consideration, separated by a comma.
notes=FIXME,
      XXX,
      TODO

# Regular expression of note tags to take in consideration.
notes-rgx=


[REFACTORING]

# Maximum number of nested blocks for function / method body
max-nested-blocks=5

# Complete name of functions that never returns. When checking for
# inconsistent-return-statements if a never returning function is called then
# it will be considered as an explicit return statement and no message will be
# printed.
never-returning-functions=sys.exit,argparse.parse_error


[REPORTS]

# Python expression which should return a score less than or equal to 10. You
# have access to the variables 'fatal', 'error', 'warning', 'refactor',
# 'convention', and 'info' which contain the number of messages in each
# category, as well as 'statement' which is the total number of statements
# analyzed. This score is used by the global evaluation report (RP0004).
evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))

# Template used to display messages. This is a python new-style format string
# used to format the message information. See doc for all details.
msg-template=

# Set the output format. Available formats are: text, parseable, colorized,
# json2 (improved json format), json (old json format) and msvs (visual
# studio). You can also give a reporter class, e.g.
# mypackage.mymodule.MyReporterClass.
#output-format=

# Tells whether to display a full report or only the messages.
reports=no

# Activate the evaluation score.
score=yes


[SIMILARITIES]

# Comments are removed from the similarity computation
ignore-comments=yes

# Docstrings are removed from the similarity computation
ignore-docstrings=yes

# Imports are removed from the similarity computation
ignore-imports=yes

# Signatures are removed from the similarity computation
ignore-signatures=yes

# Minimum lines number of a similarity.
min-similarity-lines=4


[SPELLING]

# Limits count of emitted suggestions for spelling mistakes.
max-spelling-suggestions=4

# Spelling dictionary name. No available dictionaries : You need to install
# both the python package and the system dependency for enchant to work.
spelling-dict=

# List of comma separated words that should be considered directives if they
# appear at the beginning of a comment and should not be checked.
spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:

# List of comma separated words that should not be checked.
spelling-ignore-words=

# A path to a file that contains the private dictionary; one word per line.
spelling-private-dict-file=

# Tells whether to store unknown words to the private dictionary (see the
# --spelling-private-dict-file option) instead of raising a message.
spelling-store-unknown-words=no


[STRING]

# This flag controls whether inconsistent-quotes generates a warning when the
# character used as a quote delimiter is used inconsistently within a module.
check-quote-consistency=no

# This flag controls whether the implicit-str-concat should generate a warning
# on implicit string concatenation in sequences defined over several lines.
check-str-concat-over-line-jumps=no


[TYPECHECK]

# List of decorators that produce context managers, such as
# contextlib.contextmanager. Add to this list to register other decorators that
# produce valid context managers.
contextmanager-decorators=contextlib.contextmanager

# List of members which are set dynamically and missed by pylint inference
# system, and so shouldn't trigger E1101 when accessed. Python regular
# expressions are accepted.
generated-members=

# Tells whether to warn about missing members when the owner of the attribute
# is inferred to be None.
ignore-none=yes

# This flag controls whether pylint should warn about no-member and similar
# checks whenever an opaque object is returned when inferring. The inference
# can return multiple potential results while evaluating a Python object, but
# some branches might not be evaluated, which results in partial inference. In
# that case, it might be useful to still emit no-member and other checks for
# the rest of the inferred objects.
ignore-on-opaque-inference=yes

# List of symbolic message names to ignore for Mixin members.
ignored-checks-for-mixins=no-member,
                          not-async-context-manager,
                          not-context-manager,
                          attribute-defined-outside-init

# List of class names for which member attributes should not be checked (useful
# for classes with dynamically set attributes). This supports the use of
# qualified names.
ignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace

# Show a hint with possible names when a member name was not found. The aspect
# of finding the hint is based on edit distance.
missing-member-hint=yes

# The minimum edit distance a name should have in order to be considered a
# similar match for a missing member name.
missing-member-hint-distance=1

# The total number of similar names that should be taken in consideration when
# showing a hint for a missing member.
missing-member-max-choices=1

# Regex pattern to define which classes are considered mixins.
mixin-class-rgx=.*[Mm]ixin

# List of decorators that change the signature of a decorated function.
signature-mutators=


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of names allowed to shadow builtins
allowed-redefined-builtins=id,object

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored.
ignored-argument-names=_.*|^ignored_|^unused_

# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
</file>

<file path="packages/mistralai_gcp/pyproject.toml">
[project]
name = "mistralai-gcp"
version = "1.6.0"
description = "Python Client SDK for the Mistral AI API in GCP."
authors = [{ name = "Mistral" },]
readme = "README-PYPI.md"
requires-python = ">=3.9"
dependencies = [
    "eval-type-backport >=0.2.0",
    "google-auth (>=2.31.0,<3.0.0)",
    "httpx >=0.28.1",
    "pydantic >=2.10.3",
    "python-dateutil >=2.8.2",
    "requests (>=2.32.3,<3.0.0)",
    "typing-inspection >=0.4.0",
]

[tool.poetry]
packages = [
    { include = "mistralai_gcp", from = "src" }
]
include = ["py.typed", "src/mistralai_gcp/py.typed"]

[tool.setuptools.package-data]
"*" = ["py.typed", "src/mistralai_gcp/py.typed"]

[virtualenvs]
in-project = true

[tool.poetry.group.dev.dependencies]
mypy = "==1.14.1"
pylint = "==3.2.3"
pytest = "^8.2.2"
pytest-asyncio = "^0.23.7"
types-python-dateutil = "^2.9.0.20240316"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["src"]

[tool.mypy]
disable_error_code = "misc"

[[tool.mypy.overrides]]
module = "typing_inspect"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "jsonpath"
ignore_missing_imports = true

[tool.pyright]
venvPath = "."
venv = ".venv"
</file>

<file path="packages/mistralai_gcp/README.md">
# Mistral on GCP Python Client


**Prerequisites**

Before you begin, you will need to create a Google Cloud project and enable the Mistral API. To do this, follow the instructions [here](https://docs.mistral.ai/deployment/cloud/vertex/).

To run this locally you will also need to ensure you are authenticated with Google Cloud. You can do this by running

```bash
gcloud auth application-default login
```

## SDK Installation

Install the extras dependencies specific to Google Cloud:

```bash
pip install mistralai[gcp]
```

<!-- Start SDK Example Usage [usage] -->
## SDK Example Usage

### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai_gcp import MistralGCP
import os
)


res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="mistral-small-latest")

if res is not None:
    # handle response
    pass
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai_gcp import MistralGCP
import os

async def main():
    s = MistralGCP(
        api_key=os.getenv("API_KEY", ""),
    )
    res = await s.chat.complete_async(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], model="mistral-small-latest")
    if res is not None:
        # handle response
        pass

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->

<!-- Start Available Resources and Operations [operations] -->
## Available Resources and Operations

### [chat](docs/sdks/chat/README.md)

* [stream](docs/sdks/chat/README.md#stream) - Stream chat completion
* [create](docs/sdks/chat/README.md#create) - Chat Completion

### [fim](docs/sdks/fim/README.md)

* [stream](docs/sdks/fim/README.md#stream) - Stream fim completion
* [create](docs/sdks/fim/README.md#create) - Fim Completion
<!-- End Available Resources and Operations [operations] -->

<!-- Start Server-sent event streaming [eventstream] -->
## Server-sent event streaming

[Server-sent events][mdn-sse] are used to stream content from certain
operations. These operations will expose the stream as [Generator][generator] that
can be consumed using a simple `for` loop. The loop will
terminate when the server no longer has any events to send and closes the
underlying connection.

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.chat.stream(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="mistral-small-latest")

if res is not None:
    for event in res:
        # handle event
        print(event)

```

[mdn-sse]: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events
[generator]: https://wiki.python.org/moin/Generators
<!-- End Server-sent event streaming [eventstream] -->

<!-- Start Retries [retries] -->
## Retries

Some of the endpoints in this SDK support retries. If you use the SDK without any configuration, it will fall back to the default retry strategy provided by the API. However, the default retry strategy can be overridden on a per-operation basis, or across the entire SDK.

To change the default retry strategy for a single API call, simply provide a `RetryConfig` object to the call:
```python
from mistralai_gcp import MistralGCP
from mistralgcp.utils import BackoffStrategy, RetryConfig
import os

s = MistralGCP()


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="mistral-small-latest",
    retries=RetryConfig(
        "backoff",
        BackoffStrategy(1, 50, 1.1, 100),
        False
    )
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```

If you'd like to override the default retry strategy for all operations that support retries, you can use the `retry_config` optional parameter when initializing the SDK:
```python
from mistralai_gcp import MistralGCP
from mistralgcp.utils import BackoffStrategy, RetryConfig
import os

s = MistralGCP(
    retry_config=RetryConfig("backoff", BackoffStrategy(1, 50, 1.1, 100), False),
)


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ],
    model="mistral-small-latest"
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Retries [retries] -->

<!-- Start Error Handling [errors] -->
## Error Handling

Handling errors in this SDK should largely match your expectations.  All operations return a response object or raise an error.  If Error objects are specified in your OpenAPI Spec, the SDK will raise the appropriate Error type.

| Error Object               | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4xx-5xx     | */*              |

### Example

```python
from mistralai_gcp import MistralGCP, models
import os

s = MistralGCP()

res = None
try:
    res = s.chat.complete(
        messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ],
        model="mistral-small-latest"
    )

except models.HTTPValidationError as e:
    # handle exception
    raise(e)
except models.SDKError as e:
    # handle exception
    raise(e)

if res is not None:
    # handle response
    pass

```
<!-- End Error Handling [errors] -->

<!-- Start Server Selection [server] -->
## Server Selection

### Select Server by Name

You can override the default server globally by passing a server name to the `server: str` optional parameter when initializing the SDK client instance. The selected server will then be used as the default on the operations that use it. This table lists the names associated with the available servers:

| Name   | Server                   | Variables |
| ------ | ------------------------ | --------- |
| `prod` | `https://api.mistral.ai` | None      |

#### Example

```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP(server="prod")


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="mistral-small-latest"
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```


### Override Server URL Per-Client

The default server can also be overridden globally by passing a URL to the `server_url: str` optional parameter when initializing the SDK client instance. For example:
```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP(
    server_url="https://api.mistral.ai",
)


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="mistral-small-latest"
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Server Selection [server] -->

<!-- Start Custom HTTP Client [http-client] -->
## Custom HTTP Client

The Python SDK makes API calls using the [httpx](https://www.python-httpx.org/) HTTP library.  In order to provide a convenient way to configure timeouts, cookies, proxies, custom headers, and other low-level configuration, you can initialize the SDK client with your own HTTP client instance.
Depending on whether you are using the sync or async version of the SDK, you can pass an instance of `HttpClient` or `AsyncHttpClient` respectively, which are Protocol's ensuring that the client has the necessary methods to make API calls.
This allows you to wrap the client with your own custom logic, such as adding custom headers, logging, or error handling, or you can just pass an instance of `httpx.Client` or `httpx.AsyncClient` directly.

For example, you could specify a header for every request that this sdk makes as follows:
```python
from mistralai_gcp import MistralGCP
import httpx

http_client = httpx.Client(headers={"x-custom-header": "someValue"})
s = MistralGCP(client=http_client)
```

or you could wrap the client with your own custom logic:
```python
from mistralai_gcp import MistralGCP
from mistralai_gcp.httpclient import AsyncHttpClient
import httpx

class CustomClient(AsyncHttpClient):
    client: AsyncHttpClient

    def __init__(self, client: AsyncHttpClient):
        self.client = client

    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        request.headers["Client-Level-Header"] = "added by client"

        return await self.client.send(
            request, stream=stream, auth=auth, follow_redirects=follow_redirects
        )

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        return self.client.build_request(
            method,
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )

s = MistralGCP(async_client=CustomClient(httpx.AsyncClient()))
```
<!-- End Custom HTTP Client [http-client] -->

<!-- Start Authentication [security] -->
## Authentication

### Per-Client Security Schemes

This SDK supports the following security scheme globally:

| Name      | Type | Scheme      |
| --------- | ---- | ----------- |
| `api_key` | http | HTTP Bearer |

To authenticate with the API the `api_key` parameter must be set when initializing the SDK client instance. For example:
```python
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.chat.stream(
    messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], 
    model="mistral-small-latest"
)

if res is not None:
    for event in res:
        # handle event
        print(event)

```
<!-- End Authentication [security] -->

<!-- Placeholder for Future Speakeasy SDK Sections -->

# Development

## Contributions

While we value open-source contributions to this SDK, this library is generated programmatically. Any manual changes added to internal files will be overwritten on the next generation. 
We look forward to hearing your feedback. Feel free to open a PR or an issue with a proof of concept and we'll do our best to include it in a future release.
</file>

<file path="packages/mistralai_gcp/USAGE.md">
<!-- Start SDK Example Usage [usage] -->
### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai_gcp import MistralGCP
import os

s = MistralGCP()


res = s.chat.complete(messages=[
    {
        "content": "Who is the best French painter? Answer in one short sentence.",
        "role": "user",
    },
], model="mistral-small-latest")

if res is not None:
    # handle response
    pass
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai_gcp import MistralGCP
import os

async def main():
    s = MistralGCP(
        api_key=os.getenv("API_KEY", ""),
    )
    res = await s.chat.complete_async(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], model="mistral-small-latest")
    if res is not None:
        # handle response
        pass

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->
</file>

<file path="scripts/lint_custom_code.sh">
#!/usr/bin/env bash

set -e

ERRORS=0

echo "Running mypy..."
# TODO: Uncomment once the examples are fixed
# poetry run mypy examples/ || ERRORS=1
echo "-> running on extra"
poetry run mypy src/mistralai/extra/ || ERRORS=1
echo "-> running on hooks"
poetry run mypy src/mistralai/_hooks/ \
--exclude __init__.py --exclude sdkhooks.py --exclude types.py || ERRORS=1

echo "Running pyright..."
# TODO: Uncomment once the examples are fixed
# poetry run pyright examples/ || ERRORS=1
echo "-> running on extra"
poetry run pyright src/mistralai/extra/ || ERRORS=1
echo "-> running on hooks"
poetry run pyright src/mistralai/_hooks/ || ERRORS=1

echo "Running ruff..."
echo "-> running on examples"
poetry run ruff check examples/ || ERRORS=1
echo "-> running on extra"
poetry run ruff check src/mistralai/extra/ || ERRORS=1
echo "-> running on hooks"
poetry run ruff check src/mistralai/_hooks/ \
--exclude __init__.py --exclude sdkhooks.py --exclude types.py || ERRORS=1

if [ "$ERRORS" -ne 0 ]; then
echo " One or more linters failed"
exit 1
else
echo " All linters passed"
fi
</file>

<file path="scripts/prepare_readme.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import re
import shutil

try:
    with open("README.md", "r", encoding="utf-8") as rh:
        readme_contents = rh.read()
        GITHUB_URL = "https://github.com/mistralai/client-python.git"
        GITHUB_URL = (
            GITHUB_URL[: -len(".git")] if GITHUB_URL.endswith(".git") else GITHUB_URL
        )
        # links on PyPI should have absolute URLs
        readme_contents = re.sub(
            r"(\[[^\]]+\]\()((?!https?:)[^\)]+)(\))",
            lambda m: m.group(1)
            + GITHUB_URL
            + "/blob/master/"
            + m.group(2)
            + m.group(3),
            readme_contents,
        )

        with open("README-PYPI.md", "w", encoding="utf-8") as wh:
            wh.write(readme_contents)
except Exception as e:
    try:
        print("Failed to rewrite README.md to README-PYPI.md, copying original instead")
        print(e)
        shutil.copyfile("README.md", "README-PYPI.md")
    except Exception as ie:
        print("Failed to copy README.md to README-PYPI.md")
        print(ie)
</file>

<file path="scripts/publish.sh">
#!/usr/bin/env bash

export POETRY_PYPI_TOKEN_PYPI=${PYPI_TOKEN}

poetry run python scripts/prepare_readme.py

poetry publish --build --skip-existing
</file>

<file path="scripts/run_examples.sh">
#!/bin/bash

# List of files to exclude
exclude_files=(
 "examples/chatbot_with_streaming.py"
 "examples/async_conversation_run_mcp_remote_auth.py"
)

# Check if the first argument is "no-extra-dep" then remove all the files that require the extra dependencies
if [ "$1" = "--no-extra-dep" ]; then
    # Add more files to the exclude list
    exclude_files+=(
      "examples/async_conversation_run_mcp_remote.py"
      "examples/async_conversation_run_mcp.py"
      "examples/async_conversation_run_stream.py"
      "examples/async_conversation_run.py"
    )
fi

failed=0

for file in examples/*.py; do
    # Check if the file is not in the exclude list
    if [ -f "$file" ] && [[ ! " ${exclude_files[@]} " =~ " $file " ]]; then
        echo "Running $file"
        # Run the script and capture the exit status
        if python3 "$file" > /dev/null; then
            echo "Success"
        else
            echo "Failed"
            failed=1
        fi
    else
      echo "Skipped $file"
    fi
done

# If one of the example scripts failed, then exit
if [ $failed -ne 0 ]; then
    exit 1
fi
</file>

<file path="src/mistralai/_hooks/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkhooks import *
from .types import *
from .registration import *
</file>

<file path="src/mistralai/_hooks/custom_user_agent.py">
# MAKE SURE YOU UPDATE THE COPIES OF THIS FILES IN THE PROVIDERS'S PACKAGES WHEN YOU MAKE CHANGES HERE
from typing import Union

import httpx

from .types import BeforeRequestContext, BeforeRequestHook

PREFIX = "mistral-client-python/"

class CustomUserAgentHook(BeforeRequestHook):
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        current = request.headers["user-agent"]
        if current.startswith(PREFIX):
            return request
        
        request.headers["user-agent"] = (
            PREFIX + current.split(" ")[1]
        )

        return request
</file>

<file path="src/mistralai/_hooks/deprecation_warning.py">
import logging
from typing import Union

import httpx

from .types import AfterSuccessContext, AfterSuccessHook

logger = logging.getLogger(__name__)

HEADER_MODEL_DEPRECATION_TIMESTAMP = "x-model-deprecation-timestamp"


class DeprecationWarningHook(AfterSuccessHook):

    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> Union[httpx.Response, Exception]:
        if HEADER_MODEL_DEPRECATION_TIMESTAMP in response.headers:
            model = response.json()["model"]
            # pylint: disable=logging-fstring-interpolation
            logger.warning(
                "WARNING: The model %s is deprecated and will be removed on %s. Please refer to https://docs.mistral.ai/getting-started/models/#api-versioning for more information.",
                model,
                response.headers[HEADER_MODEL_DEPRECATION_TIMESTAMP],
            )
        return response
</file>

<file path="src/mistralai/_hooks/registration.py">
from .custom_user_agent import CustomUserAgentHook
from .deprecation_warning import DeprecationWarningHook
from .types import Hooks

# This file is only ever generated once on the first generation and then is free to be modified.
# Any hooks you wish to add should be registered in the init_hooks function. Feel free to define them
# in this file or in separate files in the hooks folder.


def init_hooks(hooks: Hooks):
    # pylint: disable=unused-argument
    """Add hooks by calling hooks.register{sdk_init/before_request/after_success/after_error}Hook
    with an instance of a hook that implements that specific Hook interface
    Hooks are registered per SDK instance, and are valid for the lifetime of the SDK instance
    """
    hooks.register_before_request_hook(CustomUserAgentHook())
    hooks.register_after_success_hook(DeprecationWarningHook())
</file>

<file path="src/mistralai/_hooks/sdkhooks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
from .types import (
    SDKInitHook,
    BeforeRequestContext,
    BeforeRequestHook,
    AfterSuccessContext,
    AfterSuccessHook,
    AfterErrorContext,
    AfterErrorHook,
    Hooks,
)
from .registration import init_hooks
from typing import List, Optional, Tuple
from mistralai.httpclient import HttpClient


class SDKHooks(Hooks):
    def __init__(self) -> None:
        self.sdk_init_hooks: List[SDKInitHook] = []
        self.before_request_hooks: List[BeforeRequestHook] = []
        self.after_success_hooks: List[AfterSuccessHook] = []
        self.after_error_hooks: List[AfterErrorHook] = []
        init_hooks(self)

    def register_sdk_init_hook(self, hook: SDKInitHook) -> None:
        self.sdk_init_hooks.append(hook)

    def register_before_request_hook(self, hook: BeforeRequestHook) -> None:
        self.before_request_hooks.append(hook)

    def register_after_success_hook(self, hook: AfterSuccessHook) -> None:
        self.after_success_hooks.append(hook)

    def register_after_error_hook(self, hook: AfterErrorHook) -> None:
        self.after_error_hooks.append(hook)

    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        for hook in self.sdk_init_hooks:
            base_url, client = hook.sdk_init(base_url, client)
        return base_url, client

    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> httpx.Request:
        for hook in self.before_request_hooks:
            out = hook.before_request(hook_ctx, request)
            if isinstance(out, Exception):
                raise out
            request = out

        return request

    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> httpx.Response:
        for hook in self.after_success_hooks:
            out = hook.after_success(hook_ctx, response)
            if isinstance(out, Exception):
                raise out
            response = out
        return response

    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Tuple[Optional[httpx.Response], Optional[Exception]]:
        for hook in self.after_error_hooks:
            result = hook.after_error(hook_ctx, response, error)
            if isinstance(result, Exception):
                raise result
            response, error = result
        return response, error
</file>

<file path="src/mistralai/_hooks/types.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from abc import ABC, abstractmethod
import httpx
from mistralai.httpclient import HttpClient
from typing import Any, Callable, List, Optional, Tuple, Union


class HookContext:
    base_url: str
    operation_id: str
    oauth2_scopes: Optional[List[str]] = None
    security_source: Optional[Union[Any, Callable[[], Any]]] = None

    def __init__(
        self,
        base_url: str,
        operation_id: str,
        oauth2_scopes: Optional[List[str]],
        security_source: Optional[Union[Any, Callable[[], Any]]],
    ):
        self.base_url = base_url
        self.operation_id = operation_id
        self.oauth2_scopes = oauth2_scopes
        self.security_source = security_source


class BeforeRequestContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterSuccessContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class AfterErrorContext(HookContext):
    def __init__(self, hook_ctx: HookContext):
        super().__init__(
            hook_ctx.base_url,
            hook_ctx.operation_id,
            hook_ctx.oauth2_scopes,
            hook_ctx.security_source,
        )


class SDKInitHook(ABC):
    @abstractmethod
    def sdk_init(self, base_url: str, client: HttpClient) -> Tuple[str, HttpClient]:
        pass


class BeforeRequestHook(ABC):
    @abstractmethod
    def before_request(
        self, hook_ctx: BeforeRequestContext, request: httpx.Request
    ) -> Union[httpx.Request, Exception]:
        pass


class AfterSuccessHook(ABC):
    @abstractmethod
    def after_success(
        self, hook_ctx: AfterSuccessContext, response: httpx.Response
    ) -> Union[httpx.Response, Exception]:
        pass


class AfterErrorHook(ABC):
    @abstractmethod
    def after_error(
        self,
        hook_ctx: AfterErrorContext,
        response: Optional[httpx.Response],
        error: Optional[Exception],
    ) -> Union[Tuple[Optional[httpx.Response], Optional[Exception]], Exception]:
        pass


class Hooks(ABC):
    @abstractmethod
    def register_sdk_init_hook(self, hook: SDKInitHook):
        pass

    @abstractmethod
    def register_before_request_hook(self, hook: BeforeRequestHook):
        pass

    @abstractmethod
    def register_after_success_hook(self, hook: AfterSuccessHook):
        pass

    @abstractmethod
    def register_after_error_hook(self, hook: AfterErrorHook):
        pass
</file>

<file path="src/mistralai/extra/mcp/auth.py">
from typing import Optional

from authlib.oauth2.rfc8414 import AuthorizationServerMetadata
from authlib.integrations.httpx_client import AsyncOAuth2Client as AsyncOAuth2ClientBase
import httpx
import logging

from mistralai.types import BaseModel

logger = logging.getLogger(__name__)


class Oauth2AuthorizationScheme(BaseModel):
    """Information about the oauth flow to perform with the authorization server."""

    authorization_url: str
    token_url: str
    scope: list[str]
    description: Optional[str] = None
    refresh_url: Optional[str] = None


class OAuthParams(BaseModel):
    """Required params for authorization."""

    scheme: Oauth2AuthorizationScheme
    client_id: str
    client_secret: str


class AsyncOAuth2Client(AsyncOAuth2ClientBase):
    """Subclass of the Async httpx oauth client which provides a constructor from OAuthParams."""

    @classmethod
    def from_oauth_params(cls, oauth_params: OAuthParams) -> "AsyncOAuth2Client":
        return cls(
            client_id=oauth_params.client_id,
            client_secret=oauth_params.client_secret,
            scope=oauth_params.scheme.scope,
        )


async def get_well_known_authorization_server_metadata(
    server_url: str,
) -> Optional[AuthorizationServerMetadata]:
    """Fetch the metadata from the well-known location.

    This should be available on MCP servers as described by the specification:
    https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization#2-3-server-metadata-discovery.
    """
    well_known_url = f"{server_url}/.well-known/oauth-authorization-server"
    response = await httpx.AsyncClient().get(well_known_url)
    if 200 <= response.status_code < 300:
        try:
            server_metadata = AuthorizationServerMetadata(**response.json())
            server_metadata.validate()
            return server_metadata
        except ValueError:
            logger.exception("Failed to parse oauth well-known metadata")
            return None
    else:
        logger.error(f"Failed to get oauth well-known metadata from {server_url}")
        return None


async def get_oauth_server_metadata(server_url: str) -> AuthorizationServerMetadata:
    """Fetch the metadata from the authorization server to perform the oauth flow."""
    # 1) attempt to get the metadata from the resource server at /.well-known/oauth-protected-resource
    # TODO: new self-discovery protocol, not released yet

    # 2) attempt to get the metadata from the authorization server at /.well-known/oauth-authorization-server
    metadata = await get_well_known_authorization_server_metadata(server_url=server_url)
    if metadata is not None:
        return metadata

    # 3) fallback on default endpoints
    # https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization#2-3-3-fallbacks-for-servers-without-metadata-discovery
    return AuthorizationServerMetadata(
        issuer=server_url,
        authorization_endpoint=f"{server_url}/authorize",
        token_endpoint=f"{server_url}/token",
        register_endpoint=f"{server_url}/register",
        response_types_supported=["code"],
        response_modes_supported=["query"],
        grant_types_supported=["authorization_code", "refresh_token"],
        token_endpoint_auth_methods_supported=["client_secret_basic"],
        code_challenge_methods_supported=["S256", "plain"],
    )


async def dynamic_client_registration(
    register_endpoint: str,
    redirect_url: str,
    async_client: httpx.AsyncClient,
) -> tuple[str, str]:
    """Try to register the client dynamically with an MCP server.

    Returns a client_id and client_secret.
    """
    # Construct the registration request payload
    registration_payload = {
        "client_name": "MistralSDKClient",
        "grant_types": ["authorization_code", "refresh_token"],
        "token_endpoint_auth_method": "client_secret_basic",
        "response_types": ["code"],
        "redirect_uris": [redirect_url],
    }

    # Make the registration request
    response = await async_client.post(register_endpoint, json=registration_payload)
    try:
        response.raise_for_status()
        registration_info = response.json()
        client_id = registration_info["client_id"]
        client_secret = registration_info["client_secret"]
    except Exception as e:
        raise ValueError(
            f"Client registration failed: status={response.status_code}, error={response.text}"
        ) from e
    return client_id, client_secret


async def build_oauth_params(
    server_url: str,
    redirect_url: str,
    client_id: Optional[str] = None,
    client_secret: Optional[str] = None,
    scope: Optional[list[str]] = None,
    async_client: Optional[httpx.AsyncClient] = None,
) -> OAuthParams:
    """Get issuer metadata and build the oauth required params."""
    metadata = await get_oauth_server_metadata(server_url=server_url)
    oauth_scheme = Oauth2AuthorizationScheme(
        authorization_url=metadata.authorization_endpoint,
        token_url=metadata.token_endpoint,
        scope=scope or [],
        refresh_url=metadata.token_endpoint
        if "refresh_token" in metadata.grant_types_supported
        else None,
    )
    if client_id and client_secret:
        return OAuthParams(
            client_id=client_id,
            client_secret=client_secret,
            scheme=oauth_scheme,
        )

    # Try to dynamically register the client
    if async_client:
        reg_client_id, reg_client_secret = await dynamic_client_registration(
            register_endpoint=metadata.registration_endpoint,
            redirect_url=redirect_url,
            async_client=async_client,
        )
    else:
        async with httpx.AsyncClient() as async_client:
            reg_client_id, reg_client_secret = await dynamic_client_registration(
                register_endpoint=metadata.registration_endpoint,
                redirect_url=redirect_url,
                async_client=async_client,
            )
    return OAuthParams(
        client_id=reg_client_id,
        client_secret=reg_client_secret,
        scheme=oauth_scheme,
    )
</file>

<file path="src/mistralai/extra/mcp/base.py">
from typing import Optional, Union
import logging
import typing
from contextlib import AsyncExitStack
from typing import Protocol, Any

from mcp import ClientSession
from mcp.types import ListPromptsResult, EmbeddedResource, ImageContent, TextContent

from mistralai.extra.exceptions import MCPException
from mistralai.models import (
    FunctionTool,
    Function,
    SystemMessageTypedDict,
    AssistantMessageTypedDict,
    TextChunkTypedDict,
)

logger = logging.getLogger(__name__)


class MCPSystemPrompt(typing.TypedDict):
    description: Optional[str]
    messages: list[Union[SystemMessageTypedDict, AssistantMessageTypedDict]]


class MCPClientProtocol(Protocol):
    """MCP client that converts MCP artifacts to Mistral format."""

    _name: str

    async def initialize(self, exit_stack: Optional[AsyncExitStack]) -> None:
        ...

    async def aclose(self) -> None:
        ...

    async def get_tools(self) -> list[FunctionTool]:
        ...

    async def execute_tool(
        self, name: str, arguments: dict
    ) -> list[TextChunkTypedDict]:
        ...

    async def get_system_prompt(
        self, name: str, arguments: dict[str, Any]
    ) -> MCPSystemPrompt:
        ...

    async def list_system_prompts(self) -> ListPromptsResult:
        ...


class MCPClientBase(MCPClientProtocol):
    """Base class to implement functionalities from an initialized MCP session."""

    _session: ClientSession

    def __init__(self, name: Optional[str] = None):
        self._name = name or self.__class__.__name__
        self._exit_stack: Optional[AsyncExitStack] = None
        self._is_initialized = False

    def _convert_content(
        self, mcp_content: Union[TextContent, ImageContent, EmbeddedResource]
    ) -> TextChunkTypedDict:
        if not mcp_content.type == "text":
            raise MCPException("Only supporting text tool responses for now.")
        return {"type": "text", "text": mcp_content.text}

    def _convert_content_list(
        self, mcp_contents: list[Union[TextContent, ImageContent, EmbeddedResource]]
    ) -> list[TextChunkTypedDict]:
        content_chunks = []
        for mcp_content in mcp_contents:
            content_chunks.append(self._convert_content(mcp_content))
        return content_chunks

    async def get_tools(self) -> list[FunctionTool]:
        mcp_tools = await self._session.list_tools()
        tools = []
        for mcp_tool in mcp_tools.tools:
            tools.append(
                FunctionTool(
                    type="function",
                    function=Function(
                        name=mcp_tool.name,
                        description=mcp_tool.description,
                        parameters=mcp_tool.inputSchema,
                        strict=True,
                    ),
                )
            )
        return tools

    async def execute_tool(
        self, name: str, arguments: dict[str, Any]
    ) -> list[TextChunkTypedDict]:
        contents = await self._session.call_tool(name=name, arguments=arguments)
        return self._convert_content_list(contents.content)

    async def get_system_prompt(
        self, name: str, arguments: dict[str, Any]
    ) -> MCPSystemPrompt:
        prompt_result = await self._session.get_prompt(name=name, arguments=arguments)
        return {
            "description": prompt_result.description,
            "messages": [
                typing.cast(
                    Union[SystemMessageTypedDict, AssistantMessageTypedDict],
                    {
                        "role": message.role,
                        "content": self._convert_content(mcp_content=message.content),
                    },
                )
                for message in prompt_result.messages
            ],
        }

    async def list_system_prompts(self) -> ListPromptsResult:
        return await self._session.list_prompts()

    async def initialize(self, exit_stack: Optional[AsyncExitStack] = None) -> None:
        """Initialize the MCP session."""
        # client is already initialized so return
        if self._is_initialized:
            return
        if exit_stack is None:
            self._exit_stack = AsyncExitStack()
            exit_stack = self._exit_stack
        stdio_transport = await self._get_transport(exit_stack=exit_stack)
        mcp_session = await exit_stack.enter_async_context(
            ClientSession(
                read_stream=stdio_transport[0],
                write_stream=stdio_transport[1],
            )
        )
        await mcp_session.initialize()
        self._session = mcp_session
        self._is_initialized = True

    async def aclose(self):
        """Close the MCP session."""
        if self._exit_stack:
            await self._exit_stack.aclose()

    def __repr__(self):
        return f"<{self.__class__.__name__} name={self._name!r} id=0x{id(self):x}>"

    def __str__(self):
        return f"{self.__class__.__name__}(name={self._name})"

    async def _get_transport(self, exit_stack: AsyncExitStack):
        raise NotImplementedError
</file>

<file path="src/mistralai/extra/mcp/sse.py">
import http
import logging
import typing
from typing import Any, Optional
from contextlib import AsyncExitStack
from functools import cached_property

import httpx

from mistralai.extra.exceptions import MCPAuthException
from mistralai.extra.mcp.base import (
    MCPClientBase,
)
from mistralai.extra.mcp.auth import OAuthParams, AsyncOAuth2Client
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream

from mcp.client.sse import sse_client
from mcp.shared.message import SessionMessage
from authlib.oauth2.rfc6749 import OAuth2Token

from mistralai.types import BaseModel

logger = logging.getLogger(__name__)


class SSEServerParams(BaseModel):
    """Parameters required for a MCPClient with SSE transport"""

    url: str
    headers: Optional[dict[str, Any]] = None
    timeout: float = 5
    sse_read_timeout: float = 60 * 5


class MCPClientSSE(MCPClientBase):
    """MCP client that uses sse for communication.

    The client provides authentication for OAuth2 protocol following the current MCP authorization spec:
    https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization.

    This is possibly going to change in the future since the protocol has ongoing discussions.
    """

    _oauth_params: Optional[OAuthParams]
    _sse_params: SSEServerParams

    def __init__(
        self,
        sse_params: SSEServerParams,
        name: Optional[str] = None,
        oauth_params: Optional[OAuthParams] = None,
        auth_token: Optional[OAuth2Token] = None,
    ):
        super().__init__(name=name)
        self._sse_params = sse_params
        self._oauth_params: Optional[OAuthParams] = oauth_params
        self._auth_token: Optional[OAuth2Token] = auth_token

    @cached_property
    def base_url(self) -> str:
        return self._sse_params.url.rstrip("/sse")

    def set_oauth_params(self, oauth_params: OAuthParams):
        """Update the oauth params and client accordingly."""
        if self._oauth_params is not None:
            logger.warning(f"Overriding current oauth params for {self._name}")
        self._oauth_params = oauth_params

    async def get_auth_url_and_state(self, redirect_url: str) -> tuple[str, str]:
        """Create the authorization url for client to start oauth flow."""
        if self._oauth_params is None:
            raise MCPAuthException(
                "Can't generate an authorization url without oauth_params being set, "
                "make sure the oauth params have been set."
            )
        oauth_client = AsyncOAuth2Client.from_oauth_params(self._oauth_params)
        auth_url, state = oauth_client.create_authorization_url(
            self._oauth_params.scheme.authorization_url, redirect_uri=redirect_url
        )
        return auth_url, state

    async def get_token_from_auth_response(
        self,
        authorization_response: str,
        redirect_url: str,
        state: str,
    ) -> OAuth2Token:
        """Fetch the authentication token from the server."""
        if self._oauth_params is None:
            raise MCPAuthException(
                "Can't fetch a token without oauth_params, make sure they have been set."
            )
        oauth_client = AsyncOAuth2Client.from_oauth_params(self._oauth_params)
        oauth_token = await oauth_client.fetch_token(
            url=self._oauth_params.scheme.token_url,
            authorization_response=authorization_response,
            redirect_uri=redirect_url,
            headers={"Content-Type": "application/x-www-form-urlencoded"},
            state=state,
        )
        return oauth_token

    async def refresh_auth_token(self):
        """Refresh an expired token."""
        if self._oauth_params is None or self._oauth_params.scheme.refresh_url is None:
            raise MCPAuthException(
                "Can't refresh a token without a refresh url make sure the oauth params have been set."
            )
        if self._auth_token is None:
            raise MCPAuthException(
                "Can't refresh a token without a refresh token, use the `set_auth_token` to add a OAuth2Token."
            )
        oauth_client = AsyncOAuth2Client.from_oauth_params(self._oauth_params)
        oauth_token = await oauth_client.refresh_token(
            url=self._oauth_params.scheme.refresh_url,
            refresh_token=self._auth_token["refresh_token"],
            headers={"Content-Type": "application/x-www-form-urlencoded"},
        )
        self.set_auth_token(oauth_token)

    def set_auth_token(self, token: OAuth2Token) -> None:
        """Register the authentication token with this client."""
        self._auth_token = token

    def _format_headers(self) -> dict[str, str]:
        headers: dict[str, str] = {}
        if self._sse_params.headers:
            headers |= self._sse_params.headers
        if self._auth_token:
            headers["Authorization"] = f"Bearer {self._auth_token['access_token']}"
        return headers

    async def requires_auth(self) -> bool:
        """Check if the client requires authentication to communicate with the server."""
        response = httpx.get(
            self._sse_params.url,
            headers=self._format_headers(),
            timeout=self._sse_params.timeout,
        )
        return response.status_code == http.HTTPStatus.UNAUTHORIZED

    async def _get_transport(
        self, exit_stack: AsyncExitStack
    ) -> tuple[
        MemoryObjectReceiveStream[typing.Union[SessionMessage, Exception]],
        MemoryObjectSendStream[SessionMessage],
    ]:
        try:
            return await exit_stack.enter_async_context(
                sse_client(
                    url=self._sse_params.url,
                    headers=self._format_headers(),
                    timeout=self._sse_params.timeout,
                    sse_read_timeout=self._sse_params.sse_read_timeout,
                )
            )
        except Exception as e:
            if isinstance(e, httpx.HTTPStatusError):
                if e.response.status_code == http.HTTPStatus.UNAUTHORIZED:
                    if self._oauth_params is None:
                        raise MCPAuthException(
                            "Authentication required but no auth params provided."
                        ) from e
                    raise MCPAuthException("Authentication required.") from e
            raise
</file>

<file path="src/mistralai/extra/mcp/stdio.py">
from typing import Optional
import logging
from contextlib import AsyncExitStack

from mistralai.extra.mcp.base import (
    MCPClientBase,
)

from mcp import stdio_client, StdioServerParameters

logger = logging.getLogger(__name__)


class MCPClientSTDIO(MCPClientBase):
    """MCP client that uses stdio for communication."""

    def __init__(self, stdio_params: StdioServerParameters, name: Optional[str] = None):
        super().__init__(name=name)
        self._stdio_params = stdio_params

    async def _get_transport(self, exit_stack: AsyncExitStack):
        return await exit_stack.enter_async_context(stdio_client(self._stdio_params))
</file>

<file path="src/mistralai/extra/run/context.py">
import asyncio
import inspect
import typing
from contextlib import AsyncExitStack
from functools import wraps
from collections.abc import Callable

from dataclasses import dataclass, field
from typing import Union, Optional

import pydantic

from mistralai.extra import (
    response_format_from_pydantic_model,
)
from mistralai.extra.exceptions import RunException
from mistralai.extra.mcp.base import MCPClientProtocol
from mistralai.extra.run.result import RunResult
from mistralai.types.basemodel import OptionalNullable, BaseModel, UNSET
from mistralai.models import (
    ResponseFormat,
    FunctionCallEntry,
    Tools,
    ToolsTypedDict,
    CompletionArgs,
    CompletionArgsTypedDict,
    FunctionResultEntry,
    ConversationInputs,
    ConversationInputsTypedDict,
    FunctionTool,
    MessageInputEntry,
    InputEntries,
)

from logging import getLogger

from mistralai.extra.run.tools import (
    create_function_result,
    RunFunction,
    create_tool_call,
    RunTool,
    RunMCPTool,
    RunCoroutine,
)

if typing.TYPE_CHECKING:
    from mistralai import Beta, OptionalNullable

logger = getLogger(__name__)


class AgentRequestKwargs(typing.TypedDict):
    agent_id: str


class ModelRequestKwargs(typing.TypedDict):
    model: str
    instructions: OptionalNullable[str]
    tools: OptionalNullable[Union[list[Tools], list[ToolsTypedDict]]]
    completion_args: OptionalNullable[Union[CompletionArgs, CompletionArgsTypedDict]]


@dataclass
class RunContext:
    """A context for running a conversation with an agent or a model.

    The context can be used to execute function calls, connect to MCP server, and keep track of information about
    the run.

    Args:
        conversation_id (Options[str]): The unique identifier for the conversation. This is
          passed if the user wants to continue an existing conversation.
        model (Options[str]): The model name to be used for the conversation. Can't be used along with 'agent_id'.
        agent_id (Options[str]): The agent id to be used for the conversation. Can't be used along with 'model'.
        output_format (Optional[type[BaseModel]]): The output format expected from the conversation. It represents
          the `response_format` which is part of the `CompletionArgs`.
        request_count (int): The number of requests made in the current `RunContext`.
        continue_on_fn_error (bool): Flag to determine if the conversation should continue when function execution
          resulted in an error.
    """

    _exit_stack: AsyncExitStack = field(init=False)
    _callable_tools: dict[str, RunTool] = field(init=False, default_factory=dict)
    _mcp_clients: list[MCPClientProtocol] = field(init=False, default_factory=list)

    conversation_id: Optional[str] = field(default=None)
    model: Optional[str] = field(default=None)
    agent_id: Optional[str] = field(default=None)
    output_format: Optional[type[BaseModel]] = field(default=None)
    request_count: int = field(default=0)
    continue_on_fn_error: bool = field(default=False)

    def __post_init__(self):
        if self.model and self.agent_id:
            raise RunException("Only one for model or agent_id should be set")
        self._exit_stack = AsyncExitStack()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self._exit_stack.aclose()
        for mcp_client in self._mcp_clients:
            await mcp_client.aclose()

    def register_func(self, func: Callable):
        """Add a function to the context."""
        if not inspect.isfunction(func):
            raise RunException(
                "Only object of type function can be registered at the moment."
            )

        if inspect.iscoroutinefunction(func):
            self._callable_tools[func.__name__] = RunCoroutine(
                name=func.__name__,
                awaitable=func,
                tool=create_tool_call(func),
            )
        else:
            self._callable_tools[func.__name__] = RunFunction(
                name=func.__name__,
                callable=func,
                tool=create_tool_call(func),
            )

        @wraps(func)
        def wrapper(*args, **kwargs):
            logger.info(f"Executing {func.__name__}")
            return func(*args, **kwargs)

        return wrapper

    async def register_mcp_clients(self, mcp_clients: list[MCPClientProtocol]) -> None:
        """Registering multiple MCP clients at the same time in the same asyncio.Task."""
        for mcp_client in mcp_clients:
            await self.register_mcp_client(mcp_client)

    async def register_mcp_client(self, mcp_client: MCPClientProtocol) -> None:
        """Add a MCP client to the context."""
        await mcp_client.initialize(exit_stack=self._exit_stack)
        tools = await mcp_client.get_tools()
        for tool in tools:
            logger.info(
                f"Adding tool {tool.function.name} from {mcp_client._name or 'mcp client'}"
            )
            self._callable_tools[tool.function.name] = RunMCPTool(
                name=tool.function.name,
                tool=tool,
                mcp_client=mcp_client,
            )
        self._mcp_clients.append(mcp_client)

    async def execute_function_calls(
        self, function_calls: list[FunctionCallEntry]
    ) -> list[FunctionResultEntry]:
        """Execute function calls and create function results from them."""
        if not all(
            function_call.name in self._callable_tools
            for function_call in function_calls
        ):
            logger.warning("Can't execute all functions, stopping run here")
            return []
        function_result_tasks = []
        for function_call in function_calls:
            function_result_tasks.append(
                asyncio.create_task(
                    create_function_result(
                        function_call=function_call,
                        run_tool=self._callable_tools[function_call.name],
                        continue_on_fn_error=self.continue_on_fn_error,
                    )
                )
            )
        await asyncio.gather(*function_result_tasks)
        return [task.result() for task in function_result_tasks]

    def get_tools(self) -> list[FunctionTool]:
        """Get the tools that are part of the context."""
        callable_tools = [
            run_functions.tool for run_functions in self._callable_tools.values()
        ]
        return callable_tools

    async def prepare_agent_request(self, beta_client: "Beta") -> AgentRequestKwargs:
        """Prepare an agent request with the functions added to the context.

        Update the agent definition before making the request.
        """
        if self.agent_id is None:
            raise RunException(
                "Can't prepare an agent request, if no agent_id is provided"
            )
        agent = await beta_client.agents.get_async(agent_id=self.agent_id)
        agent_tools = agent.tools or []
        updated_tools = []
        for i in range(len(agent_tools)):
            tool = agent_tools[i]
            if tool.type != "function":
                updated_tools.append(tool)
            elif tool.function.name in self._callable_tools:
                # function already exists in the agent, don't add it again
                continue
            else:
                updated_tools.append(tool)
        updated_tools += self.get_tools()
        completion_args = (
            CompletionArgs(response_format=self.response_format)
            if self.output_format
            else None
        )
        beta_client.agents.update(
            agent_id=self.agent_id, tools=updated_tools, completion_args=completion_args
        )
        return AgentRequestKwargs(agent_id=self.agent_id)

    async def prepare_model_request(
        self,
        tools: OptionalNullable[Union[list[Tools], list[ToolsTypedDict]]] = UNSET,
        completion_args: OptionalNullable[
            Union[CompletionArgs, CompletionArgsTypedDict]
        ] = UNSET,
        instructions: OptionalNullable[str] = None,
    ) -> ModelRequestKwargs:
        if self.model is None:
            raise RunException("Can't prepare a model request, if no model is provided")
        if not completion_args and self.output_format:
            completion_args = CompletionArgs(response_format=self.response_format)
        elif isinstance(completion_args, CompletionArgs) and self.output_format:
            completion_args.response_format = self.response_format
        elif isinstance(completion_args, dict) and self.output_format:
            completion_args = CompletionArgs.model_validate(completion_args)
            completion_args.response_format = self.response_format
        request_tools = []
        if isinstance(tools, list):
            for tool in tools:
                request_tools.append(typing.cast(Tools, tool))
        for tool in self.get_tools():
            request_tools.append(tool)
        return ModelRequestKwargs(
            model=self.model,
            tools=request_tools,
            instructions=instructions,
            completion_args=completion_args,
        )

    @property
    def response_format(self) -> ResponseFormat:
        if not self.output_format:
            raise RunException("No response format exist for the current RunContext.")
        return response_format_from_pydantic_model(self.output_format)


async def _validate_run(
    *,
    beta_client: "Beta",
    run_ctx: RunContext,
    inputs: Union[ConversationInputs, ConversationInputsTypedDict],
    instructions: OptionalNullable[str] = UNSET,
    tools: OptionalNullable[Union[list[Tools], list[ToolsTypedDict]]] = UNSET,
    completion_args: OptionalNullable[
        Union[CompletionArgs, CompletionArgsTypedDict]
    ] = UNSET,
) -> tuple[
    Union[AgentRequestKwargs, ModelRequestKwargs], RunResult, list[InputEntries]
]:
    input_entries: list[InputEntries] = []
    if isinstance(inputs, str):
        input_entries.append(MessageInputEntry(role="user", content=inputs))
    else:
        for input in inputs:
            if isinstance(input, dict):
                input_entries.append(
                    pydantic.TypeAdapter(InputEntries).validate_python(input)
                )
    run_result = RunResult(
        input_entries=input_entries,
        output_model=run_ctx.output_format,
        conversation_id=run_ctx.conversation_id,
    )
    req: Union[AgentRequestKwargs, ModelRequestKwargs]
    if run_ctx.agent_id:
        if tools or completion_args:
            raise RunException("Can't set tools or completion_args when using an agent")
        req = await run_ctx.prepare_agent_request(beta_client=beta_client)
    elif run_ctx.model:
        req = await run_ctx.prepare_model_request(
            instructions=instructions,
            tools=tools,
            completion_args=completion_args,
        )
    else:
        raise RunException("Either agent_id or model must be set in the run context")
    return req, run_result, input_entries
</file>

<file path="src/mistralai/extra/run/result.py">
import datetime
import json
import typing
from typing import Union, Annotated, Optional, Literal
from dataclasses import dataclass, field
from pydantic import Discriminator, Tag, BaseModel

from mistralai.extra.utils.response_format import pydantic_model_from_json
from mistralai.models import (
    FunctionResultEntry,
    FunctionCallEntry,
    MessageOutputEntry,
    AgentHandoffEntry,
    ToolExecutionEntry,
    MessageInputEntry,
    AgentHandoffDoneEvent,
    AgentHandoffStartedEvent,
    ResponseDoneEvent,
    ResponseErrorEvent,
    ResponseStartedEvent,
    FunctionCallEvent,
    MessageOutputEvent,
    ToolExecutionDoneEvent,
    ToolExecutionStartedEvent,
    ConversationEventsData,
    MessageOutputEventContent,
    MessageOutputEntryContent,
    TextChunk,
    MessageOutputContentChunks,
    SSETypes,
    InputEntries,
    ToolFileChunk,
    ToolReferenceChunk,
    FunctionCallEntryArguments,
)
from mistralai.utils import get_discriminator

RunOutputEntries = typing.Union[
    MessageOutputEntry,
    FunctionCallEntry,
    FunctionResultEntry,
    AgentHandoffEntry,
    ToolExecutionEntry,
]

RunEntries = typing.Union[RunOutputEntries, MessageInputEntry]


def as_text(entry: RunOutputEntries) -> str:
    """Keep only the messages and turn content into textual representation."""
    text = ""
    if isinstance(entry, MessageOutputEntry):
        if isinstance(entry.content, str):
            text += entry.content
        else:
            for chunk in entry.content:
                if isinstance(chunk, TextChunk):
                    text += chunk.text
                elif isinstance(chunk, ToolFileChunk):
                    text += f"<File id={chunk.file_id} name={chunk.file_name}>"
                elif isinstance(chunk, ToolReferenceChunk):
                    text += f"<Reference title={chunk.title}>"
    return text


def reconstitute_message_content(
    chunks: list[MessageOutputEventContent],
) -> MessageOutputEntryContent:
    """Given a list of MessageOutputEventContent, recreate a normalised MessageOutputEntryContent."""
    if all(isinstance(chunk, str) for chunk in chunks):
        return "".join(typing.cast(list[str], chunks))
    content: list[MessageOutputContentChunks] = []
    for chunk in chunks:
        if isinstance(chunk, str):
            chunk = TextChunk(text=chunk)
        if isinstance(chunk, TextChunk):
            if len(content) and isinstance(content[-1], TextChunk):
                content[-1].text += chunk.text
            else:
                content.append(chunk)
        else:
            content.append(chunk)
    return content


def reconstitute_function_call_args(chunks: list[str]) -> FunctionCallEntryArguments:
    """Recreates function call arguments from stream"""
    return typing.cast(FunctionCallEntryArguments, "".join(chunks))


def reconstitue_entries(
    received_event_tracker: dict[int, list[ConversationEventsData]],
) -> list[RunOutputEntries]:
    """Given a list of events, recreate the corresponding entries."""
    run_entries: list[RunOutputEntries] = []
    for idx, events in sorted(received_event_tracker.items(), key=lambda x: x[0]):
        first_event = events[0]
        if isinstance(first_event, MessageOutputEvent):
            message_events = typing.cast(list[MessageOutputEvent], events)
            run_entries.append(
                MessageOutputEntry(
                    content=reconstitute_message_content(
                        chunks=[
                            message_event.content for message_event in message_events
                        ]
                    ),
                    created_at=first_event.created_at,
                    id=first_event.id,
                    agent_id=first_event.agent_id,
                    model=first_event.model,
                    role=first_event.role,
                )
            )
        elif isinstance(first_event, FunctionCallEvent):
            function_call_events = typing.cast(list[FunctionCallEvent], events)
            run_entries.append(
                FunctionCallEntry(
                    name=first_event.name,
                    arguments=reconstitute_function_call_args(
                        chunks=[
                            function_call_event.arguments
                            for function_call_event in function_call_events
                        ]
                    ),
                    created_at=first_event.created_at,
                    id=first_event.id,
                    tool_call_id=first_event.tool_call_id,
                )
            )
    return run_entries


@dataclass
class RunFiles:
    id: str
    name: str
    content: bytes


@dataclass
class RunResult:
    input_entries: list[InputEntries]
    conversation_id: Optional[str] = field(default=None)
    output_entries: list[RunOutputEntries] = field(default_factory=list)
    files: dict[str, RunFiles] = field(default_factory=dict)
    output_model: Optional[type[BaseModel]] = field(default=None)

    def get_file(self, file_id: str) -> Optional[RunFiles]:
        return self.files.get(file_id)

    @property
    def entries(self) -> list[RunEntries]:
        return [*self.input_entries, *self.output_entries]

    @property
    def output_as_text(self) -> str:
        if not self.output_entries:
            raise ValueError("No output entries were started.")
        return "\n".join(
            as_text(entry)
            for entry in self.output_entries
            if entry.type == "message.output"
        )

    @property
    def output_as_model(self) -> BaseModel:
        if self.output_model is None:
            raise ValueError("No output format was not set.")
        return pydantic_model_from_json(
            json.loads(self.output_as_text), self.output_model
        )


class FunctionResultEvent(BaseModel):
    id: Optional[str] = None

    type: Optional[Literal["function.result"]] = "function.result"

    result: str

    tool_call_id: str

    created_at: Optional[datetime.datetime] = datetime.datetime.now(
        tz=datetime.timezone.utc
    )

    output_index: Optional[int] = 0


RunResultEventsType = typing.Union[SSETypes, Literal["function.result"]]

RunResultEventsData = typing.Annotated[
    Union[
        Annotated[AgentHandoffDoneEvent, Tag("agent.handoff.done")],
        Annotated[AgentHandoffStartedEvent, Tag("agent.handoff.started")],
        Annotated[ResponseDoneEvent, Tag("conversation.response.done")],
        Annotated[ResponseErrorEvent, Tag("conversation.response.error")],
        Annotated[ResponseStartedEvent, Tag("conversation.response.started")],
        Annotated[FunctionCallEvent, Tag("function.call.delta")],
        Annotated[MessageOutputEvent, Tag("message.output.delta")],
        Annotated[ToolExecutionDoneEvent, Tag("tool.execution.done")],
        Annotated[ToolExecutionStartedEvent, Tag("tool.execution.started")],
        Annotated[FunctionResultEvent, Tag("function.result")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class RunResultEvents(BaseModel):
    event: RunResultEventsType

    data: RunResultEventsData
</file>

<file path="src/mistralai/extra/run/tools.py">
import itertools
import logging
from dataclasses import dataclass
import inspect

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
import json
from typing import cast, Callable, Sequence, Any, ForwardRef, get_type_hints, Union

from griffe import (
    Docstring,
    DocstringSectionKind,
    DocstringSectionText,
    DocstringParameter,
    DocstringSection,
)

from mistralai.extra.exceptions import RunException
from mistralai.extra.mcp.base import MCPClientProtocol
from mistralai.extra.run.result import RunOutputEntries
from mistralai.models import (
    FunctionResultEntry,
    FunctionTool,
    Function,
    FunctionCallEntry,
)


logger = logging.getLogger(__name__)


@dataclass
class RunFunction:
    name: str
    callable: Callable
    tool: FunctionTool


@dataclass
class RunCoroutine:
    name: str
    awaitable: Callable
    tool: FunctionTool


@dataclass
class RunMCPTool:
    name: str
    tool: FunctionTool
    mcp_client: MCPClientProtocol


RunTool = Union[RunFunction, RunCoroutine, RunMCPTool]


def _get_function_description(docstring_sections: list[DocstringSection]) -> str:
    """Given a list of docstring sections create a description for the function."""
    text_sections: list[DocstringSectionText] = []
    for section in docstring_sections:
        if section.kind == DocstringSectionKind.text:
            text_sections.append(cast(DocstringSectionText, section))
    return "\n".join(text.value for text in text_sections)


def _get_function_parameters(
    docstring_sections: list[DocstringSection],
    params_from_sig: list[inspect.Parameter],
    type_hints: dict[str, Any],
):
    """Given a list of docstring sections and type annotations create the most accurate tool parameters"""
    params_from_docstrings: list[DocstringParameter] = list(
        itertools.chain.from_iterable(
            section.value
            for section in docstring_sections
            if section.kind
            in (DocstringSectionKind.parameters, DocstringSectionKind.other_parameters)
        )
    )

    # Extract all description and annotation
    param_descriptions = {}
    param_annotations = {}

    for param_doc in params_from_docstrings:
        param_descriptions[param_doc.name] = param_doc.description

    for param in params_from_sig:
        if param.name not in param_descriptions:
            param_descriptions[param.name] = ""
        param_annotations[param.name] = type_hints.get(param.name)

    # resolve all params into Field and create the parameters schema
    fields: dict[str, tuple[type, FieldInfo]] = {}
    for p in params_from_sig:
        default = p.default if p.default is not inspect.Parameter.empty else ...
        annotation = (
            p.annotation if p.annotation is not inspect.Parameter.empty else Any
        )
        # handle forward ref with the help of get_type_hints
        if isinstance(annotation, str):
            annotation = type_hints[p.name]

        if isinstance(default, FieldInfo):
            field_info = default
        else:
            # If the annotation is Annotated[..., Field(...)] extract the Field and annotation
            # Otherwise, just use the annotation as-is
            field_info = None
            # If it's Annotated[..., SomeFieldMarker(...)], find it
            if hasattr(annotation, "__metadata__") and hasattr(annotation, "__args__"):
                # It's Annotated
                # e.g. Annotated[str, Field(...)]
                # Extract the first Field(...) or None if not found
                for meta in annotation.__metadata__:  # type: ignore
                    if isinstance(meta, FieldInfo):
                        field_info = meta
                        break
                # The actual annotation is the first part of Annotated
                annotation = annotation.__args__[0]  # type: ignore

                # handle forward ref with the help of get_type_hints
                if isinstance(annotation, ForwardRef):
                    annotation = param_annotations[p.name]

        # no Field
        if field_info is None:
            if default is ...:
                field_info = Field()
            else:
                field_info = Field(default=default)

        field_info.description = param_descriptions[p.name]
        fields[p.name] = (cast(type, annotation), field_info)

    schema = create_model("_", **fields).model_json_schema()  # type: ignore[call-overload]
    schema.pop("title", None)
    for prop in schema.get("properties", {}).values():
        prop.pop("title", None)
    return schema


def create_tool_call(func: Callable) -> FunctionTool:
    """Parse a function docstring / type annotations to create a FunctionTool."""
    name = func.__name__

    # Inspect and parse the docstring of the function
    doc = inspect.getdoc(func)
    docstring_sections: list[DocstringSection]
    if not doc:
        logger.warning(
            f"Function '{name}' without a docstring is being parsed, add docstring for more accurate result."
        )
        docstring_sections = []
    else:
        docstring = Docstring(doc, parser="google")
        docstring_sections = docstring.parse(warnings=False)
        if len(docstring_sections) == 0:
            logger.warning(
                f"Function '{name}' has no relevant docstring sections, add docstring for more accurate result."
            )

    # Extract the function's signature and type hints
    sig = inspect.signature(func)
    params_from_sig = list(sig.parameters.values())
    type_hints = get_type_hints(func, include_extras=True, localns=None, globalns=None)

    return FunctionTool(
        type="function",
        function=Function(
            name=name,
            description=_get_function_description(docstring_sections),
            parameters=_get_function_parameters(
                docstring_sections=docstring_sections,
                params_from_sig=params_from_sig,
                type_hints=type_hints,
            ),
            strict=True,
        ),
    )


async def create_function_result(
    function_call: FunctionCallEntry,
    run_tool: RunTool,
    continue_on_fn_error: bool = False,
) -> FunctionResultEntry:
    """Run the function with arguments of a FunctionCallEntry."""
    arguments = (
        json.loads(function_call.arguments)
        if isinstance(function_call.arguments, str)
        else function_call.arguments
    )
    try:
        if isinstance(run_tool, RunFunction):
            res = run_tool.callable(**arguments)
        elif isinstance(run_tool, RunCoroutine):
            res = await run_tool.awaitable(**arguments)
        elif isinstance(run_tool, RunMCPTool):
            res = await run_tool.mcp_client.execute_tool(function_call.name, arguments)
    except Exception as e:
        if continue_on_fn_error is True:
            return FunctionResultEntry(
                tool_call_id=function_call.tool_call_id,
                result=f"Error while executing {function_call.name}: {str(e)}",
            )
        raise RunException(
            f"Failed to execute tool {function_call.name} with arguments '{function_call.arguments}'"
        ) from e

    return FunctionResultEntry(
        tool_call_id=function_call.tool_call_id,
        result=res if isinstance(res, str) else json.dumps(res),
    )


def get_function_calls(
    output_entries: Sequence[RunOutputEntries],
) -> list[FunctionCallEntry]:
    """Extract all FunctionCallEntry from a conversation response"""
    function_calls = []
    for entry in output_entries:
        if isinstance(entry, FunctionCallEntry):
            function_calls.append(entry)
    return function_calls
</file>

<file path="src/mistralai/extra/run/utils.py">
import importlib.util
import sys
from typing import Callable, TypeVar, Any, cast
from functools import wraps

from mistralai.extra.exceptions import MistralClientException

F = TypeVar("F", bound=Callable[..., Any])


REQUIRED_PYTHON_VERSION = (3, 10)
REQUIRED_PYTHON_VERSION_STR = "3.10"
REQUIRED_PACKAGES = ["mcp"]


def is_module_installed(module_name: str) -> bool:
    spec = importlib.util.find_spec(module_name)
    return spec is not None


def run_requirements(func: F) -> F:
    @wraps(func)
    def wrapper(*args, **kwargs):
        if sys.version_info < REQUIRED_PYTHON_VERSION:
            raise MistralClientException(
                f"{func.__name__} requires a Python version higher than {REQUIRED_PYTHON_VERSION_STR}."
                f"You are using Python {sys.version_info.major}.{sys.version_info.minor}."
            )
        for package in REQUIRED_PACKAGES:
            if not is_module_installed(package):
                raise MistralClientException(
                    f"{func.__name__} requires the sdk to be installed with 'agents' extra dependencies."
                )
        return func(*args, **kwargs)

    return cast(F, wrapper)
</file>

<file path="src/mistralai/extra/tests/test_struct_chat.py">
import unittest
from ..struct_chat import (
    convert_to_parsed_chat_completion_response,
    ParsedChatCompletionResponse,
    ParsedChatCompletionChoice,
    ParsedAssistantMessage,
)
from ...models import (
    ChatCompletionResponse,
    UsageInfo,
    ChatCompletionChoice,
    AssistantMessage,
)
from pydantic import BaseModel


class Explanation(BaseModel):
    explanation: str
    output: str


class MathDemonstration(BaseModel):
    steps: list[Explanation]
    final_answer: str


mock_cc_response = ChatCompletionResponse(
    id="c0271b2098954c6094231703875ca0bc",
    object="chat.completion",
    model="mistral-large-latest",
    usage=UsageInfo(prompt_tokens=75, completion_tokens=220, total_tokens=295),
    created=1737727558,
    choices=[
        ChatCompletionChoice(
            index=0,
            message=AssistantMessage(
                content='{\n  "final_answer": "x = -4",\n  "steps": [\n    {\n      "explanation": "Start with the given equation.",\n      "output": "8x + 7 = -23"\n    },\n    {\n      "explanation": "Subtract 7 from both sides to isolate the term with x.",\n      "output": "8x = -23 - 7"\n    },\n    {\n      "explanation": "Simplify the right side of the equation.",\n      "output": "8x = -30"\n    },\n    {\n      "explanation": "Divide both sides by 8 to solve for x.",\n      "output": "x = -30 / 8"\n    },\n    {\n      "explanation": "Simplify the fraction to get the final answer.",\n      "output": "x = -4"\n    }\n  ]\n}',
                tool_calls=None,
                prefix=False,
                role="assistant",
            ),
            finish_reason="stop",
        )
    ],
)


expected_response: ParsedChatCompletionResponse = ParsedChatCompletionResponse(
    choices=[
        ParsedChatCompletionChoice(
            index=0,
            message=ParsedAssistantMessage(
                content='{\n  "final_answer": "x = -4",\n  "steps": [\n    {\n      "explanation": "Start with the given equation.",\n      "output": "8x + 7 = -23"\n    },\n    {\n      "explanation": "Subtract 7 from both sides to isolate the term with x.",\n      "output": "8x = -23 - 7"\n    },\n    {\n      "explanation": "Simplify the right side of the equation.",\n      "output": "8x = -30"\n    },\n    {\n      "explanation": "Divide both sides by 8 to solve for x.",\n      "output": "x = -30 / 8"\n    },\n    {\n      "explanation": "Simplify the fraction to get the final answer.",\n      "output": "x = -4"\n    }\n  ]\n}',
                tool_calls=None,
                prefix=False,
                role="assistant",
                parsed=MathDemonstration(
                    steps=[
                        Explanation(
                            explanation="Start with the given equation.",
                            output="8x + 7 = -23",
                        ),
                        Explanation(
                            explanation="Subtract 7 from both sides to isolate the term with x.",
                            output="8x = -23 - 7",
                        ),
                        Explanation(
                            explanation="Simplify the right side of the equation.",
                            output="8x = -30",
                        ),
                        Explanation(
                            explanation="Divide both sides by 8 to solve for x.",
                            output="x = -30 / 8",
                        ),
                        Explanation(
                            explanation="Simplify the fraction to get the final answer.",
                            output="x = -4",
                        ),
                    ],
                    final_answer="x = -4",
                ),
            ),
            finish_reason="stop",
        )
    ],
    created=1737727558,
    id="c0271b2098954c6094231703875ca0bc",
    model="mistral-large-latest",
    object="chat.completion",
    usage=UsageInfo(prompt_tokens=75, completion_tokens=220, total_tokens=295),
)


class TestConvertToParsedChatCompletionResponse(unittest.TestCase):
    def test_convert_to_parsed_chat_completion_response(self):
        output = convert_to_parsed_chat_completion_response(
            mock_cc_response, MathDemonstration
        )
        self.assertEqual(output, expected_response)


if __name__ == "__main__":
    unittest.main()
</file>

<file path="src/mistralai/extra/tests/test_utils.py">
from ..utils.response_format import (
    pydantic_model_from_json,
    response_format_from_pydantic_model,
    rec_strict_json_schema,
)
from pydantic import BaseModel, ValidationError

from ...models import ResponseFormat, JSONSchema
from ...types.basemodel import Unset

import unittest


class Student(BaseModel):
    name: str
    age: int


class Explanation(BaseModel):
    explanation: str
    output: str


class MathDemonstration(BaseModel):
    steps: list[Explanation]
    final_answer: str


mathdemo_schema = {
    "$defs": {
        "Explanation": {
            "properties": {
                "explanation": {"title": "Explanation", "type": "string"},
                "output": {"title": "Output", "type": "string"},
            },
            "required": ["explanation", "output"],
            "title": "Explanation",
            "type": "object",
        }
    },
    "properties": {
        "steps": {
            "items": {"$ref": "#/$defs/Explanation"},
            "title": "Steps",
            "type": "array",
        },
        "final_answer": {"title": "Final Answer", "type": "string"},
    },
    "required": ["steps", "final_answer"],
    "title": "MathDemonstration",
    "type": "object",
}

mathdemo_strict_schema = mathdemo_schema.copy()
mathdemo_strict_schema["$defs"]["Explanation"]["additionalProperties"] = False # type: ignore
mathdemo_strict_schema["additionalProperties"] = False

mathdemo_response_format = ResponseFormat(
    type="json_schema",
    json_schema=JSONSchema(
        name="MathDemonstration",
        schema_definition=mathdemo_strict_schema,
        description=Unset(),
        strict=True,
    ),
)


class TestResponseFormat(unittest.TestCase):
    def test_pydantic_model_from_json(self):
        missing_json_data = {"name": "Jean Dupont"}
        good_json_data = {"name": "Jean Dupont", "age": 25}
        extra_json_data = {
            "name": "Jean Dupont",
            "age": 25,
            "extra_field": "extra_value",
        }
        complex_json_data = {
            "final_answer": "x = -4",
            "steps": [
                {
                    "explanation": "Start with the given equation.",
                    "output": "8x + 7 = -23",
                },
                {
                    "explanation": "Subtract 7 from both sides to isolate the term with x.",
                    "output": "8x = -23 - 7",
                },
                {
                    "explanation": "Simplify the right side of the equation.",
                    "output": "8x = -30",
                },
                {
                    "explanation": "Divide both sides by 8 to solve for x.",
                    "output": "x = -30 / 8",
                },
                {
                    "explanation": "Simplify the fraction to get the final answer.",
                    "output": "x = -4",
                },
            ],
        }

        self.assertEqual(
            pydantic_model_from_json(good_json_data, Student),
            Student(name="Jean Dupont", age=25),
        )
        self.assertEqual(
            pydantic_model_from_json(extra_json_data, Student),
            Student(name="Jean Dupont", age=25),
        )
        self.assertEqual(
            pydantic_model_from_json(complex_json_data, MathDemonstration),
            MathDemonstration(
                steps=[
                    Explanation(
                        explanation="Start with the given equation.",
                        output="8x + 7 = -23",
                    ),
                    Explanation(
                        explanation="Subtract 7 from both sides to isolate the term with x.",
                        output="8x = -23 - 7",
                    ),
                    Explanation(
                        explanation="Simplify the right side of the equation.",
                        output="8x = -30",
                    ),
                    Explanation(
                        explanation="Divide both sides by 8 to solve for x.",
                        output="x = -30 / 8",
                    ),
                    Explanation(
                        explanation="Simplify the fraction to get the final answer.",
                        output="x = -4",
                    ),
                ],
                final_answer="x = -4",
            ),
        )

        # Check it raises a validation error
        with self.assertRaises(ValidationError):
            pydantic_model_from_json(missing_json_data, Student)  # type: ignore

    def test_response_format_from_pydantic_model(self):
        self.assertEqual(
            response_format_from_pydantic_model(MathDemonstration),
            mathdemo_response_format,
        )

    def test_rec_strict_json_schema(self):
        invalid_schema = mathdemo_schema | {"wrong_value": 1}
        self.assertEqual(
            rec_strict_json_schema(mathdemo_schema), mathdemo_strict_schema
        )

        with self.assertRaises(ValueError):
            rec_strict_json_schema(invalid_schema)


if __name__ == "__main__":
    unittest.main()
</file>

<file path="src/mistralai/extra/utils/__init__.py">
from .response_format import response_format_from_pydantic_model

__all__ = ["response_format_from_pydantic_model"]
</file>

<file path="src/mistralai/extra/utils/_pydantic_helper.py">
from typing import Any


def rec_strict_json_schema(schema_node: Any) -> Any:
    """
    Recursively set the additionalProperties property to False for all objects in the JSON Schema.
    This makes the JSON Schema strict (i.e. no additional properties are allowed).
    """
    if isinstance(schema_node, (str, bool)) or schema_node is None:
        return schema_node
    if isinstance(schema_node, dict):
        if "type" in schema_node and schema_node["type"] == "object":
            schema_node["additionalProperties"] = False
        for key, value in schema_node.items():
            schema_node[key] = rec_strict_json_schema(value)
    elif isinstance(schema_node, list):
        for i, value in enumerate(schema_node):
            schema_node[i] = rec_strict_json_schema(value)
    else:
        raise ValueError(f"Unexpected type: {schema_node}")
    return schema_node
</file>

<file path="src/mistralai/extra/utils/response_format.py">
from pydantic import BaseModel
from typing import TypeVar, Any, Type, Dict
from ...models import JSONSchema, ResponseFormat
from ._pydantic_helper import rec_strict_json_schema

CustomPydanticModel = TypeVar("CustomPydanticModel", bound=BaseModel)


def response_format_from_pydantic_model(
    model: Type[CustomPydanticModel],
) -> ResponseFormat:
    """Generate a strict JSON schema from a pydantic model."""
    model_schema = rec_strict_json_schema(model.model_json_schema())
    json_schema = JSONSchema.model_validate(
        {"name": model.__name__, "schema": model_schema, "strict": True}
    )
    return ResponseFormat(type="json_schema", json_schema=json_schema)


def pydantic_model_from_json(
    json_data: Dict[str, Any], pydantic_model: Type[CustomPydanticModel]
) -> CustomPydanticModel:
    """Parse a JSON schema into a pydantic model."""
    return pydantic_model.model_validate(json_data)
</file>

<file path="src/mistralai/extra/__init__.py">
from .struct_chat import (
    ParsedChatCompletionResponse,
    convert_to_parsed_chat_completion_response,
)
from .utils import response_format_from_pydantic_model
from .utils.response_format import CustomPydanticModel

__all__ = [
    "convert_to_parsed_chat_completion_response",
    "response_format_from_pydantic_model",
    "CustomPydanticModel",
    "ParsedChatCompletionResponse",
]
</file>

<file path="src/mistralai/extra/exceptions.py">
class MistralClientException(Exception):
    """Base exception for all the client errors."""


class RunException(MistralClientException):
    """Exception raised for errors during a conversation run."""


class MCPException(MistralClientException):
    """Exception raised for errors related to MCP operations."""


class MCPAuthException(MCPException):
    """Exception raised for authentication errors with an MCP server."""
</file>

<file path="src/mistralai/extra/README.md">
## Context

The extra package contains the custom logic which is too complex to be generated by Speakeasy from the OpenAPI specs. It was introduced to add the Structured Outputs feature.

## Development / Contributing

To add custom code in the SDK, you need to use [Speakeasy custom code regions](https://www.speakeasy.com/docs/customize/code/code-regions/overview) as below.

### Runbook of SDK customization

1. Add the code you want to import in the `src/mistralai/extra/` package. To have it importable from the SDK, you need to add it in the `__init__.py` file:
```python
from .my_custom_file import my_custom_function

__all__ = ["my_custom_function"]
```

2. Add a new custom code region in the SDK files, e.g in `src/mistralai/chat.py`:
```python
# region imports
from typing import Type
from mistralai.extra import my_custom_function
# endregion imports

class Chat(BaseSDK):
    r"""Chat Completion API."""

    # region sdk-class-body
    def my_custom_method(self, param: str) -> Type[some_type]:
        output = my_custom_function(param1)
        return output
    # endregion sdk-class-body
```

3. Now build the SDK with the custom code:
```bash
rm -rf dist; poetry build; python3 -m pip install ~/client-python/dist/mistralai-1.4.1-py3-none-any.whl --force-reinstall
```

4. And now you should be able to call the custom method:
```python
import os
from mistralai import Mistral

api_key = os.environ["MISTRAL_API_KEY"]
client = Mistral(api_key=api_key)

client.chat.my_custom_method(param="test")
```

### Run the unit tests

To run the unit tests for the `extra` package, you can run the following command from the root of the repository:
```bash
python3.12 -m unittest discover -s src/mistralai/extra/tests -t src
```
</file>

<file path="src/mistralai/extra/struct_chat.py">
from ..models import ChatCompletionResponse, ChatCompletionChoice, AssistantMessage
from .utils.response_format import  CustomPydanticModel, pydantic_model_from_json
from typing import List, Optional, Type, Generic
from pydantic import BaseModel
import json

class ParsedAssistantMessage(AssistantMessage, Generic[CustomPydanticModel]):
    parsed: Optional[CustomPydanticModel]

class ParsedChatCompletionChoice(ChatCompletionChoice, Generic[CustomPydanticModel]):
    message: Optional[ParsedAssistantMessage[CustomPydanticModel]]  # type: ignore

class ParsedChatCompletionResponse(ChatCompletionResponse, Generic[CustomPydanticModel]):
    choices: Optional[List[ParsedChatCompletionChoice[CustomPydanticModel]]] # type: ignore

def convert_to_parsed_chat_completion_response(response: ChatCompletionResponse, response_format: Type[BaseModel]) -> ParsedChatCompletionResponse:
    parsed_choices = []

    if response.choices:
        for choice in response.choices:
            if choice.message:
                parsed_message: ParsedAssistantMessage = ParsedAssistantMessage(
                    **choice.message.model_dump(),
                    parsed=None
                )
                if isinstance(parsed_message.content, str):
                    parsed_message.parsed = pydantic_model_from_json(json.loads(parsed_message.content), response_format)
                elif parsed_message.content is None:
                    parsed_message.parsed = None
                else:
                    raise TypeError(f"Unexpected type for message.content: {type(parsed_message.content)}")
                choice_dict = choice.model_dump()
                choice_dict["message"] = parsed_message
                parsed_choice: ParsedChatCompletionChoice = ParsedChatCompletionChoice(**choice_dict)
                parsed_choices.append(parsed_choice)
            else:
                parsed_choice = ParsedChatCompletionChoice(**choice.model_dump())
                parsed_choices.append(parsed_choice)
    response_dict = response.model_dump()
    response_dict["choices"] = parsed_choices
    return ParsedChatCompletionResponse(**response_dict)
</file>

<file path="src/mistralai/models/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .agent import Agent, AgentObject, AgentTools, AgentToolsTypedDict, AgentTypedDict
from .agentconversation import (
    AgentConversation,
    AgentConversationObject,
    AgentConversationTypedDict,
)
from .agentcreationrequest import (
    AgentCreationRequest,
    AgentCreationRequestTools,
    AgentCreationRequestToolsTypedDict,
    AgentCreationRequestTypedDict,
)
from .agenthandoffdoneevent import (
    AgentHandoffDoneEvent,
    AgentHandoffDoneEventType,
    AgentHandoffDoneEventTypedDict,
)
from .agenthandoffentry import (
    AgentHandoffEntry,
    AgentHandoffEntryObject,
    AgentHandoffEntryType,
    AgentHandoffEntryTypedDict,
)
from .agenthandoffstartedevent import (
    AgentHandoffStartedEvent,
    AgentHandoffStartedEventType,
    AgentHandoffStartedEventTypedDict,
)
from .agents_api_v1_agents_getop import (
    AgentsAPIV1AgentsGetRequest,
    AgentsAPIV1AgentsGetRequestTypedDict,
)
from .agents_api_v1_agents_listop import (
    AgentsAPIV1AgentsListRequest,
    AgentsAPIV1AgentsListRequestTypedDict,
)
from .agents_api_v1_agents_update_versionop import (
    AgentsAPIV1AgentsUpdateVersionRequest,
    AgentsAPIV1AgentsUpdateVersionRequestTypedDict,
)
from .agents_api_v1_agents_updateop import (
    AgentsAPIV1AgentsUpdateRequest,
    AgentsAPIV1AgentsUpdateRequestTypedDict,
)
from .agents_api_v1_conversations_append_streamop import (
    AgentsAPIV1ConversationsAppendStreamRequest,
    AgentsAPIV1ConversationsAppendStreamRequestTypedDict,
)
from .agents_api_v1_conversations_appendop import (
    AgentsAPIV1ConversationsAppendRequest,
    AgentsAPIV1ConversationsAppendRequestTypedDict,
)
from .agents_api_v1_conversations_getop import (
    AgentsAPIV1ConversationsGetRequest,
    AgentsAPIV1ConversationsGetRequestTypedDict,
    AgentsAPIV1ConversationsGetResponseV1ConversationsGet,
    AgentsAPIV1ConversationsGetResponseV1ConversationsGetTypedDict,
)
from .agents_api_v1_conversations_historyop import (
    AgentsAPIV1ConversationsHistoryRequest,
    AgentsAPIV1ConversationsHistoryRequestTypedDict,
)
from .agents_api_v1_conversations_listop import (
    AgentsAPIV1ConversationsListRequest,
    AgentsAPIV1ConversationsListRequestTypedDict,
    ResponseBody,
    ResponseBodyTypedDict,
)
from .agents_api_v1_conversations_messagesop import (
    AgentsAPIV1ConversationsMessagesRequest,
    AgentsAPIV1ConversationsMessagesRequestTypedDict,
)
from .agents_api_v1_conversations_restart_streamop import (
    AgentsAPIV1ConversationsRestartStreamRequest,
    AgentsAPIV1ConversationsRestartStreamRequestTypedDict,
)
from .agents_api_v1_conversations_restartop import (
    AgentsAPIV1ConversationsRestartRequest,
    AgentsAPIV1ConversationsRestartRequestTypedDict,
)
from .agentscompletionrequest import (
    AgentsCompletionRequest,
    AgentsCompletionRequestMessages,
    AgentsCompletionRequestMessagesTypedDict,
    AgentsCompletionRequestStop,
    AgentsCompletionRequestStopTypedDict,
    AgentsCompletionRequestToolChoice,
    AgentsCompletionRequestToolChoiceTypedDict,
    AgentsCompletionRequestTypedDict,
)
from .agentscompletionstreamrequest import (
    AgentsCompletionStreamRequest,
    AgentsCompletionStreamRequestMessages,
    AgentsCompletionStreamRequestMessagesTypedDict,
    AgentsCompletionStreamRequestStop,
    AgentsCompletionStreamRequestStopTypedDict,
    AgentsCompletionStreamRequestToolChoice,
    AgentsCompletionStreamRequestToolChoiceTypedDict,
    AgentsCompletionStreamRequestTypedDict,
)
from .agentupdaterequest import (
    AgentUpdateRequest,
    AgentUpdateRequestTools,
    AgentUpdateRequestToolsTypedDict,
    AgentUpdateRequestTypedDict,
)
from .apiendpoint import APIEndpoint
from .archiveftmodelout import (
    ArchiveFTModelOut,
    ArchiveFTModelOutObject,
    ArchiveFTModelOutTypedDict,
)
from .assistantmessage import (
    AssistantMessage,
    AssistantMessageContent,
    AssistantMessageContentTypedDict,
    AssistantMessageRole,
    AssistantMessageTypedDict,
)
from .basemodelcard import BaseModelCard, BaseModelCardTypedDict, Type
from .batcherror import BatchError, BatchErrorTypedDict
from .batchjobin import BatchJobIn, BatchJobInTypedDict
from .batchjobout import BatchJobOut, BatchJobOutObject, BatchJobOutTypedDict
from .batchjobsout import BatchJobsOut, BatchJobsOutObject, BatchJobsOutTypedDict
from .batchjobstatus import BatchJobStatus
from .builtinconnectors import BuiltInConnectors
from .chatclassificationrequest import (
    ChatClassificationRequest,
    ChatClassificationRequestTypedDict,
)
from .chatcompletionchoice import (
    ChatCompletionChoice,
    ChatCompletionChoiceTypedDict,
    FinishReason,
)
from .chatcompletionrequest import (
    ChatCompletionRequest,
    ChatCompletionRequestToolChoice,
    ChatCompletionRequestToolChoiceTypedDict,
    ChatCompletionRequestTypedDict,
    Messages,
    MessagesTypedDict,
    Stop,
    StopTypedDict,
)
from .chatcompletionresponse import (
    ChatCompletionResponse,
    ChatCompletionResponseTypedDict,
)
from .chatcompletionstreamrequest import (
    ChatCompletionStreamRequest,
    ChatCompletionStreamRequestMessages,
    ChatCompletionStreamRequestMessagesTypedDict,
    ChatCompletionStreamRequestStop,
    ChatCompletionStreamRequestStopTypedDict,
    ChatCompletionStreamRequestToolChoice,
    ChatCompletionStreamRequestToolChoiceTypedDict,
    ChatCompletionStreamRequestTypedDict,
)
from .chatmoderationrequest import (
    ChatModerationRequest,
    ChatModerationRequestInputs,
    ChatModerationRequestInputsTypedDict,
    ChatModerationRequestTypedDict,
    One,
    OneTypedDict,
    Two,
    TwoTypedDict,
)
from .checkpointout import CheckpointOut, CheckpointOutTypedDict
from .classificationrequest import (
    ClassificationRequest,
    ClassificationRequestInputs,
    ClassificationRequestInputsTypedDict,
    ClassificationRequestTypedDict,
)
from .classificationresponse import (
    ClassificationResponse,
    ClassificationResponseTypedDict,
)
from .classificationtargetresult import (
    ClassificationTargetResult,
    ClassificationTargetResultTypedDict,
)
from .classifierdetailedjobout import (
    ClassifierDetailedJobOut,
    ClassifierDetailedJobOutIntegrations,
    ClassifierDetailedJobOutIntegrationsTypedDict,
    ClassifierDetailedJobOutJobType,
    ClassifierDetailedJobOutObject,
    ClassifierDetailedJobOutStatus,
    ClassifierDetailedJobOutTypedDict,
)
from .classifierftmodelout import (
    ClassifierFTModelOut,
    ClassifierFTModelOutModelType,
    ClassifierFTModelOutObject,
    ClassifierFTModelOutTypedDict,
)
from .classifierjobout import (
    ClassifierJobOut,
    ClassifierJobOutIntegrations,
    ClassifierJobOutIntegrationsTypedDict,
    ClassifierJobOutJobType,
    ClassifierJobOutObject,
    ClassifierJobOutStatus,
    ClassifierJobOutTypedDict,
)
from .classifiertargetin import ClassifierTargetIn, ClassifierTargetInTypedDict
from .classifiertargetout import ClassifierTargetOut, ClassifierTargetOutTypedDict
from .classifiertrainingparameters import (
    ClassifierTrainingParameters,
    ClassifierTrainingParametersTypedDict,
)
from .classifiertrainingparametersin import (
    ClassifierTrainingParametersIn,
    ClassifierTrainingParametersInTypedDict,
)
from .codeinterpretertool import (
    CodeInterpreterTool,
    CodeInterpreterToolType,
    CodeInterpreterToolTypedDict,
)
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .completionargsstop import CompletionArgsStop, CompletionArgsStopTypedDict
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from .completiondetailedjobout import (
    CompletionDetailedJobOut,
    CompletionDetailedJobOutIntegrations,
    CompletionDetailedJobOutIntegrationsTypedDict,
    CompletionDetailedJobOutJobType,
    CompletionDetailedJobOutObject,
    CompletionDetailedJobOutRepositories,
    CompletionDetailedJobOutRepositoriesTypedDict,
    CompletionDetailedJobOutStatus,
    CompletionDetailedJobOutTypedDict,
)
from .completionevent import CompletionEvent, CompletionEventTypedDict
from .completionftmodelout import (
    CompletionFTModelOut,
    CompletionFTModelOutObject,
    CompletionFTModelOutTypedDict,
    ModelType,
)
from .completionjobout import (
    CompletionJobOut,
    CompletionJobOutObject,
    CompletionJobOutTypedDict,
    Integrations,
    IntegrationsTypedDict,
    JobType,
    Repositories,
    RepositoriesTypedDict,
    Status,
)
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceFinishReason,
    CompletionResponseStreamChoiceTypedDict,
)
from .completiontrainingparameters import (
    CompletionTrainingParameters,
    CompletionTrainingParametersTypedDict,
)
from .completiontrainingparametersin import (
    CompletionTrainingParametersIn,
    CompletionTrainingParametersInTypedDict,
)
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .conversationappendrequest import (
    ConversationAppendRequest,
    ConversationAppendRequestHandoffExecution,
    ConversationAppendRequestTypedDict,
)
from .conversationappendstreamrequest import (
    ConversationAppendStreamRequest,
    ConversationAppendStreamRequestHandoffExecution,
    ConversationAppendStreamRequestTypedDict,
)
from .conversationevents import (
    ConversationEvents,
    ConversationEventsData,
    ConversationEventsDataTypedDict,
    ConversationEventsTypedDict,
)
from .conversationhistory import (
    ConversationHistory,
    ConversationHistoryObject,
    ConversationHistoryTypedDict,
    Entries,
    EntriesTypedDict,
)
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from .conversationmessages import (
    ConversationMessages,
    ConversationMessagesObject,
    ConversationMessagesTypedDict,
)
from .conversationrequest import (
    ConversationRequest,
    ConversationRequestTypedDict,
    HandoffExecution,
    Tools,
    ToolsTypedDict,
)
from .conversationresponse import (
    ConversationResponse,
    ConversationResponseObject,
    ConversationResponseTypedDict,
    Outputs,
    OutputsTypedDict,
)
from .conversationrestartrequest import (
    ConversationRestartRequest,
    ConversationRestartRequestHandoffExecution,
    ConversationRestartRequestTypedDict,
)
from .conversationrestartstreamrequest import (
    ConversationRestartStreamRequest,
    ConversationRestartStreamRequestHandoffExecution,
    ConversationRestartStreamRequestTypedDict,
)
from .conversationstreamrequest import (
    ConversationStreamRequest,
    ConversationStreamRequestHandoffExecution,
    ConversationStreamRequestTools,
    ConversationStreamRequestToolsTypedDict,
    ConversationStreamRequestTypedDict,
)
from .conversationusageinfo import ConversationUsageInfo, ConversationUsageInfoTypedDict
from .delete_model_v1_models_model_id_deleteop import (
    DeleteModelV1ModelsModelIDDeleteRequest,
    DeleteModelV1ModelsModelIDDeleteRequestTypedDict,
)
from .deletefileout import DeleteFileOut, DeleteFileOutTypedDict
from .deletemodelout import DeleteModelOut, DeleteModelOutTypedDict
from .deltamessage import Content, ContentTypedDict, DeltaMessage, DeltaMessageTypedDict
from .documentlibrarytool import (
    DocumentLibraryTool,
    DocumentLibraryToolType,
    DocumentLibraryToolTypedDict,
)
from .documenturlchunk import (
    DocumentURLChunk,
    DocumentURLChunkType,
    DocumentURLChunkTypedDict,
)
from .embeddingdtype import EmbeddingDtype
from .embeddingrequest import (
    EmbeddingRequest,
    EmbeddingRequestInputs,
    EmbeddingRequestInputsTypedDict,
    EmbeddingRequestTypedDict,
)
from .embeddingresponse import EmbeddingResponse, EmbeddingResponseTypedDict
from .embeddingresponsedata import EmbeddingResponseData, EmbeddingResponseDataTypedDict
from .eventout import EventOut, EventOutTypedDict
from .filepurpose import FilePurpose
from .files_api_routes_delete_fileop import (
    FilesAPIRoutesDeleteFileRequest,
    FilesAPIRoutesDeleteFileRequestTypedDict,
)
from .files_api_routes_download_fileop import (
    FilesAPIRoutesDownloadFileRequest,
    FilesAPIRoutesDownloadFileRequestTypedDict,
)
from .files_api_routes_get_signed_urlop import (
    FilesAPIRoutesGetSignedURLRequest,
    FilesAPIRoutesGetSignedURLRequestTypedDict,
)
from .files_api_routes_list_filesop import (
    FilesAPIRoutesListFilesRequest,
    FilesAPIRoutesListFilesRequestTypedDict,
)
from .files_api_routes_retrieve_fileop import (
    FilesAPIRoutesRetrieveFileRequest,
    FilesAPIRoutesRetrieveFileRequestTypedDict,
)
from .files_api_routes_upload_fileop import (
    File,
    FileTypedDict,
    FilesAPIRoutesUploadFileMultiPartBodyParams,
    FilesAPIRoutesUploadFileMultiPartBodyParamsTypedDict,
)
from .fileschema import FileSchema, FileSchemaTypedDict
from .filesignedurl import FileSignedURL, FileSignedURLTypedDict
from .fimcompletionrequest import (
    FIMCompletionRequest,
    FIMCompletionRequestStop,
    FIMCompletionRequestStopTypedDict,
    FIMCompletionRequestTypedDict,
)
from .fimcompletionresponse import FIMCompletionResponse, FIMCompletionResponseTypedDict
from .fimcompletionstreamrequest import (
    FIMCompletionStreamRequest,
    FIMCompletionStreamRequestStop,
    FIMCompletionStreamRequestStopTypedDict,
    FIMCompletionStreamRequestTypedDict,
)
from .finetuneablemodeltype import FineTuneableModelType
from .ftclassifierlossfunction import FTClassifierLossFunction
from .ftmodelcapabilitiesout import (
    FTModelCapabilitiesOut,
    FTModelCapabilitiesOutTypedDict,
)
from .ftmodelcard import FTModelCard, FTModelCardType, FTModelCardTypedDict
from .function import Function, FunctionTypedDict
from .functioncall import (
    Arguments,
    ArgumentsTypedDict,
    FunctionCall,
    FunctionCallTypedDict,
)
from .functioncallentry import (
    FunctionCallEntry,
    FunctionCallEntryObject,
    FunctionCallEntryType,
    FunctionCallEntryTypedDict,
)
from .functioncallentryarguments import (
    FunctionCallEntryArguments,
    FunctionCallEntryArgumentsTypedDict,
)
from .functioncallevent import (
    FunctionCallEvent,
    FunctionCallEventType,
    FunctionCallEventTypedDict,
)
from .functionname import FunctionName, FunctionNameTypedDict
from .functionresultentry import (
    FunctionResultEntry,
    FunctionResultEntryObject,
    FunctionResultEntryType,
    FunctionResultEntryTypedDict,
)
from .functiontool import FunctionTool, FunctionToolType, FunctionToolTypedDict
from .githubrepositoryin import (
    GithubRepositoryIn,
    GithubRepositoryInType,
    GithubRepositoryInTypedDict,
)
from .githubrepositoryout import (
    GithubRepositoryOut,
    GithubRepositoryOutType,
    GithubRepositoryOutTypedDict,
)
from .httpvalidationerror import HTTPValidationError, HTTPValidationErrorData
from .imagegenerationtool import (
    ImageGenerationTool,
    ImageGenerationToolType,
    ImageGenerationToolTypedDict,
)
from .imageurl import ImageURL, ImageURLTypedDict
from .imageurlchunk import (
    ImageURLChunk,
    ImageURLChunkImageURL,
    ImageURLChunkImageURLTypedDict,
    ImageURLChunkType,
    ImageURLChunkTypedDict,
)
from .inputentries import InputEntries, InputEntriesTypedDict
from .inputs import (
    Inputs,
    InputsTypedDict,
    InstructRequestInputs,
    InstructRequestInputsMessages,
    InstructRequestInputsMessagesTypedDict,
    InstructRequestInputsTypedDict,
)
from .instructrequest import (
    InstructRequest,
    InstructRequestMessages,
    InstructRequestMessagesTypedDict,
    InstructRequestTypedDict,
)
from .jobin import (
    Hyperparameters,
    HyperparametersTypedDict,
    JobIn,
    JobInIntegrations,
    JobInIntegrationsTypedDict,
    JobInRepositories,
    JobInRepositoriesTypedDict,
    JobInTypedDict,
)
from .jobmetadataout import JobMetadataOut, JobMetadataOutTypedDict
from .jobs_api_routes_batch_cancel_batch_jobop import (
    JobsAPIRoutesBatchCancelBatchJobRequest,
    JobsAPIRoutesBatchCancelBatchJobRequestTypedDict,
)
from .jobs_api_routes_batch_get_batch_jobop import (
    JobsAPIRoutesBatchGetBatchJobRequest,
    JobsAPIRoutesBatchGetBatchJobRequestTypedDict,
)
from .jobs_api_routes_batch_get_batch_jobsop import (
    JobsAPIRoutesBatchGetBatchJobsRequest,
    JobsAPIRoutesBatchGetBatchJobsRequestTypedDict,
)
from .jobs_api_routes_fine_tuning_archive_fine_tuned_modelop import (
    JobsAPIRoutesFineTuningArchiveFineTunedModelRequest,
    JobsAPIRoutesFineTuningArchiveFineTunedModelRequestTypedDict,
)
from .jobs_api_routes_fine_tuning_cancel_fine_tuning_jobop import (
    JobsAPIRoutesFineTuningCancelFineTuningJobRequest,
    JobsAPIRoutesFineTuningCancelFineTuningJobRequestTypedDict,
    JobsAPIRoutesFineTuningCancelFineTuningJobResponse,
    JobsAPIRoutesFineTuningCancelFineTuningJobResponseTypedDict,
)
from .jobs_api_routes_fine_tuning_create_fine_tuning_jobop import (
    JobsAPIRoutesFineTuningCreateFineTuningJobResponse,
    JobsAPIRoutesFineTuningCreateFineTuningJobResponseTypedDict,
    Response1,
    Response1TypedDict,
)
from .jobs_api_routes_fine_tuning_get_fine_tuning_jobop import (
    JobsAPIRoutesFineTuningGetFineTuningJobRequest,
    JobsAPIRoutesFineTuningGetFineTuningJobRequestTypedDict,
    JobsAPIRoutesFineTuningGetFineTuningJobResponse,
    JobsAPIRoutesFineTuningGetFineTuningJobResponseTypedDict,
)
from .jobs_api_routes_fine_tuning_get_fine_tuning_jobsop import (
    JobsAPIRoutesFineTuningGetFineTuningJobsRequest,
    JobsAPIRoutesFineTuningGetFineTuningJobsRequestTypedDict,
    QueryParamStatus,
)
from .jobs_api_routes_fine_tuning_start_fine_tuning_jobop import (
    JobsAPIRoutesFineTuningStartFineTuningJobRequest,
    JobsAPIRoutesFineTuningStartFineTuningJobRequestTypedDict,
    JobsAPIRoutesFineTuningStartFineTuningJobResponse,
    JobsAPIRoutesFineTuningStartFineTuningJobResponseTypedDict,
)
from .jobs_api_routes_fine_tuning_unarchive_fine_tuned_modelop import (
    JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest,
    JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequestTypedDict,
)
from .jobs_api_routes_fine_tuning_update_fine_tuned_modelop import (
    JobsAPIRoutesFineTuningUpdateFineTunedModelRequest,
    JobsAPIRoutesFineTuningUpdateFineTunedModelRequestTypedDict,
    JobsAPIRoutesFineTuningUpdateFineTunedModelResponse,
    JobsAPIRoutesFineTuningUpdateFineTunedModelResponseTypedDict,
)
from .jobsout import (
    JobsOut,
    JobsOutData,
    JobsOutDataTypedDict,
    JobsOutObject,
    JobsOutTypedDict,
)
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .legacyjobmetadataout import (
    LegacyJobMetadataOut,
    LegacyJobMetadataOutObject,
    LegacyJobMetadataOutTypedDict,
)
from .listfilesout import ListFilesOut, ListFilesOutTypedDict
from .messageentries import MessageEntries, MessageEntriesTypedDict
from .messageinputcontentchunks import (
    MessageInputContentChunks,
    MessageInputContentChunksTypedDict,
)
from .messageinputentry import (
    MessageInputEntry,
    MessageInputEntryContent,
    MessageInputEntryContentTypedDict,
    MessageInputEntryRole,
    MessageInputEntryType,
    MessageInputEntryTypedDict,
    Object,
)
from .messageoutputcontentchunks import (
    MessageOutputContentChunks,
    MessageOutputContentChunksTypedDict,
)
from .messageoutputentry import (
    MessageOutputEntry,
    MessageOutputEntryContent,
    MessageOutputEntryContentTypedDict,
    MessageOutputEntryObject,
    MessageOutputEntryRole,
    MessageOutputEntryType,
    MessageOutputEntryTypedDict,
)
from .messageoutputevent import (
    MessageOutputEvent,
    MessageOutputEventContent,
    MessageOutputEventContentTypedDict,
    MessageOutputEventRole,
    MessageOutputEventType,
    MessageOutputEventTypedDict,
)
from .metricout import MetricOut, MetricOutTypedDict
from .mistralpromptmode import MistralPromptMode
from .modelcapabilities import ModelCapabilities, ModelCapabilitiesTypedDict
from .modelconversation import (
    ModelConversation,
    ModelConversationObject,
    ModelConversationTools,
    ModelConversationToolsTypedDict,
    ModelConversationTypedDict,
)
from .modellist import Data, DataTypedDict, ModelList, ModelListTypedDict
from .moderationobject import ModerationObject, ModerationObjectTypedDict
from .moderationresponse import ModerationResponse, ModerationResponseTypedDict
from .ocrimageobject import OCRImageObject, OCRImageObjectTypedDict
from .ocrpagedimensions import OCRPageDimensions, OCRPageDimensionsTypedDict
from .ocrpageobject import OCRPageObject, OCRPageObjectTypedDict
from .ocrrequest import Document, DocumentTypedDict, OCRRequest, OCRRequestTypedDict
from .ocrresponse import OCRResponse, OCRResponseTypedDict
from .ocrusageinfo import OCRUsageInfo, OCRUsageInfoTypedDict
from .outputcontentchunks import OutputContentChunks, OutputContentChunksTypedDict
from .prediction import Prediction, PredictionTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkType, ReferenceChunkTypedDict
from .responsedoneevent import (
    ResponseDoneEvent,
    ResponseDoneEventType,
    ResponseDoneEventTypedDict,
)
from .responseerrorevent import (
    ResponseErrorEvent,
    ResponseErrorEventType,
    ResponseErrorEventTypedDict,
)
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .responseformats import ResponseFormats
from .responsestartedevent import (
    ResponseStartedEvent,
    ResponseStartedEventType,
    ResponseStartedEventTypedDict,
)
from .retrieve_model_v1_models_model_id_getop import (
    RetrieveModelV1ModelsModelIDGetRequest,
    RetrieveModelV1ModelsModelIDGetRequestTypedDict,
    RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet,
    RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGetTypedDict,
)
from .retrievefileout import RetrieveFileOut, RetrieveFileOutTypedDict
from .sampletype import SampleType
from .sdkerror import SDKError
from .security import Security, SecurityTypedDict
from .source import Source
from .ssetypes import SSETypes
from .systemmessage import (
    Role,
    SystemMessage,
    SystemMessageContent,
    SystemMessageContentTypedDict,
    SystemMessageTypedDict,
)
from .textchunk import TextChunk, TextChunkType, TextChunkTypedDict
from .tool import Tool, ToolTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolexecutiondoneevent import (
    ToolExecutionDoneEvent,
    ToolExecutionDoneEventType,
    ToolExecutionDoneEventTypedDict,
)
from .toolexecutionentry import (
    ToolExecutionEntry,
    ToolExecutionEntryObject,
    ToolExecutionEntryType,
    ToolExecutionEntryTypedDict,
)
from .toolexecutionstartedevent import (
    ToolExecutionStartedEvent,
    ToolExecutionStartedEventType,
    ToolExecutionStartedEventTypedDict,
)
from .toolfilechunk import ToolFileChunk, ToolFileChunkType, ToolFileChunkTypedDict
from .toolmessage import (
    ToolMessage,
    ToolMessageContent,
    ToolMessageContentTypedDict,
    ToolMessageRole,
    ToolMessageTypedDict,
)
from .toolreferencechunk import (
    ToolReferenceChunk,
    ToolReferenceChunkType,
    ToolReferenceChunkTypedDict,
)
from .tooltypes import ToolTypes
from .trainingfile import TrainingFile, TrainingFileTypedDict
from .unarchiveftmodelout import (
    UnarchiveFTModelOut,
    UnarchiveFTModelOutObject,
    UnarchiveFTModelOutTypedDict,
)
from .updateftmodelin import UpdateFTModelIn, UpdateFTModelInTypedDict
from .uploadfileout import UploadFileOut, UploadFileOutTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from .usermessage import (
    UserMessage,
    UserMessageContent,
    UserMessageContentTypedDict,
    UserMessageRole,
    UserMessageTypedDict,
)
from .validationerror import (
    Loc,
    LocTypedDict,
    ValidationError,
    ValidationErrorTypedDict,
)
from .wandbintegration import (
    WandbIntegration,
    WandbIntegrationType,
    WandbIntegrationTypedDict,
)
from .wandbintegrationout import (
    WandbIntegrationOut,
    WandbIntegrationOutType,
    WandbIntegrationOutTypedDict,
)
from .websearchpremiumtool import (
    WebSearchPremiumTool,
    WebSearchPremiumToolType,
    WebSearchPremiumToolTypedDict,
)
from .websearchtool import WebSearchTool, WebSearchToolType, WebSearchToolTypedDict


__all__ = [
    "APIEndpoint",
    "Agent",
    "AgentConversation",
    "AgentConversationObject",
    "AgentConversationTypedDict",
    "AgentCreationRequest",
    "AgentCreationRequestTools",
    "AgentCreationRequestToolsTypedDict",
    "AgentCreationRequestTypedDict",
    "AgentHandoffDoneEvent",
    "AgentHandoffDoneEventType",
    "AgentHandoffDoneEventTypedDict",
    "AgentHandoffEntry",
    "AgentHandoffEntryObject",
    "AgentHandoffEntryType",
    "AgentHandoffEntryTypedDict",
    "AgentHandoffStartedEvent",
    "AgentHandoffStartedEventType",
    "AgentHandoffStartedEventTypedDict",
    "AgentObject",
    "AgentTools",
    "AgentToolsTypedDict",
    "AgentTypedDict",
    "AgentUpdateRequest",
    "AgentUpdateRequestTools",
    "AgentUpdateRequestToolsTypedDict",
    "AgentUpdateRequestTypedDict",
    "AgentsAPIV1AgentsGetRequest",
    "AgentsAPIV1AgentsGetRequestTypedDict",
    "AgentsAPIV1AgentsListRequest",
    "AgentsAPIV1AgentsListRequestTypedDict",
    "AgentsAPIV1AgentsUpdateRequest",
    "AgentsAPIV1AgentsUpdateRequestTypedDict",
    "AgentsAPIV1AgentsUpdateVersionRequest",
    "AgentsAPIV1AgentsUpdateVersionRequestTypedDict",
    "AgentsAPIV1ConversationsAppendRequest",
    "AgentsAPIV1ConversationsAppendRequestTypedDict",
    "AgentsAPIV1ConversationsAppendStreamRequest",
    "AgentsAPIV1ConversationsAppendStreamRequestTypedDict",
    "AgentsAPIV1ConversationsGetRequest",
    "AgentsAPIV1ConversationsGetRequestTypedDict",
    "AgentsAPIV1ConversationsGetResponseV1ConversationsGet",
    "AgentsAPIV1ConversationsGetResponseV1ConversationsGetTypedDict",
    "AgentsAPIV1ConversationsHistoryRequest",
    "AgentsAPIV1ConversationsHistoryRequestTypedDict",
    "AgentsAPIV1ConversationsListRequest",
    "AgentsAPIV1ConversationsListRequestTypedDict",
    "AgentsAPIV1ConversationsMessagesRequest",
    "AgentsAPIV1ConversationsMessagesRequestTypedDict",
    "AgentsAPIV1ConversationsRestartRequest",
    "AgentsAPIV1ConversationsRestartRequestTypedDict",
    "AgentsAPIV1ConversationsRestartStreamRequest",
    "AgentsAPIV1ConversationsRestartStreamRequestTypedDict",
    "AgentsCompletionRequest",
    "AgentsCompletionRequestMessages",
    "AgentsCompletionRequestMessagesTypedDict",
    "AgentsCompletionRequestStop",
    "AgentsCompletionRequestStopTypedDict",
    "AgentsCompletionRequestToolChoice",
    "AgentsCompletionRequestToolChoiceTypedDict",
    "AgentsCompletionRequestTypedDict",
    "AgentsCompletionStreamRequest",
    "AgentsCompletionStreamRequestMessages",
    "AgentsCompletionStreamRequestMessagesTypedDict",
    "AgentsCompletionStreamRequestStop",
    "AgentsCompletionStreamRequestStopTypedDict",
    "AgentsCompletionStreamRequestToolChoice",
    "AgentsCompletionStreamRequestToolChoiceTypedDict",
    "AgentsCompletionStreamRequestTypedDict",
    "ArchiveFTModelOut",
    "ArchiveFTModelOutObject",
    "ArchiveFTModelOutTypedDict",
    "Arguments",
    "ArgumentsTypedDict",
    "AssistantMessage",
    "AssistantMessageContent",
    "AssistantMessageContentTypedDict",
    "AssistantMessageRole",
    "AssistantMessageTypedDict",
    "BaseModelCard",
    "BaseModelCardTypedDict",
    "BatchError",
    "BatchErrorTypedDict",
    "BatchJobIn",
    "BatchJobInTypedDict",
    "BatchJobOut",
    "BatchJobOutObject",
    "BatchJobOutTypedDict",
    "BatchJobStatus",
    "BatchJobsOut",
    "BatchJobsOutObject",
    "BatchJobsOutTypedDict",
    "BuiltInConnectors",
    "ChatClassificationRequest",
    "ChatClassificationRequestTypedDict",
    "ChatCompletionChoice",
    "ChatCompletionChoiceTypedDict",
    "ChatCompletionRequest",
    "ChatCompletionRequestToolChoice",
    "ChatCompletionRequestToolChoiceTypedDict",
    "ChatCompletionRequestTypedDict",
    "ChatCompletionResponse",
    "ChatCompletionResponseTypedDict",
    "ChatCompletionStreamRequest",
    "ChatCompletionStreamRequestMessages",
    "ChatCompletionStreamRequestMessagesTypedDict",
    "ChatCompletionStreamRequestStop",
    "ChatCompletionStreamRequestStopTypedDict",
    "ChatCompletionStreamRequestToolChoice",
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    "ChatCompletionStreamRequestTypedDict",
    "ChatModerationRequest",
    "ChatModerationRequestInputs",
    "ChatModerationRequestInputsTypedDict",
    "ChatModerationRequestTypedDict",
    "CheckpointOut",
    "CheckpointOutTypedDict",
    "ClassificationRequest",
    "ClassificationRequestInputs",
    "ClassificationRequestInputsTypedDict",
    "ClassificationRequestTypedDict",
    "ClassificationResponse",
    "ClassificationResponseTypedDict",
    "ClassificationTargetResult",
    "ClassificationTargetResultTypedDict",
    "ClassifierDetailedJobOut",
    "ClassifierDetailedJobOutIntegrations",
    "ClassifierDetailedJobOutIntegrationsTypedDict",
    "ClassifierDetailedJobOutJobType",
    "ClassifierDetailedJobOutObject",
    "ClassifierDetailedJobOutStatus",
    "ClassifierDetailedJobOutTypedDict",
    "ClassifierFTModelOut",
    "ClassifierFTModelOutModelType",
    "ClassifierFTModelOutObject",
    "ClassifierFTModelOutTypedDict",
    "ClassifierJobOut",
    "ClassifierJobOutIntegrations",
    "ClassifierJobOutIntegrationsTypedDict",
    "ClassifierJobOutJobType",
    "ClassifierJobOutObject",
    "ClassifierJobOutStatus",
    "ClassifierJobOutTypedDict",
    "ClassifierTargetIn",
    "ClassifierTargetInTypedDict",
    "ClassifierTargetOut",
    "ClassifierTargetOutTypedDict",
    "ClassifierTrainingParameters",
    "ClassifierTrainingParametersIn",
    "ClassifierTrainingParametersInTypedDict",
    "ClassifierTrainingParametersTypedDict",
    "CodeInterpreterTool",
    "CodeInterpreterToolType",
    "CodeInterpreterToolTypedDict",
    "CompletionArgs",
    "CompletionArgsStop",
    "CompletionArgsStopTypedDict",
    "CompletionArgsTypedDict",
    "CompletionChunk",
    "CompletionChunkTypedDict",
    "CompletionDetailedJobOut",
    "CompletionDetailedJobOutIntegrations",
    "CompletionDetailedJobOutIntegrationsTypedDict",
    "CompletionDetailedJobOutJobType",
    "CompletionDetailedJobOutObject",
    "CompletionDetailedJobOutRepositories",
    "CompletionDetailedJobOutRepositoriesTypedDict",
    "CompletionDetailedJobOutStatus",
    "CompletionDetailedJobOutTypedDict",
    "CompletionEvent",
    "CompletionEventTypedDict",
    "CompletionFTModelOut",
    "CompletionFTModelOutObject",
    "CompletionFTModelOutTypedDict",
    "CompletionJobOut",
    "CompletionJobOutObject",
    "CompletionJobOutTypedDict",
    "CompletionResponseStreamChoice",
    "CompletionResponseStreamChoiceFinishReason",
    "CompletionResponseStreamChoiceTypedDict",
    "CompletionTrainingParameters",
    "CompletionTrainingParametersIn",
    "CompletionTrainingParametersInTypedDict",
    "CompletionTrainingParametersTypedDict",
    "Content",
    "ContentChunk",
    "ContentChunkTypedDict",
    "ContentTypedDict",
    "ConversationAppendRequest",
    "ConversationAppendRequestHandoffExecution",
    "ConversationAppendRequestTypedDict",
    "ConversationAppendStreamRequest",
    "ConversationAppendStreamRequestHandoffExecution",
    "ConversationAppendStreamRequestTypedDict",
    "ConversationEvents",
    "ConversationEventsData",
    "ConversationEventsDataTypedDict",
    "ConversationEventsTypedDict",
    "ConversationHistory",
    "ConversationHistoryObject",
    "ConversationHistoryTypedDict",
    "ConversationInputs",
    "ConversationInputsTypedDict",
    "ConversationMessages",
    "ConversationMessagesObject",
    "ConversationMessagesTypedDict",
    "ConversationRequest",
    "ConversationRequestTypedDict",
    "ConversationResponse",
    "ConversationResponseObject",
    "ConversationResponseTypedDict",
    "ConversationRestartRequest",
    "ConversationRestartRequestHandoffExecution",
    "ConversationRestartRequestTypedDict",
    "ConversationRestartStreamRequest",
    "ConversationRestartStreamRequestHandoffExecution",
    "ConversationRestartStreamRequestTypedDict",
    "ConversationStreamRequest",
    "ConversationStreamRequestHandoffExecution",
    "ConversationStreamRequestTools",
    "ConversationStreamRequestToolsTypedDict",
    "ConversationStreamRequestTypedDict",
    "ConversationUsageInfo",
    "ConversationUsageInfoTypedDict",
    "Data",
    "DataTypedDict",
    "DeleteFileOut",
    "DeleteFileOutTypedDict",
    "DeleteModelOut",
    "DeleteModelOutTypedDict",
    "DeleteModelV1ModelsModelIDDeleteRequest",
    "DeleteModelV1ModelsModelIDDeleteRequestTypedDict",
    "DeltaMessage",
    "DeltaMessageTypedDict",
    "Document",
    "DocumentLibraryTool",
    "DocumentLibraryToolType",
    "DocumentLibraryToolTypedDict",
    "DocumentTypedDict",
    "DocumentURLChunk",
    "DocumentURLChunkType",
    "DocumentURLChunkTypedDict",
    "EmbeddingDtype",
    "EmbeddingRequest",
    "EmbeddingRequestInputs",
    "EmbeddingRequestInputsTypedDict",
    "EmbeddingRequestTypedDict",
    "EmbeddingResponse",
    "EmbeddingResponseData",
    "EmbeddingResponseDataTypedDict",
    "EmbeddingResponseTypedDict",
    "Entries",
    "EntriesTypedDict",
    "EventOut",
    "EventOutTypedDict",
    "FIMCompletionRequest",
    "FIMCompletionRequestStop",
    "FIMCompletionRequestStopTypedDict",
    "FIMCompletionRequestTypedDict",
    "FIMCompletionResponse",
    "FIMCompletionResponseTypedDict",
    "FIMCompletionStreamRequest",
    "FIMCompletionStreamRequestStop",
    "FIMCompletionStreamRequestStopTypedDict",
    "FIMCompletionStreamRequestTypedDict",
    "FTClassifierLossFunction",
    "FTModelCapabilitiesOut",
    "FTModelCapabilitiesOutTypedDict",
    "FTModelCard",
    "FTModelCardType",
    "FTModelCardTypedDict",
    "File",
    "FilePurpose",
    "FileSchema",
    "FileSchemaTypedDict",
    "FileSignedURL",
    "FileSignedURLTypedDict",
    "FileTypedDict",
    "FilesAPIRoutesDeleteFileRequest",
    "FilesAPIRoutesDeleteFileRequestTypedDict",
    "FilesAPIRoutesDownloadFileRequest",
    "FilesAPIRoutesDownloadFileRequestTypedDict",
    "FilesAPIRoutesGetSignedURLRequest",
    "FilesAPIRoutesGetSignedURLRequestTypedDict",
    "FilesAPIRoutesListFilesRequest",
    "FilesAPIRoutesListFilesRequestTypedDict",
    "FilesAPIRoutesRetrieveFileRequest",
    "FilesAPIRoutesRetrieveFileRequestTypedDict",
    "FilesAPIRoutesUploadFileMultiPartBodyParams",
    "FilesAPIRoutesUploadFileMultiPartBodyParamsTypedDict",
    "FineTuneableModelType",
    "FinishReason",
    "Function",
    "FunctionCall",
    "FunctionCallEntry",
    "FunctionCallEntryArguments",
    "FunctionCallEntryArgumentsTypedDict",
    "FunctionCallEntryObject",
    "FunctionCallEntryType",
    "FunctionCallEntryTypedDict",
    "FunctionCallEvent",
    "FunctionCallEventType",
    "FunctionCallEventTypedDict",
    "FunctionCallTypedDict",
    "FunctionName",
    "FunctionNameTypedDict",
    "FunctionResultEntry",
    "FunctionResultEntryObject",
    "FunctionResultEntryType",
    "FunctionResultEntryTypedDict",
    "FunctionTool",
    "FunctionToolType",
    "FunctionToolTypedDict",
    "FunctionTypedDict",
    "GithubRepositoryIn",
    "GithubRepositoryInType",
    "GithubRepositoryInTypedDict",
    "GithubRepositoryOut",
    "GithubRepositoryOutType",
    "GithubRepositoryOutTypedDict",
    "HTTPValidationError",
    "HTTPValidationErrorData",
    "HandoffExecution",
    "Hyperparameters",
    "HyperparametersTypedDict",
    "ImageGenerationTool",
    "ImageGenerationToolType",
    "ImageGenerationToolTypedDict",
    "ImageURL",
    "ImageURLChunk",
    "ImageURLChunkImageURL",
    "ImageURLChunkImageURLTypedDict",
    "ImageURLChunkType",
    "ImageURLChunkTypedDict",
    "ImageURLTypedDict",
    "InputEntries",
    "InputEntriesTypedDict",
    "Inputs",
    "InputsTypedDict",
    "InstructRequest",
    "InstructRequestInputs",
    "InstructRequestInputsMessages",
    "InstructRequestInputsMessagesTypedDict",
    "InstructRequestInputsTypedDict",
    "InstructRequestMessages",
    "InstructRequestMessagesTypedDict",
    "InstructRequestTypedDict",
    "Integrations",
    "IntegrationsTypedDict",
    "JSONSchema",
    "JSONSchemaTypedDict",
    "JobIn",
    "JobInIntegrations",
    "JobInIntegrationsTypedDict",
    "JobInRepositories",
    "JobInRepositoriesTypedDict",
    "JobInTypedDict",
    "JobMetadataOut",
    "JobMetadataOutTypedDict",
    "JobType",
    "JobsAPIRoutesBatchCancelBatchJobRequest",
    "JobsAPIRoutesBatchCancelBatchJobRequestTypedDict",
    "JobsAPIRoutesBatchGetBatchJobRequest",
    "JobsAPIRoutesBatchGetBatchJobRequestTypedDict",
    "JobsAPIRoutesBatchGetBatchJobsRequest",
    "JobsAPIRoutesBatchGetBatchJobsRequestTypedDict",
    "JobsAPIRoutesFineTuningArchiveFineTunedModelRequest",
    "JobsAPIRoutesFineTuningArchiveFineTunedModelRequestTypedDict",
    "JobsAPIRoutesFineTuningCancelFineTuningJobRequest",
    "JobsAPIRoutesFineTuningCancelFineTuningJobRequestTypedDict",
    "JobsAPIRoutesFineTuningCancelFineTuningJobResponse",
    "JobsAPIRoutesFineTuningCancelFineTuningJobResponseTypedDict",
    "JobsAPIRoutesFineTuningCreateFineTuningJobResponse",
    "JobsAPIRoutesFineTuningCreateFineTuningJobResponseTypedDict",
    "JobsAPIRoutesFineTuningGetFineTuningJobRequest",
    "JobsAPIRoutesFineTuningGetFineTuningJobRequestTypedDict",
    "JobsAPIRoutesFineTuningGetFineTuningJobResponse",
    "JobsAPIRoutesFineTuningGetFineTuningJobResponseTypedDict",
    "JobsAPIRoutesFineTuningGetFineTuningJobsRequest",
    "JobsAPIRoutesFineTuningGetFineTuningJobsRequestTypedDict",
    "JobsAPIRoutesFineTuningStartFineTuningJobRequest",
    "JobsAPIRoutesFineTuningStartFineTuningJobRequestTypedDict",
    "JobsAPIRoutesFineTuningStartFineTuningJobResponse",
    "JobsAPIRoutesFineTuningStartFineTuningJobResponseTypedDict",
    "JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest",
    "JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequestTypedDict",
    "JobsAPIRoutesFineTuningUpdateFineTunedModelRequest",
    "JobsAPIRoutesFineTuningUpdateFineTunedModelRequestTypedDict",
    "JobsAPIRoutesFineTuningUpdateFineTunedModelResponse",
    "JobsAPIRoutesFineTuningUpdateFineTunedModelResponseTypedDict",
    "JobsOut",
    "JobsOutData",
    "JobsOutDataTypedDict",
    "JobsOutObject",
    "JobsOutTypedDict",
    "LegacyJobMetadataOut",
    "LegacyJobMetadataOutObject",
    "LegacyJobMetadataOutTypedDict",
    "ListFilesOut",
    "ListFilesOutTypedDict",
    "Loc",
    "LocTypedDict",
    "MessageEntries",
    "MessageEntriesTypedDict",
    "MessageInputContentChunks",
    "MessageInputContentChunksTypedDict",
    "MessageInputEntry",
    "MessageInputEntryContent",
    "MessageInputEntryContentTypedDict",
    "MessageInputEntryRole",
    "MessageInputEntryType",
    "MessageInputEntryTypedDict",
    "MessageOutputContentChunks",
    "MessageOutputContentChunksTypedDict",
    "MessageOutputEntry",
    "MessageOutputEntryContent",
    "MessageOutputEntryContentTypedDict",
    "MessageOutputEntryObject",
    "MessageOutputEntryRole",
    "MessageOutputEntryType",
    "MessageOutputEntryTypedDict",
    "MessageOutputEvent",
    "MessageOutputEventContent",
    "MessageOutputEventContentTypedDict",
    "MessageOutputEventRole",
    "MessageOutputEventType",
    "MessageOutputEventTypedDict",
    "Messages",
    "MessagesTypedDict",
    "MetricOut",
    "MetricOutTypedDict",
    "MistralPromptMode",
    "ModelCapabilities",
    "ModelCapabilitiesTypedDict",
    "ModelConversation",
    "ModelConversationObject",
    "ModelConversationTools",
    "ModelConversationToolsTypedDict",
    "ModelConversationTypedDict",
    "ModelList",
    "ModelListTypedDict",
    "ModelType",
    "ModerationObject",
    "ModerationObjectTypedDict",
    "ModerationResponse",
    "ModerationResponseTypedDict",
    "OCRImageObject",
    "OCRImageObjectTypedDict",
    "OCRPageDimensions",
    "OCRPageDimensionsTypedDict",
    "OCRPageObject",
    "OCRPageObjectTypedDict",
    "OCRRequest",
    "OCRRequestTypedDict",
    "OCRResponse",
    "OCRResponseTypedDict",
    "OCRUsageInfo",
    "OCRUsageInfoTypedDict",
    "Object",
    "One",
    "OneTypedDict",
    "OutputContentChunks",
    "OutputContentChunksTypedDict",
    "Outputs",
    "OutputsTypedDict",
    "Prediction",
    "PredictionTypedDict",
    "QueryParamStatus",
    "ReferenceChunk",
    "ReferenceChunkType",
    "ReferenceChunkTypedDict",
    "Repositories",
    "RepositoriesTypedDict",
    "Response1",
    "Response1TypedDict",
    "ResponseBody",
    "ResponseBodyTypedDict",
    "ResponseDoneEvent",
    "ResponseDoneEventType",
    "ResponseDoneEventTypedDict",
    "ResponseErrorEvent",
    "ResponseErrorEventType",
    "ResponseErrorEventTypedDict",
    "ResponseFormat",
    "ResponseFormatTypedDict",
    "ResponseFormats",
    "ResponseStartedEvent",
    "ResponseStartedEventType",
    "ResponseStartedEventTypedDict",
    "RetrieveFileOut",
    "RetrieveFileOutTypedDict",
    "RetrieveModelV1ModelsModelIDGetRequest",
    "RetrieveModelV1ModelsModelIDGetRequestTypedDict",
    "RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet",
    "RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGetTypedDict",
    "Role",
    "SDKError",
    "SSETypes",
    "SampleType",
    "Security",
    "SecurityTypedDict",
    "Source",
    "Status",
    "Stop",
    "StopTypedDict",
    "SystemMessage",
    "SystemMessageContent",
    "SystemMessageContentTypedDict",
    "SystemMessageTypedDict",
    "TextChunk",
    "TextChunkType",
    "TextChunkTypedDict",
    "Tool",
    "ToolCall",
    "ToolCallTypedDict",
    "ToolChoice",
    "ToolChoiceEnum",
    "ToolChoiceTypedDict",
    "ToolExecutionDoneEvent",
    "ToolExecutionDoneEventType",
    "ToolExecutionDoneEventTypedDict",
    "ToolExecutionEntry",
    "ToolExecutionEntryObject",
    "ToolExecutionEntryType",
    "ToolExecutionEntryTypedDict",
    "ToolExecutionStartedEvent",
    "ToolExecutionStartedEventType",
    "ToolExecutionStartedEventTypedDict",
    "ToolFileChunk",
    "ToolFileChunkType",
    "ToolFileChunkTypedDict",
    "ToolMessage",
    "ToolMessageContent",
    "ToolMessageContentTypedDict",
    "ToolMessageRole",
    "ToolMessageTypedDict",
    "ToolReferenceChunk",
    "ToolReferenceChunkType",
    "ToolReferenceChunkTypedDict",
    "ToolTypedDict",
    "ToolTypes",
    "Tools",
    "ToolsTypedDict",
    "TrainingFile",
    "TrainingFileTypedDict",
    "Two",
    "TwoTypedDict",
    "Type",
    "UnarchiveFTModelOut",
    "UnarchiveFTModelOutObject",
    "UnarchiveFTModelOutTypedDict",
    "UpdateFTModelIn",
    "UpdateFTModelInTypedDict",
    "UploadFileOut",
    "UploadFileOutTypedDict",
    "UsageInfo",
    "UsageInfoTypedDict",
    "UserMessage",
    "UserMessageContent",
    "UserMessageContentTypedDict",
    "UserMessageRole",
    "UserMessageTypedDict",
    "ValidationError",
    "ValidationErrorTypedDict",
    "WandbIntegration",
    "WandbIntegrationOut",
    "WandbIntegrationOutType",
    "WandbIntegrationOutTypedDict",
    "WandbIntegrationType",
    "WandbIntegrationTypedDict",
    "WebSearchPremiumTool",
    "WebSearchPremiumToolType",
    "WebSearchPremiumToolTypedDict",
    "WebSearchTool",
    "WebSearchToolType",
    "WebSearchToolTypedDict",
]
</file>

<file path="src/mistralai/models/agent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentToolsTypedDict = TypeAliasType(
    "AgentToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


AgentTools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


AgentObject = Literal["agent"]


class AgentTypedDict(TypedDict):
    model: str
    name: str
    id: str
    version: int
    created_at: datetime
    updated_at: datetime
    instructions: NotRequired[Nullable[str]]
    r"""Instruction prompt the model will follow during the conversation."""
    tools: NotRequired[List[AgentToolsTypedDict]]
    r"""List of tools which are available to the model during the conversation."""
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""
    description: NotRequired[Nullable[str]]
    handoffs: NotRequired[Nullable[List[str]]]
    object: NotRequired[AgentObject]


class Agent(BaseModel):
    model: str

    name: str

    id: str

    version: int

    created_at: datetime

    updated_at: datetime

    instructions: OptionalNullable[str] = UNSET
    r"""Instruction prompt the model will follow during the conversation."""

    tools: Optional[List[AgentTools]] = None
    r"""List of tools which are available to the model during the conversation."""

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""

    description: OptionalNullable[str] = UNSET

    handoffs: OptionalNullable[List[str]] = UNSET

    object: Optional[AgentObject] = "agent"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "instructions",
            "tools",
            "completion_args",
            "description",
            "handoffs",
            "object",
        ]
        nullable_fields = ["instructions", "description", "handoffs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agentconversation.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


AgentConversationObject = Literal["conversation"]


class AgentConversationTypedDict(TypedDict):
    id: str
    created_at: datetime
    updated_at: datetime
    agent_id: str
    name: NotRequired[Nullable[str]]
    r"""Name given to the conversation."""
    description: NotRequired[Nullable[str]]
    r"""Description of the what the conversation is about."""
    object: NotRequired[AgentConversationObject]


class AgentConversation(BaseModel):
    id: str

    created_at: datetime

    updated_at: datetime

    agent_id: str

    name: OptionalNullable[str] = UNSET
    r"""Name given to the conversation."""

    description: OptionalNullable[str] = UNSET
    r"""Description of the what the conversation is about."""

    object: Optional[AgentConversationObject] = "conversation"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["name", "description", "object"]
        nullable_fields = ["name", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agentcreationrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentCreationRequestToolsTypedDict = TypeAliasType(
    "AgentCreationRequestToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


AgentCreationRequestTools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class AgentCreationRequestTypedDict(TypedDict):
    model: str
    name: str
    instructions: NotRequired[Nullable[str]]
    r"""Instruction prompt the model will follow during the conversation."""
    tools: NotRequired[List[AgentCreationRequestToolsTypedDict]]
    r"""List of tools which are available to the model during the conversation."""
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""
    description: NotRequired[Nullable[str]]
    handoffs: NotRequired[Nullable[List[str]]]


class AgentCreationRequest(BaseModel):
    model: str

    name: str

    instructions: OptionalNullable[str] = UNSET
    r"""Instruction prompt the model will follow during the conversation."""

    tools: Optional[List[AgentCreationRequestTools]] = None
    r"""List of tools which are available to the model during the conversation."""

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""

    description: OptionalNullable[str] = UNSET

    handoffs: OptionalNullable[List[str]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "instructions",
            "tools",
            "completion_args",
            "description",
            "handoffs",
        ]
        nullable_fields = ["instructions", "description", "handoffs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agenthandoffdoneevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


AgentHandoffDoneEventType = Literal["agent.handoff.done"]


class AgentHandoffDoneEventTypedDict(TypedDict):
    id: str
    next_agent_id: str
    next_agent_name: str
    type: NotRequired[AgentHandoffDoneEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]


class AgentHandoffDoneEvent(BaseModel):
    id: str

    next_agent_id: str

    next_agent_name: str

    type: Optional[AgentHandoffDoneEventType] = "agent.handoff.done"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0
</file>

<file path="src/mistralai/models/agenthandoffentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


AgentHandoffEntryObject = Literal["entry"]

AgentHandoffEntryType = Literal["agent.handoff"]


class AgentHandoffEntryTypedDict(TypedDict):
    previous_agent_id: str
    previous_agent_name: str
    next_agent_id: str
    next_agent_name: str
    object: NotRequired[AgentHandoffEntryObject]
    type: NotRequired[AgentHandoffEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]


class AgentHandoffEntry(BaseModel):
    previous_agent_id: str

    previous_agent_name: str

    next_agent_id: str

    next_agent_name: str

    object: Optional[AgentHandoffEntryObject] = "entry"

    type: Optional[AgentHandoffEntryType] = "agent.handoff"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["object", "type", "created_at", "completed_at", "id"]
        nullable_fields = ["completed_at"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agenthandoffstartedevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


AgentHandoffStartedEventType = Literal["agent.handoff.started"]


class AgentHandoffStartedEventTypedDict(TypedDict):
    id: str
    previous_agent_id: str
    previous_agent_name: str
    type: NotRequired[AgentHandoffStartedEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]


class AgentHandoffStartedEvent(BaseModel):
    id: str

    previous_agent_id: str

    previous_agent_name: str

    type: Optional[AgentHandoffStartedEventType] = "agent.handoff.started"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0
</file>

<file path="src/mistralai/models/agents_api_v1_agents_getop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1AgentsGetRequestTypedDict(TypedDict):
    agent_id: str


class AgentsAPIV1AgentsGetRequest(BaseModel):
    agent_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_agents_listop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, QueryParamMetadata
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class AgentsAPIV1AgentsListRequestTypedDict(TypedDict):
    page: NotRequired[int]
    page_size: NotRequired[int]


class AgentsAPIV1AgentsListRequest(BaseModel):
    page: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 0

    page_size: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 20
</file>

<file path="src/mistralai/models/agents_api_v1_agents_update_versionop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, QueryParamMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1AgentsUpdateVersionRequestTypedDict(TypedDict):
    agent_id: str
    version: int


class AgentsAPIV1AgentsUpdateVersionRequest(BaseModel):
    agent_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]

    version: Annotated[
        int, FieldMetadata(query=QueryParamMetadata(style="form", explode=True))
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_agents_updateop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agentupdaterequest import AgentUpdateRequest, AgentUpdateRequestTypedDict
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1AgentsUpdateRequestTypedDict(TypedDict):
    agent_id: str
    agent_update_request: AgentUpdateRequestTypedDict


class AgentsAPIV1AgentsUpdateRequest(BaseModel):
    agent_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]

    agent_update_request: Annotated[
        AgentUpdateRequest,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_append_streamop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .conversationappendstreamrequest import (
    ConversationAppendStreamRequest,
    ConversationAppendStreamRequestTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsAppendStreamRequestTypedDict(TypedDict):
    conversation_id: str
    r"""ID of the conversation to which we append entries."""
    conversation_append_stream_request: ConversationAppendStreamRequestTypedDict


class AgentsAPIV1ConversationsAppendStreamRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""ID of the conversation to which we append entries."""

    conversation_append_stream_request: Annotated[
        ConversationAppendStreamRequest,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_appendop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .conversationappendrequest import (
    ConversationAppendRequest,
    ConversationAppendRequestTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsAppendRequestTypedDict(TypedDict):
    conversation_id: str
    r"""ID of the conversation to which we append entries."""
    conversation_append_request: ConversationAppendRequestTypedDict


class AgentsAPIV1ConversationsAppendRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""ID of the conversation to which we append entries."""

    conversation_append_request: Annotated[
        ConversationAppendRequest,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_getop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agentconversation import AgentConversation, AgentConversationTypedDict
from .modelconversation import ModelConversation, ModelConversationTypedDict
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class AgentsAPIV1ConversationsGetRequestTypedDict(TypedDict):
    conversation_id: str


class AgentsAPIV1ConversationsGetRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]


AgentsAPIV1ConversationsGetResponseV1ConversationsGetTypedDict = TypeAliasType(
    "AgentsAPIV1ConversationsGetResponseV1ConversationsGetTypedDict",
    Union[AgentConversationTypedDict, ModelConversationTypedDict],
)
r"""Successful Response"""


AgentsAPIV1ConversationsGetResponseV1ConversationsGet = TypeAliasType(
    "AgentsAPIV1ConversationsGetResponseV1ConversationsGet",
    Union[AgentConversation, ModelConversation],
)
r"""Successful Response"""
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_historyop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsHistoryRequestTypedDict(TypedDict):
    conversation_id: str


class AgentsAPIV1ConversationsHistoryRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_listop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agentconversation import AgentConversation, AgentConversationTypedDict
from .modelconversation import ModelConversation, ModelConversationTypedDict
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, QueryParamMetadata
from typing import Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class AgentsAPIV1ConversationsListRequestTypedDict(TypedDict):
    page: NotRequired[int]
    page_size: NotRequired[int]


class AgentsAPIV1ConversationsListRequest(BaseModel):
    page: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 0

    page_size: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 100


ResponseBodyTypedDict = TypeAliasType(
    "ResponseBodyTypedDict",
    Union[AgentConversationTypedDict, ModelConversationTypedDict],
)


ResponseBody = TypeAliasType(
    "ResponseBody", Union[AgentConversation, ModelConversation]
)
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_messagesop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsMessagesRequestTypedDict(TypedDict):
    conversation_id: str


class AgentsAPIV1ConversationsMessagesRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_restart_streamop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .conversationrestartstreamrequest import (
    ConversationRestartStreamRequest,
    ConversationRestartStreamRequestTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsRestartStreamRequestTypedDict(TypedDict):
    conversation_id: str
    conversation_restart_stream_request: ConversationRestartStreamRequestTypedDict


class AgentsAPIV1ConversationsRestartStreamRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]

    conversation_restart_stream_request: Annotated[
        ConversationRestartStreamRequest,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
</file>

<file path="src/mistralai/models/agents_api_v1_conversations_restartop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .conversationrestartrequest import (
    ConversationRestartRequest,
    ConversationRestartRequestTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from typing_extensions import Annotated, TypedDict


class AgentsAPIV1ConversationsRestartRequestTypedDict(TypedDict):
    conversation_id: str
    conversation_restart_request: ConversationRestartRequestTypedDict


class AgentsAPIV1ConversationsRestartRequest(BaseModel):
    conversation_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]

    conversation_restart_request: Annotated[
        ConversationRestartRequest,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
</file>

<file path="src/mistralai/models/agentscompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .mistralpromptmode import MistralPromptMode
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator, validate_open_enum
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentsCompletionRequestStopTypedDict = TypeAliasType(
    "AgentsCompletionRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


AgentsCompletionRequestStop = TypeAliasType(
    "AgentsCompletionRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


AgentsCompletionRequestMessagesTypedDict = TypeAliasType(
    "AgentsCompletionRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


AgentsCompletionRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


AgentsCompletionRequestToolChoiceTypedDict = TypeAliasType(
    "AgentsCompletionRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


AgentsCompletionRequestToolChoice = TypeAliasType(
    "AgentsCompletionRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class AgentsCompletionRequestTypedDict(TypedDict):
    messages: List[AgentsCompletionRequestMessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    agent_id: str
    r"""The ID of the agent to use for this completion."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[AgentsCompletionRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[AgentsCompletionRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    prompt_mode: NotRequired[Nullable[MistralPromptMode]]


class AgentsCompletionRequest(BaseModel):
    messages: List[AgentsCompletionRequestMessages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    agent_id: str
    r"""The ID of the agent to use for this completion."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[AgentsCompletionRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[AgentsCompletionRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    prompt_mode: Annotated[
        OptionalNullable[MistralPromptMode], PlainValidator(validate_open_enum(False))
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "prompt_mode",
        ]
        nullable_fields = ["max_tokens", "random_seed", "tools", "n", "prompt_mode"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agentscompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .mistralpromptmode import MistralPromptMode
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator, validate_open_enum
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentsCompletionStreamRequestStopTypedDict = TypeAliasType(
    "AgentsCompletionStreamRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


AgentsCompletionStreamRequestStop = TypeAliasType(
    "AgentsCompletionStreamRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


AgentsCompletionStreamRequestMessagesTypedDict = TypeAliasType(
    "AgentsCompletionStreamRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


AgentsCompletionStreamRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


AgentsCompletionStreamRequestToolChoiceTypedDict = TypeAliasType(
    "AgentsCompletionStreamRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


AgentsCompletionStreamRequestToolChoice = TypeAliasType(
    "AgentsCompletionStreamRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class AgentsCompletionStreamRequestTypedDict(TypedDict):
    messages: List[AgentsCompletionStreamRequestMessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    agent_id: str
    r"""The ID of the agent to use for this completion."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[AgentsCompletionStreamRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[AgentsCompletionStreamRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    prompt_mode: NotRequired[Nullable[MistralPromptMode]]


class AgentsCompletionStreamRequest(BaseModel):
    messages: List[AgentsCompletionStreamRequestMessages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    agent_id: str
    r"""The ID of the agent to use for this completion."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[AgentsCompletionStreamRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[AgentsCompletionStreamRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    prompt_mode: Annotated[
        OptionalNullable[MistralPromptMode], PlainValidator(validate_open_enum(False))
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "prompt_mode",
        ]
        nullable_fields = ["max_tokens", "random_seed", "tools", "n", "prompt_mode"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/agentupdaterequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


AgentUpdateRequestToolsTypedDict = TypeAliasType(
    "AgentUpdateRequestToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


AgentUpdateRequestTools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class AgentUpdateRequestTypedDict(TypedDict):
    instructions: NotRequired[Nullable[str]]
    r"""Instruction prompt the model will follow during the conversation."""
    tools: NotRequired[List[AgentUpdateRequestToolsTypedDict]]
    r"""List of tools which are available to the model during the conversation."""
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""
    model: NotRequired[Nullable[str]]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    handoffs: NotRequired[Nullable[List[str]]]


class AgentUpdateRequest(BaseModel):
    instructions: OptionalNullable[str] = UNSET
    r"""Instruction prompt the model will follow during the conversation."""

    tools: Optional[List[AgentUpdateRequestTools]] = None
    r"""List of tools which are available to the model during the conversation."""

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""

    model: OptionalNullable[str] = UNSET

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    handoffs: OptionalNullable[List[str]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "instructions",
            "tools",
            "completion_args",
            "model",
            "name",
            "description",
            "handoffs",
        ]
        nullable_fields = ["instructions", "model", "name", "description", "handoffs"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/apiendpoint.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


APIEndpoint = Union[
    Literal[
        "/v1/chat/completions",
        "/v1/embeddings",
        "/v1/fim/completions",
        "/v1/moderations",
        "/v1/chat/moderations",
    ],
    UnrecognizedStr,
]
</file>

<file path="src/mistralai/models/archiveftmodelout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ArchiveFTModelOutObject = Literal["model"]


class ArchiveFTModelOutTypedDict(TypedDict):
    id: str
    object: NotRequired[ArchiveFTModelOutObject]
    archived: NotRequired[bool]


class ArchiveFTModelOut(BaseModel):
    id: str

    object: Optional[ArchiveFTModelOutObject] = "model"

    archived: Optional[bool] = True
</file>

<file path="src/mistralai/models/assistantmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


AssistantMessageContentTypedDict = TypeAliasType(
    "AssistantMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


AssistantMessageContent = TypeAliasType(
    "AssistantMessageContent", Union[str, List[ContentChunk]]
)


AssistantMessageRole = Literal["assistant"]


class AssistantMessageTypedDict(TypedDict):
    content: NotRequired[Nullable[AssistantMessageContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]
    prefix: NotRequired[bool]
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""
    role: NotRequired[AssistantMessageRole]


class AssistantMessage(BaseModel):
    content: OptionalNullable[AssistantMessageContent] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    prefix: Optional[bool] = False
    r"""Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message."""

    role: Optional[AssistantMessageRole] = "assistant"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["content", "tool_calls", "prefix", "role"]
        nullable_fields = ["content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/basemodelcard.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .modelcapabilities import ModelCapabilities, ModelCapabilitiesTypedDict
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_const
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import AfterValidator
from typing import List, Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


Type = Literal["base"]


class BaseModelCardTypedDict(TypedDict):
    id: str
    capabilities: ModelCapabilitiesTypedDict
    object: NotRequired[str]
    created: NotRequired[int]
    owned_by: NotRequired[str]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    max_context_length: NotRequired[int]
    aliases: NotRequired[List[str]]
    deprecation: NotRequired[Nullable[datetime]]
    default_model_temperature: NotRequired[Nullable[float]]
    type: Type


class BaseModelCard(BaseModel):
    id: str

    capabilities: ModelCapabilities

    object: Optional[str] = "model"

    created: Optional[int] = None

    owned_by: Optional[str] = "mistralai"

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    max_context_length: Optional[int] = 32768

    aliases: Optional[List[str]] = None

    deprecation: OptionalNullable[datetime] = UNSET

    default_model_temperature: OptionalNullable[float] = UNSET

    TYPE: Annotated[
        Annotated[Optional[Type], AfterValidator(validate_const("base"))],
        pydantic.Field(alias="type"),
    ] = "base"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "created",
            "owned_by",
            "name",
            "description",
            "max_context_length",
            "aliases",
            "deprecation",
            "default_model_temperature",
            "type",
        ]
        nullable_fields = [
            "name",
            "description",
            "deprecation",
            "default_model_temperature",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/batcherror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class BatchErrorTypedDict(TypedDict):
    message: str
    count: NotRequired[int]


class BatchError(BaseModel):
    message: str

    count: Optional[int] = 1
</file>

<file path="src/mistralai/models/batchjobin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .apiendpoint import APIEndpoint
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_open_enum
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing import Dict, List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class BatchJobInTypedDict(TypedDict):
    input_files: List[str]
    endpoint: APIEndpoint
    model: str
    metadata: NotRequired[Nullable[Dict[str, str]]]
    timeout_hours: NotRequired[int]


class BatchJobIn(BaseModel):
    input_files: List[str]

    endpoint: Annotated[APIEndpoint, PlainValidator(validate_open_enum(False))]

    model: str

    metadata: OptionalNullable[Dict[str, str]] = UNSET

    timeout_hours: Optional[int] = 24

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["metadata", "timeout_hours"]
        nullable_fields = ["metadata"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/batchjobout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .batcherror import BatchError, BatchErrorTypedDict
from .batchjobstatus import BatchJobStatus
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


BatchJobOutObject = Literal["batch"]


class BatchJobOutTypedDict(TypedDict):
    id: str
    input_files: List[str]
    endpoint: str
    model: str
    errors: List[BatchErrorTypedDict]
    status: BatchJobStatus
    created_at: int
    total_requests: int
    completed_requests: int
    succeeded_requests: int
    failed_requests: int
    object: NotRequired[BatchJobOutObject]
    metadata: NotRequired[Nullable[Dict[str, Any]]]
    output_file: NotRequired[Nullable[str]]
    error_file: NotRequired[Nullable[str]]
    started_at: NotRequired[Nullable[int]]
    completed_at: NotRequired[Nullable[int]]


class BatchJobOut(BaseModel):
    id: str

    input_files: List[str]

    endpoint: str

    model: str

    errors: List[BatchError]

    status: BatchJobStatus

    created_at: int

    total_requests: int

    completed_requests: int

    succeeded_requests: int

    failed_requests: int

    object: Optional[BatchJobOutObject] = "batch"

    metadata: OptionalNullable[Dict[str, Any]] = UNSET

    output_file: OptionalNullable[str] = UNSET

    error_file: OptionalNullable[str] = UNSET

    started_at: OptionalNullable[int] = UNSET

    completed_at: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "metadata",
            "output_file",
            "error_file",
            "started_at",
            "completed_at",
        ]
        nullable_fields = [
            "metadata",
            "output_file",
            "error_file",
            "started_at",
            "completed_at",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/batchjobsout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .batchjobout import BatchJobOut, BatchJobOutTypedDict
from mistralai.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


BatchJobsOutObject = Literal["list"]


class BatchJobsOutTypedDict(TypedDict):
    total: int
    data: NotRequired[List[BatchJobOutTypedDict]]
    object: NotRequired[BatchJobsOutObject]


class BatchJobsOut(BaseModel):
    total: int

    data: Optional[List[BatchJobOut]] = None

    object: Optional[BatchJobsOutObject] = "list"
</file>

<file path="src/mistralai/models/batchjobstatus.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


BatchJobStatus = Literal[
    "QUEUED",
    "RUNNING",
    "SUCCESS",
    "FAILED",
    "TIMEOUT_EXCEEDED",
    "CANCELLATION_REQUESTED",
    "CANCELLED",
]
</file>

<file path="src/mistralai/models/builtinconnectors.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


BuiltInConnectors = Literal[
    "web_search",
    "web_search_premium",
    "code_interpreter",
    "image_generation",
    "document_library",
]
</file>

<file path="src/mistralai/models/chatclassificationrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .inputs import Inputs, InputsTypedDict
from mistralai.types import BaseModel
import pydantic
from typing_extensions import Annotated, TypedDict


class ChatClassificationRequestTypedDict(TypedDict):
    model: str
    inputs: InputsTypedDict
    r"""Chat to classify"""


class ChatClassificationRequest(BaseModel):
    model: str

    inputs: Annotated[Inputs, pydantic.Field(alias="input")]
    r"""Chat to classify"""
</file>

<file path="src/mistralai/models/chatcompletionchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from mistralai.types import BaseModel, UnrecognizedStr
from mistralai.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


FinishReason = Union[
    Literal["stop", "length", "model_length", "error", "tool_calls"], UnrecognizedStr
]


class ChatCompletionChoiceTypedDict(TypedDict):
    index: int
    message: AssistantMessageTypedDict
    finish_reason: FinishReason


class ChatCompletionChoice(BaseModel):
    index: int

    message: AssistantMessage

    finish_reason: Annotated[FinishReason, PlainValidator(validate_open_enum(False))]
</file>

<file path="src/mistralai/models/chatcompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .mistralpromptmode import MistralPromptMode
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator, validate_open_enum
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


Stop = TypeAliasType("Stop", Union[str, List[str]])
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


MessagesTypedDict = TypeAliasType(
    "MessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


Messages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionRequestToolChoice = TypeAliasType(
    "ChatCompletionRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""
    messages: List[MessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[StopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    prompt_mode: NotRequired[Nullable[MistralPromptMode]]
    safe_prompt: NotRequired[bool]
    r"""Whether to inject a safety prompt before all conversations."""


class ChatCompletionRequest(BaseModel):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""

    messages: List[Messages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[Stop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    prompt_mode: Annotated[
        OptionalNullable[MistralPromptMode], PlainValidator(validate_open_enum(False))
    ] = UNSET

    safe_prompt: Optional[bool] = None
    r"""Whether to inject a safety prompt before all conversations."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "prompt_mode",
            "safe_prompt",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "tools",
            "n",
            "prompt_mode",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/chatcompletionresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionchoice import ChatCompletionChoice, ChatCompletionChoiceTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class ChatCompletionResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    created: int
    choices: List[ChatCompletionChoiceTypedDict]


class ChatCompletionResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    created: int

    choices: List[ChatCompletionChoice]
</file>

<file path="src/mistralai/models/chatcompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .mistralpromptmode import MistralPromptMode
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .tool import Tool, ToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
from .toolchoiceenum import ToolChoiceEnum
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator, validate_open_enum
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ChatCompletionStreamRequestStopTypedDict = TypeAliasType(
    "ChatCompletionStreamRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionStreamRequestStop = TypeAliasType(
    "ChatCompletionStreamRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


ChatCompletionStreamRequestMessagesTypedDict = TypeAliasType(
    "ChatCompletionStreamRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


ChatCompletionStreamRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatCompletionStreamRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionStreamRequestToolChoiceTypedDict",
    Union[ToolChoiceTypedDict, ToolChoiceEnum],
)


ChatCompletionStreamRequestToolChoice = TypeAliasType(
    "ChatCompletionStreamRequestToolChoice", Union[ToolChoice, ToolChoiceEnum]
)


class ChatCompletionStreamRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""
    messages: List[ChatCompletionStreamRequestMessagesTypedDict]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[ChatCompletionStreamRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    response_format: NotRequired[ResponseFormatTypedDict]
    tools: NotRequired[Nullable[List[ToolTypedDict]]]
    tool_choice: NotRequired[ChatCompletionStreamRequestToolChoiceTypedDict]
    presence_penalty: NotRequired[float]
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""
    frequency_penalty: NotRequired[float]
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""
    n: NotRequired[Nullable[int]]
    r"""Number of completions to return for each request, input tokens are only billed once."""
    prediction: NotRequired[PredictionTypedDict]
    parallel_tool_calls: NotRequired[bool]
    prompt_mode: NotRequired[Nullable[MistralPromptMode]]
    safe_prompt: NotRequired[bool]
    r"""Whether to inject a safety prompt before all conversations."""


class ChatCompletionStreamRequest(BaseModel):
    model: str
    r"""ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions."""

    messages: List[ChatCompletionStreamRequestMessages]
    r"""The prompt(s) to generate completions for, encoded as a list of dict with role and content."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = None
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[ChatCompletionStreamRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    response_format: Optional[ResponseFormat] = None

    tools: OptionalNullable[List[Tool]] = UNSET

    tool_choice: Optional[ChatCompletionStreamRequestToolChoice] = None

    presence_penalty: Optional[float] = None
    r"""presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative."""

    frequency_penalty: Optional[float] = None
    r"""frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition."""

    n: OptionalNullable[int] = UNSET
    r"""Number of completions to return for each request, input tokens are only billed once."""

    prediction: Optional[Prediction] = None

    parallel_tool_calls: Optional[bool] = None

    prompt_mode: Annotated[
        OptionalNullable[MistralPromptMode], PlainValidator(validate_open_enum(False))
    ] = UNSET

    safe_prompt: Optional[bool] = None
    r"""Whether to inject a safety prompt before all conversations."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "response_format",
            "tools",
            "tool_choice",
            "presence_penalty",
            "frequency_penalty",
            "n",
            "prediction",
            "parallel_tool_calls",
            "prompt_mode",
            "safe_prompt",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "tools",
            "n",
            "prompt_mode",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/chatmoderationrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
import pydantic
from pydantic import Discriminator, Tag
from typing import List, Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


TwoTypedDict = TypeAliasType(
    "TwoTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


Two = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


OneTypedDict = TypeAliasType(
    "OneTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


One = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


ChatModerationRequestInputsTypedDict = TypeAliasType(
    "ChatModerationRequestInputsTypedDict",
    Union[List[OneTypedDict], List[List[TwoTypedDict]]],
)
r"""Chat to classify"""


ChatModerationRequestInputs = TypeAliasType(
    "ChatModerationRequestInputs", Union[List[One], List[List[Two]]]
)
r"""Chat to classify"""


class ChatModerationRequestTypedDict(TypedDict):
    inputs: ChatModerationRequestInputsTypedDict
    r"""Chat to classify"""
    model: str


class ChatModerationRequest(BaseModel):
    inputs: Annotated[ChatModerationRequestInputs, pydantic.Field(alias="input")]
    r"""Chat to classify"""

    model: str
</file>

<file path="src/mistralai/models/checkpointout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .metricout import MetricOut, MetricOutTypedDict
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class CheckpointOutTypedDict(TypedDict):
    metrics: MetricOutTypedDict
    r"""Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase)."""
    step_number: int
    r"""The step number that the checkpoint was created at."""
    created_at: int
    r"""The UNIX timestamp (in seconds) for when the checkpoint was created."""


class CheckpointOut(BaseModel):
    metrics: MetricOut
    r"""Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase)."""

    step_number: int
    r"""The step number that the checkpoint was created at."""

    created_at: int
    r"""The UNIX timestamp (in seconds) for when the checkpoint was created."""
</file>

<file path="src/mistralai/models/classificationrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
import pydantic
from typing import List, Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


ClassificationRequestInputsTypedDict = TypeAliasType(
    "ClassificationRequestInputsTypedDict", Union[str, List[str]]
)
r"""Text to classify."""


ClassificationRequestInputs = TypeAliasType(
    "ClassificationRequestInputs", Union[str, List[str]]
)
r"""Text to classify."""


class ClassificationRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use."""
    inputs: ClassificationRequestInputsTypedDict
    r"""Text to classify."""


class ClassificationRequest(BaseModel):
    model: str
    r"""ID of the model to use."""

    inputs: Annotated[ClassificationRequestInputs, pydantic.Field(alias="input")]
    r"""Text to classify."""
</file>

<file path="src/mistralai/models/classificationresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classificationtargetresult import (
    ClassificationTargetResult,
    ClassificationTargetResultTypedDict,
)
from mistralai.types import BaseModel
from typing import Dict, List
from typing_extensions import TypedDict


class ClassificationResponseTypedDict(TypedDict):
    id: str
    model: str
    results: List[Dict[str, ClassificationTargetResultTypedDict]]


class ClassificationResponse(BaseModel):
    id: str

    model: str

    results: List[Dict[str, ClassificationTargetResult]]
</file>

<file path="src/mistralai/models/classificationtargetresult.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Dict
from typing_extensions import TypedDict


class ClassificationTargetResultTypedDict(TypedDict):
    scores: Dict[str, float]


class ClassificationTargetResult(BaseModel):
    scores: Dict[str, float]
</file>

<file path="src/mistralai/models/classifierdetailedjobout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .checkpointout import CheckpointOut, CheckpointOutTypedDict
from .classifiertargetout import ClassifierTargetOut, ClassifierTargetOutTypedDict
from .classifiertrainingparameters import (
    ClassifierTrainingParameters,
    ClassifierTrainingParametersTypedDict,
)
from .eventout import EventOut, EventOutTypedDict
from .jobmetadataout import JobMetadataOut, JobMetadataOutTypedDict
from .wandbintegrationout import WandbIntegrationOut, WandbIntegrationOutTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ClassifierDetailedJobOutStatus = Literal[
    "QUEUED",
    "STARTED",
    "VALIDATING",
    "VALIDATED",
    "RUNNING",
    "FAILED_VALIDATION",
    "FAILED",
    "SUCCESS",
    "CANCELLED",
    "CANCELLATION_REQUESTED",
]

ClassifierDetailedJobOutObject = Literal["job"]

ClassifierDetailedJobOutIntegrationsTypedDict = WandbIntegrationOutTypedDict


ClassifierDetailedJobOutIntegrations = WandbIntegrationOut


ClassifierDetailedJobOutJobType = Literal["classifier"]


class ClassifierDetailedJobOutTypedDict(TypedDict):
    id: str
    auto_start: bool
    model: str
    r"""The name of the model to fine-tune."""
    status: ClassifierDetailedJobOutStatus
    created_at: int
    modified_at: int
    training_files: List[str]
    hyperparameters: ClassifierTrainingParametersTypedDict
    classifier_targets: List[ClassifierTargetOutTypedDict]
    validation_files: NotRequired[Nullable[List[str]]]
    object: NotRequired[ClassifierDetailedJobOutObject]
    fine_tuned_model: NotRequired[Nullable[str]]
    suffix: NotRequired[Nullable[str]]
    integrations: NotRequired[
        Nullable[List[ClassifierDetailedJobOutIntegrationsTypedDict]]
    ]
    trained_tokens: NotRequired[Nullable[int]]
    metadata: NotRequired[Nullable[JobMetadataOutTypedDict]]
    job_type: NotRequired[ClassifierDetailedJobOutJobType]
    events: NotRequired[List[EventOutTypedDict]]
    r"""Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here."""
    checkpoints: NotRequired[List[CheckpointOutTypedDict]]


class ClassifierDetailedJobOut(BaseModel):
    id: str

    auto_start: bool

    model: str
    r"""The name of the model to fine-tune."""

    status: ClassifierDetailedJobOutStatus

    created_at: int

    modified_at: int

    training_files: List[str]

    hyperparameters: ClassifierTrainingParameters

    classifier_targets: List[ClassifierTargetOut]

    validation_files: OptionalNullable[List[str]] = UNSET

    object: Optional[ClassifierDetailedJobOutObject] = "job"

    fine_tuned_model: OptionalNullable[str] = UNSET

    suffix: OptionalNullable[str] = UNSET

    integrations: OptionalNullable[List[ClassifierDetailedJobOutIntegrations]] = UNSET

    trained_tokens: OptionalNullable[int] = UNSET

    metadata: OptionalNullable[JobMetadataOut] = UNSET

    job_type: Optional[ClassifierDetailedJobOutJobType] = "classifier"

    events: Optional[List[EventOut]] = None
    r"""Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here."""

    checkpoints: Optional[List[CheckpointOut]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "validation_files",
            "object",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
            "job_type",
            "events",
            "checkpoints",
        ]
        nullable_fields = [
            "validation_files",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/classifierftmodelout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifiertargetout import ClassifierTargetOut, ClassifierTargetOutTypedDict
from .ftmodelcapabilitiesout import (
    FTModelCapabilitiesOut,
    FTModelCapabilitiesOutTypedDict,
)
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ClassifierFTModelOutObject = Literal["model"]

ClassifierFTModelOutModelType = Literal["classifier"]


class ClassifierFTModelOutTypedDict(TypedDict):
    id: str
    created: int
    owned_by: str
    root: str
    archived: bool
    capabilities: FTModelCapabilitiesOutTypedDict
    job: str
    classifier_targets: List[ClassifierTargetOutTypedDict]
    object: NotRequired[ClassifierFTModelOutObject]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    max_context_length: NotRequired[int]
    aliases: NotRequired[List[str]]
    model_type: NotRequired[ClassifierFTModelOutModelType]


class ClassifierFTModelOut(BaseModel):
    id: str

    created: int

    owned_by: str

    root: str

    archived: bool

    capabilities: FTModelCapabilitiesOut

    job: str

    classifier_targets: List[ClassifierTargetOut]

    object: Optional[ClassifierFTModelOutObject] = "model"

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    max_context_length: Optional[int] = 32768

    aliases: Optional[List[str]] = None

    model_type: Optional[ClassifierFTModelOutModelType] = "classifier"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "name",
            "description",
            "max_context_length",
            "aliases",
            "model_type",
        ]
        nullable_fields = ["name", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/classifierjobout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifiertrainingparameters import (
    ClassifierTrainingParameters,
    ClassifierTrainingParametersTypedDict,
)
from .jobmetadataout import JobMetadataOut, JobMetadataOutTypedDict
from .wandbintegrationout import WandbIntegrationOut, WandbIntegrationOutTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ClassifierJobOutStatus = Literal[
    "QUEUED",
    "STARTED",
    "VALIDATING",
    "VALIDATED",
    "RUNNING",
    "FAILED_VALIDATION",
    "FAILED",
    "SUCCESS",
    "CANCELLED",
    "CANCELLATION_REQUESTED",
]
r"""The current status of the fine-tuning job."""

ClassifierJobOutObject = Literal["job"]
r"""The object type of the fine-tuning job."""

ClassifierJobOutIntegrationsTypedDict = WandbIntegrationOutTypedDict


ClassifierJobOutIntegrations = WandbIntegrationOut


ClassifierJobOutJobType = Literal["classifier"]
r"""The type of job (`FT` for fine-tuning)."""


class ClassifierJobOutTypedDict(TypedDict):
    id: str
    r"""The ID of the job."""
    auto_start: bool
    model: str
    r"""The name of the model to fine-tune."""
    status: ClassifierJobOutStatus
    r"""The current status of the fine-tuning job."""
    created_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was created."""
    modified_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was last modified."""
    training_files: List[str]
    r"""A list containing the IDs of uploaded files that contain training data."""
    hyperparameters: ClassifierTrainingParametersTypedDict
    validation_files: NotRequired[Nullable[List[str]]]
    r"""A list containing the IDs of uploaded files that contain validation data."""
    object: NotRequired[ClassifierJobOutObject]
    r"""The object type of the fine-tuning job."""
    fine_tuned_model: NotRequired[Nullable[str]]
    r"""The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    integrations: NotRequired[Nullable[List[ClassifierJobOutIntegrationsTypedDict]]]
    r"""A list of integrations enabled for your fine-tuning job."""
    trained_tokens: NotRequired[Nullable[int]]
    r"""Total number of tokens trained."""
    metadata: NotRequired[Nullable[JobMetadataOutTypedDict]]
    job_type: NotRequired[ClassifierJobOutJobType]
    r"""The type of job (`FT` for fine-tuning)."""


class ClassifierJobOut(BaseModel):
    id: str
    r"""The ID of the job."""

    auto_start: bool

    model: str
    r"""The name of the model to fine-tune."""

    status: ClassifierJobOutStatus
    r"""The current status of the fine-tuning job."""

    created_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was created."""

    modified_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was last modified."""

    training_files: List[str]
    r"""A list containing the IDs of uploaded files that contain training data."""

    hyperparameters: ClassifierTrainingParameters

    validation_files: OptionalNullable[List[str]] = UNSET
    r"""A list containing the IDs of uploaded files that contain validation data."""

    object: Optional[ClassifierJobOutObject] = "job"
    r"""The object type of the fine-tuning job."""

    fine_tuned_model: OptionalNullable[str] = UNSET
    r"""The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    integrations: OptionalNullable[List[ClassifierJobOutIntegrations]] = UNSET
    r"""A list of integrations enabled for your fine-tuning job."""

    trained_tokens: OptionalNullable[int] = UNSET
    r"""Total number of tokens trained."""

    metadata: OptionalNullable[JobMetadataOut] = UNSET

    job_type: Optional[ClassifierJobOutJobType] = "classifier"
    r"""The type of job (`FT` for fine-tuning)."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "validation_files",
            "object",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
            "job_type",
        ]
        nullable_fields = [
            "validation_files",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/classifiertargetin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .ftclassifierlossfunction import FTClassifierLossFunction
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class ClassifierTargetInTypedDict(TypedDict):
    name: str
    labels: List[str]
    weight: NotRequired[float]
    loss_function: NotRequired[Nullable[FTClassifierLossFunction]]


class ClassifierTargetIn(BaseModel):
    name: str

    labels: List[str]

    weight: Optional[float] = 1

    loss_function: OptionalNullable[FTClassifierLossFunction] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["weight", "loss_function"]
        nullable_fields = ["loss_function"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/classifiertargetout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .ftclassifierlossfunction import FTClassifierLossFunction
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class ClassifierTargetOutTypedDict(TypedDict):
    name: str
    labels: List[str]
    weight: float
    loss_function: FTClassifierLossFunction


class ClassifierTargetOut(BaseModel):
    name: str

    labels: List[str]

    weight: float

    loss_function: FTClassifierLossFunction
</file>

<file path="src/mistralai/models/classifiertrainingparameters.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ClassifierTrainingParametersTypedDict(TypedDict):
    training_steps: NotRequired[Nullable[int]]
    learning_rate: NotRequired[float]
    weight_decay: NotRequired[Nullable[float]]
    warmup_fraction: NotRequired[Nullable[float]]
    epochs: NotRequired[Nullable[float]]
    seq_len: NotRequired[Nullable[int]]


class ClassifierTrainingParameters(BaseModel):
    training_steps: OptionalNullable[int] = UNSET

    learning_rate: Optional[float] = 0.0001

    weight_decay: OptionalNullable[float] = UNSET

    warmup_fraction: OptionalNullable[float] = UNSET

    epochs: OptionalNullable[float] = UNSET

    seq_len: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "training_steps",
            "learning_rate",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
        ]
        nullable_fields = [
            "training_steps",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/classifiertrainingparametersin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ClassifierTrainingParametersInTypedDict(TypedDict):
    r"""The fine-tuning hyperparameter settings used in a classifier fine-tune job."""

    training_steps: NotRequired[Nullable[int]]
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""
    learning_rate: NotRequired[float]
    r"""A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process."""
    weight_decay: NotRequired[Nullable[float]]
    r"""(Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large."""
    warmup_fraction: NotRequired[Nullable[float]]
    r"""(Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune)"""
    epochs: NotRequired[Nullable[float]]
    seq_len: NotRequired[Nullable[int]]


class ClassifierTrainingParametersIn(BaseModel):
    r"""The fine-tuning hyperparameter settings used in a classifier fine-tune job."""

    training_steps: OptionalNullable[int] = UNSET
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""

    learning_rate: Optional[float] = 0.0001
    r"""A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process."""

    weight_decay: OptionalNullable[float] = UNSET
    r"""(Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large."""

    warmup_fraction: OptionalNullable[float] = UNSET
    r"""(Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune)"""

    epochs: OptionalNullable[float] = UNSET

    seq_len: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "training_steps",
            "learning_rate",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
        ]
        nullable_fields = [
            "training_steps",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/codeinterpretertool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


CodeInterpreterToolType = Literal["code_interpreter"]


class CodeInterpreterToolTypedDict(TypedDict):
    type: NotRequired[CodeInterpreterToolType]


class CodeInterpreterTool(BaseModel):
    type: Optional[CodeInterpreterToolType] = "code_interpreter"
</file>

<file path="src/mistralai/models/completionargs.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionargsstop import CompletionArgsStop, CompletionArgsStopTypedDict
from .prediction import Prediction, PredictionTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .toolchoiceenum import ToolChoiceEnum
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class CompletionArgsTypedDict(TypedDict):
    r"""White-listed arguments from the completion API"""

    stop: NotRequired[Nullable[CompletionArgsStopTypedDict]]
    presence_penalty: NotRequired[Nullable[float]]
    frequency_penalty: NotRequired[Nullable[float]]
    temperature: NotRequired[float]
    top_p: NotRequired[Nullable[float]]
    max_tokens: NotRequired[Nullable[int]]
    random_seed: NotRequired[Nullable[int]]
    prediction: NotRequired[Nullable[PredictionTypedDict]]
    response_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    tool_choice: NotRequired[ToolChoiceEnum]


class CompletionArgs(BaseModel):
    r"""White-listed arguments from the completion API"""

    stop: OptionalNullable[CompletionArgsStop] = UNSET

    presence_penalty: OptionalNullable[float] = UNSET

    frequency_penalty: OptionalNullable[float] = UNSET

    temperature: Optional[float] = 0.3

    top_p: OptionalNullable[float] = UNSET

    max_tokens: OptionalNullable[int] = UNSET

    random_seed: OptionalNullable[int] = UNSET

    prediction: OptionalNullable[Prediction] = UNSET

    response_format: OptionalNullable[ResponseFormat] = UNSET

    tool_choice: Optional[ToolChoiceEnum] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stop",
            "presence_penalty",
            "frequency_penalty",
            "temperature",
            "top_p",
            "max_tokens",
            "random_seed",
            "prediction",
            "response_format",
            "tool_choice",
        ]
        nullable_fields = [
            "stop",
            "presence_penalty",
            "frequency_penalty",
            "top_p",
            "max_tokens",
            "random_seed",
            "prediction",
            "response_format",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completionargsstop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import List, Union
from typing_extensions import TypeAliasType


CompletionArgsStopTypedDict = TypeAliasType(
    "CompletionArgsStopTypedDict", Union[str, List[str]]
)


CompletionArgsStop = TypeAliasType("CompletionArgsStop", Union[str, List[str]])
</file>

<file path="src/mistralai/models/completionchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionresponsestreamchoice import (
    CompletionResponseStreamChoice,
    CompletionResponseStreamChoiceTypedDict,
)
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class CompletionChunkTypedDict(TypedDict):
    id: str
    model: str
    choices: List[CompletionResponseStreamChoiceTypedDict]
    object: NotRequired[str]
    created: NotRequired[int]
    usage: NotRequired[UsageInfoTypedDict]


class CompletionChunk(BaseModel):
    id: str

    model: str

    choices: List[CompletionResponseStreamChoice]

    object: Optional[str] = None

    created: Optional[int] = None

    usage: Optional[UsageInfo] = None
</file>

<file path="src/mistralai/models/completiondetailedjobout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .checkpointout import CheckpointOut, CheckpointOutTypedDict
from .completiontrainingparameters import (
    CompletionTrainingParameters,
    CompletionTrainingParametersTypedDict,
)
from .eventout import EventOut, EventOutTypedDict
from .githubrepositoryout import GithubRepositoryOut, GithubRepositoryOutTypedDict
from .jobmetadataout import JobMetadataOut, JobMetadataOutTypedDict
from .wandbintegrationout import WandbIntegrationOut, WandbIntegrationOutTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


CompletionDetailedJobOutStatus = Literal[
    "QUEUED",
    "STARTED",
    "VALIDATING",
    "VALIDATED",
    "RUNNING",
    "FAILED_VALIDATION",
    "FAILED",
    "SUCCESS",
    "CANCELLED",
    "CANCELLATION_REQUESTED",
]

CompletionDetailedJobOutObject = Literal["job"]

CompletionDetailedJobOutIntegrationsTypedDict = WandbIntegrationOutTypedDict


CompletionDetailedJobOutIntegrations = WandbIntegrationOut


CompletionDetailedJobOutJobType = Literal["completion"]

CompletionDetailedJobOutRepositoriesTypedDict = GithubRepositoryOutTypedDict


CompletionDetailedJobOutRepositories = GithubRepositoryOut


class CompletionDetailedJobOutTypedDict(TypedDict):
    id: str
    auto_start: bool
    model: str
    r"""The name of the model to fine-tune."""
    status: CompletionDetailedJobOutStatus
    created_at: int
    modified_at: int
    training_files: List[str]
    hyperparameters: CompletionTrainingParametersTypedDict
    validation_files: NotRequired[Nullable[List[str]]]
    object: NotRequired[CompletionDetailedJobOutObject]
    fine_tuned_model: NotRequired[Nullable[str]]
    suffix: NotRequired[Nullable[str]]
    integrations: NotRequired[
        Nullable[List[CompletionDetailedJobOutIntegrationsTypedDict]]
    ]
    trained_tokens: NotRequired[Nullable[int]]
    metadata: NotRequired[Nullable[JobMetadataOutTypedDict]]
    job_type: NotRequired[CompletionDetailedJobOutJobType]
    repositories: NotRequired[List[CompletionDetailedJobOutRepositoriesTypedDict]]
    events: NotRequired[List[EventOutTypedDict]]
    r"""Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here."""
    checkpoints: NotRequired[List[CheckpointOutTypedDict]]


class CompletionDetailedJobOut(BaseModel):
    id: str

    auto_start: bool

    model: str
    r"""The name of the model to fine-tune."""

    status: CompletionDetailedJobOutStatus

    created_at: int

    modified_at: int

    training_files: List[str]

    hyperparameters: CompletionTrainingParameters

    validation_files: OptionalNullable[List[str]] = UNSET

    object: Optional[CompletionDetailedJobOutObject] = "job"

    fine_tuned_model: OptionalNullable[str] = UNSET

    suffix: OptionalNullable[str] = UNSET

    integrations: OptionalNullable[List[CompletionDetailedJobOutIntegrations]] = UNSET

    trained_tokens: OptionalNullable[int] = UNSET

    metadata: OptionalNullable[JobMetadataOut] = UNSET

    job_type: Optional[CompletionDetailedJobOutJobType] = "completion"

    repositories: Optional[List[CompletionDetailedJobOutRepositories]] = None

    events: Optional[List[EventOut]] = None
    r"""Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here."""

    checkpoints: Optional[List[CheckpointOut]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "validation_files",
            "object",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
            "job_type",
            "repositories",
            "events",
            "checkpoints",
        ]
        nullable_fields = [
            "validation_files",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completionevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionchunk import CompletionChunk, CompletionChunkTypedDict
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class CompletionEventTypedDict(TypedDict):
    data: CompletionChunkTypedDict


class CompletionEvent(BaseModel):
    data: CompletionChunk
</file>

<file path="src/mistralai/models/completionftmodelout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .ftmodelcapabilitiesout import (
    FTModelCapabilitiesOut,
    FTModelCapabilitiesOutTypedDict,
)
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


CompletionFTModelOutObject = Literal["model"]

ModelType = Literal["completion"]


class CompletionFTModelOutTypedDict(TypedDict):
    id: str
    created: int
    owned_by: str
    root: str
    archived: bool
    capabilities: FTModelCapabilitiesOutTypedDict
    job: str
    object: NotRequired[CompletionFTModelOutObject]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    max_context_length: NotRequired[int]
    aliases: NotRequired[List[str]]
    model_type: NotRequired[ModelType]


class CompletionFTModelOut(BaseModel):
    id: str

    created: int

    owned_by: str

    root: str

    archived: bool

    capabilities: FTModelCapabilitiesOut

    job: str

    object: Optional[CompletionFTModelOutObject] = "model"

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    max_context_length: Optional[int] = 32768

    aliases: Optional[List[str]] = None

    model_type: Optional[ModelType] = "completion"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "name",
            "description",
            "max_context_length",
            "aliases",
            "model_type",
        ]
        nullable_fields = ["name", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completionjobout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completiontrainingparameters import (
    CompletionTrainingParameters,
    CompletionTrainingParametersTypedDict,
)
from .githubrepositoryout import GithubRepositoryOut, GithubRepositoryOutTypedDict
from .jobmetadataout import JobMetadataOut, JobMetadataOutTypedDict
from .wandbintegrationout import WandbIntegrationOut, WandbIntegrationOutTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


Status = Literal[
    "QUEUED",
    "STARTED",
    "VALIDATING",
    "VALIDATED",
    "RUNNING",
    "FAILED_VALIDATION",
    "FAILED",
    "SUCCESS",
    "CANCELLED",
    "CANCELLATION_REQUESTED",
]
r"""The current status of the fine-tuning job."""

CompletionJobOutObject = Literal["job"]
r"""The object type of the fine-tuning job."""

IntegrationsTypedDict = WandbIntegrationOutTypedDict


Integrations = WandbIntegrationOut


JobType = Literal["completion"]
r"""The type of job (`FT` for fine-tuning)."""

RepositoriesTypedDict = GithubRepositoryOutTypedDict


Repositories = GithubRepositoryOut


class CompletionJobOutTypedDict(TypedDict):
    id: str
    r"""The ID of the job."""
    auto_start: bool
    model: str
    r"""The name of the model to fine-tune."""
    status: Status
    r"""The current status of the fine-tuning job."""
    created_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was created."""
    modified_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was last modified."""
    training_files: List[str]
    r"""A list containing the IDs of uploaded files that contain training data."""
    hyperparameters: CompletionTrainingParametersTypedDict
    validation_files: NotRequired[Nullable[List[str]]]
    r"""A list containing the IDs of uploaded files that contain validation data."""
    object: NotRequired[CompletionJobOutObject]
    r"""The object type of the fine-tuning job."""
    fine_tuned_model: NotRequired[Nullable[str]]
    r"""The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    integrations: NotRequired[Nullable[List[IntegrationsTypedDict]]]
    r"""A list of integrations enabled for your fine-tuning job."""
    trained_tokens: NotRequired[Nullable[int]]
    r"""Total number of tokens trained."""
    metadata: NotRequired[Nullable[JobMetadataOutTypedDict]]
    job_type: NotRequired[JobType]
    r"""The type of job (`FT` for fine-tuning)."""
    repositories: NotRequired[List[RepositoriesTypedDict]]


class CompletionJobOut(BaseModel):
    id: str
    r"""The ID of the job."""

    auto_start: bool

    model: str
    r"""The name of the model to fine-tune."""

    status: Status
    r"""The current status of the fine-tuning job."""

    created_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was created."""

    modified_at: int
    r"""The UNIX timestamp (in seconds) for when the fine-tuning job was last modified."""

    training_files: List[str]
    r"""A list containing the IDs of uploaded files that contain training data."""

    hyperparameters: CompletionTrainingParameters

    validation_files: OptionalNullable[List[str]] = UNSET
    r"""A list containing the IDs of uploaded files that contain validation data."""

    object: Optional[CompletionJobOutObject] = "job"
    r"""The object type of the fine-tuning job."""

    fine_tuned_model: OptionalNullable[str] = UNSET
    r"""The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    integrations: OptionalNullable[List[Integrations]] = UNSET
    r"""A list of integrations enabled for your fine-tuning job."""

    trained_tokens: OptionalNullable[int] = UNSET
    r"""Total number of tokens trained."""

    metadata: OptionalNullable[JobMetadataOut] = UNSET

    job_type: Optional[JobType] = "completion"
    r"""The type of job (`FT` for fine-tuning)."""

    repositories: Optional[List[Repositories]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "validation_files",
            "object",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
            "job_type",
            "repositories",
        ]
        nullable_fields = [
            "validation_files",
            "fine_tuned_model",
            "suffix",
            "integrations",
            "trained_tokens",
            "metadata",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completionresponsestreamchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .deltamessage import DeltaMessage, DeltaMessageTypedDict
from mistralai.types import BaseModel, Nullable, UNSET_SENTINEL, UnrecognizedStr
from mistralai.utils import validate_open_enum
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing import Literal, Union
from typing_extensions import Annotated, TypedDict


CompletionResponseStreamChoiceFinishReason = Union[
    Literal["stop", "length", "error", "tool_calls"], UnrecognizedStr
]


class CompletionResponseStreamChoiceTypedDict(TypedDict):
    index: int
    delta: DeltaMessageTypedDict
    finish_reason: Nullable[CompletionResponseStreamChoiceFinishReason]


class CompletionResponseStreamChoice(BaseModel):
    index: int

    delta: DeltaMessage

    finish_reason: Annotated[
        Nullable[CompletionResponseStreamChoiceFinishReason],
        PlainValidator(validate_open_enum(False)),
    ]

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["finish_reason"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completiontrainingparameters.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class CompletionTrainingParametersTypedDict(TypedDict):
    training_steps: NotRequired[Nullable[int]]
    learning_rate: NotRequired[float]
    weight_decay: NotRequired[Nullable[float]]
    warmup_fraction: NotRequired[Nullable[float]]
    epochs: NotRequired[Nullable[float]]
    seq_len: NotRequired[Nullable[int]]
    fim_ratio: NotRequired[Nullable[float]]


class CompletionTrainingParameters(BaseModel):
    training_steps: OptionalNullable[int] = UNSET

    learning_rate: Optional[float] = 0.0001

    weight_decay: OptionalNullable[float] = UNSET

    warmup_fraction: OptionalNullable[float] = UNSET

    epochs: OptionalNullable[float] = UNSET

    seq_len: OptionalNullable[int] = UNSET

    fim_ratio: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "training_steps",
            "learning_rate",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
            "fim_ratio",
        ]
        nullable_fields = [
            "training_steps",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
            "fim_ratio",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/completiontrainingparametersin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class CompletionTrainingParametersInTypedDict(TypedDict):
    r"""The fine-tuning hyperparameter settings used in a fine-tune job."""

    training_steps: NotRequired[Nullable[int]]
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""
    learning_rate: NotRequired[float]
    r"""A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process."""
    weight_decay: NotRequired[Nullable[float]]
    r"""(Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large."""
    warmup_fraction: NotRequired[Nullable[float]]
    r"""(Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune)"""
    epochs: NotRequired[Nullable[float]]
    seq_len: NotRequired[Nullable[int]]
    fim_ratio: NotRequired[Nullable[float]]


class CompletionTrainingParametersIn(BaseModel):
    r"""The fine-tuning hyperparameter settings used in a fine-tune job."""

    training_steps: OptionalNullable[int] = UNSET
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""

    learning_rate: Optional[float] = 0.0001
    r"""A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process."""

    weight_decay: OptionalNullable[float] = UNSET
    r"""(Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large."""

    warmup_fraction: OptionalNullable[float] = UNSET
    r"""(Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune)"""

    epochs: OptionalNullable[float] = UNSET

    seq_len: OptionalNullable[int] = UNSET

    fim_ratio: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "training_steps",
            "learning_rate",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
            "fim_ratio",
        ]
        nullable_fields = [
            "training_steps",
            "weight_decay",
            "warmup_fraction",
            "epochs",
            "seq_len",
            "fim_ratio",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/contentchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .documenturlchunk import DocumentURLChunk, DocumentURLChunkTypedDict
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .referencechunk import ReferenceChunk, ReferenceChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType


ContentChunkTypedDict = TypeAliasType(
    "ContentChunkTypedDict",
    Union[
        TextChunkTypedDict,
        ImageURLChunkTypedDict,
        ReferenceChunkTypedDict,
        DocumentURLChunkTypedDict,
    ],
)


ContentChunk = Annotated[
    Union[
        Annotated[ImageURLChunk, Tag("image_url")],
        Annotated[DocumentURLChunk, Tag("document_url")],
        Annotated[TextChunk, Tag("text")],
        Annotated[ReferenceChunk, Tag("reference")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
</file>

<file path="src/mistralai/models/conversationappendrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ConversationAppendRequestHandoffExecution = Literal["client", "server"]


class ConversationAppendRequestTypedDict(TypedDict):
    inputs: ConversationInputsTypedDict
    stream: NotRequired[bool]
    store: NotRequired[bool]
    r"""Whether to store the results into our servers or not."""
    handoff_execution: NotRequired[ConversationAppendRequestHandoffExecution]
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""


class ConversationAppendRequest(BaseModel):
    inputs: ConversationInputs

    stream: Optional[bool] = False

    store: Optional[bool] = True
    r"""Whether to store the results into our servers or not."""

    handoff_execution: Optional[ConversationAppendRequestHandoffExecution] = "server"

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""
</file>

<file path="src/mistralai/models/conversationappendstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ConversationAppendStreamRequestHandoffExecution = Literal["client", "server"]


class ConversationAppendStreamRequestTypedDict(TypedDict):
    inputs: ConversationInputsTypedDict
    stream: NotRequired[bool]
    store: NotRequired[bool]
    r"""Whether to store the results into our servers or not."""
    handoff_execution: NotRequired[ConversationAppendStreamRequestHandoffExecution]
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""


class ConversationAppendStreamRequest(BaseModel):
    inputs: ConversationInputs

    stream: Optional[bool] = True

    store: Optional[bool] = True
    r"""Whether to store the results into our servers or not."""

    handoff_execution: Optional[ConversationAppendStreamRequestHandoffExecution] = (
        "server"
    )

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""
</file>

<file path="src/mistralai/models/conversationevents.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agenthandoffdoneevent import AgentHandoffDoneEvent, AgentHandoffDoneEventTypedDict
from .agenthandoffstartedevent import (
    AgentHandoffStartedEvent,
    AgentHandoffStartedEventTypedDict,
)
from .functioncallevent import FunctionCallEvent, FunctionCallEventTypedDict
from .messageoutputevent import MessageOutputEvent, MessageOutputEventTypedDict
from .responsedoneevent import ResponseDoneEvent, ResponseDoneEventTypedDict
from .responseerrorevent import ResponseErrorEvent, ResponseErrorEventTypedDict
from .responsestartedevent import ResponseStartedEvent, ResponseStartedEventTypedDict
from .ssetypes import SSETypes
from .toolexecutiondoneevent import (
    ToolExecutionDoneEvent,
    ToolExecutionDoneEventTypedDict,
)
from .toolexecutionstartedevent import (
    ToolExecutionStartedEvent,
    ToolExecutionStartedEventTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


ConversationEventsDataTypedDict = TypeAliasType(
    "ConversationEventsDataTypedDict",
    Union[
        ResponseStartedEventTypedDict,
        ResponseDoneEventTypedDict,
        ResponseErrorEventTypedDict,
        ToolExecutionStartedEventTypedDict,
        ToolExecutionDoneEventTypedDict,
        AgentHandoffStartedEventTypedDict,
        AgentHandoffDoneEventTypedDict,
        FunctionCallEventTypedDict,
        MessageOutputEventTypedDict,
    ],
)


ConversationEventsData = Annotated[
    Union[
        Annotated[AgentHandoffDoneEvent, Tag("agent.handoff.done")],
        Annotated[AgentHandoffStartedEvent, Tag("agent.handoff.started")],
        Annotated[ResponseDoneEvent, Tag("conversation.response.done")],
        Annotated[ResponseErrorEvent, Tag("conversation.response.error")],
        Annotated[ResponseStartedEvent, Tag("conversation.response.started")],
        Annotated[FunctionCallEvent, Tag("function.call.delta")],
        Annotated[MessageOutputEvent, Tag("message.output.delta")],
        Annotated[ToolExecutionDoneEvent, Tag("tool.execution.done")],
        Annotated[ToolExecutionStartedEvent, Tag("tool.execution.started")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class ConversationEventsTypedDict(TypedDict):
    event: SSETypes
    r"""Server side events sent when streaming a conversation response."""
    data: ConversationEventsDataTypedDict


class ConversationEvents(BaseModel):
    event: SSETypes
    r"""Server side events sent when streaming a conversation response."""

    data: ConversationEventsData
</file>

<file path="src/mistralai/models/conversationhistory.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agenthandoffentry import AgentHandoffEntry, AgentHandoffEntryTypedDict
from .functioncallentry import FunctionCallEntry, FunctionCallEntryTypedDict
from .functionresultentry import FunctionResultEntry, FunctionResultEntryTypedDict
from .messageinputentry import MessageInputEntry, MessageInputEntryTypedDict
from .messageoutputentry import MessageOutputEntry, MessageOutputEntryTypedDict
from .toolexecutionentry import ToolExecutionEntry, ToolExecutionEntryTypedDict
from mistralai.types import BaseModel
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ConversationHistoryObject = Literal["conversation.history"]

EntriesTypedDict = TypeAliasType(
    "EntriesTypedDict",
    Union[
        MessageInputEntryTypedDict,
        FunctionResultEntryTypedDict,
        ToolExecutionEntryTypedDict,
        FunctionCallEntryTypedDict,
        MessageOutputEntryTypedDict,
        AgentHandoffEntryTypedDict,
    ],
)


Entries = TypeAliasType(
    "Entries",
    Union[
        MessageInputEntry,
        FunctionResultEntry,
        ToolExecutionEntry,
        FunctionCallEntry,
        MessageOutputEntry,
        AgentHandoffEntry,
    ],
)


class ConversationHistoryTypedDict(TypedDict):
    r"""Retrieve all entries in a conversation."""

    conversation_id: str
    entries: List[EntriesTypedDict]
    object: NotRequired[ConversationHistoryObject]


class ConversationHistory(BaseModel):
    r"""Retrieve all entries in a conversation."""

    conversation_id: str

    entries: List[Entries]

    object: Optional[ConversationHistoryObject] = "conversation.history"
</file>

<file path="src/mistralai/models/conversationinputs.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .inputentries import InputEntries, InputEntriesTypedDict
from typing import List, Union
from typing_extensions import TypeAliasType


ConversationInputsTypedDict = TypeAliasType(
    "ConversationInputsTypedDict", Union[str, List[InputEntriesTypedDict]]
)


ConversationInputs = TypeAliasType("ConversationInputs", Union[str, List[InputEntries]])
</file>

<file path="src/mistralai/models/conversationmessages.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .messageentries import MessageEntries, MessageEntriesTypedDict
from mistralai.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ConversationMessagesObject = Literal["conversation.messages"]


class ConversationMessagesTypedDict(TypedDict):
    r"""Similar to the conversation history but only keep the messages"""

    conversation_id: str
    messages: List[MessageEntriesTypedDict]
    object: NotRequired[ConversationMessagesObject]


class ConversationMessages(BaseModel):
    r"""Similar to the conversation history but only keep the messages"""

    conversation_id: str

    messages: List[MessageEntries]

    object: Optional[ConversationMessagesObject] = "conversation.messages"
</file>

<file path="src/mistralai/models/conversationrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


HandoffExecution = Literal["client", "server"]

ToolsTypedDict = TypeAliasType(
    "ToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


Tools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class ConversationRequestTypedDict(TypedDict):
    inputs: ConversationInputsTypedDict
    stream: NotRequired[bool]
    store: NotRequired[Nullable[bool]]
    handoff_execution: NotRequired[Nullable[HandoffExecution]]
    instructions: NotRequired[Nullable[str]]
    tools: NotRequired[Nullable[List[ToolsTypedDict]]]
    completion_args: NotRequired[Nullable[CompletionArgsTypedDict]]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    agent_id: NotRequired[Nullable[str]]
    model: NotRequired[Nullable[str]]


class ConversationRequest(BaseModel):
    inputs: ConversationInputs

    stream: Optional[bool] = False

    store: OptionalNullable[bool] = UNSET

    handoff_execution: OptionalNullable[HandoffExecution] = UNSET

    instructions: OptionalNullable[str] = UNSET

    tools: OptionalNullable[List[Tools]] = UNSET

    completion_args: OptionalNullable[CompletionArgs] = UNSET

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    agent_id: OptionalNullable[str] = UNSET

    model: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "store",
            "handoff_execution",
            "instructions",
            "tools",
            "completion_args",
            "name",
            "description",
            "agent_id",
            "model",
        ]
        nullable_fields = [
            "store",
            "handoff_execution",
            "instructions",
            "tools",
            "completion_args",
            "name",
            "description",
            "agent_id",
            "model",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/conversationresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .agenthandoffentry import AgentHandoffEntry, AgentHandoffEntryTypedDict
from .conversationusageinfo import ConversationUsageInfo, ConversationUsageInfoTypedDict
from .functioncallentry import FunctionCallEntry, FunctionCallEntryTypedDict
from .messageoutputentry import MessageOutputEntry, MessageOutputEntryTypedDict
from .toolexecutionentry import ToolExecutionEntry, ToolExecutionEntryTypedDict
from mistralai.types import BaseModel
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ConversationResponseObject = Literal["conversation.response"]

OutputsTypedDict = TypeAliasType(
    "OutputsTypedDict",
    Union[
        ToolExecutionEntryTypedDict,
        FunctionCallEntryTypedDict,
        MessageOutputEntryTypedDict,
        AgentHandoffEntryTypedDict,
    ],
)


Outputs = TypeAliasType(
    "Outputs",
    Union[ToolExecutionEntry, FunctionCallEntry, MessageOutputEntry, AgentHandoffEntry],
)


class ConversationResponseTypedDict(TypedDict):
    r"""The response after appending new entries to the conversation."""

    conversation_id: str
    outputs: List[OutputsTypedDict]
    usage: ConversationUsageInfoTypedDict
    object: NotRequired[ConversationResponseObject]


class ConversationResponse(BaseModel):
    r"""The response after appending new entries to the conversation."""

    conversation_id: str

    outputs: List[Outputs]

    usage: ConversationUsageInfo

    object: Optional[ConversationResponseObject] = "conversation.response"
</file>

<file path="src/mistralai/models/conversationrestartrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ConversationRestartRequestHandoffExecution = Literal["client", "server"]


class ConversationRestartRequestTypedDict(TypedDict):
    r"""Request to restart a new conversation from a given entry in the conversation."""

    inputs: ConversationInputsTypedDict
    from_entry_id: str
    stream: NotRequired[bool]
    store: NotRequired[bool]
    r"""Whether to store the results into our servers or not."""
    handoff_execution: NotRequired[ConversationRestartRequestHandoffExecution]
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""


class ConversationRestartRequest(BaseModel):
    r"""Request to restart a new conversation from a given entry in the conversation."""

    inputs: ConversationInputs

    from_entry_id: str

    stream: Optional[bool] = False

    store: Optional[bool] = True
    r"""Whether to store the results into our servers or not."""

    handoff_execution: Optional[ConversationRestartRequestHandoffExecution] = "server"

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""
</file>

<file path="src/mistralai/models/conversationrestartstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ConversationRestartStreamRequestHandoffExecution = Literal["client", "server"]


class ConversationRestartStreamRequestTypedDict(TypedDict):
    r"""Request to restart a new conversation from a given entry in the conversation."""

    inputs: ConversationInputsTypedDict
    from_entry_id: str
    stream: NotRequired[bool]
    store: NotRequired[bool]
    r"""Whether to store the results into our servers or not."""
    handoff_execution: NotRequired[ConversationRestartStreamRequestHandoffExecution]
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""


class ConversationRestartStreamRequest(BaseModel):
    r"""Request to restart a new conversation from a given entry in the conversation."""

    inputs: ConversationInputs

    from_entry_id: str

    stream: Optional[bool] = True

    store: Optional[bool] = True
    r"""Whether to store the results into our servers or not."""

    handoff_execution: Optional[ConversationRestartStreamRequestHandoffExecution] = (
        "server"
    )

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""
</file>

<file path="src/mistralai/models/conversationstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .conversationinputs import ConversationInputs, ConversationInputsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ConversationStreamRequestHandoffExecution = Literal["client", "server"]

ConversationStreamRequestToolsTypedDict = TypeAliasType(
    "ConversationStreamRequestToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


ConversationStreamRequestTools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class ConversationStreamRequestTypedDict(TypedDict):
    inputs: ConversationInputsTypedDict
    stream: NotRequired[bool]
    store: NotRequired[Nullable[bool]]
    handoff_execution: NotRequired[Nullable[ConversationStreamRequestHandoffExecution]]
    instructions: NotRequired[Nullable[str]]
    tools: NotRequired[Nullable[List[ConversationStreamRequestToolsTypedDict]]]
    completion_args: NotRequired[Nullable[CompletionArgsTypedDict]]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    agent_id: NotRequired[Nullable[str]]
    model: NotRequired[Nullable[str]]


class ConversationStreamRequest(BaseModel):
    inputs: ConversationInputs

    stream: Optional[bool] = True

    store: OptionalNullable[bool] = UNSET

    handoff_execution: OptionalNullable[ConversationStreamRequestHandoffExecution] = (
        UNSET
    )

    instructions: OptionalNullable[str] = UNSET

    tools: OptionalNullable[List[ConversationStreamRequestTools]] = UNSET

    completion_args: OptionalNullable[CompletionArgs] = UNSET

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    agent_id: OptionalNullable[str] = UNSET

    model: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "stream",
            "store",
            "handoff_execution",
            "instructions",
            "tools",
            "completion_args",
            "name",
            "description",
            "agent_id",
            "model",
        ]
        nullable_fields = [
            "store",
            "handoff_execution",
            "instructions",
            "tools",
            "completion_args",
            "name",
            "description",
            "agent_id",
            "model",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/conversationusageinfo.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Dict, Optional
from typing_extensions import NotRequired, TypedDict


class ConversationUsageInfoTypedDict(TypedDict):
    prompt_tokens: NotRequired[int]
    completion_tokens: NotRequired[int]
    total_tokens: NotRequired[int]
    connector_tokens: NotRequired[Nullable[int]]
    connectors: NotRequired[Nullable[Dict[str, int]]]


class ConversationUsageInfo(BaseModel):
    prompt_tokens: Optional[int] = 0

    completion_tokens: Optional[int] = 0

    total_tokens: Optional[int] = 0

    connector_tokens: OptionalNullable[int] = UNSET

    connectors: OptionalNullable[Dict[str, int]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt_tokens",
            "completion_tokens",
            "total_tokens",
            "connector_tokens",
            "connectors",
        ]
        nullable_fields = ["connector_tokens", "connectors"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/delete_model_v1_models_model_id_deleteop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class DeleteModelV1ModelsModelIDDeleteRequestTypedDict(TypedDict):
    model_id: str
    r"""The ID of the model to delete."""


class DeleteModelV1ModelsModelIDDeleteRequest(BaseModel):
    model_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the model to delete."""
</file>

<file path="src/mistralai/models/deletefileout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class DeleteFileOutTypedDict(TypedDict):
    id: str
    r"""The ID of the deleted file."""
    object: str
    r"""The object type that was deleted"""
    deleted: bool
    r"""The deletion status."""


class DeleteFileOut(BaseModel):
    id: str
    r"""The ID of the deleted file."""

    object: str
    r"""The object type that was deleted"""

    deleted: bool
    r"""The deletion status."""
</file>

<file path="src/mistralai/models/deletemodelout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class DeleteModelOutTypedDict(TypedDict):
    id: str
    r"""The ID of the deleted model."""
    object: NotRequired[str]
    r"""The object type that was deleted"""
    deleted: NotRequired[bool]
    r"""The deletion status"""


class DeleteModelOut(BaseModel):
    id: str
    r"""The ID of the deleted model."""

    object: Optional[str] = "model"
    r"""The object type that was deleted"""

    deleted: Optional[bool] = True
    r"""The deletion status"""
</file>

<file path="src/mistralai/models/deltamessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from .toolcall import ToolCall, ToolCallTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ContentTypedDict = TypeAliasType(
    "ContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


Content = TypeAliasType("Content", Union[str, List[ContentChunk]])


class DeltaMessageTypedDict(TypedDict):
    role: NotRequired[Nullable[str]]
    content: NotRequired[Nullable[ContentTypedDict]]
    tool_calls: NotRequired[Nullable[List[ToolCallTypedDict]]]


class DeltaMessage(BaseModel):
    role: OptionalNullable[str] = UNSET

    content: OptionalNullable[Content] = UNSET

    tool_calls: OptionalNullable[List[ToolCall]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role", "content", "tool_calls"]
        nullable_fields = ["role", "content", "tool_calls"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/documentlibrarytool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


DocumentLibraryToolType = Literal["document_library"]


class DocumentLibraryToolTypedDict(TypedDict):
    library_ids: List[str]
    r"""Ids of the library in which to search."""
    type: NotRequired[DocumentLibraryToolType]


class DocumentLibraryTool(BaseModel):
    library_ids: List[str]
    r"""Ids of the library in which to search."""

    type: Optional[DocumentLibraryToolType] = "document_library"
</file>

<file path="src/mistralai/models/documenturlchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


DocumentURLChunkType = Literal["document_url"]


class DocumentURLChunkTypedDict(TypedDict):
    document_url: str
    document_name: NotRequired[Nullable[str]]
    r"""The filename of the document"""
    type: NotRequired[DocumentURLChunkType]


class DocumentURLChunk(BaseModel):
    document_url: str

    document_name: OptionalNullable[str] = UNSET
    r"""The filename of the document"""

    type: Optional[DocumentURLChunkType] = "document_url"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["document_name", "type"]
        nullable_fields = ["document_name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/embeddingdtype.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


EmbeddingDtype = Literal["float", "int8", "uint8", "binary", "ubinary"]
</file>

<file path="src/mistralai/models/embeddingrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .embeddingdtype import EmbeddingDtype
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
import pydantic
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


EmbeddingRequestInputsTypedDict = TypeAliasType(
    "EmbeddingRequestInputsTypedDict", Union[str, List[str]]
)
r"""Text to embed."""


EmbeddingRequestInputs = TypeAliasType("EmbeddingRequestInputs", Union[str, List[str]])
r"""Text to embed."""


class EmbeddingRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use."""
    inputs: EmbeddingRequestInputsTypedDict
    r"""Text to embed."""
    output_dimension: NotRequired[Nullable[int]]
    r"""The dimension of the output embeddings."""
    output_dtype: NotRequired[EmbeddingDtype]


class EmbeddingRequest(BaseModel):
    model: str
    r"""ID of the model to use."""

    inputs: Annotated[EmbeddingRequestInputs, pydantic.Field(alias="input")]
    r"""Text to embed."""

    output_dimension: OptionalNullable[int] = UNSET
    r"""The dimension of the output embeddings."""

    output_dtype: Optional[EmbeddingDtype] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["output_dimension", "output_dtype"]
        nullable_fields = ["output_dimension"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/embeddingresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .embeddingresponsedata import EmbeddingResponseData, EmbeddingResponseDataTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class EmbeddingResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    data: List[EmbeddingResponseDataTypedDict]


class EmbeddingResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    data: List[EmbeddingResponseData]
</file>

<file path="src/mistralai/models/embeddingresponsedata.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import List, Optional
from typing_extensions import NotRequired, TypedDict


class EmbeddingResponseDataTypedDict(TypedDict):
    object: NotRequired[str]
    embedding: NotRequired[List[float]]
    index: NotRequired[int]


class EmbeddingResponseData(BaseModel):
    object: Optional[str] = None

    embedding: Optional[List[float]] = None

    index: Optional[int] = None
</file>

<file path="src/mistralai/models/eventout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Any, Dict
from typing_extensions import NotRequired, TypedDict


class EventOutTypedDict(TypedDict):
    name: str
    r"""The name of the event."""
    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""
    data: NotRequired[Nullable[Dict[str, Any]]]


class EventOut(BaseModel):
    name: str
    r"""The name of the event."""

    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""

    data: OptionalNullable[Dict[str, Any]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["data"]
        nullable_fields = ["data"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/filepurpose.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


FilePurpose = Union[Literal["fine-tune", "batch", "ocr"], UnrecognizedStr]
</file>

<file path="src/mistralai/models/files_api_routes_delete_fileop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class FilesAPIRoutesDeleteFileRequestTypedDict(TypedDict):
    file_id: str


class FilesAPIRoutesDeleteFileRequest(BaseModel):
    file_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/files_api_routes_download_fileop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class FilesAPIRoutesDownloadFileRequestTypedDict(TypedDict):
    file_id: str


class FilesAPIRoutesDownloadFileRequest(BaseModel):
    file_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/files_api_routes_get_signed_urlop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, QueryParamMetadata
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class FilesAPIRoutesGetSignedURLRequestTypedDict(TypedDict):
    file_id: str
    expiry: NotRequired[int]
    r"""Number of hours before the url becomes invalid. Defaults to 24h"""


class FilesAPIRoutesGetSignedURLRequest(BaseModel):
    file_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]

    expiry: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 24
    r"""Number of hours before the url becomes invalid. Defaults to 24h"""
</file>

<file path="src/mistralai/models/files_api_routes_list_filesop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .filepurpose import FilePurpose
from .sampletype import SampleType
from .source import Source
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import FieldMetadata, QueryParamMetadata, validate_open_enum
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class FilesAPIRoutesListFilesRequestTypedDict(TypedDict):
    page: NotRequired[int]
    page_size: NotRequired[int]
    sample_type: NotRequired[Nullable[List[SampleType]]]
    source: NotRequired[Nullable[List[Source]]]
    search: NotRequired[Nullable[str]]
    purpose: NotRequired[Nullable[FilePurpose]]


class FilesAPIRoutesListFilesRequest(BaseModel):
    page: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 0

    page_size: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 100

    sample_type: Annotated[
        OptionalNullable[
            List[Annotated[SampleType, PlainValidator(validate_open_enum(False))]]
        ],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    source: Annotated[
        OptionalNullable[
            List[Annotated[Source, PlainValidator(validate_open_enum(False))]]
        ],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    search: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    purpose: Annotated[
        Annotated[
            OptionalNullable[FilePurpose], PlainValidator(validate_open_enum(False))
        ],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "page",
            "page_size",
            "sample_type",
            "source",
            "search",
            "purpose",
        ]
        nullable_fields = ["sample_type", "source", "search", "purpose"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/files_api_routes_retrieve_fileop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class FilesAPIRoutesRetrieveFileRequestTypedDict(TypedDict):
    file_id: str


class FilesAPIRoutesRetrieveFileRequest(BaseModel):
    file_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/files_api_routes_upload_fileop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .filepurpose import FilePurpose
import io
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, MultipartFormMetadata, validate_open_enum
import pydantic
from pydantic.functional_validators import PlainValidator
from typing import IO, Optional, Union
from typing_extensions import Annotated, NotRequired, TypedDict


class FileTypedDict(TypedDict):
    file_name: str
    content: Union[bytes, IO[bytes], io.BufferedReader]
    content_type: NotRequired[str]


class File(BaseModel):
    file_name: Annotated[
        str, pydantic.Field(alias="fileName"), FieldMetadata(multipart=True)
    ]

    content: Annotated[
        Union[bytes, IO[bytes], io.BufferedReader],
        pydantic.Field(alias=""),
        FieldMetadata(multipart=MultipartFormMetadata(content=True)),
    ]

    content_type: Annotated[
        Optional[str],
        pydantic.Field(alias="Content-Type"),
        FieldMetadata(multipart=True),
    ] = None


class FilesAPIRoutesUploadFileMultiPartBodyParamsTypedDict(TypedDict):
    file: FileTypedDict
    r"""The File object (not file name) to be uploaded.
    To upload a file and specify a custom file name you should format your request as such:
    ```bash
    file=@path/to/your/file.jsonl;filename=custom_name.jsonl
    ```
    Otherwise, you can just keep the original file name:
    ```bash
    file=@path/to/your/file.jsonl
    ```
    """
    purpose: NotRequired[FilePurpose]


class FilesAPIRoutesUploadFileMultiPartBodyParams(BaseModel):
    file: Annotated[File, FieldMetadata(multipart=MultipartFormMetadata(file=True))]
    r"""The File object (not file name) to be uploaded.
    To upload a file and specify a custom file name you should format your request as such:
    ```bash
    file=@path/to/your/file.jsonl;filename=custom_name.jsonl
    ```
    Otherwise, you can just keep the original file name:
    ```bash
    file=@path/to/your/file.jsonl
    ```
    """

    purpose: Annotated[
        Annotated[Optional[FilePurpose], PlainValidator(validate_open_enum(False))],
        FieldMetadata(multipart=True),
    ] = None
</file>

<file path="src/mistralai/models/fileschema.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .filepurpose import FilePurpose
from .sampletype import SampleType
from .source import Source
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_open_enum
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing_extensions import Annotated, NotRequired, TypedDict


class FileSchemaTypedDict(TypedDict):
    id: str
    r"""The unique identifier of the file."""
    object: str
    r"""The object type, which is always \"file\"."""
    size_bytes: int
    r"""The size of the file, in bytes."""
    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""
    filename: str
    r"""The name of the uploaded file."""
    purpose: FilePurpose
    sample_type: SampleType
    source: Source
    num_lines: NotRequired[Nullable[int]]


class FileSchema(BaseModel):
    id: str
    r"""The unique identifier of the file."""

    object: str
    r"""The object type, which is always \"file\"."""

    size_bytes: Annotated[int, pydantic.Field(alias="bytes")]
    r"""The size of the file, in bytes."""

    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""

    filename: str
    r"""The name of the uploaded file."""

    purpose: Annotated[FilePurpose, PlainValidator(validate_open_enum(False))]

    sample_type: Annotated[SampleType, PlainValidator(validate_open_enum(False))]

    source: Annotated[Source, PlainValidator(validate_open_enum(False))]

    num_lines: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["num_lines"]
        nullable_fields = ["num_lines"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/filesignedurl.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class FileSignedURLTypedDict(TypedDict):
    url: str


class FileSignedURL(BaseModel):
    url: str
</file>

<file path="src/mistralai/models/fimcompletionrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


FIMCompletionRequestStopTypedDict = TypeAliasType(
    "FIMCompletionRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


FIMCompletionRequestStop = TypeAliasType(
    "FIMCompletionRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


class FIMCompletionRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """
    prompt: str
    r"""The text/code to complete."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""
    stop: NotRequired[FIMCompletionRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    min_tokens: NotRequired[Nullable[int]]
    r"""The minimum number of tokens to generate in the completion."""


class FIMCompletionRequest(BaseModel):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """

    prompt: str
    r"""The text/code to complete."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = 1
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON."""

    stop: Optional[FIMCompletionRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    min_tokens: OptionalNullable[int] = UNSET
    r"""The minimum number of tokens to generate in the completion."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/fimcompletionresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionchoice import ChatCompletionChoice, ChatCompletionChoiceTypedDict
from .usageinfo import UsageInfo, UsageInfoTypedDict
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class FIMCompletionResponseTypedDict(TypedDict):
    id: str
    object: str
    model: str
    usage: UsageInfoTypedDict
    created: int
    choices: List[ChatCompletionChoiceTypedDict]


class FIMCompletionResponse(BaseModel):
    id: str

    object: str

    model: str

    usage: UsageInfo

    created: int

    choices: List[ChatCompletionChoice]
</file>

<file path="src/mistralai/models/fimcompletionstreamrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


FIMCompletionStreamRequestStopTypedDict = TypeAliasType(
    "FIMCompletionStreamRequestStopTypedDict", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


FIMCompletionStreamRequestStop = TypeAliasType(
    "FIMCompletionStreamRequestStop", Union[str, List[str]]
)
r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""


class FIMCompletionStreamRequestTypedDict(TypedDict):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """
    prompt: str
    r"""The text/code to complete."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""
    top_p: NotRequired[float]
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""
    stream: NotRequired[bool]
    stop: NotRequired[FIMCompletionStreamRequestStopTypedDict]
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""
    min_tokens: NotRequired[Nullable[int]]
    r"""The minimum number of tokens to generate in the completion."""


class FIMCompletionStreamRequest(BaseModel):
    model: str
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`
    """

    prompt: str
    r"""The text/code to complete."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value."""

    top_p: Optional[float] = 1
    r"""Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both."""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length."""

    stream: Optional[bool] = True

    stop: Optional[FIMCompletionStreamRequestStop] = None
    r"""Stop generation if this token is detected. Or if one of these tokens is detected when providing an array"""

    random_seed: OptionalNullable[int] = UNSET
    r"""The seed to use for random sampling. If set, different calls will generate deterministic results."""

    suffix: OptionalNullable[str] = UNSET
    r"""Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`."""

    min_tokens: OptionalNullable[int] = UNSET
    r"""The minimum number of tokens to generate in the completion."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "temperature",
            "top_p",
            "max_tokens",
            "stream",
            "stop",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        nullable_fields = [
            "temperature",
            "max_tokens",
            "random_seed",
            "suffix",
            "min_tokens",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/finetuneablemodeltype.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


FineTuneableModelType = Literal["completion", "classifier"]
</file>

<file path="src/mistralai/models/ftclassifierlossfunction.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


FTClassifierLossFunction = Literal["single_class", "multi_class"]
</file>

<file path="src/mistralai/models/ftmodelcapabilitiesout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class FTModelCapabilitiesOutTypedDict(TypedDict):
    completion_chat: NotRequired[bool]
    completion_fim: NotRequired[bool]
    function_calling: NotRequired[bool]
    fine_tuning: NotRequired[bool]
    classification: NotRequired[bool]


class FTModelCapabilitiesOut(BaseModel):
    completion_chat: Optional[bool] = True

    completion_fim: Optional[bool] = False

    function_calling: Optional[bool] = False

    fine_tuning: Optional[bool] = False

    classification: Optional[bool] = False
</file>

<file path="src/mistralai/models/ftmodelcard.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .modelcapabilities import ModelCapabilities, ModelCapabilitiesTypedDict
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_const
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import AfterValidator
from typing import List, Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


FTModelCardType = Literal["fine-tuned"]


class FTModelCardTypedDict(TypedDict):
    r"""Extra fields for fine-tuned models."""

    id: str
    capabilities: ModelCapabilitiesTypedDict
    job: str
    root: str
    object: NotRequired[str]
    created: NotRequired[int]
    owned_by: NotRequired[str]
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]
    max_context_length: NotRequired[int]
    aliases: NotRequired[List[str]]
    deprecation: NotRequired[Nullable[datetime]]
    default_model_temperature: NotRequired[Nullable[float]]
    type: FTModelCardType
    archived: NotRequired[bool]


class FTModelCard(BaseModel):
    r"""Extra fields for fine-tuned models."""

    id: str

    capabilities: ModelCapabilities

    job: str

    root: str

    object: Optional[str] = "model"

    created: Optional[int] = None

    owned_by: Optional[str] = "mistralai"

    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    max_context_length: Optional[int] = 32768

    aliases: Optional[List[str]] = None

    deprecation: OptionalNullable[datetime] = UNSET

    default_model_temperature: OptionalNullable[float] = UNSET

    TYPE: Annotated[
        Annotated[
            Optional[FTModelCardType], AfterValidator(validate_const("fine-tuned"))
        ],
        pydantic.Field(alias="type"),
    ] = "fine-tuned"

    archived: Optional[bool] = False

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "created",
            "owned_by",
            "name",
            "description",
            "max_context_length",
            "aliases",
            "deprecation",
            "default_model_temperature",
            "type",
            "archived",
        ]
        nullable_fields = [
            "name",
            "description",
            "deprecation",
            "default_model_temperature",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/function.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Any, Dict, Optional
from typing_extensions import NotRequired, TypedDict


class FunctionTypedDict(TypedDict):
    name: str
    parameters: Dict[str, Any]
    description: NotRequired[str]
    strict: NotRequired[bool]


class Function(BaseModel):
    name: str

    parameters: Dict[str, Any]

    description: Optional[str] = None

    strict: Optional[bool] = None
</file>

<file path="src/mistralai/models/functioncall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Any, Dict, Union
from typing_extensions import TypeAliasType, TypedDict


ArgumentsTypedDict = TypeAliasType("ArgumentsTypedDict", Union[Dict[str, Any], str])


Arguments = TypeAliasType("Arguments", Union[Dict[str, Any], str])


class FunctionCallTypedDict(TypedDict):
    name: str
    arguments: ArgumentsTypedDict


class FunctionCall(BaseModel):
    name: str

    arguments: Arguments
</file>

<file path="src/mistralai/models/functioncallentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functioncallentryarguments import (
    FunctionCallEntryArguments,
    FunctionCallEntryArgumentsTypedDict,
)
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


FunctionCallEntryObject = Literal["entry"]

FunctionCallEntryType = Literal["function.call"]


class FunctionCallEntryTypedDict(TypedDict):
    tool_call_id: str
    name: str
    arguments: FunctionCallEntryArgumentsTypedDict
    object: NotRequired[FunctionCallEntryObject]
    type: NotRequired[FunctionCallEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]


class FunctionCallEntry(BaseModel):
    tool_call_id: str

    name: str

    arguments: FunctionCallEntryArguments

    object: Optional[FunctionCallEntryObject] = "entry"

    type: Optional[FunctionCallEntryType] = "function.call"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["object", "type", "created_at", "completed_at", "id"]
        nullable_fields = ["completed_at"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/functioncallentryarguments.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Any, Dict, Union
from typing_extensions import TypeAliasType


FunctionCallEntryArgumentsTypedDict = TypeAliasType(
    "FunctionCallEntryArgumentsTypedDict", Union[Dict[str, Any], str]
)


FunctionCallEntryArguments = TypeAliasType(
    "FunctionCallEntryArguments", Union[Dict[str, Any], str]
)
</file>

<file path="src/mistralai/models/functioncallevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


FunctionCallEventType = Literal["function.call.delta"]


class FunctionCallEventTypedDict(TypedDict):
    id: str
    name: str
    tool_call_id: str
    arguments: str
    type: NotRequired[FunctionCallEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]


class FunctionCallEvent(BaseModel):
    id: str

    name: str

    tool_call_id: str

    arguments: str

    type: Optional[FunctionCallEventType] = "function.call.delta"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0
</file>

<file path="src/mistralai/models/functionname.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class FunctionNameTypedDict(TypedDict):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str


class FunctionName(BaseModel):
    r"""this restriction of `Function` is used to select a specific function to call"""

    name: str
</file>

<file path="src/mistralai/models/functionresultentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


FunctionResultEntryObject = Literal["entry"]

FunctionResultEntryType = Literal["function.result"]


class FunctionResultEntryTypedDict(TypedDict):
    tool_call_id: str
    result: str
    object: NotRequired[FunctionResultEntryObject]
    type: NotRequired[FunctionResultEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]


class FunctionResultEntry(BaseModel):
    tool_call_id: str

    result: str

    object: Optional[FunctionResultEntryObject] = "entry"

    type: Optional[FunctionResultEntryType] = "function.result"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["object", "type", "created_at", "completed_at", "id"]
        nullable_fields = ["completed_at"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/functiontool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .function import Function, FunctionTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


FunctionToolType = Literal["function"]


class FunctionToolTypedDict(TypedDict):
    function: FunctionTypedDict
    type: NotRequired[FunctionToolType]


class FunctionTool(BaseModel):
    function: Function

    type: Optional[FunctionToolType] = "function"
</file>

<file path="src/mistralai/models/githubrepositoryin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


GithubRepositoryInType = Literal["github"]


class GithubRepositoryInTypedDict(TypedDict):
    name: str
    owner: str
    token: str
    type: NotRequired[GithubRepositoryInType]
    ref: NotRequired[Nullable[str]]
    weight: NotRequired[float]


class GithubRepositoryIn(BaseModel):
    name: str

    owner: str

    token: str

    type: Optional[GithubRepositoryInType] = "github"

    ref: OptionalNullable[str] = UNSET

    weight: Optional[float] = 1

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "ref", "weight"]
        nullable_fields = ["ref"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/githubrepositoryout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


GithubRepositoryOutType = Literal["github"]


class GithubRepositoryOutTypedDict(TypedDict):
    name: str
    owner: str
    commit_id: str
    type: NotRequired[GithubRepositoryOutType]
    ref: NotRequired[Nullable[str]]
    weight: NotRequired[float]


class GithubRepositoryOut(BaseModel):
    name: str

    owner: str

    commit_id: str

    type: Optional[GithubRepositoryOutType] = "github"

    ref: OptionalNullable[str] = UNSET

    weight: Optional[float] = 1

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "ref", "weight"]
        nullable_fields = ["ref"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/httpvalidationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .validationerror import ValidationError
from mistralai import utils
from mistralai.types import BaseModel
from typing import List, Optional


class HTTPValidationErrorData(BaseModel):
    detail: Optional[List[ValidationError]] = None


class HTTPValidationError(Exception):
    data: HTTPValidationErrorData

    def __init__(self, data: HTTPValidationErrorData):
        self.data = data

    def __str__(self) -> str:
        return utils.marshal_json(self.data, HTTPValidationErrorData)
</file>

<file path="src/mistralai/models/imagegenerationtool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ImageGenerationToolType = Literal["image_generation"]


class ImageGenerationToolTypedDict(TypedDict):
    type: NotRequired[ImageGenerationToolType]


class ImageGenerationTool(BaseModel):
    type: Optional[ImageGenerationToolType] = "image_generation"
</file>

<file path="src/mistralai/models/imageurl.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class ImageURLTypedDict(TypedDict):
    url: str
    detail: NotRequired[Nullable[str]]


class ImageURL(BaseModel):
    url: str

    detail: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["detail"]
        nullable_fields = ["detail"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/imageurlchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .imageurl import ImageURL, ImageURLTypedDict
from mistralai.types import BaseModel
from typing import Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ImageURLChunkImageURLTypedDict = TypeAliasType(
    "ImageURLChunkImageURLTypedDict", Union[ImageURLTypedDict, str]
)


ImageURLChunkImageURL = TypeAliasType("ImageURLChunkImageURL", Union[ImageURL, str])


ImageURLChunkType = Literal["image_url"]


class ImageURLChunkTypedDict(TypedDict):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURLTypedDict
    type: NotRequired[ImageURLChunkType]


class ImageURLChunk(BaseModel):
    r"""{\"type\":\"image_url\",\"image_url\":{\"url\":\"data:image/png;base64,iVBORw0"""

    image_url: ImageURLChunkImageURL

    type: Optional[ImageURLChunkType] = "image_url"
</file>

<file path="src/mistralai/models/inputentries.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functionresultentry import FunctionResultEntry, FunctionResultEntryTypedDict
from .messageinputentry import MessageInputEntry, MessageInputEntryTypedDict
from typing import Union
from typing_extensions import TypeAliasType


InputEntriesTypedDict = TypeAliasType(
    "InputEntriesTypedDict",
    Union[MessageInputEntryTypedDict, FunctionResultEntryTypedDict],
)


InputEntries = TypeAliasType(
    "InputEntries", Union[MessageInputEntry, FunctionResultEntry]
)
</file>

<file path="src/mistralai/models/inputs.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .instructrequest import InstructRequest, InstructRequestTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import List, Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


InstructRequestInputsMessagesTypedDict = TypeAliasType(
    "InstructRequestInputsMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


InstructRequestInputsMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


class InstructRequestInputsTypedDict(TypedDict):
    messages: List[InstructRequestInputsMessagesTypedDict]


class InstructRequestInputs(BaseModel):
    messages: List[InstructRequestInputsMessages]


InputsTypedDict = TypeAliasType(
    "InputsTypedDict",
    Union[InstructRequestInputsTypedDict, List[InstructRequestTypedDict]],
)
r"""Chat to classify"""


Inputs = TypeAliasType("Inputs", Union[InstructRequestInputs, List[InstructRequest]])
r"""Chat to classify"""
</file>

<file path="src/mistralai/models/instructrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .assistantmessage import AssistantMessage, AssistantMessageTypedDict
from .systemmessage import SystemMessage, SystemMessageTypedDict
from .toolmessage import ToolMessage, ToolMessageTypedDict
from .usermessage import UserMessage, UserMessageTypedDict
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import List, Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


InstructRequestMessagesTypedDict = TypeAliasType(
    "InstructRequestMessagesTypedDict",
    Union[
        SystemMessageTypedDict,
        UserMessageTypedDict,
        AssistantMessageTypedDict,
        ToolMessageTypedDict,
    ],
)


InstructRequestMessages = Annotated[
    Union[
        Annotated[AssistantMessage, Tag("assistant")],
        Annotated[SystemMessage, Tag("system")],
        Annotated[ToolMessage, Tag("tool")],
        Annotated[UserMessage, Tag("user")],
    ],
    Discriminator(lambda m: get_discriminator(m, "role", "role")),
]


class InstructRequestTypedDict(TypedDict):
    messages: List[InstructRequestMessagesTypedDict]


class InstructRequest(BaseModel):
    messages: List[InstructRequestMessages]
</file>

<file path="src/mistralai/models/jobin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifiertargetin import ClassifierTargetIn, ClassifierTargetInTypedDict
from .classifiertrainingparametersin import (
    ClassifierTrainingParametersIn,
    ClassifierTrainingParametersInTypedDict,
)
from .completiontrainingparametersin import (
    CompletionTrainingParametersIn,
    CompletionTrainingParametersInTypedDict,
)
from .finetuneablemodeltype import FineTuneableModelType
from .githubrepositoryin import GithubRepositoryIn, GithubRepositoryInTypedDict
from .trainingfile import TrainingFile, TrainingFileTypedDict
from .wandbintegration import WandbIntegration, WandbIntegrationTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


JobInIntegrationsTypedDict = WandbIntegrationTypedDict


JobInIntegrations = WandbIntegration


HyperparametersTypedDict = TypeAliasType(
    "HyperparametersTypedDict",
    Union[
        ClassifierTrainingParametersInTypedDict, CompletionTrainingParametersInTypedDict
    ],
)


Hyperparameters = TypeAliasType(
    "Hyperparameters",
    Union[ClassifierTrainingParametersIn, CompletionTrainingParametersIn],
)


JobInRepositoriesTypedDict = GithubRepositoryInTypedDict


JobInRepositories = GithubRepositoryIn


class JobInTypedDict(TypedDict):
    model: str
    r"""The name of the model to fine-tune."""
    hyperparameters: HyperparametersTypedDict
    training_files: NotRequired[List[TrainingFileTypedDict]]
    validation_files: NotRequired[Nullable[List[str]]]
    r"""A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files."""
    suffix: NotRequired[Nullable[str]]
    r"""A string that will be added to your fine-tuning model name. For example, a suffix of \"my-great-model\" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`"""
    integrations: NotRequired[Nullable[List[JobInIntegrationsTypedDict]]]
    r"""A list of integrations to enable for your fine-tuning job."""
    auto_start: NotRequired[bool]
    r"""This field will be required in a future release."""
    invalid_sample_skip_percentage: NotRequired[float]
    job_type: NotRequired[Nullable[FineTuneableModelType]]
    repositories: NotRequired[Nullable[List[JobInRepositoriesTypedDict]]]
    classifier_targets: NotRequired[Nullable[List[ClassifierTargetInTypedDict]]]


class JobIn(BaseModel):
    model: str
    r"""The name of the model to fine-tune."""

    hyperparameters: Hyperparameters

    training_files: Optional[List[TrainingFile]] = None

    validation_files: OptionalNullable[List[str]] = UNSET
    r"""A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files."""

    suffix: OptionalNullable[str] = UNSET
    r"""A string that will be added to your fine-tuning model name. For example, a suffix of \"my-great-model\" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`"""

    integrations: OptionalNullable[List[JobInIntegrations]] = UNSET
    r"""A list of integrations to enable for your fine-tuning job."""

    auto_start: Optional[bool] = None
    r"""This field will be required in a future release."""

    invalid_sample_skip_percentage: Optional[float] = 0

    job_type: OptionalNullable[FineTuneableModelType] = UNSET

    repositories: OptionalNullable[List[JobInRepositories]] = UNSET

    classifier_targets: OptionalNullable[List[ClassifierTargetIn]] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "training_files",
            "validation_files",
            "suffix",
            "integrations",
            "auto_start",
            "invalid_sample_skip_percentage",
            "job_type",
            "repositories",
            "classifier_targets",
        ]
        nullable_fields = [
            "validation_files",
            "suffix",
            "integrations",
            "job_type",
            "repositories",
            "classifier_targets",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/jobmetadataout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class JobMetadataOutTypedDict(TypedDict):
    expected_duration_seconds: NotRequired[Nullable[int]]
    cost: NotRequired[Nullable[float]]
    cost_currency: NotRequired[Nullable[str]]
    train_tokens_per_step: NotRequired[Nullable[int]]
    train_tokens: NotRequired[Nullable[int]]
    data_tokens: NotRequired[Nullable[int]]
    estimated_start_time: NotRequired[Nullable[int]]


class JobMetadataOut(BaseModel):
    expected_duration_seconds: OptionalNullable[int] = UNSET

    cost: OptionalNullable[float] = UNSET

    cost_currency: OptionalNullable[str] = UNSET

    train_tokens_per_step: OptionalNullable[int] = UNSET

    train_tokens: OptionalNullable[int] = UNSET

    data_tokens: OptionalNullable[int] = UNSET

    estimated_start_time: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "expected_duration_seconds",
            "cost",
            "cost_currency",
            "train_tokens_per_step",
            "train_tokens",
            "data_tokens",
            "estimated_start_time",
        ]
        nullable_fields = [
            "expected_duration_seconds",
            "cost",
            "cost_currency",
            "train_tokens_per_step",
            "train_tokens",
            "data_tokens",
            "estimated_start_time",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/jobs_api_routes_batch_cancel_batch_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class JobsAPIRoutesBatchCancelBatchJobRequestTypedDict(TypedDict):
    job_id: str


class JobsAPIRoutesBatchCancelBatchJobRequest(BaseModel):
    job_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/jobs_api_routes_batch_get_batch_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class JobsAPIRoutesBatchGetBatchJobRequestTypedDict(TypedDict):
    job_id: str


class JobsAPIRoutesBatchGetBatchJobRequest(BaseModel):
    job_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
</file>

<file path="src/mistralai/models/jobs_api_routes_batch_get_batch_jobsop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .batchjobstatus import BatchJobStatus
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import FieldMetadata, QueryParamMetadata
from pydantic import model_serializer
from typing import Any, Dict, List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class JobsAPIRoutesBatchGetBatchJobsRequestTypedDict(TypedDict):
    page: NotRequired[int]
    page_size: NotRequired[int]
    model: NotRequired[Nullable[str]]
    metadata: NotRequired[Nullable[Dict[str, Any]]]
    created_after: NotRequired[Nullable[datetime]]
    created_by_me: NotRequired[bool]
    status: NotRequired[Nullable[List[BatchJobStatus]]]


class JobsAPIRoutesBatchGetBatchJobsRequest(BaseModel):
    page: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 0

    page_size: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 100

    model: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    metadata: Annotated[
        OptionalNullable[Dict[str, Any]],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    created_after: Annotated[
        OptionalNullable[datetime],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    created_by_me: Annotated[
        Optional[bool],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = False

    status: Annotated[
        OptionalNullable[List[BatchJobStatus]],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "page",
            "page_size",
            "model",
            "metadata",
            "created_after",
            "created_by_me",
            "status",
        ]
        nullable_fields = ["model", "metadata", "created_after", "status"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_archive_fine_tuned_modelop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class JobsAPIRoutesFineTuningArchiveFineTunedModelRequestTypedDict(TypedDict):
    model_id: str
    r"""The ID of the model to archive."""


class JobsAPIRoutesFineTuningArchiveFineTunedModelRequest(BaseModel):
    model_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the model to archive."""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_cancel_fine_tuning_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierdetailedjobout import (
    ClassifierDetailedJobOut,
    ClassifierDetailedJobOutTypedDict,
)
from .completiondetailedjobout import (
    CompletionDetailedJobOut,
    CompletionDetailedJobOutTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class JobsAPIRoutesFineTuningCancelFineTuningJobRequestTypedDict(TypedDict):
    job_id: str
    r"""The ID of the job to cancel."""


class JobsAPIRoutesFineTuningCancelFineTuningJobRequest(BaseModel):
    job_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the job to cancel."""


JobsAPIRoutesFineTuningCancelFineTuningJobResponseTypedDict = TypeAliasType(
    "JobsAPIRoutesFineTuningCancelFineTuningJobResponseTypedDict",
    Union[CompletionDetailedJobOutTypedDict, ClassifierDetailedJobOutTypedDict],
)
r"""OK"""


JobsAPIRoutesFineTuningCancelFineTuningJobResponse = Annotated[
    Union[
        Annotated[ClassifierDetailedJobOut, Tag("classifier")],
        Annotated[CompletionDetailedJobOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "job_type", "job_type")),
]
r"""OK"""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_create_fine_tuning_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierjobout import ClassifierJobOut, ClassifierJobOutTypedDict
from .completionjobout import CompletionJobOut, CompletionJobOutTypedDict
from .legacyjobmetadataout import LegacyJobMetadataOut, LegacyJobMetadataOutTypedDict
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType


Response1TypedDict = TypeAliasType(
    "Response1TypedDict", Union[ClassifierJobOutTypedDict, CompletionJobOutTypedDict]
)


Response1 = Annotated[
    Union[
        Annotated[ClassifierJobOut, Tag("classifier")],
        Annotated[CompletionJobOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "job_type", "job_type")),
]


JobsAPIRoutesFineTuningCreateFineTuningJobResponseTypedDict = TypeAliasType(
    "JobsAPIRoutesFineTuningCreateFineTuningJobResponseTypedDict",
    Union[LegacyJobMetadataOutTypedDict, Response1TypedDict],
)
r"""OK"""


JobsAPIRoutesFineTuningCreateFineTuningJobResponse = TypeAliasType(
    "JobsAPIRoutesFineTuningCreateFineTuningJobResponse",
    Union[LegacyJobMetadataOut, Response1],
)
r"""OK"""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_get_fine_tuning_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierdetailedjobout import (
    ClassifierDetailedJobOut,
    ClassifierDetailedJobOutTypedDict,
)
from .completiondetailedjobout import (
    CompletionDetailedJobOut,
    CompletionDetailedJobOutTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class JobsAPIRoutesFineTuningGetFineTuningJobRequestTypedDict(TypedDict):
    job_id: str
    r"""The ID of the job to analyse."""


class JobsAPIRoutesFineTuningGetFineTuningJobRequest(BaseModel):
    job_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the job to analyse."""


JobsAPIRoutesFineTuningGetFineTuningJobResponseTypedDict = TypeAliasType(
    "JobsAPIRoutesFineTuningGetFineTuningJobResponseTypedDict",
    Union[CompletionDetailedJobOutTypedDict, ClassifierDetailedJobOutTypedDict],
)
r"""OK"""


JobsAPIRoutesFineTuningGetFineTuningJobResponse = Annotated[
    Union[
        Annotated[ClassifierDetailedJobOut, Tag("classifier")],
        Annotated[CompletionDetailedJobOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "job_type", "job_type")),
]
r"""OK"""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_get_fine_tuning_jobsop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import FieldMetadata, QueryParamMetadata
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


QueryParamStatus = Literal[
    "QUEUED",
    "STARTED",
    "VALIDATING",
    "VALIDATED",
    "RUNNING",
    "FAILED_VALIDATION",
    "FAILED",
    "SUCCESS",
    "CANCELLED",
    "CANCELLATION_REQUESTED",
]
r"""The current job state to filter on. When set, the other results are not displayed."""


class JobsAPIRoutesFineTuningGetFineTuningJobsRequestTypedDict(TypedDict):
    page: NotRequired[int]
    r"""The page number of the results to be returned."""
    page_size: NotRequired[int]
    r"""The number of items to return per page."""
    model: NotRequired[Nullable[str]]
    r"""The model name used for fine-tuning to filter on. When set, the other results are not displayed."""
    created_after: NotRequired[Nullable[datetime]]
    r"""The date/time to filter on. When set, the results for previous creation times are not displayed."""
    created_before: NotRequired[Nullable[datetime]]
    created_by_me: NotRequired[bool]
    r"""When set, only return results for jobs created by the API caller. Other results are not displayed."""
    status: NotRequired[Nullable[QueryParamStatus]]
    r"""The current job state to filter on. When set, the other results are not displayed."""
    wandb_project: NotRequired[Nullable[str]]
    r"""The Weights and Biases project to filter on. When set, the other results are not displayed."""
    wandb_name: NotRequired[Nullable[str]]
    r"""The Weight and Biases run name to filter on. When set, the other results are not displayed."""
    suffix: NotRequired[Nullable[str]]
    r"""The model suffix to filter on. When set, the other results are not displayed."""


class JobsAPIRoutesFineTuningGetFineTuningJobsRequest(BaseModel):
    page: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 0
    r"""The page number of the results to be returned."""

    page_size: Annotated[
        Optional[int],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = 100
    r"""The number of items to return per page."""

    model: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The model name used for fine-tuning to filter on. When set, the other results are not displayed."""

    created_after: Annotated[
        OptionalNullable[datetime],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The date/time to filter on. When set, the results for previous creation times are not displayed."""

    created_before: Annotated[
        OptionalNullable[datetime],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET

    created_by_me: Annotated[
        Optional[bool],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = False
    r"""When set, only return results for jobs created by the API caller. Other results are not displayed."""

    status: Annotated[
        OptionalNullable[QueryParamStatus],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The current job state to filter on. When set, the other results are not displayed."""

    wandb_project: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The Weights and Biases project to filter on. When set, the other results are not displayed."""

    wandb_name: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The Weight and Biases run name to filter on. When set, the other results are not displayed."""

    suffix: Annotated[
        OptionalNullable[str],
        FieldMetadata(query=QueryParamMetadata(style="form", explode=True)),
    ] = UNSET
    r"""The model suffix to filter on. When set, the other results are not displayed."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "page",
            "page_size",
            "model",
            "created_after",
            "created_before",
            "created_by_me",
            "status",
            "wandb_project",
            "wandb_name",
            "suffix",
        ]
        nullable_fields = [
            "model",
            "created_after",
            "created_before",
            "status",
            "wandb_project",
            "wandb_name",
            "suffix",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_start_fine_tuning_jobop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierdetailedjobout import (
    ClassifierDetailedJobOut,
    ClassifierDetailedJobOutTypedDict,
)
from .completiondetailedjobout import (
    CompletionDetailedJobOut,
    CompletionDetailedJobOutTypedDict,
)
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class JobsAPIRoutesFineTuningStartFineTuningJobRequestTypedDict(TypedDict):
    job_id: str


class JobsAPIRoutesFineTuningStartFineTuningJobRequest(BaseModel):
    job_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]


JobsAPIRoutesFineTuningStartFineTuningJobResponseTypedDict = TypeAliasType(
    "JobsAPIRoutesFineTuningStartFineTuningJobResponseTypedDict",
    Union[CompletionDetailedJobOutTypedDict, ClassifierDetailedJobOutTypedDict],
)
r"""OK"""


JobsAPIRoutesFineTuningStartFineTuningJobResponse = Annotated[
    Union[
        Annotated[ClassifierDetailedJobOut, Tag("classifier")],
        Annotated[CompletionDetailedJobOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "job_type", "job_type")),
]
r"""OK"""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_unarchive_fine_tuned_modelop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata
from typing_extensions import Annotated, TypedDict


class JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequestTypedDict(TypedDict):
    model_id: str
    r"""The ID of the model to unarchive."""


class JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest(BaseModel):
    model_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the model to unarchive."""
</file>

<file path="src/mistralai/models/jobs_api_routes_fine_tuning_update_fine_tuned_modelop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierftmodelout import ClassifierFTModelOut, ClassifierFTModelOutTypedDict
from .completionftmodelout import CompletionFTModelOut, CompletionFTModelOutTypedDict
from .updateftmodelin import UpdateFTModelIn, UpdateFTModelInTypedDict
from mistralai.types import BaseModel
from mistralai.utils import (
    FieldMetadata,
    PathParamMetadata,
    RequestMetadata,
    get_discriminator,
)
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class JobsAPIRoutesFineTuningUpdateFineTunedModelRequestTypedDict(TypedDict):
    model_id: str
    r"""The ID of the model to update."""
    update_ft_model_in: UpdateFTModelInTypedDict


class JobsAPIRoutesFineTuningUpdateFineTunedModelRequest(BaseModel):
    model_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the model to update."""

    update_ft_model_in: Annotated[
        UpdateFTModelIn,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]


JobsAPIRoutesFineTuningUpdateFineTunedModelResponseTypedDict = TypeAliasType(
    "JobsAPIRoutesFineTuningUpdateFineTunedModelResponseTypedDict",
    Union[CompletionFTModelOutTypedDict, ClassifierFTModelOutTypedDict],
)
r"""OK"""


JobsAPIRoutesFineTuningUpdateFineTunedModelResponse = Annotated[
    Union[
        Annotated[ClassifierFTModelOut, Tag("classifier")],
        Annotated[CompletionFTModelOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_type", "model_type")),
]
r"""OK"""
</file>

<file path="src/mistralai/models/jobsout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .classifierjobout import ClassifierJobOut, ClassifierJobOutTypedDict
from .completionjobout import CompletionJobOut, CompletionJobOutTypedDict
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


JobsOutDataTypedDict = TypeAliasType(
    "JobsOutDataTypedDict", Union[ClassifierJobOutTypedDict, CompletionJobOutTypedDict]
)


JobsOutData = Annotated[
    Union[
        Annotated[ClassifierJobOut, Tag("classifier")],
        Annotated[CompletionJobOut, Tag("completion")],
    ],
    Discriminator(lambda m: get_discriminator(m, "job_type", "job_type")),
]


JobsOutObject = Literal["list"]


class JobsOutTypedDict(TypedDict):
    total: int
    data: NotRequired[List[JobsOutDataTypedDict]]
    object: NotRequired[JobsOutObject]


class JobsOut(BaseModel):
    total: int

    data: Optional[List[JobsOutData]] = None

    object: Optional[JobsOutObject] = "list"
</file>

<file path="src/mistralai/models/jsonschema.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
import pydantic
from pydantic import model_serializer
from typing import Any, Dict, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class JSONSchemaTypedDict(TypedDict):
    name: str
    schema_definition: Dict[str, Any]
    description: NotRequired[Nullable[str]]
    strict: NotRequired[bool]


class JSONSchema(BaseModel):
    name: str

    schema_definition: Annotated[Dict[str, Any], pydantic.Field(alias="schema")]

    description: OptionalNullable[str] = UNSET

    strict: Optional[bool] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["description", "strict"]
        nullable_fields = ["description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/legacyjobmetadataout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


LegacyJobMetadataOutObject = Literal["job.metadata"]


class LegacyJobMetadataOutTypedDict(TypedDict):
    details: str
    expected_duration_seconds: NotRequired[Nullable[int]]
    r"""The approximated time (in seconds) for the fine-tuning process to complete."""
    cost: NotRequired[Nullable[float]]
    r"""The cost of the fine-tuning job."""
    cost_currency: NotRequired[Nullable[str]]
    r"""The currency used for the fine-tuning job cost."""
    train_tokens_per_step: NotRequired[Nullable[int]]
    r"""The number of tokens consumed by one training step."""
    train_tokens: NotRequired[Nullable[int]]
    r"""The total number of tokens used during the fine-tuning process."""
    data_tokens: NotRequired[Nullable[int]]
    r"""The total number of tokens in the training dataset."""
    estimated_start_time: NotRequired[Nullable[int]]
    deprecated: NotRequired[bool]
    epochs: NotRequired[Nullable[float]]
    r"""The number of complete passes through the entire training dataset."""
    training_steps: NotRequired[Nullable[int]]
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""
    object: NotRequired[LegacyJobMetadataOutObject]


class LegacyJobMetadataOut(BaseModel):
    details: str

    expected_duration_seconds: OptionalNullable[int] = UNSET
    r"""The approximated time (in seconds) for the fine-tuning process to complete."""

    cost: OptionalNullable[float] = UNSET
    r"""The cost of the fine-tuning job."""

    cost_currency: OptionalNullable[str] = UNSET
    r"""The currency used for the fine-tuning job cost."""

    train_tokens_per_step: OptionalNullable[int] = UNSET
    r"""The number of tokens consumed by one training step."""

    train_tokens: OptionalNullable[int] = UNSET
    r"""The total number of tokens used during the fine-tuning process."""

    data_tokens: OptionalNullable[int] = UNSET
    r"""The total number of tokens in the training dataset."""

    estimated_start_time: OptionalNullable[int] = UNSET

    deprecated: Optional[bool] = True

    epochs: OptionalNullable[float] = UNSET
    r"""The number of complete passes through the entire training dataset."""

    training_steps: OptionalNullable[int] = UNSET
    r"""The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset."""

    object: Optional[LegacyJobMetadataOutObject] = "job.metadata"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "expected_duration_seconds",
            "cost",
            "cost_currency",
            "train_tokens_per_step",
            "train_tokens",
            "data_tokens",
            "estimated_start_time",
            "deprecated",
            "epochs",
            "training_steps",
            "object",
        ]
        nullable_fields = [
            "expected_duration_seconds",
            "cost",
            "cost_currency",
            "train_tokens_per_step",
            "train_tokens",
            "data_tokens",
            "estimated_start_time",
            "epochs",
            "training_steps",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/listfilesout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .fileschema import FileSchema, FileSchemaTypedDict
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class ListFilesOutTypedDict(TypedDict):
    data: List[FileSchemaTypedDict]
    object: str
    total: int


class ListFilesOut(BaseModel):
    data: List[FileSchema]

    object: str

    total: int
</file>

<file path="src/mistralai/models/messageentries.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .messageinputentry import MessageInputEntry, MessageInputEntryTypedDict
from .messageoutputentry import MessageOutputEntry, MessageOutputEntryTypedDict
from typing import Union
from typing_extensions import TypeAliasType


MessageEntriesTypedDict = TypeAliasType(
    "MessageEntriesTypedDict",
    Union[MessageInputEntryTypedDict, MessageOutputEntryTypedDict],
)


MessageEntries = TypeAliasType(
    "MessageEntries", Union[MessageInputEntry, MessageOutputEntry]
)
</file>

<file path="src/mistralai/models/messageinputcontentchunks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .documenturlchunk import DocumentURLChunk, DocumentURLChunkTypedDict
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from .toolfilechunk import ToolFileChunk, ToolFileChunkTypedDict
from typing import Union
from typing_extensions import TypeAliasType


MessageInputContentChunksTypedDict = TypeAliasType(
    "MessageInputContentChunksTypedDict",
    Union[
        TextChunkTypedDict,
        ImageURLChunkTypedDict,
        DocumentURLChunkTypedDict,
        ToolFileChunkTypedDict,
    ],
)


MessageInputContentChunks = TypeAliasType(
    "MessageInputContentChunks",
    Union[TextChunk, ImageURLChunk, DocumentURLChunk, ToolFileChunk],
)
</file>

<file path="src/mistralai/models/messageinputentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .messageinputcontentchunks import (
    MessageInputContentChunks,
    MessageInputContentChunksTypedDict,
)
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


Object = Literal["entry"]

MessageInputEntryType = Literal["message.input"]

MessageInputEntryRole = Literal["assistant", "user"]

MessageInputEntryContentTypedDict = TypeAliasType(
    "MessageInputEntryContentTypedDict",
    Union[str, List[MessageInputContentChunksTypedDict]],
)


MessageInputEntryContent = TypeAliasType(
    "MessageInputEntryContent", Union[str, List[MessageInputContentChunks]]
)


class MessageInputEntryTypedDict(TypedDict):
    r"""Representation of an input message inside the conversation."""

    role: MessageInputEntryRole
    content: MessageInputEntryContentTypedDict
    object: NotRequired[Object]
    type: NotRequired[MessageInputEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]


class MessageInputEntry(BaseModel):
    r"""Representation of an input message inside the conversation."""

    role: MessageInputEntryRole

    content: MessageInputEntryContent

    object: Optional[Object] = "entry"

    type: Optional[MessageInputEntryType] = "message.input"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["object", "type", "created_at", "completed_at", "id"]
        nullable_fields = ["completed_at"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/messageoutputcontentchunks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .documenturlchunk import DocumentURLChunk, DocumentURLChunkTypedDict
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from .toolfilechunk import ToolFileChunk, ToolFileChunkTypedDict
from .toolreferencechunk import ToolReferenceChunk, ToolReferenceChunkTypedDict
from typing import Union
from typing_extensions import TypeAliasType


MessageOutputContentChunksTypedDict = TypeAliasType(
    "MessageOutputContentChunksTypedDict",
    Union[
        TextChunkTypedDict,
        ImageURLChunkTypedDict,
        DocumentURLChunkTypedDict,
        ToolFileChunkTypedDict,
        ToolReferenceChunkTypedDict,
    ],
)


MessageOutputContentChunks = TypeAliasType(
    "MessageOutputContentChunks",
    Union[
        TextChunk, ImageURLChunk, DocumentURLChunk, ToolFileChunk, ToolReferenceChunk
    ],
)
</file>

<file path="src/mistralai/models/messageoutputentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .messageoutputcontentchunks import (
    MessageOutputContentChunks,
    MessageOutputContentChunksTypedDict,
)
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


MessageOutputEntryObject = Literal["entry"]

MessageOutputEntryType = Literal["message.output"]

MessageOutputEntryRole = Literal["assistant"]

MessageOutputEntryContentTypedDict = TypeAliasType(
    "MessageOutputEntryContentTypedDict",
    Union[str, List[MessageOutputContentChunksTypedDict]],
)


MessageOutputEntryContent = TypeAliasType(
    "MessageOutputEntryContent", Union[str, List[MessageOutputContentChunks]]
)


class MessageOutputEntryTypedDict(TypedDict):
    content: MessageOutputEntryContentTypedDict
    object: NotRequired[MessageOutputEntryObject]
    type: NotRequired[MessageOutputEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]
    agent_id: NotRequired[Nullable[str]]
    model: NotRequired[Nullable[str]]
    role: NotRequired[MessageOutputEntryRole]


class MessageOutputEntry(BaseModel):
    content: MessageOutputEntryContent

    object: Optional[MessageOutputEntryObject] = "entry"

    type: Optional[MessageOutputEntryType] = "message.output"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    agent_id: OptionalNullable[str] = UNSET

    model: OptionalNullable[str] = UNSET

    role: Optional[MessageOutputEntryRole] = "assistant"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "object",
            "type",
            "created_at",
            "completed_at",
            "id",
            "agent_id",
            "model",
            "role",
        ]
        nullable_fields = ["completed_at", "agent_id", "model"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/messageoutputevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .outputcontentchunks import OutputContentChunks, OutputContentChunksTypedDict
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


MessageOutputEventType = Literal["message.output.delta"]

MessageOutputEventRole = Literal["assistant"]

MessageOutputEventContentTypedDict = TypeAliasType(
    "MessageOutputEventContentTypedDict", Union[str, OutputContentChunksTypedDict]
)


MessageOutputEventContent = TypeAliasType(
    "MessageOutputEventContent", Union[str, OutputContentChunks]
)


class MessageOutputEventTypedDict(TypedDict):
    id: str
    content: MessageOutputEventContentTypedDict
    type: NotRequired[MessageOutputEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]
    content_index: NotRequired[int]
    model: NotRequired[Nullable[str]]
    agent_id: NotRequired[Nullable[str]]
    role: NotRequired[MessageOutputEventRole]


class MessageOutputEvent(BaseModel):
    id: str

    content: MessageOutputEventContent

    type: Optional[MessageOutputEventType] = "message.output.delta"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0

    content_index: Optional[int] = 0

    model: OptionalNullable[str] = UNSET

    agent_id: OptionalNullable[str] = UNSET

    role: Optional[MessageOutputEventRole] = "assistant"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "type",
            "created_at",
            "output_index",
            "content_index",
            "model",
            "agent_id",
            "role",
        ]
        nullable_fields = ["model", "agent_id"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/metricout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class MetricOutTypedDict(TypedDict):
    r"""Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase)."""

    train_loss: NotRequired[Nullable[float]]
    valid_loss: NotRequired[Nullable[float]]
    valid_mean_token_accuracy: NotRequired[Nullable[float]]


class MetricOut(BaseModel):
    r"""Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase)."""

    train_loss: OptionalNullable[float] = UNSET

    valid_loss: OptionalNullable[float] = UNSET

    valid_mean_token_accuracy: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["train_loss", "valid_loss", "valid_mean_token_accuracy"]
        nullable_fields = ["train_loss", "valid_loss", "valid_mean_token_accuracy"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/mistralpromptmode.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


MistralPromptMode = Union[Literal["reasoning"], UnrecognizedStr]
</file>

<file path="src/mistralai/models/modelcapabilities.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ModelCapabilitiesTypedDict(TypedDict):
    completion_chat: NotRequired[bool]
    completion_fim: NotRequired[bool]
    function_calling: NotRequired[bool]
    fine_tuning: NotRequired[bool]
    vision: NotRequired[bool]


class ModelCapabilities(BaseModel):
    completion_chat: Optional[bool] = True

    completion_fim: Optional[bool] = False

    function_calling: Optional[bool] = True

    fine_tuning: Optional[bool] = False

    vision: Optional[bool] = False
</file>

<file path="src/mistralai/models/modelconversation.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .codeinterpretertool import CodeInterpreterTool, CodeInterpreterToolTypedDict
from .completionargs import CompletionArgs, CompletionArgsTypedDict
from .documentlibrarytool import DocumentLibraryTool, DocumentLibraryToolTypedDict
from .functiontool import FunctionTool, FunctionToolTypedDict
from .imagegenerationtool import ImageGenerationTool, ImageGenerationToolTypedDict
from .websearchpremiumtool import WebSearchPremiumTool, WebSearchPremiumToolTypedDict
from .websearchtool import WebSearchTool, WebSearchToolTypedDict
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag, model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ModelConversationToolsTypedDict = TypeAliasType(
    "ModelConversationToolsTypedDict",
    Union[
        WebSearchToolTypedDict,
        WebSearchPremiumToolTypedDict,
        CodeInterpreterToolTypedDict,
        ImageGenerationToolTypedDict,
        FunctionToolTypedDict,
        DocumentLibraryToolTypedDict,
    ],
)


ModelConversationTools = Annotated[
    Union[
        Annotated[CodeInterpreterTool, Tag("code_interpreter")],
        Annotated[DocumentLibraryTool, Tag("document_library")],
        Annotated[FunctionTool, Tag("function")],
        Annotated[ImageGenerationTool, Tag("image_generation")],
        Annotated[WebSearchTool, Tag("web_search")],
        Annotated[WebSearchPremiumTool, Tag("web_search_premium")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


ModelConversationObject = Literal["conversation"]


class ModelConversationTypedDict(TypedDict):
    id: str
    created_at: datetime
    updated_at: datetime
    model: str
    instructions: NotRequired[Nullable[str]]
    r"""Instruction prompt the model will follow during the conversation."""
    tools: NotRequired[List[ModelConversationToolsTypedDict]]
    r"""List of tools which are available to the model during the conversation."""
    completion_args: NotRequired[CompletionArgsTypedDict]
    r"""White-listed arguments from the completion API"""
    name: NotRequired[Nullable[str]]
    r"""Name given to the conversation."""
    description: NotRequired[Nullable[str]]
    r"""Description of the what the conversation is about."""
    object: NotRequired[ModelConversationObject]


class ModelConversation(BaseModel):
    id: str

    created_at: datetime

    updated_at: datetime

    model: str

    instructions: OptionalNullable[str] = UNSET
    r"""Instruction prompt the model will follow during the conversation."""

    tools: Optional[List[ModelConversationTools]] = None
    r"""List of tools which are available to the model during the conversation."""

    completion_args: Optional[CompletionArgs] = None
    r"""White-listed arguments from the completion API"""

    name: OptionalNullable[str] = UNSET
    r"""Name given to the conversation."""

    description: OptionalNullable[str] = UNSET
    r"""Description of the what the conversation is about."""

    object: Optional[ModelConversationObject] = "conversation"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "instructions",
            "tools",
            "completion_args",
            "name",
            "description",
            "object",
        ]
        nullable_fields = ["instructions", "name", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/modellist.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .basemodelcard import BaseModelCard, BaseModelCardTypedDict
from .ftmodelcard import FTModelCard, FTModelCardTypedDict
from mistralai.types import BaseModel
from mistralai.utils import get_discriminator
from pydantic import Discriminator, Tag
from typing import List, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


DataTypedDict = TypeAliasType(
    "DataTypedDict", Union[BaseModelCardTypedDict, FTModelCardTypedDict]
)


Data = Annotated[
    Union[
        Annotated[BaseModelCard, Tag("base")], Annotated[FTModelCard, Tag("fine-tuned")]
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


class ModelListTypedDict(TypedDict):
    object: NotRequired[str]
    data: NotRequired[List[DataTypedDict]]


class ModelList(BaseModel):
    object: Optional[str] = "list"

    data: Optional[List[Data]] = None
</file>

<file path="src/mistralai/models/moderationobject.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Dict, Optional
from typing_extensions import NotRequired, TypedDict


class ModerationObjectTypedDict(TypedDict):
    categories: NotRequired[Dict[str, bool]]
    r"""Moderation result thresholds"""
    category_scores: NotRequired[Dict[str, float]]
    r"""Moderation result"""


class ModerationObject(BaseModel):
    categories: Optional[Dict[str, bool]] = None
    r"""Moderation result thresholds"""

    category_scores: Optional[Dict[str, float]] = None
    r"""Moderation result"""
</file>

<file path="src/mistralai/models/moderationresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .moderationobject import ModerationObject, ModerationObjectTypedDict
from mistralai.types import BaseModel
from typing import List
from typing_extensions import TypedDict


class ModerationResponseTypedDict(TypedDict):
    id: str
    model: str
    results: List[ModerationObjectTypedDict]


class ModerationResponse(BaseModel):
    id: str

    model: str

    results: List[ModerationObject]
</file>

<file path="src/mistralai/models/ocrimageobject.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class OCRImageObjectTypedDict(TypedDict):
    id: str
    r"""Image ID for extracted image in a page"""
    top_left_x: Nullable[int]
    r"""X coordinate of top-left corner of the extracted image"""
    top_left_y: Nullable[int]
    r"""Y coordinate of top-left corner of the extracted image"""
    bottom_right_x: Nullable[int]
    r"""X coordinate of bottom-right corner of the extracted image"""
    bottom_right_y: Nullable[int]
    r"""Y coordinate of bottom-right corner of the extracted image"""
    image_base64: NotRequired[Nullable[str]]
    r"""Base64 string of the extracted image"""
    image_annotation: NotRequired[Nullable[str]]
    r"""Annotation of the extracted image in json str"""


class OCRImageObject(BaseModel):
    id: str
    r"""Image ID for extracted image in a page"""

    top_left_x: Nullable[int]
    r"""X coordinate of top-left corner of the extracted image"""

    top_left_y: Nullable[int]
    r"""Y coordinate of top-left corner of the extracted image"""

    bottom_right_x: Nullable[int]
    r"""X coordinate of bottom-right corner of the extracted image"""

    bottom_right_y: Nullable[int]
    r"""Y coordinate of bottom-right corner of the extracted image"""

    image_base64: OptionalNullable[str] = UNSET
    r"""Base64 string of the extracted image"""

    image_annotation: OptionalNullable[str] = UNSET
    r"""Annotation of the extracted image in json str"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["image_base64", "image_annotation"]
        nullable_fields = [
            "top_left_x",
            "top_left_y",
            "bottom_right_x",
            "bottom_right_y",
            "image_base64",
            "image_annotation",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/ocrpagedimensions.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class OCRPageDimensionsTypedDict(TypedDict):
    dpi: int
    r"""Dots per inch of the page-image"""
    height: int
    r"""Height of the image in pixels"""
    width: int
    r"""Width of the image in pixels"""


class OCRPageDimensions(BaseModel):
    dpi: int
    r"""Dots per inch of the page-image"""

    height: int
    r"""Height of the image in pixels"""

    width: int
    r"""Width of the image in pixels"""
</file>

<file path="src/mistralai/models/ocrpageobject.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .ocrimageobject import OCRImageObject, OCRImageObjectTypedDict
from .ocrpagedimensions import OCRPageDimensions, OCRPageDimensionsTypedDict
from mistralai.types import BaseModel, Nullable, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List
from typing_extensions import TypedDict


class OCRPageObjectTypedDict(TypedDict):
    index: int
    r"""The page index in a pdf document starting from 0"""
    markdown: str
    r"""The markdown string response of the page"""
    images: List[OCRImageObjectTypedDict]
    r"""List of all extracted images in the page"""
    dimensions: Nullable[OCRPageDimensionsTypedDict]
    r"""The dimensions of the PDF Page's screenshot image"""


class OCRPageObject(BaseModel):
    index: int
    r"""The page index in a pdf document starting from 0"""

    markdown: str
    r"""The markdown string response of the page"""

    images: List[OCRImageObject]
    r"""List of all extracted images in the page"""

    dimensions: Nullable[OCRPageDimensions]
    r"""The dimensions of the PDF Page's screenshot image"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = []
        nullable_fields = ["dimensions"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/ocrrequest.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .documenturlchunk import DocumentURLChunk, DocumentURLChunkTypedDict
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


DocumentTypedDict = TypeAliasType(
    "DocumentTypedDict", Union[ImageURLChunkTypedDict, DocumentURLChunkTypedDict]
)
r"""Document to run OCR on"""


Document = TypeAliasType("Document", Union[ImageURLChunk, DocumentURLChunk])
r"""Document to run OCR on"""


class OCRRequestTypedDict(TypedDict):
    model: Nullable[str]
    document: DocumentTypedDict
    r"""Document to run OCR on"""
    id: NotRequired[str]
    pages: NotRequired[Nullable[List[int]]]
    r"""Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0"""
    include_image_base64: NotRequired[Nullable[bool]]
    r"""Include image URLs in response"""
    image_limit: NotRequired[Nullable[int]]
    r"""Max images to extract"""
    image_min_size: NotRequired[Nullable[int]]
    r"""Minimum height and width of image to extract"""
    bbox_annotation_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    r"""Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field"""
    document_annotation_format: NotRequired[Nullable[ResponseFormatTypedDict]]
    r"""Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field"""


class OCRRequest(BaseModel):
    model: Nullable[str]

    document: Document
    r"""Document to run OCR on"""

    id: Optional[str] = None

    pages: OptionalNullable[List[int]] = UNSET
    r"""Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0"""

    include_image_base64: OptionalNullable[bool] = UNSET
    r"""Include image URLs in response"""

    image_limit: OptionalNullable[int] = UNSET
    r"""Max images to extract"""

    image_min_size: OptionalNullable[int] = UNSET
    r"""Minimum height and width of image to extract"""

    bbox_annotation_format: OptionalNullable[ResponseFormat] = UNSET
    r"""Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field"""

    document_annotation_format: OptionalNullable[ResponseFormat] = UNSET
    r"""Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "id",
            "pages",
            "include_image_base64",
            "image_limit",
            "image_min_size",
            "bbox_annotation_format",
            "document_annotation_format",
        ]
        nullable_fields = [
            "model",
            "pages",
            "include_image_base64",
            "image_limit",
            "image_min_size",
            "bbox_annotation_format",
            "document_annotation_format",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/ocrresponse.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .ocrpageobject import OCRPageObject, OCRPageObjectTypedDict
from .ocrusageinfo import OCRUsageInfo, OCRUsageInfoTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List
from typing_extensions import NotRequired, TypedDict


class OCRResponseTypedDict(TypedDict):
    pages: List[OCRPageObjectTypedDict]
    r"""List of OCR info for pages."""
    model: str
    r"""The model used to generate the OCR."""
    usage_info: OCRUsageInfoTypedDict
    document_annotation: NotRequired[Nullable[str]]
    r"""Formatted response in the request_format if provided in json str"""


class OCRResponse(BaseModel):
    pages: List[OCRPageObject]
    r"""List of OCR info for pages."""

    model: str
    r"""The model used to generate the OCR."""

    usage_info: OCRUsageInfo

    document_annotation: OptionalNullable[str] = UNSET
    r"""Formatted response in the request_format if provided in json str"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["document_annotation"]
        nullable_fields = ["document_annotation"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/ocrusageinfo.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class OCRUsageInfoTypedDict(TypedDict):
    pages_processed: int
    r"""Number of pages processed"""
    doc_size_bytes: NotRequired[Nullable[int]]
    r"""Document size in bytes"""


class OCRUsageInfo(BaseModel):
    pages_processed: int
    r"""Number of pages processed"""

    doc_size_bytes: OptionalNullable[int] = UNSET
    r"""Document size in bytes"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["doc_size_bytes"]
        nullable_fields = ["doc_size_bytes"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/outputcontentchunks.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .documenturlchunk import DocumentURLChunk, DocumentURLChunkTypedDict
from .imageurlchunk import ImageURLChunk, ImageURLChunkTypedDict
from .textchunk import TextChunk, TextChunkTypedDict
from .toolfilechunk import ToolFileChunk, ToolFileChunkTypedDict
from .toolreferencechunk import ToolReferenceChunk, ToolReferenceChunkTypedDict
from typing import Union
from typing_extensions import TypeAliasType


OutputContentChunksTypedDict = TypeAliasType(
    "OutputContentChunksTypedDict",
    Union[
        TextChunkTypedDict,
        ImageURLChunkTypedDict,
        DocumentURLChunkTypedDict,
        ToolFileChunkTypedDict,
        ToolReferenceChunkTypedDict,
    ],
)


OutputContentChunks = TypeAliasType(
    "OutputContentChunks",
    Union[
        TextChunk, ImageURLChunk, DocumentURLChunk, ToolFileChunk, ToolReferenceChunk
    ],
)
</file>

<file path="src/mistralai/models/prediction.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import validate_const
import pydantic
from pydantic.functional_validators import AfterValidator
from typing import Literal, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class PredictionTypedDict(TypedDict):
    type: Literal["content"]
    content: NotRequired[str]


class Prediction(BaseModel):
    TYPE: Annotated[
        Annotated[
            Optional[Literal["content"]], AfterValidator(validate_const("content"))
        ],
        pydantic.Field(alias="type"),
    ] = "content"

    content: Optional[str] = ""
</file>

<file path="src/mistralai/models/referencechunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import List, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ReferenceChunkType = Literal["reference"]


class ReferenceChunkTypedDict(TypedDict):
    reference_ids: List[int]
    type: NotRequired[ReferenceChunkType]


class ReferenceChunk(BaseModel):
    reference_ids: List[int]

    type: Optional[ReferenceChunkType] = "reference"
</file>

<file path="src/mistralai/models/responsedoneevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .conversationusageinfo import ConversationUsageInfo, ConversationUsageInfoTypedDict
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ResponseDoneEventType = Literal["conversation.response.done"]


class ResponseDoneEventTypedDict(TypedDict):
    usage: ConversationUsageInfoTypedDict
    type: NotRequired[ResponseDoneEventType]
    created_at: NotRequired[datetime]


class ResponseDoneEvent(BaseModel):
    usage: ConversationUsageInfo

    type: Optional[ResponseDoneEventType] = "conversation.response.done"

    created_at: Optional[datetime] = None
</file>

<file path="src/mistralai/models/responseerrorevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ResponseErrorEventType = Literal["conversation.response.error"]


class ResponseErrorEventTypedDict(TypedDict):
    message: str
    code: int
    type: NotRequired[ResponseErrorEventType]
    created_at: NotRequired[datetime]


class ResponseErrorEvent(BaseModel):
    message: str

    code: int

    type: Optional[ResponseErrorEventType] = "conversation.response.error"

    created_at: Optional[datetime] = None
</file>

<file path="src/mistralai/models/responseformat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .jsonschema import JSONSchema, JSONSchemaTypedDict
from .responseformats import ResponseFormats
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class ResponseFormatTypedDict(TypedDict):
    type: NotRequired[ResponseFormats]
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
    json_schema: NotRequired[Nullable[JSONSchemaTypedDict]]


class ResponseFormat(BaseModel):
    type: Optional[ResponseFormats] = None
    r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""

    json_schema: OptionalNullable[JSONSchema] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "json_schema"]
        nullable_fields = ["json_schema"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/responseformats.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ResponseFormats = Literal["text", "json_object", "json_schema"]
r"""An object specifying the format that the model must output. Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message."""
</file>

<file path="src/mistralai/models/responsestartedevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ResponseStartedEventType = Literal["conversation.response.started"]


class ResponseStartedEventTypedDict(TypedDict):
    conversation_id: str
    type: NotRequired[ResponseStartedEventType]
    created_at: NotRequired[datetime]


class ResponseStartedEvent(BaseModel):
    conversation_id: str

    type: Optional[ResponseStartedEventType] = "conversation.response.started"

    created_at: Optional[datetime] = None
</file>

<file path="src/mistralai/models/retrieve_model_v1_models_model_id_getop.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .basemodelcard import BaseModelCard, BaseModelCardTypedDict
from .ftmodelcard import FTModelCard, FTModelCardTypedDict
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, PathParamMetadata, get_discriminator
from pydantic import Discriminator, Tag
from typing import Union
from typing_extensions import Annotated, TypeAliasType, TypedDict


class RetrieveModelV1ModelsModelIDGetRequestTypedDict(TypedDict):
    model_id: str
    r"""The ID of the model to retrieve."""


class RetrieveModelV1ModelsModelIDGetRequest(BaseModel):
    model_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the model to retrieve."""


RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGetTypedDict = TypeAliasType(
    "RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGetTypedDict",
    Union[BaseModelCardTypedDict, FTModelCardTypedDict],
)
r"""Successful Response"""


RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet = Annotated[
    Union[
        Annotated[BaseModelCard, Tag("base")], Annotated[FTModelCard, Tag("fine-tuned")]
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]
r"""Successful Response"""
</file>

<file path="src/mistralai/models/retrievefileout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .filepurpose import FilePurpose
from .sampletype import SampleType
from .source import Source
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_open_enum
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing_extensions import Annotated, NotRequired, TypedDict


class RetrieveFileOutTypedDict(TypedDict):
    id: str
    r"""The unique identifier of the file."""
    object: str
    r"""The object type, which is always \"file\"."""
    size_bytes: int
    r"""The size of the file, in bytes."""
    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""
    filename: str
    r"""The name of the uploaded file."""
    purpose: FilePurpose
    sample_type: SampleType
    source: Source
    deleted: bool
    num_lines: NotRequired[Nullable[int]]


class RetrieveFileOut(BaseModel):
    id: str
    r"""The unique identifier of the file."""

    object: str
    r"""The object type, which is always \"file\"."""

    size_bytes: Annotated[int, pydantic.Field(alias="bytes")]
    r"""The size of the file, in bytes."""

    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""

    filename: str
    r"""The name of the uploaded file."""

    purpose: Annotated[FilePurpose, PlainValidator(validate_open_enum(False))]

    sample_type: Annotated[SampleType, PlainValidator(validate_open_enum(False))]

    source: Annotated[Source, PlainValidator(validate_open_enum(False))]

    deleted: bool

    num_lines: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["num_lines"]
        nullable_fields = ["num_lines"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/sampletype.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


SampleType = Union[
    Literal["pretrain", "instruct", "batch_request", "batch_result", "batch_error"],
    UnrecognizedStr,
]
</file>

<file path="src/mistralai/models/sdkerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from dataclasses import dataclass
from typing import Optional
import httpx


@dataclass
class SDKError(Exception):
    """Represents an error returned by the API."""

    message: str
    status_code: int = -1
    body: str = ""
    raw_response: Optional[httpx.Response] = None

    def __str__(self):
        body = ""
        if len(self.body) > 0:
            body = f"\n{self.body}"

        return f"{self.message}: Status {self.status_code}{body}"
</file>

<file path="src/mistralai/models/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from mistralai.utils import FieldMetadata, SecurityMetadata
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class SecurityTypedDict(TypedDict):
    api_key: NotRequired[str]


class Security(BaseModel):
    api_key: Annotated[
        Optional[str],
        FieldMetadata(
            security=SecurityMetadata(
                scheme=True,
                scheme_type="http",
                sub_type="bearer",
                field_name="Authorization",
            )
        ),
    ] = None
</file>

<file path="src/mistralai/models/source.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


Source = Union[Literal["upload", "repository", "mistral"], UnrecognizedStr]
</file>

<file path="src/mistralai/models/ssetypes.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


SSETypes = Literal[
    "conversation.response.started",
    "conversation.response.done",
    "conversation.response.error",
    "message.output.delta",
    "tool.execution.started",
    "tool.execution.done",
    "agent.handoff.started",
    "agent.handoff.done",
    "function.call.delta",
]
r"""Server side events sent when streaming a conversation response."""
</file>

<file path="src/mistralai/models/systemmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .textchunk import TextChunk, TextChunkTypedDict
from mistralai.types import BaseModel
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


SystemMessageContentTypedDict = TypeAliasType(
    "SystemMessageContentTypedDict", Union[str, List[TextChunkTypedDict]]
)


SystemMessageContent = TypeAliasType(
    "SystemMessageContent", Union[str, List[TextChunk]]
)


Role = Literal["system"]


class SystemMessageTypedDict(TypedDict):
    content: SystemMessageContentTypedDict
    role: NotRequired[Role]


class SystemMessage(BaseModel):
    content: SystemMessageContent

    role: Optional[Role] = "system"
</file>

<file path="src/mistralai/models/textchunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


TextChunkType = Literal["text"]


class TextChunkTypedDict(TypedDict):
    text: str
    type: NotRequired[TextChunkType]


class TextChunk(BaseModel):
    text: str

    type: Optional[TextChunkType] = "text"
</file>

<file path="src/mistralai/models/tool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .function import Function, FunctionTypedDict
from .tooltypes import ToolTypes
from mistralai.types import BaseModel
from mistralai.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolTypedDict(TypedDict):
    function: FunctionTypedDict
    type: NotRequired[ToolTypes]


class Tool(BaseModel):
    function: Function

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="src/mistralai/models/toolcall.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functioncall import FunctionCall, FunctionCallTypedDict
from .tooltypes import ToolTypes
from mistralai.types import BaseModel
from mistralai.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolCallTypedDict(TypedDict):
    function: FunctionCallTypedDict
    id: NotRequired[str]
    type: NotRequired[ToolTypes]
    index: NotRequired[int]


class ToolCall(BaseModel):
    function: FunctionCall

    id: Optional[str] = "null"

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )

    index: Optional[int] = 0
</file>

<file path="src/mistralai/models/toolchoice.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .functionname import FunctionName, FunctionNameTypedDict
from .tooltypes import ToolTypes
from mistralai.types import BaseModel
from mistralai.utils import validate_open_enum
from pydantic.functional_validators import PlainValidator
from typing import Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class ToolChoiceTypedDict(TypedDict):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionNameTypedDict
    r"""this restriction of `Function` is used to select a specific function to call"""
    type: NotRequired[ToolTypes]


class ToolChoice(BaseModel):
    r"""ToolChoice is either a ToolChoiceEnum or a ToolChoice"""

    function: FunctionName
    r"""this restriction of `Function` is used to select a specific function to call"""

    type: Annotated[Optional[ToolTypes], PlainValidator(validate_open_enum(False))] = (
        None
    )
</file>

<file path="src/mistralai/models/toolchoiceenum.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from typing import Literal


ToolChoiceEnum = Literal["auto", "none", "any", "required"]
</file>

<file path="src/mistralai/models/toolexecutiondoneevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .builtinconnectors import BuiltInConnectors
from datetime import datetime
from mistralai.types import BaseModel
from typing import Any, Dict, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ToolExecutionDoneEventType = Literal["tool.execution.done"]


class ToolExecutionDoneEventTypedDict(TypedDict):
    id: str
    name: BuiltInConnectors
    type: NotRequired[ToolExecutionDoneEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]
    info: NotRequired[Dict[str, Any]]


class ToolExecutionDoneEvent(BaseModel):
    id: str

    name: BuiltInConnectors

    type: Optional[ToolExecutionDoneEventType] = "tool.execution.done"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0

    info: Optional[Dict[str, Any]] = None
</file>

<file path="src/mistralai/models/toolexecutionentry.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .builtinconnectors import BuiltInConnectors
from datetime import datetime
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Any, Dict, Literal, Optional
from typing_extensions import NotRequired, TypedDict


ToolExecutionEntryObject = Literal["entry"]

ToolExecutionEntryType = Literal["tool.execution"]


class ToolExecutionEntryTypedDict(TypedDict):
    name: BuiltInConnectors
    object: NotRequired[ToolExecutionEntryObject]
    type: NotRequired[ToolExecutionEntryType]
    created_at: NotRequired[datetime]
    completed_at: NotRequired[Nullable[datetime]]
    id: NotRequired[str]
    info: NotRequired[Dict[str, Any]]


class ToolExecutionEntry(BaseModel):
    name: BuiltInConnectors

    object: Optional[ToolExecutionEntryObject] = "entry"

    type: Optional[ToolExecutionEntryType] = "tool.execution"

    created_at: Optional[datetime] = None

    completed_at: OptionalNullable[datetime] = UNSET

    id: Optional[str] = None

    info: Optional[Dict[str, Any]] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["object", "type", "created_at", "completed_at", "id", "info"]
        nullable_fields = ["completed_at"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/toolexecutionstartedevent.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .builtinconnectors import BuiltInConnectors
from datetime import datetime
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ToolExecutionStartedEventType = Literal["tool.execution.started"]


class ToolExecutionStartedEventTypedDict(TypedDict):
    id: str
    name: BuiltInConnectors
    type: NotRequired[ToolExecutionStartedEventType]
    created_at: NotRequired[datetime]
    output_index: NotRequired[int]


class ToolExecutionStartedEvent(BaseModel):
    id: str

    name: BuiltInConnectors

    type: Optional[ToolExecutionStartedEventType] = "tool.execution.started"

    created_at: Optional[datetime] = None

    output_index: Optional[int] = 0
</file>

<file path="src/mistralai/models/toolfilechunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .builtinconnectors import BuiltInConnectors
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ToolFileChunkType = Literal["tool_file"]


class ToolFileChunkTypedDict(TypedDict):
    tool: BuiltInConnectors
    file_id: str
    type: NotRequired[ToolFileChunkType]
    file_name: NotRequired[Nullable[str]]
    file_type: NotRequired[Nullable[str]]


class ToolFileChunk(BaseModel):
    tool: BuiltInConnectors

    file_id: str

    type: Optional[ToolFileChunkType] = "tool_file"

    file_name: OptionalNullable[str] = UNSET

    file_type: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "file_name", "file_type"]
        nullable_fields = ["file_name", "file_type"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/toolmessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


ToolMessageContentTypedDict = TypeAliasType(
    "ToolMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


ToolMessageContent = TypeAliasType("ToolMessageContent", Union[str, List[ContentChunk]])


ToolMessageRole = Literal["tool"]


class ToolMessageTypedDict(TypedDict):
    content: Nullable[ToolMessageContentTypedDict]
    tool_call_id: NotRequired[Nullable[str]]
    name: NotRequired[Nullable[str]]
    role: NotRequired[ToolMessageRole]


class ToolMessage(BaseModel):
    content: Nullable[ToolMessageContent]

    tool_call_id: OptionalNullable[str] = UNSET

    name: OptionalNullable[str] = UNSET

    role: Optional[ToolMessageRole] = "tool"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["tool_call_id", "name", "role"]
        nullable_fields = ["content", "tool_call_id", "name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/toolreferencechunk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .builtinconnectors import BuiltInConnectors
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


ToolReferenceChunkType = Literal["tool_reference"]


class ToolReferenceChunkTypedDict(TypedDict):
    tool: BuiltInConnectors
    title: str
    type: NotRequired[ToolReferenceChunkType]
    url: NotRequired[Nullable[str]]
    source: NotRequired[Nullable[str]]


class ToolReferenceChunk(BaseModel):
    tool: BuiltInConnectors

    title: str

    type: Optional[ToolReferenceChunkType] = "tool_reference"

    url: OptionalNullable[str] = UNSET

    source: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "url", "source"]
        nullable_fields = ["url", "source"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/tooltypes.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import UnrecognizedStr
from typing import Literal, Union


ToolTypes = Union[Literal["function"], UnrecognizedStr]
</file>

<file path="src/mistralai/models/trainingfile.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class TrainingFileTypedDict(TypedDict):
    file_id: str
    weight: NotRequired[float]


class TrainingFile(BaseModel):
    file_id: str

    weight: Optional[float] = 1
</file>

<file path="src/mistralai/models/unarchiveftmodelout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


UnarchiveFTModelOutObject = Literal["model"]


class UnarchiveFTModelOutTypedDict(TypedDict):
    id: str
    object: NotRequired[UnarchiveFTModelOutObject]
    archived: NotRequired[bool]


class UnarchiveFTModelOut(BaseModel):
    id: str

    object: Optional[UnarchiveFTModelOutObject] = "model"

    archived: Optional[bool] = False
</file>

<file path="src/mistralai/models/updateftmodelin.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing_extensions import NotRequired, TypedDict


class UpdateFTModelInTypedDict(TypedDict):
    name: NotRequired[Nullable[str]]
    description: NotRequired[Nullable[str]]


class UpdateFTModelIn(BaseModel):
    name: OptionalNullable[str] = UNSET

    description: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["name", "description"]
        nullable_fields = ["name", "description"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/uploadfileout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .filepurpose import FilePurpose
from .sampletype import SampleType
from .source import Source
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from mistralai.utils import validate_open_enum
import pydantic
from pydantic import model_serializer
from pydantic.functional_validators import PlainValidator
from typing_extensions import Annotated, NotRequired, TypedDict


class UploadFileOutTypedDict(TypedDict):
    id: str
    r"""The unique identifier of the file."""
    object: str
    r"""The object type, which is always \"file\"."""
    size_bytes: int
    r"""The size of the file, in bytes."""
    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""
    filename: str
    r"""The name of the uploaded file."""
    purpose: FilePurpose
    sample_type: SampleType
    source: Source
    num_lines: NotRequired[Nullable[int]]


class UploadFileOut(BaseModel):
    id: str
    r"""The unique identifier of the file."""

    object: str
    r"""The object type, which is always \"file\"."""

    size_bytes: Annotated[int, pydantic.Field(alias="bytes")]
    r"""The size of the file, in bytes."""

    created_at: int
    r"""The UNIX timestamp (in seconds) of the event."""

    filename: str
    r"""The name of the uploaded file."""

    purpose: Annotated[FilePurpose, PlainValidator(validate_open_enum(False))]

    sample_type: Annotated[SampleType, PlainValidator(validate_open_enum(False))]

    source: Annotated[Source, PlainValidator(validate_open_enum(False))]

    num_lines: OptionalNullable[int] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["num_lines"]
        nullable_fields = ["num_lines"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/usageinfo.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing_extensions import TypedDict


class UsageInfoTypedDict(TypedDict):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class UsageInfo(BaseModel):
    prompt_tokens: int

    completion_tokens: int

    total_tokens: int
</file>

<file path="src/mistralai/models/usermessage.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .contentchunk import ContentChunk, ContentChunkTypedDict
from mistralai.types import BaseModel, Nullable, UNSET_SENTINEL
from pydantic import model_serializer
from typing import List, Literal, Optional, Union
from typing_extensions import NotRequired, TypeAliasType, TypedDict


UserMessageContentTypedDict = TypeAliasType(
    "UserMessageContentTypedDict", Union[str, List[ContentChunkTypedDict]]
)


UserMessageContent = TypeAliasType("UserMessageContent", Union[str, List[ContentChunk]])


UserMessageRole = Literal["user"]


class UserMessageTypedDict(TypedDict):
    content: Nullable[UserMessageContentTypedDict]
    role: NotRequired[UserMessageRole]


class UserMessage(BaseModel):
    content: Nullable[UserMessageContent]

    role: Optional[UserMessageRole] = "user"

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["role"]
        nullable_fields = ["content"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/validationerror.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import List, Union
from typing_extensions import TypeAliasType, TypedDict


LocTypedDict = TypeAliasType("LocTypedDict", Union[str, int])


Loc = TypeAliasType("Loc", Union[str, int])


class ValidationErrorTypedDict(TypedDict):
    loc: List[LocTypedDict]
    msg: str
    type: str


class ValidationError(BaseModel):
    loc: List[Loc]

    msg: str

    type: str
</file>

<file path="src/mistralai/models/wandbintegration.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


WandbIntegrationType = Literal["wandb"]


class WandbIntegrationTypedDict(TypedDict):
    project: str
    r"""The name of the project that the new run will be created under."""
    api_key: str
    r"""The WandB API key to use for authentication."""
    type: NotRequired[WandbIntegrationType]
    name: NotRequired[Nullable[str]]
    r"""A display name to set for the run. If not set, will use the job ID as the name."""
    run_name: NotRequired[Nullable[str]]


class WandbIntegration(BaseModel):
    project: str
    r"""The name of the project that the new run will be created under."""

    api_key: str
    r"""The WandB API key to use for authentication."""

    type: Optional[WandbIntegrationType] = "wandb"

    name: OptionalNullable[str] = UNSET
    r"""A display name to set for the run. If not set, will use the job ID as the name."""

    run_name: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "name", "run_name"]
        nullable_fields = ["name", "run_name"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/wandbintegrationout.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


WandbIntegrationOutType = Literal["wandb"]


class WandbIntegrationOutTypedDict(TypedDict):
    project: str
    r"""The name of the project that the new run will be created under."""
    type: NotRequired[WandbIntegrationOutType]
    name: NotRequired[Nullable[str]]
    r"""A display name to set for the run. If not set, will use the job ID as the name."""
    run_name: NotRequired[Nullable[str]]
    url: NotRequired[Nullable[str]]


class WandbIntegrationOut(BaseModel):
    project: str
    r"""The name of the project that the new run will be created under."""

    type: Optional[WandbIntegrationOutType] = "wandb"

    name: OptionalNullable[str] = UNSET
    r"""A display name to set for the run. If not set, will use the job ID as the name."""

    run_name: OptionalNullable[str] = UNSET

    url: OptionalNullable[str] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["type", "name", "run_name", "url"]
        nullable_fields = ["name", "run_name", "url"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
</file>

<file path="src/mistralai/models/websearchpremiumtool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


WebSearchPremiumToolType = Literal["web_search_premium"]


class WebSearchPremiumToolTypedDict(TypedDict):
    type: NotRequired[WebSearchPremiumToolType]


class WebSearchPremiumTool(BaseModel):
    type: Optional[WebSearchPremiumToolType] = "web_search_premium"
</file>

<file path="src/mistralai/models/websearchtool.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from mistralai.types import BaseModel
from typing import Literal, Optional
from typing_extensions import NotRequired, TypedDict


WebSearchToolType = Literal["web_search"]


class WebSearchToolTypedDict(TypedDict):
    type: NotRequired[WebSearchToolType]


class WebSearchTool(BaseModel):
    type: Optional[WebSearchToolType] = "web_search"
</file>

<file path="src/mistralai/types/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basemodel import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UnrecognizedInt,
    UnrecognizedStr,
    UNSET,
    UNSET_SENTINEL,
)

__all__ = [
    "BaseModel",
    "Nullable",
    "OptionalNullable",
    "UnrecognizedInt",
    "UnrecognizedStr",
    "UNSET",
    "UNSET_SENTINEL",
]
</file>

<file path="src/mistralai/types/basemodel.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from pydantic import ConfigDict, model_serializer
from pydantic import BaseModel as PydanticBaseModel
from typing import TYPE_CHECKING, Literal, Optional, TypeVar, Union, NewType
from typing_extensions import TypeAliasType, TypeAlias


class BaseModel(PydanticBaseModel):
    model_config = ConfigDict(
        populate_by_name=True, arbitrary_types_allowed=True, protected_namespaces=()
    )


class Unset(BaseModel):
    @model_serializer(mode="plain")
    def serialize_model(self):
        return UNSET_SENTINEL

    def __bool__(self) -> Literal[False]:
        return False


UNSET = Unset()
UNSET_SENTINEL = "~?~unset~?~sentinel~?~"


T = TypeVar("T")
if TYPE_CHECKING:
    Nullable: TypeAlias = Union[T, None]
    OptionalNullable: TypeAlias = Union[Optional[Nullable[T]], Unset]
else:
    Nullable = TypeAliasType("Nullable", Union[T, None], type_params=(T,))
    OptionalNullable = TypeAliasType(
        "OptionalNullable", Union[Optional[Nullable[T]], Unset], type_params=(T,)
    )

UnrecognizedInt = NewType("UnrecognizedInt", int)
UnrecognizedStr = NewType("UnrecognizedStr", str)
</file>

<file path="src/mistralai/utils/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .annotations import get_discriminator
from .enums import OpenEnumMeta
from .headers import get_headers, get_response_headers
from .metadata import (
    FieldMetadata,
    find_metadata,
    FormMetadata,
    HeaderMetadata,
    MultipartFormMetadata,
    PathParamMetadata,
    QueryParamMetadata,
    RequestMetadata,
    SecurityMetadata,
)
from .queryparams import get_query_params
from .retries import BackoffStrategy, Retries, retry, retry_async, RetryConfig
from .requestbodies import serialize_request_body, SerializedRequestBody
from .security import get_security, get_security_from_env

from .serializers import (
    get_pydantic_model,
    marshal_json,
    unmarshal,
    unmarshal_json,
    serialize_decimal,
    serialize_float,
    serialize_int,
    stream_to_text,
    stream_to_text_async,
    stream_to_bytes,
    stream_to_bytes_async,
    validate_const,
    validate_decimal,
    validate_float,
    validate_int,
    validate_open_enum,
)
from .url import generate_url, template_url, remove_suffix
from .values import (
    get_global_from_env,
    match_content_type,
    match_status_codes,
    match_response,
    cast_partial,
)
from .logger import Logger, get_body_content, get_default_logger

__all__ = [
    "BackoffStrategy",
    "FieldMetadata",
    "find_metadata",
    "FormMetadata",
    "generate_url",
    "get_body_content",
    "get_default_logger",
    "get_discriminator",
    "get_global_from_env",
    "get_headers",
    "get_pydantic_model",
    "get_query_params",
    "get_response_headers",
    "get_security",
    "get_security_from_env",
    "HeaderMetadata",
    "Logger",
    "marshal_json",
    "match_content_type",
    "match_status_codes",
    "match_response",
    "MultipartFormMetadata",
    "OpenEnumMeta",
    "PathParamMetadata",
    "QueryParamMetadata",
    "remove_suffix",
    "Retries",
    "retry",
    "retry_async",
    "RetryConfig",
    "RequestMetadata",
    "SecurityMetadata",
    "serialize_decimal",
    "serialize_float",
    "serialize_int",
    "serialize_request_body",
    "SerializedRequestBody",
    "stream_to_text",
    "stream_to_text_async",
    "stream_to_bytes",
    "stream_to_bytes_async",
    "template_url",
    "unmarshal",
    "unmarshal_json",
    "validate_decimal",
    "validate_const",
    "validate_float",
    "validate_int",
    "validate_open_enum",
    "cast_partial",
]
</file>

<file path="src/mistralai/utils/annotations.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from enum import Enum
from typing import Any, Optional

def get_discriminator(model: Any, fieldname: str, key: str) -> str:
    """
    Recursively search for the discriminator attribute in a model.

    Args:
        model (Any): The model to search within.
        fieldname (str): The name of the field to search for.
        key (str): The key to search for in dictionaries.

    Returns:
        str: The name of the discriminator attribute.

    Raises:
        ValueError: If the discriminator attribute is not found.
    """
    upper_fieldname = fieldname.upper()

    def get_field_discriminator(field: Any) -> Optional[str]:
        """Search for the discriminator attribute in a given field."""

        if isinstance(field, dict):
            if key in field:
                return f'{field[key]}'

        if hasattr(field, fieldname):
            attr = getattr(field, fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        if hasattr(field, upper_fieldname):
            attr = getattr(field, upper_fieldname)
            if isinstance(attr, Enum):
                return f'{attr.value}'
            return f'{attr}'

        return None


    if isinstance(model, list):
        for field in model:
            discriminator = get_field_discriminator(field)
            if discriminator is not None:
                return discriminator

    discriminator = get_field_discriminator(model)
    if discriminator is not None:
        return discriminator

    raise ValueError(f'Could not find discriminator field {fieldname} in {model}')
</file>

<file path="src/mistralai/utils/enums.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import enum


class OpenEnumMeta(enum.EnumMeta):
    def __call__(
        cls, value, names=None, *, module=None, qualname=None, type=None, start=1
    ):
        # The `type` kwarg also happens to be a built-in that pylint flags as
        # redeclared. Safe to ignore this lint rule with this scope.
        # pylint: disable=redefined-builtin

        if names is not None:
            return super().__call__(
                value,
                names=names,
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )

        try:
            return super().__call__(
                value,
                names=names,  # pyright: ignore[reportArgumentType]
                module=module,
                qualname=qualname,
                type=type,
                start=start,
            )
        except ValueError:
            return value
</file>

<file path="src/mistralai/utils/eventstreaming.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import re
import json
from typing import (
    Callable,
    Generic,
    TypeVar,
    Optional,
    Generator,
    AsyncGenerator,
    Tuple,
)
import httpx

T = TypeVar("T")


class EventStream(Generic[T]):
    response: httpx.Response
    generator: Generator[T, None, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events(response, decoder, sentinel)

    def __iter__(self):
        return self

    def __next__(self):
        return next(self.generator)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.response.close()


class EventStreamAsync(Generic[T]):
    response: httpx.Response
    generator: AsyncGenerator[T, None]

    def __init__(
        self,
        response: httpx.Response,
        decoder: Callable[[str], T],
        sentinel: Optional[str] = None,
    ):
        self.response = response
        self.generator = stream_events_async(response, decoder, sentinel)

    def __aiter__(self):
        return self

    async def __anext__(self):
        return await self.generator.__anext__()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.response.aclose()


class ServerEvent:
    id: Optional[str] = None
    event: Optional[str] = None
    data: Optional[str] = None
    retry: Optional[int] = None


MESSAGE_BOUNDARIES = [
    b"\r\n\r\n",
    b"\n\n",
    b"\r\r",
]


async def stream_events_async(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> AsyncGenerator[T, None]:
    buffer = bytearray()
    position = 0
    discard = False
    async for chunk in response.aiter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def stream_events(
    response: httpx.Response,
    decoder: Callable[[str], T],
    sentinel: Optional[str] = None,
) -> Generator[T, None, None]:
    buffer = bytearray()
    position = 0
    discard = False
    for chunk in response.iter_bytes():
        # We've encountered the sentinel value and should no longer process
        # incoming data. Instead we throw new data away until the server closes
        # the connection.
        if discard:
            continue

        buffer += chunk
        for i in range(position, len(buffer)):
            char = buffer[i : i + 1]
            seq: Optional[bytes] = None
            if char in [b"\r", b"\n"]:
                for boundary in MESSAGE_BOUNDARIES:
                    seq = _peek_sequence(i, buffer, boundary)
                    if seq is not None:
                        break
            if seq is None:
                continue

            block = buffer[position:i]
            position = i + len(seq)
            event, discard = _parse_event(block, decoder, sentinel)
            if event is not None:
                yield event

        if position > 0:
            buffer = buffer[position:]
            position = 0

    event, discard = _parse_event(buffer, decoder, sentinel)
    if event is not None:
        yield event


def _parse_event(
    raw: bytearray, decoder: Callable[[str], T], sentinel: Optional[str] = None
) -> Tuple[Optional[T], bool]:
    block = raw.decode()
    lines = re.split(r"\r?\n|\r", block)
    publish = False
    event = ServerEvent()
    data = ""
    for line in lines:
        if not line:
            continue

        delim = line.find(":")
        if delim <= 0:
            continue

        field = line[0:delim]
        value = line[delim + 1 :] if delim < len(line) - 1 else ""
        if len(value) and value[0] == " ":
            value = value[1:]

        if field == "event":
            event.event = value
            publish = True
        elif field == "data":
            data += value + "\n"
            publish = True
        elif field == "id":
            event.id = value
            publish = True
        elif field == "retry":
            event.retry = int(value) if value.isdigit() else None
            publish = True

    if sentinel and data == f"{sentinel}\n":
        return None, True

    if data:
        data = data[:-1]
        event.data = data

        data_is_primitive = (
            data.isnumeric() or data == "true" or data == "false" or data == "null"
        )
        data_is_json = (
            data.startswith("{") or data.startswith("[") or data.startswith('"')
        )

        if data_is_primitive or data_is_json:
            try:
                event.data = json.loads(data)
            except Exception:
                pass

    out = None
    if publish:
        out = decoder(json.dumps(event.__dict__))

    return out, False


def _peek_sequence(position: int, buffer: bytearray, sequence: bytes):
    if len(sequence) > (len(buffer) - position):
        return None

    for i, seq in enumerate(sequence):
        if buffer[position + i] != seq:
            return None

    return sequence
</file>

<file path="src/mistralai/utils/forms.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .serializers import marshal_json

from .metadata import (
    FormMetadata,
    MultipartFormMetadata,
    find_field_metadata,
)
from .values import _is_set, _val_to_string


def _populate_form(
    field_name: str,
    explode: bool,
    obj: Any,
    delimiter: str,
    form: Dict[str, List[str]],
):
    if not _is_set(obj):
        return form

    if isinstance(obj, BaseModel):
        items = []

        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_field_name = obj_field.alias if obj_field.alias is not None else name
            if obj_field_name == "":
                continue

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                form[obj_field_name] = [_val_to_string(val)]
            else:
                items.append(f"{obj_field_name}{delimiter}{_val_to_string(val)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, Dict):
        items = []
        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                form[key] = [_val_to_string(value)]
            else:
                items.append(f"{key}{delimiter}{_val_to_string(value)}")

        if len(items) > 0:
            form[field_name] = [delimiter.join(items)]
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            if explode:
                if not field_name in form:
                    form[field_name] = []
                form[field_name].append(_val_to_string(value))
            else:
                items.append(_val_to_string(value))

        if len(items) > 0:
            form[field_name] = [delimiter.join([str(item) for item in items])]
    else:
        form[field_name] = [_val_to_string(obj)]

    return form


def serialize_multipart_form(
    media_type: str, request: Any
) -> Tuple[str, Dict[str, Any], Dict[str, Any]]:
    form: Dict[str, Any] = {}
    files: Dict[str, Any] = {}

    if not isinstance(request, BaseModel):
        raise TypeError("invalid request body type")

    request_fields: Dict[str, FieldInfo] = request.__class__.model_fields
    request_field_types = get_type_hints(request.__class__)

    for name in request_fields:
        field = request_fields[name]

        val = getattr(request, name)
        if not _is_set(val):
            continue

        field_metadata = find_field_metadata(field, MultipartFormMetadata)
        if not field_metadata:
            continue

        f_name = field.alias if field.alias else name

        if field_metadata.file:
            file_fields: Dict[str, FieldInfo] = val.__class__.model_fields

            file_name = ""
            content = None
            content_type = None

            for file_field_name in file_fields:
                file_field = file_fields[file_field_name]

                file_metadata = find_field_metadata(file_field, MultipartFormMetadata)
                if file_metadata is None:
                    continue

                if file_metadata.content:
                    content = getattr(val, file_field_name, None)
                elif file_field_name == "content_type":
                    content_type = getattr(val, file_field_name, None)
                else:
                    file_name = getattr(val, file_field_name)

            if file_name == "" or content is None:
                raise ValueError("invalid multipart/form-data file")

            if content_type is not None:
                files[f_name] = (file_name, content, content_type)
            else:
                files[f_name] = (file_name, content)
        elif field_metadata.json:
            files[f_name] = (
                None,
                marshal_json(val, request_field_types[name]),
                "application/json",
            )
        else:
            if isinstance(val, List):
                values = []

                for value in val:
                    if not _is_set(value):
                        continue
                    values.append(_val_to_string(value))

                form[f_name + "[]"] = values
            else:
                form[f_name] = _val_to_string(val)
    return media_type, form, files


def serialize_form_data(data: Any) -> Dict[str, Any]:
    form: Dict[str, List[str]] = {}

    if isinstance(data, BaseModel):
        data_fields: Dict[str, FieldInfo] = data.__class__.model_fields
        data_field_types = get_type_hints(data.__class__)
        for name in data_fields:
            field = data_fields[name]

            val = getattr(data, name)
            if not _is_set(val):
                continue

            metadata = find_field_metadata(field, FormMetadata)
            if metadata is None:
                continue

            f_name = field.alias if field.alias is not None else name

            if metadata.json:
                form[f_name] = [marshal_json(val, data_field_types[name])]
            else:
                if metadata.style == "form":
                    _populate_form(
                        f_name,
                        metadata.explode,
                        val,
                        ",",
                        form,
                    )
                else:
                    raise ValueError(f"Invalid form style for field {name}")
    elif isinstance(data, Dict):
        for key, value in data.items():
            if _is_set(value):
                form[key] = [_val_to_string(value)]
    else:
        raise TypeError(f"Invalid request body type {type(data)} for form data")

    return form
</file>

<file path="src/mistralai/utils/headers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    List,
    Optional,
)
from httpx import Headers
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    HeaderMetadata,
    find_field_metadata,
)

from .values import _is_set, _populate_from_globals, _val_to_string


def get_headers(headers_params: Any, gbls: Optional[Any] = None) -> Dict[str, str]:
    headers: Dict[str, str] = {}

    globals_already_populated = []
    if _is_set(headers_params):
        globals_already_populated = _populate_headers(headers_params, gbls, headers, [])
    if _is_set(gbls):
        _populate_headers(gbls, None, headers, globals_already_populated)

    return headers


def _populate_headers(
    headers_params: Any,
    gbls: Any,
    header_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(headers_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = headers_params.__class__.model_fields
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]
        f_name = field.alias if field.alias is not None else name

        metadata = find_field_metadata(field, HeaderMetadata)
        if metadata is None:
            continue

        value, global_found = _populate_from_globals(
            name, getattr(headers_params, name), HeaderMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)
        value = _serialize_header(metadata.explode, value)

        if value != "":
            header_values[f_name] = value

    return globals_already_populated


def _serialize_header(explode: bool, obj: Any) -> str:
    if not _is_set(obj):
        return ""

    if isinstance(obj, BaseModel):
        items = []
        obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
        for name in obj_fields:
            obj_field = obj_fields[name]
            obj_param_metadata = find_field_metadata(obj_field, HeaderMetadata)

            if not obj_param_metadata:
                continue

            f_name = obj_field.alias if obj_field.alias is not None else name

            val = getattr(obj, name)
            if not _is_set(val):
                continue

            if explode:
                items.append(f"{f_name}={_val_to_string(val)}")
            else:
                items.append(f_name)
                items.append(_val_to_string(val))

        if len(items) > 0:
            return ",".join(items)
    elif isinstance(obj, Dict):
        items = []

        for key, value in obj.items():
            if not _is_set(value):
                continue

            if explode:
                items.append(f"{key}={_val_to_string(value)}")
            else:
                items.append(key)
                items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join([str(item) for item in items])
    elif isinstance(obj, List):
        items = []

        for value in obj:
            if not _is_set(value):
                continue

            items.append(_val_to_string(value))

        if len(items) > 0:
            return ",".join(items)
    elif _is_set(obj):
        return f"{_val_to_string(obj)}"

    return ""


def get_response_headers(headers: Headers) -> Dict[str, List[str]]:
    res: Dict[str, List[str]] = {}
    for k, v in headers.items():
        if not k in res:
            res[k] = []

        res[k].append(v)
    return res
</file>

<file path="src/mistralai/utils/logger.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import httpx
import logging
import os
from typing import Any, Protocol


class Logger(Protocol):
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


class NoOpLogger:
    def debug(self, msg: str, *args: Any, **kwargs: Any) -> None:
        pass


def get_body_content(req: httpx.Request) -> str:
    return "<streaming body>" if not hasattr(req, "_content") else str(req.content)


def get_default_logger() -> Logger:
    if os.getenv("MISTRAL_DEBUG"):
        logging.basicConfig(level=logging.DEBUG)
        return logging.getLogger("mistralai")
    return NoOpLogger()
</file>

<file path="src/mistralai/utils/metadata.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import Optional, Type, TypeVar, Union
from dataclasses import dataclass
from pydantic.fields import FieldInfo


T = TypeVar("T")


@dataclass
class SecurityMetadata:
    option: bool = False
    scheme: bool = False
    scheme_type: Optional[str] = None
    sub_type: Optional[str] = None
    field_name: Optional[str] = None

    def get_field_name(self, default: str) -> str:
        return self.field_name or default


@dataclass
class ParamMetadata:
    serialization: Optional[str] = None
    style: str = "simple"
    explode: bool = False


@dataclass
class PathParamMetadata(ParamMetadata):
    pass


@dataclass
class QueryParamMetadata(ParamMetadata):
    style: str = "form"
    explode: bool = True


@dataclass
class HeaderMetadata(ParamMetadata):
    pass


@dataclass
class RequestMetadata:
    media_type: str = "application/octet-stream"


@dataclass
class MultipartFormMetadata:
    file: bool = False
    content: bool = False
    json: bool = False


@dataclass
class FormMetadata:
    json: bool = False
    style: str = "form"
    explode: bool = True


class FieldMetadata:
    security: Optional[SecurityMetadata] = None
    path: Optional[PathParamMetadata] = None
    query: Optional[QueryParamMetadata] = None
    header: Optional[HeaderMetadata] = None
    request: Optional[RequestMetadata] = None
    form: Optional[FormMetadata] = None
    multipart: Optional[MultipartFormMetadata] = None

    def __init__(
        self,
        security: Optional[SecurityMetadata] = None,
        path: Optional[Union[PathParamMetadata, bool]] = None,
        query: Optional[Union[QueryParamMetadata, bool]] = None,
        header: Optional[Union[HeaderMetadata, bool]] = None,
        request: Optional[Union[RequestMetadata, bool]] = None,
        form: Optional[Union[FormMetadata, bool]] = None,
        multipart: Optional[Union[MultipartFormMetadata, bool]] = None,
    ):
        self.security = security
        self.path = PathParamMetadata() if isinstance(path, bool) else path
        self.query = QueryParamMetadata() if isinstance(query, bool) else query
        self.header = HeaderMetadata() if isinstance(header, bool) else header
        self.request = RequestMetadata() if isinstance(request, bool) else request
        self.form = FormMetadata() if isinstance(form, bool) else form
        self.multipart = (
            MultipartFormMetadata() if isinstance(multipart, bool) else multipart
        )


def find_field_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = find_metadata(field_info, FieldMetadata)
    if not metadata:
        return None

    fields = metadata.__dict__

    for field in fields:
        if isinstance(fields[field], metadata_type):
            return fields[field]

    return None


def find_metadata(field_info: FieldInfo, metadata_type: Type[T]) -> Optional[T]:
    metadata = field_info.metadata
    if not metadata:
        return None

    for md in metadata:
        if isinstance(md, metadata_type):
            return md

    return None
</file>

<file path="src/mistralai/utils/queryparams.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
)

from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    QueryParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)
from .forms import _populate_form


def get_query_params(
    query_params: Any,
    gbls: Optional[Any] = None,
) -> Dict[str, List[str]]:
    params: Dict[str, List[str]] = {}

    globals_already_populated = _populate_query_params(query_params, gbls, params, [])
    if _is_set(gbls):
        _populate_query_params(gbls, None, params, globals_already_populated)

    return params


def _populate_query_params(
    query_params: Any,
    gbls: Any,
    query_param_values: Dict[str, List[str]],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(query_params, BaseModel):
        return globals_already_populated

    param_fields: Dict[str, FieldInfo] = query_params.__class__.model_fields
    param_field_types = get_type_hints(query_params.__class__)
    for name in param_fields:
        if name in skip_fields:
            continue

        field = param_fields[name]

        metadata = find_field_metadata(field, QueryParamMetadata)
        if not metadata:
            continue

        value = getattr(query_params, name) if _is_set(query_params) else None

        value, global_found = _populate_from_globals(
            name, value, QueryParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        f_name = field.alias if field.alias is not None else name
        serialization = metadata.serialization
        if serialization is not None:
            serialized_parms = _get_serialized_params(
                metadata, f_name, value, param_field_types[name]
            )
            for key, value in serialized_parms.items():
                if key in query_param_values:
                    query_param_values[key].extend(value)
                else:
                    query_param_values[key] = [value]
        else:
            style = metadata.style
            if style == "deepObject":
                _populate_deep_object_query_params(f_name, value, query_param_values)
            elif style == "form":
                _populate_delimited_query_params(
                    metadata, f_name, value, ",", query_param_values
                )
            elif style == "pipeDelimited":
                _populate_delimited_query_params(
                    metadata, f_name, value, "|", query_param_values
                )
            else:
                raise NotImplementedError(
                    f"query param style {style} not yet supported"
                )

    return globals_already_populated


def _populate_deep_object_query_params(
    field_name: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj):
        return

    if isinstance(obj, BaseModel):
        _populate_deep_object_query_params_basemodel(field_name, obj, params)
    elif isinstance(obj, Dict):
        _populate_deep_object_query_params_dict(field_name, obj, params)


def _populate_deep_object_query_params_basemodel(
    prior_params_key: str,
    obj: Any,
    params: Dict[str, List[str]],
):
    if not _is_set(obj) or not isinstance(obj, BaseModel):
        return

    obj_fields: Dict[str, FieldInfo] = obj.__class__.model_fields
    for name in obj_fields:
        obj_field = obj_fields[name]

        f_name = obj_field.alias if obj_field.alias is not None else name

        params_key = f"{prior_params_key}[{f_name}]"

        obj_param_metadata = find_field_metadata(obj_field, QueryParamMetadata)
        if not _is_set(obj_param_metadata):
            continue

        obj_val = getattr(obj, name)
        if not _is_set(obj_val):
            continue

        if isinstance(obj_val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, obj_val, params)
        elif isinstance(obj_val, Dict):
            _populate_deep_object_query_params_dict(params_key, obj_val, params)
        elif isinstance(obj_val, List):
            _populate_deep_object_query_params_list(params_key, obj_val, params)
        else:
            params[params_key] = [_val_to_string(obj_val)]


def _populate_deep_object_query_params_dict(
    prior_params_key: str,
    value: Dict,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for key, val in value.items():
        if not _is_set(val):
            continue

        params_key = f"{prior_params_key}[{key}]"

        if isinstance(val, BaseModel):
            _populate_deep_object_query_params_basemodel(params_key, val, params)
        elif isinstance(val, Dict):
            _populate_deep_object_query_params_dict(params_key, val, params)
        elif isinstance(val, List):
            _populate_deep_object_query_params_list(params_key, val, params)
        else:
            params[params_key] = [_val_to_string(val)]


def _populate_deep_object_query_params_list(
    params_key: str,
    value: List,
    params: Dict[str, List[str]],
):
    if not _is_set(value):
        return

    for val in value:
        if not _is_set(val):
            continue

        if params.get(params_key) is None:
            params[params_key] = []

        params[params_key].append(_val_to_string(val))


def _populate_delimited_query_params(
    metadata: QueryParamMetadata,
    field_name: str,
    obj: Any,
    delimiter: str,
    query_param_values: Dict[str, List[str]],
):
    _populate_form(
        field_name,
        metadata.explode,
        obj,
        delimiter,
        query_param_values,
    )
</file>

<file path="src/mistralai/utils/requestbodies.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import io
from dataclasses import dataclass
import re
from typing import (
    Any,
    Optional,
)

from .forms import serialize_form_data, serialize_multipart_form

from .serializers import marshal_json

SERIALIZATION_METHOD_TO_CONTENT_TYPE = {
    "json": "application/json",
    "form": "application/x-www-form-urlencoded",
    "multipart": "multipart/form-data",
    "raw": "application/octet-stream",
    "string": "text/plain",
}


@dataclass
class SerializedRequestBody:
    media_type: Optional[str] = None
    content: Optional[Any] = None
    data: Optional[Any] = None
    files: Optional[Any] = None


def serialize_request_body(
    request_body: Any,
    nullable: bool,
    optional: bool,
    serialization_method: str,
    request_body_type,
) -> Optional[SerializedRequestBody]:
    if request_body is None:
        if not nullable and optional:
            return None

    media_type = SERIALIZATION_METHOD_TO_CONTENT_TYPE[serialization_method]

    serialized_request_body = SerializedRequestBody(media_type)

    if re.match(r"(application|text)\/.*?\+*json.*", media_type) is not None:
        serialized_request_body.content = marshal_json(request_body, request_body_type)
    elif re.match(r"multipart\/.*", media_type) is not None:
        (
            serialized_request_body.media_type,
            serialized_request_body.data,
            serialized_request_body.files,
        ) = serialize_multipart_form(media_type, request_body)
    elif re.match(r"application\/x-www-form-urlencoded.*", media_type) is not None:
        serialized_request_body.data = serialize_form_data(request_body)
    elif isinstance(request_body, (bytes, bytearray, io.BytesIO, io.BufferedReader)):
        serialized_request_body.content = request_body
    elif isinstance(request_body, str):
        serialized_request_body.content = request_body
    else:
        raise TypeError(
            f"invalid request body type {type(request_body)} for mediaType {media_type}"
        )

    return serialized_request_body
</file>

<file path="src/mistralai/utils/retries.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import asyncio
import random
import time
from typing import List

import httpx


class BackoffStrategy:
    initial_interval: int
    max_interval: int
    exponent: float
    max_elapsed_time: int

    def __init__(
        self,
        initial_interval: int,
        max_interval: int,
        exponent: float,
        max_elapsed_time: int,
    ):
        self.initial_interval = initial_interval
        self.max_interval = max_interval
        self.exponent = exponent
        self.max_elapsed_time = max_elapsed_time


class RetryConfig:
    strategy: str
    backoff: BackoffStrategy
    retry_connection_errors: bool

    def __init__(
        self, strategy: str, backoff: BackoffStrategy, retry_connection_errors: bool
    ):
        self.strategy = strategy
        self.backoff = backoff
        self.retry_connection_errors = retry_connection_errors


class Retries:
    config: RetryConfig
    status_codes: List[str]

    def __init__(self, config: RetryConfig, status_codes: List[str]):
        self.config = config
        self.status_codes = status_codes


class TemporaryError(Exception):
    response: httpx.Response

    def __init__(self, response: httpx.Response):
        self.response = response


class PermanentError(Exception):
    inner: Exception

    def __init__(self, inner: Exception):
        self.inner = inner


def retry(func, retries: Retries):
    if retries.config.strategy == "backoff":

        def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return retry_with_backoff(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return func()


async def retry_async(func, retries: Retries):
    if retries.config.strategy == "backoff":

        async def do_request() -> httpx.Response:
            res: httpx.Response
            try:
                res = await func()

                for code in retries.status_codes:
                    if "X" in code.upper():
                        code_range = int(code[0])

                        status_major = res.status_code / 100

                        if code_range <= status_major < code_range + 1:
                            raise TemporaryError(res)
                    else:
                        parsed_code = int(code)

                        if res.status_code == parsed_code:
                            raise TemporaryError(res)
            except httpx.ConnectError as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except httpx.TimeoutException as exception:
                if retries.config.retry_connection_errors:
                    raise

                raise PermanentError(exception) from exception
            except TemporaryError:
                raise
            except Exception as exception:
                raise PermanentError(exception) from exception

            return res

        return await retry_with_backoff_async(
            do_request,
            retries.config.backoff.initial_interval,
            retries.config.backoff.max_interval,
            retries.config.backoff.exponent,
            retries.config.backoff.max_elapsed_time,
        )

    return await func()


def retry_with_backoff(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            time.sleep(sleep)
            retries += 1


async def retry_with_backoff_async(
    func,
    initial_interval=500,
    max_interval=60000,
    exponent=1.5,
    max_elapsed_time=3600000,
):
    start = round(time.time() * 1000)
    retries = 0

    while True:
        try:
            return await func()
        except PermanentError as exception:
            raise exception.inner
        except Exception as exception:  # pylint: disable=broad-exception-caught
            now = round(time.time() * 1000)
            if now - start > max_elapsed_time:
                if isinstance(exception, TemporaryError):
                    return exception.response

                raise
            sleep = (initial_interval / 1000) * exponent**retries + random.uniform(0, 1)
            sleep = min(sleep, max_interval / 1000)
            await asyncio.sleep(sleep)
            retries += 1
</file>

<file path="src/mistralai/utils/security.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import base64

from typing import (
    Any,
    Dict,
    List,
    Optional,
    Tuple,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    SecurityMetadata,
    find_field_metadata,
)
import os


def get_security(security: Any) -> Tuple[Dict[str, str], Dict[str, List[str]]]:
    headers: Dict[str, str] = {}
    query_params: Dict[str, List[str]] = {}

    if security is None:
        return headers, query_params

    if not isinstance(security, BaseModel):
        raise TypeError("security must be a pydantic model")

    sec_fields: Dict[str, FieldInfo] = security.__class__.model_fields
    for name in sec_fields:
        sec_field = sec_fields[name]

        value = getattr(security, name)
        if value is None:
            continue

        metadata = find_field_metadata(sec_field, SecurityMetadata)
        if metadata is None:
            continue
        if metadata.option:
            _parse_security_option(headers, query_params, value)
            return headers, query_params
        if metadata.scheme:
            # Special case for basic auth or custom auth which could be a flattened model
            if metadata.sub_type in ["basic", "custom"] and not isinstance(
                value, BaseModel
            ):
                _parse_security_scheme(headers, query_params, metadata, name, security)
            else:
                _parse_security_scheme(headers, query_params, metadata, name, value)

    return headers, query_params


def get_security_from_env(security: Any, security_class: Any) -> Optional[BaseModel]:
    if security is not None:
        return security

    if not issubclass(security_class, BaseModel):
        raise TypeError("security_class must be a pydantic model class")

    security_dict: Any = {}

    if os.getenv("MISTRAL_API_KEY"):
        security_dict["api_key"] = os.getenv("MISTRAL_API_KEY")

    return security_class(**security_dict) if security_dict else None


def _parse_security_option(
    headers: Dict[str, str], query_params: Dict[str, List[str]], option: Any
):
    if not isinstance(option, BaseModel):
        raise TypeError("security option must be a pydantic model")

    opt_fields: Dict[str, FieldInfo] = option.__class__.model_fields
    for name in opt_fields:
        opt_field = opt_fields[name]

        metadata = find_field_metadata(opt_field, SecurityMetadata)
        if metadata is None or not metadata.scheme:
            continue
        _parse_security_scheme(
            headers, query_params, metadata, name, getattr(option, name)
        )


def _parse_security_scheme(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    field_name: str,
    scheme: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    if isinstance(scheme, BaseModel):
        if scheme_type == "http":
            if sub_type == "basic":
                _parse_basic_auth_scheme(headers, scheme)
                return
            if sub_type == "custom":
                return

        scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
        for name in scheme_fields:
            scheme_field = scheme_fields[name]

            metadata = find_field_metadata(scheme_field, SecurityMetadata)
            if metadata is None or metadata.field_name is None:
                continue

            value = getattr(scheme, name)

            _parse_security_scheme_value(
                headers, query_params, scheme_metadata, metadata, name, value
            )
    else:
        _parse_security_scheme_value(
            headers, query_params, scheme_metadata, scheme_metadata, field_name, scheme
        )


def _parse_security_scheme_value(
    headers: Dict[str, str],
    query_params: Dict[str, List[str]],
    scheme_metadata: SecurityMetadata,
    security_metadata: SecurityMetadata,
    field_name: str,
    value: Any,
):
    scheme_type = scheme_metadata.scheme_type
    sub_type = scheme_metadata.sub_type

    header_name = security_metadata.get_field_name(field_name)

    if scheme_type == "apiKey":
        if sub_type == "header":
            headers[header_name] = value
        elif sub_type == "query":
            query_params[header_name] = [value]
        else:
            raise ValueError("sub type {sub_type} not supported")
    elif scheme_type == "openIdConnect":
        headers[header_name] = _apply_bearer(value)
    elif scheme_type == "oauth2":
        if sub_type != "client_credentials":
            headers[header_name] = _apply_bearer(value)
    elif scheme_type == "http":
        if sub_type == "bearer":
            headers[header_name] = _apply_bearer(value)
        elif sub_type == "custom":
            return
        else:
            raise ValueError("sub type {sub_type} not supported")
    else:
        raise ValueError("scheme type {scheme_type} not supported")


def _apply_bearer(token: str) -> str:
    return token.lower().startswith("bearer ") and token or f"Bearer {token}"


def _parse_basic_auth_scheme(headers: Dict[str, str], scheme: Any):
    username = ""
    password = ""

    if not isinstance(scheme, BaseModel):
        raise TypeError("basic auth scheme must be a pydantic model")

    scheme_fields: Dict[str, FieldInfo] = scheme.__class__.model_fields
    for name in scheme_fields:
        scheme_field = scheme_fields[name]

        metadata = find_field_metadata(scheme_field, SecurityMetadata)
        if metadata is None or metadata.field_name is None:
            continue

        field_name = metadata.field_name
        value = getattr(scheme, name)

        if field_name == "username":
            username = value
        if field_name == "password":
            password = value

    data = f"{username}:{password}".encode()
    headers["Authorization"] = f"Basic {base64.b64encode(data).decode()}"
</file>

<file path="src/mistralai/utils/serializers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
import json
from typing import Any, Dict, List, Union, get_args
import httpx
from typing_extensions import get_origin
from pydantic import ConfigDict, create_model
from pydantic_core import from_json
from typing_inspection.typing_objects import is_union

from ..types.basemodel import BaseModel, Nullable, OptionalNullable, Unset


def serialize_decimal(as_str: bool):
    def serialize(d):
        # Optional[T] is a Union[T, None]
        if is_union(type(d)) and type(None) in get_args(type(d)) and d is None:
            return None
        if isinstance(d, Unset):
            return d

        if not isinstance(d, Decimal):
            raise ValueError("Expected Decimal object")

        return str(d) if as_str else float(d)

    return serialize


def validate_decimal(d):
    if d is None:
        return None

    if isinstance(d, (Decimal, Unset)):
        return d

    if not isinstance(d, (str, int, float)):
        raise ValueError("Expected string, int or float")

    return Decimal(str(d))


def serialize_float(as_str: bool):
    def serialize(f):
        # Optional[T] is a Union[T, None]
        if is_union(type(f)) and type(None) in get_args(type(f)) and f is None:
            return None
        if isinstance(f, Unset):
            return f

        if not isinstance(f, float):
            raise ValueError("Expected float")

        return str(f) if as_str else f

    return serialize


def validate_float(f):
    if f is None:
        return None

    if isinstance(f, (float, Unset)):
        return f

    if not isinstance(f, str):
        raise ValueError("Expected string")

    return float(f)


def serialize_int(as_str: bool):
    def serialize(i):
        # Optional[T] is a Union[T, None]
        if is_union(type(i)) and type(None) in get_args(type(i)) and i is None:
            return None
        if isinstance(i, Unset):
            return i

        if not isinstance(i, int):
            raise ValueError("Expected int")

        return str(i) if as_str else i

    return serialize


def validate_int(b):
    if b is None:
        return None

    if isinstance(b, (int, Unset)):
        return b

    if not isinstance(b, str):
        raise ValueError("Expected string")

    return int(b)


def validate_open_enum(is_int: bool):
    def validate(e):
        if e is None:
            return None

        if isinstance(e, Unset):
            return e

        if is_int:
            if not isinstance(e, int):
                raise ValueError("Expected int")
        else:
            if not isinstance(e, str):
                raise ValueError("Expected string")

        return e

    return validate


def validate_const(v):
    def validate(c):
        # Optional[T] is a Union[T, None]
        if is_union(type(c)) and type(None) in get_args(type(c)) and c is None:
            return None

        if v != c:
            raise ValueError(f"Expected {v}")

        return c

    return validate


def unmarshal_json(raw, typ: Any) -> Any:
    return unmarshal(from_json(raw), typ)


def unmarshal(val, typ: Any) -> Any:
    unmarshaller = create_model(
        "Unmarshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = unmarshaller(body=val)

    # pyright: ignore[reportAttributeAccessIssue]
    return m.body  # type: ignore


def marshal_json(val, typ):
    if is_nullable(typ) and val is None:
        return "null"

    marshaller = create_model(
        "Marshaller",
        body=(typ, ...),
        __config__=ConfigDict(populate_by_name=True, arbitrary_types_allowed=True),
    )

    m = marshaller(body=val)

    d = m.model_dump(by_alias=True, mode="json", exclude_none=True)

    if len(d) == 0:
        return ""

    return json.dumps(d[next(iter(d))], separators=(",", ":"))


def is_nullable(field):
    origin = get_origin(field)
    if origin is Nullable or origin is OptionalNullable:
        return True

    if not origin is Union or type(None) not in get_args(field):
        return False

    for arg in get_args(field):
        if get_origin(arg) is Nullable or get_origin(arg) is OptionalNullable:
            return True

    return False


def stream_to_text(stream: httpx.Response) -> str:
    return "".join(stream.iter_text())


async def stream_to_text_async(stream: httpx.Response) -> str:
    return "".join([chunk async for chunk in stream.aiter_text()])


def stream_to_bytes(stream: httpx.Response) -> bytes:
    return stream.content


async def stream_to_bytes_async(stream: httpx.Response) -> bytes:
    return await stream.aread()


def get_pydantic_model(data: Any, typ: Any) -> Any:
    if not _contains_pydantic_model(data):
        return unmarshal(data, typ)

    return data


def _contains_pydantic_model(data: Any) -> bool:
    if isinstance(data, BaseModel):
        return True
    if isinstance(data, List):
        return any(_contains_pydantic_model(item) for item in data)
    if isinstance(data, Dict):
        return any(_contains_pydantic_model(value) for value in data.values())

    return False
</file>

<file path="src/mistralai/utils/url.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from decimal import Decimal
from typing import (
    Any,
    Dict,
    get_type_hints,
    List,
    Optional,
    Union,
    get_args,
    get_origin,
)
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from .metadata import (
    PathParamMetadata,
    find_field_metadata,
)
from .values import (
    _get_serialized_params,
    _is_set,
    _populate_from_globals,
    _val_to_string,
)


def generate_url(
    server_url: str,
    path: str,
    path_params: Any,
    gbls: Optional[Any] = None,
) -> str:
    path_param_values: Dict[str, str] = {}

    globals_already_populated = _populate_path_params(
        path_params, gbls, path_param_values, []
    )
    if _is_set(gbls):
        _populate_path_params(gbls, None, path_param_values, globals_already_populated)

    for key, value in path_param_values.items():
        path = path.replace("{" + key + "}", value, 1)

    return remove_suffix(server_url, "/") + path


def _populate_path_params(
    path_params: Any,
    gbls: Any,
    path_param_values: Dict[str, str],
    skip_fields: List[str],
) -> List[str]:
    globals_already_populated: List[str] = []

    if not isinstance(path_params, BaseModel):
        return globals_already_populated

    path_param_fields: Dict[str, FieldInfo] = path_params.__class__.model_fields
    path_param_field_types = get_type_hints(path_params.__class__)
    for name in path_param_fields:
        if name in skip_fields:
            continue

        field = path_param_fields[name]

        param_metadata = find_field_metadata(field, PathParamMetadata)
        if param_metadata is None:
            continue

        param = getattr(path_params, name) if _is_set(path_params) else None
        param, global_found = _populate_from_globals(
            name, param, PathParamMetadata, gbls
        )
        if global_found:
            globals_already_populated.append(name)

        if not _is_set(param):
            continue

        f_name = field.alias if field.alias is not None else name
        serialization = param_metadata.serialization
        if serialization is not None:
            serialized_params = _get_serialized_params(
                param_metadata, f_name, param, path_param_field_types[name]
            )
            for key, value in serialized_params.items():
                path_param_values[key] = value
        else:
            pp_vals: List[str] = []
            if param_metadata.style == "simple":
                if isinstance(param, List):
                    for pp_val in param:
                        if not _is_set(pp_val):
                            continue
                        pp_vals.append(_val_to_string(pp_val))
                    path_param_values[f_name] = ",".join(pp_vals)
                elif isinstance(param, Dict):
                    for pp_key in param:
                        if not _is_set(param[pp_key]):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(f"{pp_key}={_val_to_string(param[pp_key])}")
                        else:
                            pp_vals.append(f"{pp_key},{_val_to_string(param[pp_key])}")
                    path_param_values[f_name] = ",".join(pp_vals)
                elif not isinstance(param, (str, int, float, complex, bool, Decimal)):
                    param_fields: Dict[str, FieldInfo] = param.__class__.model_fields
                    for name in param_fields:
                        param_field = param_fields[name]

                        param_value_metadata = find_field_metadata(
                            param_field, PathParamMetadata
                        )
                        if param_value_metadata is None:
                            continue

                        param_name = (
                            param_field.alias if param_field.alias is not None else name
                        )

                        param_field_val = getattr(param, name)
                        if not _is_set(param_field_val):
                            continue
                        if param_metadata.explode:
                            pp_vals.append(
                                f"{param_name}={_val_to_string(param_field_val)}"
                            )
                        else:
                            pp_vals.append(
                                f"{param_name},{_val_to_string(param_field_val)}"
                            )
                    path_param_values[f_name] = ",".join(pp_vals)
                elif _is_set(param):
                    path_param_values[f_name] = _val_to_string(param)

    return globals_already_populated


def is_optional(field):
    return get_origin(field) is Union and type(None) in get_args(field)


def template_url(url_with_params: str, params: Dict[str, str]) -> str:
    for key, value in params.items():
        url_with_params = url_with_params.replace("{" + key + "}", value)

    return url_with_params


def remove_suffix(input_string, suffix):
    if suffix and input_string.endswith(suffix):
        return input_string[: -len(suffix)]
    return input_string
</file>

<file path="src/mistralai/utils/values.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from datetime import datetime
from enum import Enum
from email.message import Message
from functools import partial
import os
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast

from httpx import Response
from pydantic import BaseModel
from pydantic.fields import FieldInfo

from ..types.basemodel import Unset

from .serializers import marshal_json

from .metadata import ParamMetadata, find_field_metadata


def match_content_type(content_type: str, pattern: str) -> bool:
    if pattern in (content_type, "*", "*/*"):
        return True

    msg = Message()
    msg["content-type"] = content_type
    media_type = msg.get_content_type()

    if media_type == pattern:
        return True

    parts = media_type.split("/")
    if len(parts) == 2:
        if pattern in (f"{parts[0]}/*", f"*/{parts[1]}"):
            return True

    return False


def match_status_codes(status_codes: List[str], status_code: int) -> bool:
    if "default" in status_codes:
        return True

    for code in status_codes:
        if code == str(status_code):
            return True

        if code.endswith("XX") and code.startswith(str(status_code)[:1]):
            return True
    return False


T = TypeVar("T")

def cast_partial(typ):
    return partial(cast, typ)

def get_global_from_env(
    value: Optional[T], env_key: str, type_cast: Callable[[str], T]
) -> Optional[T]:
    if value is not None:
        return value
    env_value = os.getenv(env_key)
    if env_value is not None:
        try:
            return type_cast(env_value)
        except ValueError:
            pass
    return None


def match_response(
    response: Response, code: Union[str, List[str]], content_type: str
) -> bool:
    codes = code if isinstance(code, list) else [code]
    return match_status_codes(codes, response.status_code) and match_content_type(
        response.headers.get("content-type", "application/octet-stream"), content_type
    )


def _populate_from_globals(
    param_name: str, value: Any, param_metadata_type: type, gbls: Any
) -> Tuple[Any, bool]:
    if gbls is None:
        return value, False

    if not isinstance(gbls, BaseModel):
        raise TypeError("globals must be a pydantic model")

    global_fields: Dict[str, FieldInfo] = gbls.__class__.model_fields
    found = False
    for name in global_fields:
        field = global_fields[name]
        if name is not param_name:
            continue

        found = True

        if value is not None:
            return value, True

        global_value = getattr(gbls, name)

        param_metadata = find_field_metadata(field, param_metadata_type)
        if param_metadata is None:
            return value, True

        return global_value, True

    return value, found


def _val_to_string(val) -> str:
    if isinstance(val, bool):
        return str(val).lower()
    if isinstance(val, datetime):
        return str(val.isoformat().replace("+00:00", "Z"))
    if isinstance(val, Enum):
        return str(val.value)

    return str(val)


def _get_serialized_params(
    metadata: ParamMetadata, field_name: str, obj: Any, typ: type
) -> Dict[str, str]:
    params: Dict[str, str] = {}

    serialization = metadata.serialization
    if serialization == "json":
        params[field_name] = marshal_json(obj, typ)

    return params


def _is_set(value: Any) -> bool:
    return value is not None and not isinstance(value, Unset)
</file>

<file path="src/mistralai/__init__.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._version import (
    __title__,
    __version__,
    __openapi_doc_version__,
    __gen_version__,
    __user_agent__,
)
from .sdk import *
from .sdkconfiguration import *
from .models import *


VERSION: str = __version__
OPENAPI_DOC_VERSION = __openapi_doc_version__
SPEAKEASY_GENERATOR_VERSION = __gen_version__
USER_AGENT = __user_agent__
</file>

<file path="src/mistralai/_version.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

import importlib.metadata

__title__: str = "mistralai"
__version__: str = "1.8.2"
__openapi_doc_version__: str = "1.0.0"
__gen_version__: str = "2.548.6"
__user_agent__: str = "speakeasy-sdk/python 1.8.2 2.548.6 1.0.0 mistralai"

try:
    if __package__ is not None:
        __version__ = importlib.metadata.version(__package__)
except importlib.metadata.PackageNotFoundError:
    pass
</file>

<file path="src/mistralai/agents.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import eventstreaming, get_security_from_env
from typing import Any, List, Mapping, Optional, Union


class Agents(BaseSDK):
    r"""Agents API."""

    def complete(
        self,
        *,
        messages: Union[
            List[models.AgentsCompletionRequestMessages],
            List[models.AgentsCompletionRequestMessagesTypedDict],
        ],
        agent_id: str,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.AgentsCompletionRequestStop,
                models.AgentsCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.AgentsCompletionRequestToolChoice,
                models.AgentsCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ChatCompletionResponse:
        r"""Agents Completion

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param agent_id: The ID of the agent to use for this completion.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsCompletionRequest(
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.AgentsCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.AgentsCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            agent_id=agent_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/agents/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentsCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_completion_v1_agents_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ChatCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        messages: Union[
            List[models.AgentsCompletionRequestMessages],
            List[models.AgentsCompletionRequestMessagesTypedDict],
        ],
        agent_id: str,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.AgentsCompletionRequestStop,
                models.AgentsCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.AgentsCompletionRequestToolChoice,
                models.AgentsCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ChatCompletionResponse:
        r"""Agents Completion

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param agent_id: The ID of the agent to use for this completion.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsCompletionRequest(
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.AgentsCompletionRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.AgentsCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            agent_id=agent_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/agents/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentsCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_completion_v1_agents_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ChatCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def stream(
        self,
        *,
        messages: Union[
            List[models.AgentsCompletionStreamRequestMessages],
            List[models.AgentsCompletionStreamRequestMessagesTypedDict],
        ],
        agent_id: str,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.AgentsCompletionStreamRequestStop,
                models.AgentsCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.AgentsCompletionStreamRequestToolChoice,
                models.AgentsCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.CompletionEvent]:
        r"""Stream Agents completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param agent_id: The ID of the agent to use for this completion.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsCompletionStreamRequest(
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.AgentsCompletionStreamRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.AgentsCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            agent_id=agent_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/agents/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentsCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_agents",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        messages: Union[
            List[models.AgentsCompletionStreamRequestMessages],
            List[models.AgentsCompletionStreamRequestMessagesTypedDict],
        ],
        agent_id: str,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.AgentsCompletionStreamRequestStop,
                models.AgentsCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.AgentsCompletionStreamRequestToolChoice,
                models.AgentsCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.CompletionEvent]:
        r"""Stream Agents completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param agent_id: The ID of the agent to use for this completion.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsCompletionStreamRequest(
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.AgentsCompletionStreamRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.AgentsCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            agent_id=agent_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/agents/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentsCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_agents",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/async_client.py">
from typing import Optional

from .client import MIGRATION_MESSAGE


class MistralAsyncClient:
    def __init__(
        self,
        api_key: Optional[str] = None,
        endpoint: str = "",
        max_retries: int = 5,
        timeout: int = 120,
        max_concurrent_requests: int = 64,
    ):
        raise NotImplementedError(MIGRATION_MESSAGE)
</file>

<file path="src/mistralai/basesdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .sdkconfiguration import SDKConfiguration
import httpx
from mistralai import models, utils
from mistralai._hooks import (
    AfterErrorContext,
    AfterSuccessContext,
    BeforeRequestContext,
)
from mistralai.utils import RetryConfig, SerializedRequestBody, get_body_content
from typing import Callable, List, Mapping, Optional, Tuple
from urllib.parse import parse_qs, urlparse


class BaseSDK:
    sdk_configuration: SDKConfiguration

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        self.sdk_configuration = sdk_config

    def _get_url(self, base_url, url_variables):
        sdk_url, sdk_variables = self.sdk_configuration.get_server_details()

        if base_url is None:
            base_url = sdk_url

        if url_variables is None:
            url_variables = sdk_variables

        return utils.template_url(base_url, url_variables)

    def _build_request_async(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.async_client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request(
        self,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        client = self.sdk_configuration.client
        return self._build_request_with_client(
            client,
            method,
            path,
            base_url,
            url_variables,
            request,
            request_body_required,
            request_has_path_params,
            request_has_query_params,
            user_agent_header,
            accept_header_value,
            _globals,
            security,
            timeout_ms,
            get_serialized_body,
            url_override,
            http_headers,
        )

    def _build_request_with_client(
        self,
        client,
        method,
        path,
        base_url,
        url_variables,
        request,
        request_body_required,
        request_has_path_params,
        request_has_query_params,
        user_agent_header,
        accept_header_value,
        _globals=None,
        security=None,
        timeout_ms: Optional[int] = None,
        get_serialized_body: Optional[
            Callable[[], Optional[SerializedRequestBody]]
        ] = None,
        url_override: Optional[str] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Request:
        query_params = {}

        url = url_override
        if url is None:
            url = utils.generate_url(
                self._get_url(base_url, url_variables),
                path,
                request if request_has_path_params else None,
                _globals if request_has_path_params else None,
            )

            query_params = utils.get_query_params(
                request if request_has_query_params else None,
                _globals if request_has_query_params else None,
            )
        else:
            # Pick up the query parameter from the override so they can be
            # preserved when building the request later on (necessary as of
            # httpx 0.28).
            parsed_override = urlparse(str(url_override))
            query_params = parse_qs(parsed_override.query, keep_blank_values=True)

        headers = utils.get_headers(request, _globals)
        headers["Accept"] = accept_header_value
        headers[user_agent_header] = self.sdk_configuration.user_agent

        if security is not None:
            if callable(security):
                security = security()
        security = utils.get_security_from_env(security, models.Security)
        if security is not None:
            security_headers, security_query_params = utils.get_security(security)
            headers = {**headers, **security_headers}
            query_params = {**query_params, **security_query_params}

        serialized_request_body = SerializedRequestBody()
        if get_serialized_body is not None:
            rb = get_serialized_body()
            if request_body_required and rb is None:
                raise ValueError("request body is required")

            if rb is not None:
                serialized_request_body = rb

        if (
            serialized_request_body.media_type is not None
            and serialized_request_body.media_type
            not in (
                "multipart/form-data",
                "multipart/mixed",
            )
        ):
            headers["content-type"] = serialized_request_body.media_type

        if http_headers is not None:
            for header, value in http_headers.items():
                headers[header] = value

        timeout = timeout_ms / 1000 if timeout_ms is not None else None

        return client.build_request(
            method,
            url,
            params=query_params,
            content=serialized_request_body.content,
            data=serialized_request_body.data,
            files=serialized_request_body.files,
            headers=headers,
            timeout=timeout,
        )

    def do_request(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.client
        logger = self.sdk_configuration.debug_logger

        def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = utils.retry(do, utils.Retries(retry_config[0], retry_config[1]))
        else:
            http_res = do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res

    async def do_request_async(
        self,
        hook_ctx,
        request,
        error_status_codes,
        stream=False,
        retry_config: Optional[Tuple[RetryConfig, List[str]]] = None,
    ) -> httpx.Response:
        client = self.sdk_configuration.async_client
        logger = self.sdk_configuration.debug_logger

        async def do():
            http_res = None
            try:
                req = self.sdk_configuration.get_hooks().before_request(
                    BeforeRequestContext(hook_ctx), request
                )
                logger.debug(
                    "Request:\nMethod: %s\nURL: %s\nHeaders: %s\nBody: %s",
                    req.method,
                    req.url,
                    req.headers,
                    get_body_content(req),
                )

                if client is None:
                    raise ValueError("client is required")

                http_res = await client.send(req, stream=stream)
            except Exception as e:
                _, e = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), None, e
                )
                if e is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise e

            if http_res is None:
                logger.debug("Raising no response SDK error")
                raise models.SDKError("No response received")

            logger.debug(
                "Response:\nStatus Code: %s\nURL: %s\nHeaders: %s\nBody: %s",
                http_res.status_code,
                http_res.url,
                http_res.headers,
                "<streaming response>" if stream else http_res.text,
            )

            if utils.match_status_codes(error_status_codes, http_res.status_code):
                result, err = self.sdk_configuration.get_hooks().after_error(
                    AfterErrorContext(hook_ctx), http_res, None
                )
                if err is not None:
                    logger.debug("Request Exception", exc_info=True)
                    raise err
                if result is not None:
                    http_res = result
                else:
                    logger.debug("Raising unexpected SDK error")
                    raise models.SDKError("Unexpected error occurred")

            return http_res

        if retry_config is not None:
            http_res = await utils.retry_async(
                do, utils.Retries(retry_config[0], retry_config[1])
            )
        else:
            http_res = await do()

        if not utils.match_status_codes(error_status_codes, http_res.status_code):
            http_res = self.sdk_configuration.get_hooks().after_success(
                AfterSuccessContext(hook_ctx), http_res
            )

        return http_res
</file>

<file path="src/mistralai/batch.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from .sdkconfiguration import SDKConfiguration
from mistralai.mistral_jobs import MistralJobs


class Batch(BaseSDK):
    jobs: MistralJobs

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        BaseSDK.__init__(self, sdk_config)
        self.sdk_configuration = sdk_config
        self._init_sdks()

    def _init_sdks(self):
        self.jobs = MistralJobs(self.sdk_configuration)
</file>

<file path="src/mistralai/beta.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from .sdkconfiguration import SDKConfiguration
from mistralai.conversations import Conversations
from mistralai.mistral_agents import MistralAgents


class Beta(BaseSDK):
    conversations: Conversations
    r"""(beta) Conversations API"""
    agents: MistralAgents
    r"""(beta) Agents API"""

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        BaseSDK.__init__(self, sdk_config)
        self.sdk_configuration = sdk_config
        self._init_sdks()

    def _init_sdks(self):
        self.conversations = Conversations(self.sdk_configuration)
        self.agents = MistralAgents(self.sdk_configuration)
</file>

<file path="src/mistralai/chat.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import eventstreaming, get_security_from_env
from typing import Any, List, Mapping, Optional, Union

# region imports
from typing import Type
from mistralai.extra import (
    convert_to_parsed_chat_completion_response,
    response_format_from_pydantic_model,
    CustomPydanticModel,
    ParsedChatCompletionResponse,
)
# endregion imports


class Chat(BaseSDK):
    r"""Chat Completion API."""

    # region sdk-class-body
    # Custom .parse methods for the Structure Outputs Feature.

    def parse(
        self, response_format: Type[CustomPydanticModel], **kwargs: Any
    ) -> ParsedChatCompletionResponse[CustomPydanticModel]:
        """
        Parse the response using the provided response format.
        :param Type[CustomPydanticModel] response_format: The Pydantic model to parse the response into
        :param Any **kwargs Additional keyword arguments to pass to the .complete method
        :return: The parsed response
        """
        # Convert the input Pydantic Model to a strict JSON ready to be passed to chat.complete
        json_response_format = response_format_from_pydantic_model(response_format)
        # Run the inference
        response = self.complete(**kwargs, response_format=json_response_format)
        # Parse response back to the input pydantic model
        parsed_response = convert_to_parsed_chat_completion_response(
            response, response_format
        )
        return parsed_response

    async def parse_async(
        self, response_format: Type[CustomPydanticModel], **kwargs
    ) -> ParsedChatCompletionResponse[CustomPydanticModel]:
        """
        Asynchronously parse the response using the provided response format.
        :param Type[CustomPydanticModel] response_format: The Pydantic model to parse the response into
        :param Any **kwargs Additional keyword arguments to pass to the .complete method
        :return: The parsed response
        """
        json_response_format = response_format_from_pydantic_model(response_format)
        response = await self.complete_async(  # pylint: disable=E1125
            **kwargs, response_format=json_response_format
        )
        parsed_response = convert_to_parsed_chat_completion_response(
            response, response_format
        )
        return parsed_response

    def parse_stream(
        self, response_format: Type[CustomPydanticModel], **kwargs
    ) -> eventstreaming.EventStream[models.CompletionEvent]:
        """
        Parse the response using the provided response format.
        For now the response will be in JSON format not in the input Pydantic model.
        :param Type[CustomPydanticModel] response_format: The Pydantic model to parse the response into
        :param Any **kwargs Additional keyword arguments to pass to the .stream method
        :return: The JSON parsed response
        """
        json_response_format = response_format_from_pydantic_model(response_format)
        response = self.stream(**kwargs, response_format=json_response_format)
        return response

    async def parse_stream_async(
        self, response_format: Type[CustomPydanticModel], **kwargs
    ) -> eventstreaming.EventStreamAsync[models.CompletionEvent]:
        """
        Asynchronously parse the response using the provided response format.
        For now the response will be in JSON format not in the input Pydantic model.
        :param Type[CustomPydanticModel] response_format: The Pydantic model to parse the response into
        :param Any **kwargs Additional keyword arguments to pass to the .stream method
        :return: The JSON parsed response
        """
        json_response_format = response_format_from_pydantic_model(response_format)
        response = await self.stream_async(  # pylint: disable=E1125
            **kwargs, response_format=json_response_format
        )
        return response

    # endregion sdk-class-body

    def complete(
        self,
        *,
        model: str,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ChatCompletionResponse:
        r"""Chat Completion

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            safe_prompt=safe_prompt,
        )

        req = self._build_request(
            method="POST",
            path="/v1/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ChatCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        model: str,
        messages: Union[List[models.Messages], List[models.MessagesTypedDict]],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[Union[models.Stop, models.StopTypedDict]] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionRequestToolChoice,
                models.ChatCompletionRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ChatCompletionResponse:
        r"""Chat Completion

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(messages, List[models.Messages]),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            safe_prompt=safe_prompt,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/chat/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_completion_v1_chat_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ChatCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def stream(
        self,
        *,
        model: str,
        messages: Union[
            List[models.ChatCompletionStreamRequestMessages],
            List[models.ChatCompletionStreamRequestMessagesTypedDict],
        ],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.ChatCompletionStreamRequestStop,
                models.ChatCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.CompletionEvent]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionStreamRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            safe_prompt=safe_prompt,
        )

        req = self._build_request(
            method="POST",
            path="/v1/chat/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        model: str,
        messages: Union[
            List[models.ChatCompletionStreamRequestMessages],
            List[models.ChatCompletionStreamRequestMessagesTypedDict],
        ],
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = None,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.ChatCompletionStreamRequestStop,
                models.ChatCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        response_format: Optional[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = None,
        tools: OptionalNullable[
            Union[List[models.Tool], List[models.ToolTypedDict]]
        ] = UNSET,
        tool_choice: Optional[
            Union[
                models.ChatCompletionStreamRequestToolChoice,
                models.ChatCompletionStreamRequestToolChoiceTypedDict,
            ]
        ] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        n: OptionalNullable[int] = UNSET,
        prediction: Optional[
            Union[models.Prediction, models.PredictionTypedDict]
        ] = None,
        parallel_tool_calls: Optional[bool] = None,
        prompt_mode: OptionalNullable[models.MistralPromptMode] = UNSET,
        safe_prompt: Optional[bool] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.CompletionEvent]:
        r"""Stream chat completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions.
        :param messages: The prompt(s) to generate completions for, encoded as a list of dict with role and content.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param response_format:
        :param tools:
        :param tool_choice:
        :param presence_penalty: presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.
        :param frequency_penalty: frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.
        :param n: Number of completions to return for each request, input tokens are only billed once.
        :param prediction:
        :param parallel_tool_calls:
        :param prompt_mode:
        :param safe_prompt: Whether to inject a safety prompt before all conversations.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            messages=utils.get_pydantic_model(
                messages, List[models.ChatCompletionStreamRequestMessages]
            ),
            response_format=utils.get_pydantic_model(
                response_format, Optional[models.ResponseFormat]
            ),
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ChatCompletionStreamRequestToolChoice]
            ),
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            n=n,
            prediction=utils.get_pydantic_model(
                prediction, Optional[models.Prediction]
            ),
            parallel_tool_calls=parallel_tool_calls,
            prompt_mode=prompt_mode,
            safe_prompt=safe_prompt,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/chat/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_chat",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/classifiers.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, Mapping, Optional, Union


class Classifiers(BaseSDK):
    r"""Classifiers API."""

    def moderate(
        self,
        *,
        model: str,
        inputs: Union[
            models.ClassificationRequestInputs,
            models.ClassificationRequestInputsTypedDict,
        ],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModerationResponse:
        r"""Moderations

        :param model: ID of the model to use.
        :param inputs: Text to classify.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ClassificationRequest(
            model=model,
            inputs=inputs,
        )

        req = self._build_request(
            method="POST",
            path="/v1/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="moderations_v1_moderations_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModerationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def moderate_async(
        self,
        *,
        model: str,
        inputs: Union[
            models.ClassificationRequestInputs,
            models.ClassificationRequestInputsTypedDict,
        ],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModerationResponse:
        r"""Moderations

        :param model: ID of the model to use.
        :param inputs: Text to classify.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ClassificationRequest(
            model=model,
            inputs=inputs,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="moderations_v1_moderations_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModerationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def moderate_chat(
        self,
        *,
        inputs: Union[
            models.ChatModerationRequestInputs,
            models.ChatModerationRequestInputsTypedDict,
        ],
        model: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModerationResponse:
        r"""Chat Moderations

        :param inputs: Chat to classify
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatModerationRequest(
            inputs=utils.get_pydantic_model(inputs, models.ChatModerationRequestInputs),
            model=model,
        )

        req = self._build_request(
            method="POST",
            path="/v1/chat/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatModerationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_moderations_v1_chat_moderations_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModerationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def moderate_chat_async(
        self,
        *,
        inputs: Union[
            models.ChatModerationRequestInputs,
            models.ChatModerationRequestInputsTypedDict,
        ],
        model: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModerationResponse:
        r"""Chat Moderations

        :param inputs: Chat to classify
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatModerationRequest(
            inputs=utils.get_pydantic_model(inputs, models.ChatModerationRequestInputs),
            model=model,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/chat/moderations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatModerationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_moderations_v1_chat_moderations_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModerationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def classify(
        self,
        *,
        model: str,
        inputs: Union[
            models.ClassificationRequestInputs,
            models.ClassificationRequestInputsTypedDict,
        ],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ClassificationResponse:
        r"""Classifications

        :param model: ID of the model to use.
        :param inputs: Text to classify.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ClassificationRequest(
            model=model,
            inputs=inputs,
        )

        req = self._build_request(
            method="POST",
            path="/v1/classifications",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="classifications_v1_classifications_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ClassificationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def classify_async(
        self,
        *,
        model: str,
        inputs: Union[
            models.ClassificationRequestInputs,
            models.ClassificationRequestInputsTypedDict,
        ],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ClassificationResponse:
        r"""Classifications

        :param model: ID of the model to use.
        :param inputs: Text to classify.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ClassificationRequest(
            model=model,
            inputs=inputs,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/classifications",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="classifications_v1_classifications_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ClassificationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def classify_chat(
        self,
        *,
        model: str,
        inputs: Union[models.Inputs, models.InputsTypedDict],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ClassificationResponse:
        r"""Chat Classifications

        :param model:
        :param inputs: Chat to classify
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatClassificationRequest(
            model=model,
            inputs=utils.get_pydantic_model(inputs, models.Inputs),
        )

        req = self._build_request(
            method="POST",
            path="/v1/chat/classifications",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_classifications_v1_chat_classifications_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ClassificationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def classify_chat_async(
        self,
        *,
        model: str,
        inputs: Union[models.Inputs, models.InputsTypedDict],
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ClassificationResponse:
        r"""Chat Classifications

        :param model:
        :param inputs: Chat to classify
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ChatClassificationRequest(
            model=model,
            inputs=utils.get_pydantic_model(inputs, models.Inputs),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/chat/classifications",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ChatClassificationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="chat_classifications_v1_chat_classifications_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ClassificationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/client.py">
from typing import Optional

MIGRATION_MESSAGE = "This client is deprecated. To migrate to the new client, please refer to this guide: https://github.com/mistralai/client-python/blob/main/MIGRATION.md. If you need to use this client anyway, pin your version to 0.4.2."


class MistralClient:
    def __init__(
        self,
        api_key: Optional[str] = None,
        endpoint: str = "",
        max_retries: int = 5,
        timeout: int = 120,
    ):
        raise NotImplementedError(MIGRATION_MESSAGE)
</file>

<file path="src/mistralai/conversations.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import eventstreaming, get_security_from_env
from typing import Any, List, Mapping, Optional, Union

# region imports
import typing
from typing import AsyncGenerator
import logging
from collections import defaultdict

from mistralai.models import (
    ResponseStartedEvent,
    ConversationEventsData,
    InputEntries,
)
from mistralai.extra.run.result import (
    RunResult,
    RunResultEvents,
    FunctionResultEvent,
    reconstitue_entries,
)
from mistralai.extra.run.utils import run_requirements

logger = logging.getLogger(__name__)

if typing.TYPE_CHECKING:
    from mistralai.extra.run.context import RunContext

# endregion imports


class Conversations(BaseSDK):
    r"""(beta) Conversations API"""

    # region sdk-class-body
    # Custom run code allowing client side execution of code

    @run_requirements
    async def run_async(
        self,
        run_ctx: "RunContext",
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[List[models.Tools], List[models.ToolsTypedDict]]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> RunResult:
        """Run a conversation with the given inputs and context.

        The execution of a run will only stop when no required local execution can be done."""
        from mistralai.beta import Beta
        from mistralai.extra.run.context import _validate_run
        from mistralai.extra.run.tools import get_function_calls

        req, run_result, input_entries = await _validate_run(
            beta_client=Beta(self.sdk_configuration),
            run_ctx=run_ctx,
            inputs=inputs,
            instructions=instructions,
            tools=tools,
            completion_args=completion_args,
        )

        while True:
            if run_ctx.conversation_id is None:
                res = await self.start_async(
                    inputs=input_entries,
                    http_headers=http_headers,
                    name=name,
                    description=description,
                    retries=retries,
                    server_url=server_url,
                    timeout_ms=timeout_ms,
                    **req,
                )
                run_result.conversation_id = res.conversation_id
                run_ctx.conversation_id = res.conversation_id
                logger.info(
                    f"Started Run with conversation with id {res.conversation_id}"
                )
            else:
                res = await self.append_async(
                    conversation_id=run_ctx.conversation_id,
                    inputs=input_entries,
                    retries=retries,
                    server_url=server_url,
                    timeout_ms=timeout_ms,
                )
            run_ctx.request_count += 1
            run_result.output_entries.extend(res.outputs)
            fcalls = get_function_calls(res.outputs)
            if not fcalls:
                logger.debug("No more function calls to execute")
                break
            else:
                fresults = await run_ctx.execute_function_calls(fcalls)
                run_result.output_entries.extend(fresults)
                input_entries = typing.cast(list[InputEntries], fresults)
        return run_result

    @run_requirements
    async def run_stream_async(
        self,
        run_ctx: "RunContext",
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[List[models.Tools], List[models.ToolsTypedDict]]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> AsyncGenerator[Union[RunResultEvents, RunResult], None]:
        """Similar to `run_async` but returns a generator which streams events.

        The last streamed object is the RunResult object which summarises what happened in the run."""
        from mistralai.beta import Beta
        from mistralai.extra.run.context import _validate_run
        from mistralai.extra.run.tools import get_function_calls

        req, run_result, input_entries = await _validate_run(
            beta_client=Beta(self.sdk_configuration),
            run_ctx=run_ctx,
            inputs=inputs,
            instructions=instructions,
            tools=tools,
            completion_args=completion_args,
        )

        async def run_generator() -> (
            AsyncGenerator[Union[RunResultEvents, RunResult], None]
        ):
            current_entries = input_entries
            while True:
                received_event_tracker: defaultdict[
                    int, list[ConversationEventsData]
                ] = defaultdict(list)
                if run_ctx.conversation_id is None:
                    res = await self.start_stream_async(
                        inputs=current_entries,
                        http_headers=http_headers,
                        name=name,
                        description=description,
                        retries=retries,
                        server_url=server_url,
                        timeout_ms=timeout_ms,
                        **req,
                    )
                else:
                    res = await self.append_stream_async(
                        conversation_id=run_ctx.conversation_id,
                        inputs=current_entries,
                        retries=retries,
                        server_url=server_url,
                        timeout_ms=timeout_ms,
                    )
                async for event in res:
                    if (
                        isinstance(event.data, ResponseStartedEvent)
                        and run_ctx.conversation_id is None
                    ):
                        run_result.conversation_id = event.data.conversation_id
                        run_ctx.conversation_id = event.data.conversation_id
                        logger.info(
                            f"Started Run with conversation with id {run_ctx.conversation_id}"
                        )
                    if (
                        output_index := getattr(event.data, "output_index", None)
                    ) is not None:
                        received_event_tracker[output_index].append(event.data)
                    yield typing.cast(RunResultEvents, event)
                run_ctx.request_count += 1
                outputs = reconstitue_entries(received_event_tracker)
                run_result.output_entries.extend(outputs)
                fcalls = get_function_calls(outputs)
                if not fcalls:
                    logger.debug("No more function calls to execute")
                    break
                else:
                    fresults = await run_ctx.execute_function_calls(fcalls)
                    run_result.output_entries.extend(fresults)
                    for fresult in fresults:
                        yield RunResultEvents(
                            event="function.result",
                            data=FunctionResultEvent(
                                type="function.result",
                                result=fresult.result,
                                tool_call_id=fresult.tool_call_id,
                            ),
                        )
                    current_entries = typing.cast(list[InputEntries], fresults)
            yield run_result

        return run_generator()

    # endregion sdk-class-body

    def start(
        self,
        *,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = False,
        store: OptionalNullable[bool] = UNSET,
        handoff_execution: OptionalNullable[models.HandoffExecution] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[List[models.Tools], List[models.ToolsTypedDict]]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        agent_id: OptionalNullable[str] = UNSET,
        model: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Create a conversation and append entries to it.

        Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

        :param inputs:
        :param stream:
        :param store:
        :param handoff_execution:
        :param instructions:
        :param tools:
        :param completion_args:
        :param name:
        :param description:
        :param agent_id:
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ConversationRequest(
            inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
            stream=stream,
            store=store,
            handoff_execution=handoff_execution,
            instructions=instructions,
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tools]]),
            completion_args=utils.get_pydantic_model(
                completion_args, OptionalNullable[models.CompletionArgs]
            ),
            name=name,
            description=description,
            agent_id=agent_id,
            model=model,
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ConversationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_start",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def start_async(
        self,
        *,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = False,
        store: OptionalNullable[bool] = UNSET,
        handoff_execution: OptionalNullable[models.HandoffExecution] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[List[models.Tools], List[models.ToolsTypedDict]]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        agent_id: OptionalNullable[str] = UNSET,
        model: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Create a conversation and append entries to it.

        Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

        :param inputs:
        :param stream:
        :param store:
        :param handoff_execution:
        :param instructions:
        :param tools:
        :param completion_args:
        :param name:
        :param description:
        :param agent_id:
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ConversationRequest(
            inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
            stream=stream,
            store=store,
            handoff_execution=handoff_execution,
            instructions=instructions,
            tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tools]]),
            completion_args=utils.get_pydantic_model(
                completion_args, OptionalNullable[models.CompletionArgs]
            ),
            name=name,
            description=description,
            agent_id=agent_id,
            model=model,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ConversationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_start",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def list(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> List[models.ResponseBody]:
        r"""List all created conversations.

        Retrieve a list of conversation entities sorted by creation time.

        :param page:
        :param page_size:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsListRequest(
            page=page,
            page_size=page_size,
        )

        req = self._build_request(
            method="GET",
            path="/v1/conversations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_list",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, List[models.ResponseBody])
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> List[models.ResponseBody]:
        r"""List all created conversations.

        Retrieve a list of conversation entities sorted by creation time.

        :param page:
        :param page_size:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsListRequest(
            page=page,
            page_size=page_size,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/conversations",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_list",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, List[models.ResponseBody])
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.AgentsAPIV1ConversationsGetResponseV1ConversationsGet:
        r"""Retrieve a conversation information.

        Given a conversation_id retrieve a conversation entity with its attributes.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsGetRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/conversations/{conversation_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.AgentsAPIV1ConversationsGetResponseV1ConversationsGet,
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_async(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.AgentsAPIV1ConversationsGetResponseV1ConversationsGet:
        r"""Retrieve a conversation information.

        Given a conversation_id retrieve a conversation entity with its attributes.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsGetRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/conversations/{conversation_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.AgentsAPIV1ConversationsGetResponseV1ConversationsGet,
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def append(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = False,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationAppendRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Append new entries to an existing conversation.

        Run completion on the history of the conversation and the user entries. Return the new created entries.

        :param conversation_id: ID of the conversation to which we append entries.
        :param inputs:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsAppendRequest(
            conversation_id=conversation_id,
            conversation_append_request=models.ConversationAppendRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations/{conversation_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_append_request,
                False,
                False,
                "json",
                models.ConversationAppendRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_append",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def append_async(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = False,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationAppendRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Append new entries to an existing conversation.

        Run completion on the history of the conversation and the user entries. Return the new created entries.

        :param conversation_id: ID of the conversation to which we append entries.
        :param inputs:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsAppendRequest(
            conversation_id=conversation_id,
            conversation_append_request=models.ConversationAppendRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations/{conversation_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_append_request,
                False,
                False,
                "json",
                models.ConversationAppendRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_append",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get_history(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationHistory:
        r"""Retrieve all entries in a conversation.

        Given a conversation_id retrieve all the entries belonging to that conversation. The entries are sorted in the order they were appended, those can be messages, connectors or function_call.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsHistoryRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/conversations/{conversation_id}/history",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_history",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationHistory)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_history_async(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationHistory:
        r"""Retrieve all entries in a conversation.

        Given a conversation_id retrieve all the entries belonging to that conversation. The entries are sorted in the order they were appended, those can be messages, connectors or function_call.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsHistoryRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/conversations/{conversation_id}/history",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_history",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationHistory)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get_messages(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationMessages:
        r"""Retrieve all messages in a conversation.

        Given a conversation_id retrieve all the messages belonging to that conversation. This is similar to retrieving all entries except we filter the messages only.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsMessagesRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/conversations/{conversation_id}/messages",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_messages",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationMessages)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_messages_async(
        self,
        *,
        conversation_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationMessages:
        r"""Retrieve all messages in a conversation.

        Given a conversation_id retrieve all the messages belonging to that conversation. This is similar to retrieving all entries except we filter the messages only.

        :param conversation_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsMessagesRequest(
            conversation_id=conversation_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/conversations/{conversation_id}/messages",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_messages",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationMessages)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def restart(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        from_entry_id: str,
        stream: Optional[bool] = False,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationRestartRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Restart a conversation starting from a given entry.

        Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

        :param conversation_id:
        :param inputs:
        :param from_entry_id:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsRestartRequest(
            conversation_id=conversation_id,
            conversation_restart_request=models.ConversationRestartRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                from_entry_id=from_entry_id,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations/{conversation_id}/restart",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_restart_request,
                False,
                False,
                "json",
                models.ConversationRestartRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_restart",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def restart_async(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        from_entry_id: str,
        stream: Optional[bool] = False,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationRestartRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ConversationResponse:
        r"""Restart a conversation starting from a given entry.

        Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

        :param conversation_id:
        :param inputs:
        :param from_entry_id:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsRestartRequest(
            conversation_id=conversation_id,
            conversation_restart_request=models.ConversationRestartRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                from_entry_id=from_entry_id,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations/{conversation_id}/restart",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_restart_request,
                False,
                False,
                "json",
                models.ConversationRestartRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_restart",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ConversationResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def start_stream(
        self,
        *,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = True,
        store: OptionalNullable[bool] = UNSET,
        handoff_execution: OptionalNullable[
            models.ConversationStreamRequestHandoffExecution
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[
                List[models.ConversationStreamRequestTools],
                List[models.ConversationStreamRequestToolsTypedDict],
            ]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        agent_id: OptionalNullable[str] = UNSET,
        model: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.ConversationEvents]:
        r"""Create a conversation and append entries to it.

        Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

        :param inputs:
        :param stream:
        :param store:
        :param handoff_execution:
        :param instructions:
        :param tools:
        :param completion_args:
        :param name:
        :param description:
        :param agent_id:
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ConversationStreamRequest(
            inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
            stream=stream,
            store=store,
            handoff_execution=handoff_execution,
            instructions=instructions,
            tools=utils.get_pydantic_model(
                tools, OptionalNullable[List[models.ConversationStreamRequestTools]]
            ),
            completion_args=utils.get_pydantic_model(
                completion_args, OptionalNullable[models.CompletionArgs]
            ),
            name=name,
            description=description,
            agent_id=agent_id,
            model=model,
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ConversationStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_start_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def start_stream_async(
        self,
        *,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = True,
        store: OptionalNullable[bool] = UNSET,
        handoff_execution: OptionalNullable[
            models.ConversationStreamRequestHandoffExecution
        ] = UNSET,
        instructions: OptionalNullable[str] = UNSET,
        tools: OptionalNullable[
            Union[
                List[models.ConversationStreamRequestTools],
                List[models.ConversationStreamRequestToolsTypedDict],
            ]
        ] = UNSET,
        completion_args: OptionalNullable[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        agent_id: OptionalNullable[str] = UNSET,
        model: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.ConversationEvents]:
        r"""Create a conversation and append entries to it.

        Create a new conversation, using a base model or an agent and append entries. Completion and tool executions are run and the response is appended to the conversation.Use the returned conversation_id to continue the conversation.

        :param inputs:
        :param stream:
        :param store:
        :param handoff_execution:
        :param instructions:
        :param tools:
        :param completion_args:
        :param name:
        :param description:
        :param agent_id:
        :param model:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ConversationStreamRequest(
            inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
            stream=stream,
            store=store,
            handoff_execution=handoff_execution,
            instructions=instructions,
            tools=utils.get_pydantic_model(
                tools, OptionalNullable[List[models.ConversationStreamRequestTools]]
            ),
            completion_args=utils.get_pydantic_model(
                completion_args, OptionalNullable[models.CompletionArgs]
            ),
            name=name,
            description=description,
            agent_id=agent_id,
            model=model,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ConversationStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_start_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def append_stream(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = True,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationAppendStreamRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.ConversationEvents]:
        r"""Append new entries to an existing conversation.

        Run completion on the history of the conversation and the user entries. Return the new created entries.

        :param conversation_id: ID of the conversation to which we append entries.
        :param inputs:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsAppendStreamRequest(
            conversation_id=conversation_id,
            conversation_append_stream_request=models.ConversationAppendStreamRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations/{conversation_id}#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_append_stream_request,
                False,
                False,
                "json",
                models.ConversationAppendStreamRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_append_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def append_stream_async(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        stream: Optional[bool] = True,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationAppendStreamRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.ConversationEvents]:
        r"""Append new entries to an existing conversation.

        Run completion on the history of the conversation and the user entries. Return the new created entries.

        :param conversation_id: ID of the conversation to which we append entries.
        :param inputs:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsAppendStreamRequest(
            conversation_id=conversation_id,
            conversation_append_stream_request=models.ConversationAppendStreamRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations/{conversation_id}#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_append_stream_request,
                False,
                False,
                "json",
                models.ConversationAppendStreamRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_append_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def restart_stream(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        from_entry_id: str,
        stream: Optional[bool] = True,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationRestartStreamRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.ConversationEvents]:
        r"""Restart a conversation starting from a given entry.

        Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

        :param conversation_id:
        :param inputs:
        :param from_entry_id:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsRestartStreamRequest(
            conversation_id=conversation_id,
            conversation_restart_stream_request=models.ConversationRestartStreamRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                from_entry_id=from_entry_id,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/conversations/{conversation_id}/restart#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_restart_stream_request,
                False,
                False,
                "json",
                models.ConversationRestartStreamRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_restart_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def restart_stream_async(
        self,
        *,
        conversation_id: str,
        inputs: Union[models.ConversationInputs, models.ConversationInputsTypedDict],
        from_entry_id: str,
        stream: Optional[bool] = True,
        store: Optional[bool] = True,
        handoff_execution: Optional[
            models.ConversationRestartStreamRequestHandoffExecution
        ] = "server",
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.ConversationEvents]:
        r"""Restart a conversation starting from a given entry.

        Given a conversation_id and an id, recreate a conversation from this point and run completion. A new conversation is returned with the new entries returned.

        :param conversation_id:
        :param inputs:
        :param from_entry_id:
        :param stream:
        :param store: Whether to store the results into our servers or not.
        :param handoff_execution:
        :param completion_args: White-listed arguments from the completion API
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1ConversationsRestartStreamRequest(
            conversation_id=conversation_id,
            conversation_restart_stream_request=models.ConversationRestartStreamRequest(
                inputs=utils.get_pydantic_model(inputs, models.ConversationInputs),
                stream=stream,
                store=store,
                handoff_execution=handoff_execution,
                from_entry_id=from_entry_id,
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/conversations/{conversation_id}/restart#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.conversation_restart_stream_request,
                False,
                False,
                "json",
                models.ConversationRestartStreamRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_conversations_restart_stream",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.ConversationEvents),
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/embeddings.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, Mapping, Optional, Union


class Embeddings(BaseSDK):
    r"""Embeddings API."""

    def create(
        self,
        *,
        model: str,
        inputs: Union[
            models.EmbeddingRequestInputs, models.EmbeddingRequestInputsTypedDict
        ],
        output_dimension: OptionalNullable[int] = UNSET,
        output_dtype: Optional[models.EmbeddingDtype] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.EmbeddingResponse:
        r"""Embeddings

        Embeddings

        :param model: ID of the model to use.
        :param inputs: Text to embed.
        :param output_dimension: The dimension of the output embeddings.
        :param output_dtype:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.EmbeddingRequest(
            model=model,
            inputs=inputs,
            output_dimension=output_dimension,
            output_dtype=output_dtype,
        )

        req = self._build_request(
            method="POST",
            path="/v1/embeddings",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.EmbeddingRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="embeddings_v1_embeddings_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.EmbeddingResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def create_async(
        self,
        *,
        model: str,
        inputs: Union[
            models.EmbeddingRequestInputs, models.EmbeddingRequestInputsTypedDict
        ],
        output_dimension: OptionalNullable[int] = UNSET,
        output_dtype: Optional[models.EmbeddingDtype] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.EmbeddingResponse:
        r"""Embeddings

        Embeddings

        :param model: ID of the model to use.
        :param inputs: Text to embed.
        :param output_dimension: The dimension of the output embeddings.
        :param output_dtype:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.EmbeddingRequest(
            model=model,
            inputs=inputs,
            output_dimension=output_dimension,
            output_dtype=output_dtype,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/embeddings",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.EmbeddingRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="embeddings_v1_embeddings_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.EmbeddingResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/files.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
import httpx
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import List, Mapping, Optional, Union


class Files(BaseSDK):
    r"""Files API"""

    def upload(
        self,
        *,
        file: Union[models.File, models.FileTypedDict],
        purpose: Optional[models.FilePurpose] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.UploadFileOut:
        r"""Upload File

        Upload a file that can be used across various endpoints.

        The size of individual files can be a maximum of 512 MB. The Fine-tuning API only supports .jsonl files.

        Please contact us if you need to increase these storage limits.

        :param file: The File object (not file name) to be uploaded.  To upload a file and specify a custom file name you should format your request as such:  ```bash  file=@path/to/your/file.jsonl;filename=custom_name.jsonl  ```  Otherwise, you can just keep the original file name:  ```bash  file=@path/to/your/file.jsonl  ```
        :param purpose:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesUploadFileMultiPartBodyParams(
            file=utils.get_pydantic_model(file, models.File),
            purpose=purpose,
        )

        req = self._build_request(
            method="POST",
            path="/v1/files",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.FilesAPIRoutesUploadFileMultiPartBodyParams,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_upload_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.UploadFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def upload_async(
        self,
        *,
        file: Union[models.File, models.FileTypedDict],
        purpose: Optional[models.FilePurpose] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.UploadFileOut:
        r"""Upload File

        Upload a file that can be used across various endpoints.

        The size of individual files can be a maximum of 512 MB. The Fine-tuning API only supports .jsonl files.

        Please contact us if you need to increase these storage limits.

        :param file: The File object (not file name) to be uploaded.  To upload a file and specify a custom file name you should format your request as such:  ```bash  file=@path/to/your/file.jsonl;filename=custom_name.jsonl  ```  Otherwise, you can just keep the original file name:  ```bash  file=@path/to/your/file.jsonl  ```
        :param purpose:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesUploadFileMultiPartBodyParams(
            file=utils.get_pydantic_model(file, models.File),
            purpose=purpose,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/files",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request,
                False,
                False,
                "multipart",
                models.FilesAPIRoutesUploadFileMultiPartBodyParams,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_upload_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.UploadFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def list(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        sample_type: OptionalNullable[List[models.SampleType]] = UNSET,
        source: OptionalNullable[List[models.Source]] = UNSET,
        search: OptionalNullable[str] = UNSET,
        purpose: OptionalNullable[models.FilePurpose] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ListFilesOut:
        r"""List Files

        Returns a list of files that belong to the user's organization.

        :param page:
        :param page_size:
        :param sample_type:
        :param source:
        :param search:
        :param purpose:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesListFilesRequest(
            page=page,
            page_size=page_size,
            sample_type=sample_type,
            source=source,
            search=search,
            purpose=purpose,
        )

        req = self._build_request(
            method="GET",
            path="/v1/files",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_list_files",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ListFilesOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        sample_type: OptionalNullable[List[models.SampleType]] = UNSET,
        source: OptionalNullable[List[models.Source]] = UNSET,
        search: OptionalNullable[str] = UNSET,
        purpose: OptionalNullable[models.FilePurpose] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ListFilesOut:
        r"""List Files

        Returns a list of files that belong to the user's organization.

        :param page:
        :param page_size:
        :param sample_type:
        :param source:
        :param search:
        :param purpose:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesListFilesRequest(
            page=page,
            page_size=page_size,
            sample_type=sample_type,
            source=source,
            search=search,
            purpose=purpose,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/files",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_list_files",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ListFilesOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def retrieve(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RetrieveFileOut:
        r"""Retrieve File

        Returns information about a specific file.

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesRetrieveFileRequest(
            file_id=file_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/files/{file_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_retrieve_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.RetrieveFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def retrieve_async(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RetrieveFileOut:
        r"""Retrieve File

        Returns information about a specific file.

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesRetrieveFileRequest(
            file_id=file_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/files/{file_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_retrieve_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.RetrieveFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def delete(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.DeleteFileOut:
        r"""Delete File

        Delete a file.

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesDeleteFileRequest(
            file_id=file_id,
        )

        req = self._build_request(
            method="DELETE",
            path="/v1/files/{file_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_delete_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.DeleteFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def delete_async(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.DeleteFileOut:
        r"""Delete File

        Delete a file.

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesDeleteFileRequest(
            file_id=file_id,
        )

        req = self._build_request_async(
            method="DELETE",
            path="/v1/files/{file_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_delete_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.DeleteFileOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def download(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Response:
        r"""Download File

        Download a file

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesDownloadFileRequest(
            file_id=file_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/files/{file_id}/content",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/octet-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_download_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/octet-stream"):
            return http_res
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def download_async(
        self,
        *,
        file_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> httpx.Response:
        r"""Download File

        Download a file

        :param file_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesDownloadFileRequest(
            file_id=file_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/files/{file_id}/content",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/octet-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_download_file",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/octet-stream"):
            return http_res
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get_signed_url(
        self,
        *,
        file_id: str,
        expiry: Optional[int] = 24,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.FileSignedURL:
        r"""Get Signed Url

        :param file_id:
        :param expiry: Number of hours before the url becomes invalid. Defaults to 24h
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesGetSignedURLRequest(
            file_id=file_id,
            expiry=expiry,
        )

        req = self._build_request(
            method="GET",
            path="/v1/files/{file_id}/url",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_get_signed_url",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.FileSignedURL)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_signed_url_async(
        self,
        *,
        file_id: str,
        expiry: Optional[int] = 24,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.FileSignedURL:
        r"""Get Signed Url

        :param file_id:
        :param expiry: Number of hours before the url becomes invalid. Defaults to 24h
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FilesAPIRoutesGetSignedURLRequest(
            file_id=file_id,
            expiry=expiry,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/files/{file_id}/url",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="files_api_routes_get_signed_url",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.FileSignedURL)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/fim.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import eventstreaming, get_security_from_env
from typing import Any, Mapping, Optional, Union


class Fim(BaseSDK):
    r"""Fill-in-the-middle API."""

    def complete(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.FIMCompletionRequestStop,
                models.FIMCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.FIMCompletionResponse:
        r"""Fim Completion

        FIM completion.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request(
            method="POST",
            path="/v1/fim/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="fim_completion_v1_fim_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.FIMCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def complete_async(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = False,
        stop: Optional[
            Union[
                models.FIMCompletionRequestStop,
                models.FIMCompletionRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.FIMCompletionResponse:
        r"""Fim Completion

        FIM completion.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream: Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fim/completions",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="fim_completion_v1_fim_completions_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.FIMCompletionResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def stream(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.FIMCompletionStreamRequestStop,
                models.FIMCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.CompletionEvent]:
        r"""Stream fim completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request(
            method="POST",
            path="/v1/fim/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_fim",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def stream_async(
        self,
        *,
        model: str,
        prompt: str,
        temperature: OptionalNullable[float] = UNSET,
        top_p: Optional[float] = 1,
        max_tokens: OptionalNullable[int] = UNSET,
        stream: Optional[bool] = True,
        stop: Optional[
            Union[
                models.FIMCompletionStreamRequestStop,
                models.FIMCompletionStreamRequestStopTypedDict,
            ]
        ] = None,
        random_seed: OptionalNullable[int] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        min_tokens: OptionalNullable[int] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.CompletionEvent]:
        r"""Stream fim completion

        Mistral AI provides the ability to stream responses back to a client in order to allow partial results for certain requests. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.

        :param model: ID of the model to use. Only compatible for now with:   - `codestral-2405`   - `codestral-latest`
        :param prompt: The text/code to complete.
        :param temperature: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        :param top_p: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        :param max_tokens: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        :param stream:
        :param stop: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        :param random_seed: The seed to use for random sampling. If set, different calls will generate deterministic results.
        :param suffix: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        :param min_tokens: The minimum number of tokens to generate in the completion.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.FIMCompletionStreamRequest(
            model=model,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=stream,
            stop=stop,
            random_seed=random_seed,
            prompt=prompt,
            suffix=suffix,
            min_tokens=min_tokens,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fim/completions#stream",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.FIMCompletionStreamRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="stream_fim",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(raw, models.CompletionEvent),
                sentinel="[DONE]",
            )
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = utils.unmarshal_json(
                http_res_text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/fine_tuning.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from .sdkconfiguration import SDKConfiguration
from mistralai.jobs import Jobs


class FineTuning(BaseSDK):
    jobs: Jobs

    def __init__(self, sdk_config: SDKConfiguration) -> None:
        BaseSDK.__init__(self, sdk_config)
        self.sdk_configuration = sdk_config
        self._init_sdks()

    def _init_sdks(self):
        self.jobs = Jobs(self.sdk_configuration)
</file>

<file path="src/mistralai/httpclient.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

# pyright: reportReturnType = false
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing_extensions import Protocol, runtime_checkable
import httpx
from typing import Any, Optional, Union


@runtime_checkable
class HttpClient(Protocol):
    def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    def close(self) -> None:
        pass


@runtime_checkable
class AsyncHttpClient(Protocol):
    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        pass

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        pass

    async def aclose(self) -> None:
        pass


class ClientOwner(Protocol):
    client: Union[HttpClient, None]
    async_client: Union[AsyncHttpClient, None]


def close_clients(
    owner: ClientOwner,
    sync_client: Union[HttpClient, None],
    sync_client_supplied: bool,
    async_client: Union[AsyncHttpClient, None],
    async_client_supplied: bool,
) -> None:
    """
    A finalizer function that is meant to be used with weakref.finalize to close
    httpx clients used by an SDK so that underlying resources can be garbage
    collected.
    """

    # Unset the client/async_client properties so there are no more references
    # to them from the owning SDK instance and they can be reaped.
    owner.client = None
    owner.async_client = None

    if sync_client is not None and not sync_client_supplied:
        try:
            sync_client.close()
        except Exception:
            pass

    if async_client is not None and not async_client_supplied:
        is_async = False
        try:
            asyncio.get_running_loop()
            is_async = True
        except RuntimeError:
            pass

        try:
            # If this function is called in an async loop then start another
            # loop in a separate thread to close the async http client.
            if is_async:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_client.aclose())
                    future.result()
            else:
                asyncio.run(async_client.aclose())
        except Exception:
            pass
</file>

<file path="src/mistralai/jobs.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from datetime import datetime
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import List, Mapping, Optional, Union


class Jobs(BaseSDK):
    def list(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        model: OptionalNullable[str] = UNSET,
        created_after: OptionalNullable[datetime] = UNSET,
        created_before: OptionalNullable[datetime] = UNSET,
        created_by_me: Optional[bool] = False,
        status: OptionalNullable[models.QueryParamStatus] = UNSET,
        wandb_project: OptionalNullable[str] = UNSET,
        wandb_name: OptionalNullable[str] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsOut:
        r"""Get Fine Tuning Jobs

        Get a list of fine-tuning jobs for your organization and user.

        :param page: The page number of the results to be returned.
        :param page_size: The number of items to return per page.
        :param model: The model name used for fine-tuning to filter on. When set, the other results are not displayed.
        :param created_after: The date/time to filter on. When set, the results for previous creation times are not displayed.
        :param created_before:
        :param created_by_me: When set, only return results for jobs created by the API caller. Other results are not displayed.
        :param status: The current job state to filter on. When set, the other results are not displayed.
        :param wandb_project: The Weights and Biases project to filter on. When set, the other results are not displayed.
        :param wandb_name: The Weight and Biases run name to filter on. When set, the other results are not displayed.
        :param suffix: The model suffix to filter on. When set, the other results are not displayed.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningGetFineTuningJobsRequest(
            page=page,
            page_size=page_size,
            model=model,
            created_after=created_after,
            created_before=created_before,
            created_by_me=created_by_me,
            status=status,
            wandb_project=wandb_project,
            wandb_name=wandb_name,
            suffix=suffix,
        )

        req = self._build_request(
            method="GET",
            path="/v1/fine_tuning/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_get_fine_tuning_jobs",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.JobsOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        model: OptionalNullable[str] = UNSET,
        created_after: OptionalNullable[datetime] = UNSET,
        created_before: OptionalNullable[datetime] = UNSET,
        created_by_me: Optional[bool] = False,
        status: OptionalNullable[models.QueryParamStatus] = UNSET,
        wandb_project: OptionalNullable[str] = UNSET,
        wandb_name: OptionalNullable[str] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsOut:
        r"""Get Fine Tuning Jobs

        Get a list of fine-tuning jobs for your organization and user.

        :param page: The page number of the results to be returned.
        :param page_size: The number of items to return per page.
        :param model: The model name used for fine-tuning to filter on. When set, the other results are not displayed.
        :param created_after: The date/time to filter on. When set, the results for previous creation times are not displayed.
        :param created_before:
        :param created_by_me: When set, only return results for jobs created by the API caller. Other results are not displayed.
        :param status: The current job state to filter on. When set, the other results are not displayed.
        :param wandb_project: The Weights and Biases project to filter on. When set, the other results are not displayed.
        :param wandb_name: The Weight and Biases run name to filter on. When set, the other results are not displayed.
        :param suffix: The model suffix to filter on. When set, the other results are not displayed.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningGetFineTuningJobsRequest(
            page=page,
            page_size=page_size,
            model=model,
            created_after=created_after,
            created_before=created_before,
            created_by_me=created_by_me,
            status=status,
            wandb_project=wandb_project,
            wandb_name=wandb_name,
            suffix=suffix,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/fine_tuning/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_get_fine_tuning_jobs",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.JobsOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def create(
        self,
        *,
        model: str,
        hyperparameters: Union[models.Hyperparameters, models.HyperparametersTypedDict],
        training_files: Optional[
            Union[List[models.TrainingFile], List[models.TrainingFileTypedDict]]
        ] = None,
        validation_files: OptionalNullable[List[str]] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        integrations: OptionalNullable[
            Union[
                List[models.JobInIntegrations], List[models.JobInIntegrationsTypedDict]
            ]
        ] = UNSET,
        auto_start: Optional[bool] = None,
        invalid_sample_skip_percentage: Optional[float] = 0,
        job_type: OptionalNullable[models.FineTuneableModelType] = UNSET,
        repositories: OptionalNullable[
            Union[
                List[models.JobInRepositories], List[models.JobInRepositoriesTypedDict]
            ]
        ] = UNSET,
        classifier_targets: OptionalNullable[
            Union[
                List[models.ClassifierTargetIn],
                List[models.ClassifierTargetInTypedDict],
            ]
        ] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningCreateFineTuningJobResponse:
        r"""Create Fine Tuning Job

        Create a new fine-tuning job, it will be queued for processing.

        :param model: The name of the model to fine-tune.
        :param hyperparameters:
        :param training_files:
        :param validation_files: A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files.
        :param suffix: A string that will be added to your fine-tuning model name. For example, a suffix of \"my-great-model\" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`
        :param integrations: A list of integrations to enable for your fine-tuning job.
        :param auto_start: This field will be required in a future release.
        :param invalid_sample_skip_percentage:
        :param job_type:
        :param repositories:
        :param classifier_targets:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobIn(
            model=model,
            training_files=utils.get_pydantic_model(
                training_files, Optional[List[models.TrainingFile]]
            ),
            validation_files=validation_files,
            suffix=suffix,
            integrations=utils.get_pydantic_model(
                integrations, OptionalNullable[List[models.JobInIntegrations]]
            ),
            auto_start=auto_start,
            invalid_sample_skip_percentage=invalid_sample_skip_percentage,
            job_type=job_type,
            hyperparameters=utils.get_pydantic_model(
                hyperparameters, models.Hyperparameters
            ),
            repositories=utils.get_pydantic_model(
                repositories, OptionalNullable[List[models.JobInRepositories]]
            ),
            classifier_targets=utils.get_pydantic_model(
                classifier_targets, OptionalNullable[List[models.ClassifierTargetIn]]
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/fine_tuning/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.JobIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_create_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningCreateFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def create_async(
        self,
        *,
        model: str,
        hyperparameters: Union[models.Hyperparameters, models.HyperparametersTypedDict],
        training_files: Optional[
            Union[List[models.TrainingFile], List[models.TrainingFileTypedDict]]
        ] = None,
        validation_files: OptionalNullable[List[str]] = UNSET,
        suffix: OptionalNullable[str] = UNSET,
        integrations: OptionalNullable[
            Union[
                List[models.JobInIntegrations], List[models.JobInIntegrationsTypedDict]
            ]
        ] = UNSET,
        auto_start: Optional[bool] = None,
        invalid_sample_skip_percentage: Optional[float] = 0,
        job_type: OptionalNullable[models.FineTuneableModelType] = UNSET,
        repositories: OptionalNullable[
            Union[
                List[models.JobInRepositories], List[models.JobInRepositoriesTypedDict]
            ]
        ] = UNSET,
        classifier_targets: OptionalNullable[
            Union[
                List[models.ClassifierTargetIn],
                List[models.ClassifierTargetInTypedDict],
            ]
        ] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningCreateFineTuningJobResponse:
        r"""Create Fine Tuning Job

        Create a new fine-tuning job, it will be queued for processing.

        :param model: The name of the model to fine-tune.
        :param hyperparameters:
        :param training_files:
        :param validation_files: A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files.
        :param suffix: A string that will be added to your fine-tuning model name. For example, a suffix of \"my-great-model\" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`
        :param integrations: A list of integrations to enable for your fine-tuning job.
        :param auto_start: This field will be required in a future release.
        :param invalid_sample_skip_percentage:
        :param job_type:
        :param repositories:
        :param classifier_targets:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobIn(
            model=model,
            training_files=utils.get_pydantic_model(
                training_files, Optional[List[models.TrainingFile]]
            ),
            validation_files=validation_files,
            suffix=suffix,
            integrations=utils.get_pydantic_model(
                integrations, OptionalNullable[List[models.JobInIntegrations]]
            ),
            auto_start=auto_start,
            invalid_sample_skip_percentage=invalid_sample_skip_percentage,
            job_type=job_type,
            hyperparameters=utils.get_pydantic_model(
                hyperparameters, models.Hyperparameters
            ),
            repositories=utils.get_pydantic_model(
                repositories, OptionalNullable[List[models.JobInRepositories]]
            ),
            classifier_targets=utils.get_pydantic_model(
                classifier_targets, OptionalNullable[List[models.ClassifierTargetIn]]
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fine_tuning/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.JobIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_create_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningCreateFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningGetFineTuningJobResponse:
        r"""Get Fine Tuning Job

        Get a fine-tuned job details by its UUID.

        :param job_id: The ID of the job to analyse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningGetFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/fine_tuning/jobs/{job_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_get_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningGetFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_async(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningGetFineTuningJobResponse:
        r"""Get Fine Tuning Job

        Get a fine-tuned job details by its UUID.

        :param job_id: The ID of the job to analyse.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningGetFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/fine_tuning/jobs/{job_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_get_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningGetFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def cancel(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningCancelFineTuningJobResponse:
        r"""Cancel Fine Tuning Job

        Request the cancellation of a fine tuning job.

        :param job_id: The ID of the job to cancel.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningCancelFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/fine_tuning/jobs/{job_id}/cancel",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_cancel_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningCancelFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def cancel_async(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningCancelFineTuningJobResponse:
        r"""Cancel Fine Tuning Job

        Request the cancellation of a fine tuning job.

        :param job_id: The ID of the job to cancel.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningCancelFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fine_tuning/jobs/{job_id}/cancel",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_cancel_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningCancelFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def start(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningStartFineTuningJobResponse:
        r"""Start Fine Tuning Job

        Request the start of a validated fine tuning job.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningStartFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/fine_tuning/jobs/{job_id}/start",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_start_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningStartFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def start_async(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningStartFineTuningJobResponse:
        r"""Start Fine Tuning Job

        Request the start of a validated fine tuning job.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningStartFineTuningJobRequest(
            job_id=job_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fine_tuning/jobs/{job_id}/start",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_start_fine_tuning_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text, models.JobsAPIRoutesFineTuningStartFineTuningJobResponse
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/mistral_agents.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, List, Mapping, Optional, Union


class MistralAgents(BaseSDK):
    r"""(beta) Agents API"""

    def create(
        self,
        *,
        model: str,
        name: str,
        instructions: OptionalNullable[str] = UNSET,
        tools: Optional[
            Union[
                List[models.AgentCreationRequestTools],
                List[models.AgentCreationRequestToolsTypedDict],
            ]
        ] = None,
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        description: OptionalNullable[str] = UNSET,
        handoffs: OptionalNullable[List[str]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Create a agent that can be used within a conversation.

        Create a new agent giving it instructions, tools, description. The agent is then available to be used as a regular assistant in a conversation or as part of an agent pool from which it can be used.

        :param model:
        :param name:
        :param instructions: Instruction prompt the model will follow during the conversation.
        :param tools: List of tools which are available to the model during the conversation.
        :param completion_args: White-listed arguments from the completion API
        :param description:
        :param handoffs:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentCreationRequest(
            instructions=instructions,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.AgentCreationRequestTools]]
            ),
            completion_args=utils.get_pydantic_model(
                completion_args, Optional[models.CompletionArgs]
            ),
            model=model,
            name=name,
            description=description,
            handoffs=handoffs,
        )

        req = self._build_request(
            method="POST",
            path="/v1/agents",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentCreationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_create",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def create_async(
        self,
        *,
        model: str,
        name: str,
        instructions: OptionalNullable[str] = UNSET,
        tools: Optional[
            Union[
                List[models.AgentCreationRequestTools],
                List[models.AgentCreationRequestToolsTypedDict],
            ]
        ] = None,
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        description: OptionalNullable[str] = UNSET,
        handoffs: OptionalNullable[List[str]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Create a agent that can be used within a conversation.

        Create a new agent giving it instructions, tools, description. The agent is then available to be used as a regular assistant in a conversation or as part of an agent pool from which it can be used.

        :param model:
        :param name:
        :param instructions: Instruction prompt the model will follow during the conversation.
        :param tools: List of tools which are available to the model during the conversation.
        :param completion_args: White-listed arguments from the completion API
        :param description:
        :param handoffs:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentCreationRequest(
            instructions=instructions,
            tools=utils.get_pydantic_model(
                tools, Optional[List[models.AgentCreationRequestTools]]
            ),
            completion_args=utils.get_pydantic_model(
                completion_args, Optional[models.CompletionArgs]
            ),
            model=model,
            name=name,
            description=description,
            handoffs=handoffs,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/agents",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.AgentCreationRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_create",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def list(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 20,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> List[models.Agent]:
        r"""List agent entities.

        Retrieve a list of agent entities sorted by creation time.

        :param page:
        :param page_size:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsListRequest(
            page=page,
            page_size=page_size,
        )

        req = self._build_request(
            method="GET",
            path="/v1/agents",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_list",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, List[models.Agent])
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 20,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> List[models.Agent]:
        r"""List agent entities.

        Retrieve a list of agent entities sorted by creation time.

        :param page:
        :param page_size:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsListRequest(
            page=page,
            page_size=page_size,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/agents",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_list",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, List[models.Agent])
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get(
        self,
        *,
        agent_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Retrieve an agent entity.

        Given an agent retrieve an agent entity with its attributes.

        :param agent_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsGetRequest(
            agent_id=agent_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/agents/{agent_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_async(
        self,
        *,
        agent_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Retrieve an agent entity.

        Given an agent retrieve an agent entity with its attributes.

        :param agent_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsGetRequest(
            agent_id=agent_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/agents/{agent_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def update(
        self,
        *,
        agent_id: str,
        instructions: OptionalNullable[str] = UNSET,
        tools: Optional[
            Union[
                List[models.AgentUpdateRequestTools],
                List[models.AgentUpdateRequestToolsTypedDict],
            ]
        ] = None,
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        model: OptionalNullable[str] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        handoffs: OptionalNullable[List[str]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Update an agent entity.

        Update an agent attributes and create a new version.

        :param agent_id:
        :param instructions: Instruction prompt the model will follow during the conversation.
        :param tools: List of tools which are available to the model during the conversation.
        :param completion_args: White-listed arguments from the completion API
        :param model:
        :param name:
        :param description:
        :param handoffs:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsUpdateRequest(
            agent_id=agent_id,
            agent_update_request=models.AgentUpdateRequest(
                instructions=instructions,
                tools=utils.get_pydantic_model(
                    tools, Optional[List[models.AgentUpdateRequestTools]]
                ),
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
                model=model,
                name=name,
                description=description,
                handoffs=handoffs,
            ),
        )

        req = self._build_request(
            method="PATCH",
            path="/v1/agents/{agent_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.agent_update_request,
                False,
                False,
                "json",
                models.AgentUpdateRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_update",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def update_async(
        self,
        *,
        agent_id: str,
        instructions: OptionalNullable[str] = UNSET,
        tools: Optional[
            Union[
                List[models.AgentUpdateRequestTools],
                List[models.AgentUpdateRequestToolsTypedDict],
            ]
        ] = None,
        completion_args: Optional[
            Union[models.CompletionArgs, models.CompletionArgsTypedDict]
        ] = None,
        model: OptionalNullable[str] = UNSET,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        handoffs: OptionalNullable[List[str]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Update an agent entity.

        Update an agent attributes and create a new version.

        :param agent_id:
        :param instructions: Instruction prompt the model will follow during the conversation.
        :param tools: List of tools which are available to the model during the conversation.
        :param completion_args: White-listed arguments from the completion API
        :param model:
        :param name:
        :param description:
        :param handoffs:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsUpdateRequest(
            agent_id=agent_id,
            agent_update_request=models.AgentUpdateRequest(
                instructions=instructions,
                tools=utils.get_pydantic_model(
                    tools, Optional[List[models.AgentUpdateRequestTools]]
                ),
                completion_args=utils.get_pydantic_model(
                    completion_args, Optional[models.CompletionArgs]
                ),
                model=model,
                name=name,
                description=description,
                handoffs=handoffs,
            ),
        )

        req = self._build_request_async(
            method="PATCH",
            path="/v1/agents/{agent_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.agent_update_request,
                False,
                False,
                "json",
                models.AgentUpdateRequest,
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_update",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def update_version(
        self,
        *,
        agent_id: str,
        version: int,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Update an agent version.

        Switch the version of an agent.

        :param agent_id:
        :param version:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsUpdateVersionRequest(
            agent_id=agent_id,
            version=version,
        )

        req = self._build_request(
            method="PATCH",
            path="/v1/agents/{agent_id}/version",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_update_version",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def update_version_async(
        self,
        *,
        agent_id: str,
        version: int,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.Agent:
        r"""Update an agent version.

        Switch the version of an agent.

        :param agent_id:
        :param version:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.AgentsAPIV1AgentsUpdateVersionRequest(
            agent_id=agent_id,
            version=version,
        )

        req = self._build_request_async(
            method="PATCH",
            path="/v1/agents/{agent_id}/version",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="agents_api_v1_agents_update_version",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.Agent)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/mistral_jobs.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from datetime import datetime
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, Dict, List, Mapping, Optional


class MistralJobs(BaseSDK):
    def list(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        model: OptionalNullable[str] = UNSET,
        metadata: OptionalNullable[Dict[str, Any]] = UNSET,
        created_after: OptionalNullable[datetime] = UNSET,
        created_by_me: Optional[bool] = False,
        status: OptionalNullable[List[models.BatchJobStatus]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobsOut:
        r"""Get Batch Jobs

        Get a list of batch jobs for your organization and user.

        :param page:
        :param page_size:
        :param model:
        :param metadata:
        :param created_after:
        :param created_by_me:
        :param status:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchGetBatchJobsRequest(
            page=page,
            page_size=page_size,
            model=model,
            metadata=metadata,
            created_after=created_after,
            created_by_me=created_by_me,
            status=status,
        )

        req = self._build_request(
            method="GET",
            path="/v1/batch/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_get_batch_jobs",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobsOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        page: Optional[int] = 0,
        page_size: Optional[int] = 100,
        model: OptionalNullable[str] = UNSET,
        metadata: OptionalNullable[Dict[str, Any]] = UNSET,
        created_after: OptionalNullable[datetime] = UNSET,
        created_by_me: Optional[bool] = False,
        status: OptionalNullable[List[models.BatchJobStatus]] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobsOut:
        r"""Get Batch Jobs

        Get a list of batch jobs for your organization and user.

        :param page:
        :param page_size:
        :param model:
        :param metadata:
        :param created_after:
        :param created_by_me:
        :param status:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchGetBatchJobsRequest(
            page=page,
            page_size=page_size,
            model=model,
            metadata=metadata,
            created_after=created_after,
            created_by_me=created_by_me,
            status=status,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/batch/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_get_batch_jobs",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobsOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def create(
        self,
        *,
        input_files: List[str],
        endpoint: models.APIEndpoint,
        model: str,
        metadata: OptionalNullable[Dict[str, str]] = UNSET,
        timeout_hours: Optional[int] = 24,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Create Batch Job

        Create a new batch job, it will be queued for processing.

        :param input_files:
        :param endpoint:
        :param model:
        :param metadata:
        :param timeout_hours:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.BatchJobIn(
            input_files=input_files,
            endpoint=endpoint,
            model=model,
            metadata=metadata,
            timeout_hours=timeout_hours,
        )

        req = self._build_request(
            method="POST",
            path="/v1/batch/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.BatchJobIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_create_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def create_async(
        self,
        *,
        input_files: List[str],
        endpoint: models.APIEndpoint,
        model: str,
        metadata: OptionalNullable[Dict[str, str]] = UNSET,
        timeout_hours: Optional[int] = 24,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Create Batch Job

        Create a new batch job, it will be queued for processing.

        :param input_files:
        :param endpoint:
        :param model:
        :param metadata:
        :param timeout_hours:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.BatchJobIn(
            input_files=input_files,
            endpoint=endpoint,
            model=model,
            metadata=metadata,
            timeout_hours=timeout_hours,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/batch/jobs",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.BatchJobIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_create_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def get(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Get Batch Job

        Get a batch job details by its UUID.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchGetBatchJobRequest(
            job_id=job_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/batch/jobs/{job_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_get_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def get_async(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Get Batch Job

        Get a batch job details by its UUID.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchGetBatchJobRequest(
            job_id=job_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/batch/jobs/{job_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_get_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def cancel(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Cancel Batch Job

        Request the cancellation of a batch job.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchCancelBatchJobRequest(
            job_id=job_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/batch/jobs/{job_id}/cancel",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_cancel_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def cancel_async(
        self,
        *,
        job_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.BatchJobOut:
        r"""Cancel Batch Job

        Request the cancellation of a batch job.

        :param job_id:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesBatchCancelBatchJobRequest(
            job_id=job_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/batch/jobs/{job_id}/cancel",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_batch_cancel_batch_job",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.BatchJobOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/models_.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, Mapping, Optional


class Models(BaseSDK):
    r"""Model Management API"""

    def list(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModelList:
        r"""List Models

        List all models available to the user.

        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request(
            method="GET",
            path="/v1/models",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="list_models_v1_models_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModelList)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def list_async(
        self,
        *,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ModelList:
        r"""List Models

        List all models available to the user.

        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)
        req = self._build_request_async(
            method="GET",
            path="/v1/models",
            base_url=base_url,
            url_variables=url_variables,
            request=None,
            request_body_required=False,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="list_models_v1_models_get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ModelList)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def retrieve(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet:
        r"""Retrieve Model

        Retrieve a model information.

        :param model_id: The ID of the model to retrieve.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.RetrieveModelV1ModelsModelIDGetRequest(
            model_id=model_id,
        )

        req = self._build_request(
            method="GET",
            path="/v1/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="retrieve_model_v1_models__model_id__get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet,
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def retrieve_async(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet:
        r"""Retrieve Model

        Retrieve a model information.

        :param model_id: The ID of the model to retrieve.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.RetrieveModelV1ModelsModelIDGetRequest(
            model_id=model_id,
        )

        req = self._build_request_async(
            method="GET",
            path="/v1/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="retrieve_model_v1_models__model_id__get",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.RetrieveModelV1ModelsModelIDGetResponseRetrieveModelV1ModelsModelIDGet,
            )
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def delete(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.DeleteModelOut:
        r"""Delete Model

        Delete a fine-tuned model.

        :param model_id: The ID of the model to delete.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.DeleteModelV1ModelsModelIDDeleteRequest(
            model_id=model_id,
        )

        req = self._build_request(
            method="DELETE",
            path="/v1/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="delete_model_v1_models__model_id__delete",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.DeleteModelOut)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def delete_async(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.DeleteModelOut:
        r"""Delete Model

        Delete a fine-tuned model.

        :param model_id: The ID of the model to delete.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.DeleteModelV1ModelsModelIDDeleteRequest(
            model_id=model_id,
        )

        req = self._build_request_async(
            method="DELETE",
            path="/v1/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="delete_model_v1_models__model_id__delete",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.DeleteModelOut)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def update(
        self,
        *,
        model_id: str,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningUpdateFineTunedModelResponse:
        r"""Update Fine Tuned Model

        Update a model name or description.

        :param model_id: The ID of the model to update.
        :param name:
        :param description:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningUpdateFineTunedModelRequest(
            model_id=model_id,
            update_ft_model_in=models.UpdateFTModelIn(
                name=name,
                description=description,
            ),
        )

        req = self._build_request(
            method="PATCH",
            path="/v1/fine_tuning/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.update_ft_model_in, False, False, "json", models.UpdateFTModelIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_update_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.JobsAPIRoutesFineTuningUpdateFineTunedModelResponse,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def update_async(
        self,
        *,
        model_id: str,
        name: OptionalNullable[str] = UNSET,
        description: OptionalNullable[str] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.JobsAPIRoutesFineTuningUpdateFineTunedModelResponse:
        r"""Update Fine Tuned Model

        Update a model name or description.

        :param model_id: The ID of the model to update.
        :param name:
        :param description:
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningUpdateFineTunedModelRequest(
            model_id=model_id,
            update_ft_model_in=models.UpdateFTModelIn(
                name=name,
                description=description,
            ),
        )

        req = self._build_request_async(
            method="PATCH",
            path="/v1/fine_tuning/models/{model_id}",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request.update_ft_model_in, False, False, "json", models.UpdateFTModelIn
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_update_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(
                http_res.text,
                models.JobsAPIRoutesFineTuningUpdateFineTunedModelResponse,
            )
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def archive(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ArchiveFTModelOut:
        r"""Archive Fine Tuned Model

        Archive a fine-tuned model.

        :param model_id: The ID of the model to archive.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningArchiveFineTunedModelRequest(
            model_id=model_id,
        )

        req = self._build_request(
            method="POST",
            path="/v1/fine_tuning/models/{model_id}/archive",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_archive_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ArchiveFTModelOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def archive_async(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ArchiveFTModelOut:
        r"""Archive Fine Tuned Model

        Archive a fine-tuned model.

        :param model_id: The ID of the model to archive.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningArchiveFineTunedModelRequest(
            model_id=model_id,
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/fine_tuning/models/{model_id}/archive",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_archive_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.ArchiveFTModelOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    def unarchive(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.UnarchiveFTModelOut:
        r"""Unarchive Fine Tuned Model

        Un-archive a fine-tuned model.

        :param model_id: The ID of the model to unarchive.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest(
            model_id=model_id,
        )

        req = self._build_request(
            method="DELETE",
            path="/v1/fine_tuning/models/{model_id}/archive",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_unarchive_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.UnarchiveFTModelOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def unarchive_async(
        self,
        *,
        model_id: str,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.UnarchiveFTModelOut:
        r"""Unarchive Fine Tuned Model

        Un-archive a fine-tuned model.

        :param model_id: The ID of the model to unarchive.
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.JobsAPIRoutesFineTuningUnarchiveFineTunedModelRequest(
            model_id=model_id,
        )

        req = self._build_request_async(
            method="DELETE",
            path="/v1/fine_tuning/models/{model_id}/archive",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=False,
            request_has_path_params=True,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="jobs_api_routes_fine_tuning_unarchive_fine_tuned_model",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["4XX", "5XX"],
            retry_config=retry_config,
        )

        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.UnarchiveFTModelOut)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/ocr.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from mistralai import models, utils
from mistralai._hooks import HookContext
from mistralai.types import Nullable, OptionalNullable, UNSET
from mistralai.utils import get_security_from_env
from typing import Any, List, Mapping, Optional, Union


class Ocr(BaseSDK):
    r"""OCR API"""

    def process(
        self,
        *,
        model: Nullable[str],
        document: Union[models.Document, models.DocumentTypedDict],
        id: Optional[str] = None,
        pages: OptionalNullable[List[int]] = UNSET,
        include_image_base64: OptionalNullable[bool] = UNSET,
        image_limit: OptionalNullable[int] = UNSET,
        image_min_size: OptionalNullable[int] = UNSET,
        bbox_annotation_format: OptionalNullable[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = UNSET,
        document_annotation_format: OptionalNullable[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.OCRResponse:
        r"""OCR

        :param model:
        :param document: Document to run OCR on
        :param id:
        :param pages: Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0
        :param include_image_base64: Include image URLs in response
        :param image_limit: Max images to extract
        :param image_min_size: Minimum height and width of image to extract
        :param bbox_annotation_format: Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field
        :param document_annotation_format: Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.OCRRequest(
            model=model,
            id=id,
            document=utils.get_pydantic_model(document, models.Document),
            pages=pages,
            include_image_base64=include_image_base64,
            image_limit=image_limit,
            image_min_size=image_min_size,
            bbox_annotation_format=utils.get_pydantic_model(
                bbox_annotation_format, OptionalNullable[models.ResponseFormat]
            ),
            document_annotation_format=utils.get_pydantic_model(
                document_annotation_format, OptionalNullable[models.ResponseFormat]
            ),
        )

        req = self._build_request(
            method="POST",
            path="/v1/ocr",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.OCRRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="ocr_v1_ocr_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.OCRResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = utils.stream_to_text(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )

    async def process_async(
        self,
        *,
        model: Nullable[str],
        document: Union[models.Document, models.DocumentTypedDict],
        id: Optional[str] = None,
        pages: OptionalNullable[List[int]] = UNSET,
        include_image_base64: OptionalNullable[bool] = UNSET,
        image_limit: OptionalNullable[int] = UNSET,
        image_min_size: OptionalNullable[int] = UNSET,
        bbox_annotation_format: OptionalNullable[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = UNSET,
        document_annotation_format: OptionalNullable[
            Union[models.ResponseFormat, models.ResponseFormatTypedDict]
        ] = UNSET,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.OCRResponse:
        r"""OCR

        :param model:
        :param document: Document to run OCR on
        :param id:
        :param pages: Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0
        :param include_image_base64: Include image URLs in response
        :param image_limit: Max images to extract
        :param image_min_size: Minimum height and width of image to extract
        :param bbox_annotation_format: Structured output class for extracting useful information from each extracted bounding box / image from document. Only json_schema is valid for this field
        :param document_annotation_format: Structured output class for extracting useful information from the entire document. Only json_schema is valid for this field
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.OCRRequest(
            model=model,
            id=id,
            document=utils.get_pydantic_model(document, models.Document),
            pages=pages,
            include_image_base64=include_image_base64,
            image_limit=image_limit,
            image_min_size=image_min_size,
            bbox_annotation_format=utils.get_pydantic_model(
                bbox_annotation_format, OptionalNullable[models.ResponseFormat]
            ),
            document_annotation_format=utils.get_pydantic_model(
                document_annotation_format, OptionalNullable[models.ResponseFormat]
            ),
        )

        req = self._build_request_async(
            method="POST",
            path="/v1/ocr",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.OCRRequest
            ),
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                base_url=base_url or "",
                operation_id="ocr_v1_ocr_post",
                oauth2_scopes=[],
                security_source=get_security_from_env(
                    self.sdk_configuration.security, models.Security
                ),
            ),
            request=req,
            error_status_codes=["422", "4XX", "5XX"],
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            return utils.unmarshal_json(http_res.text, models.OCRResponse)
        if utils.match_response(http_res, "422", "application/json"):
            response_data = utils.unmarshal_json(
                http_res.text, models.HTTPValidationErrorData
            )
            raise models.HTTPValidationError(data=response_data)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise models.SDKError(
                "API error occurred", http_res.status_code, http_res_text, http_res
            )

        content_type = http_res.headers.get("Content-Type")
        http_res_text = await utils.stream_to_text_async(http_res)
        raise models.SDKError(
            f"Unexpected response received (code: {http_res.status_code}, type: {content_type})",
            http_res.status_code,
            http_res_text,
            http_res,
        )
</file>

<file path="src/mistralai/py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="src/mistralai/sdk.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from .httpclient import AsyncHttpClient, ClientOwner, HttpClient, close_clients
from .sdkconfiguration import SDKConfiguration
from .utils.logger import Logger, get_default_logger
from .utils.retries import RetryConfig
import httpx
from mistralai import models, utils
from mistralai._hooks import SDKHooks
from mistralai.agents import Agents
from mistralai.batch import Batch
from mistralai.beta import Beta
from mistralai.chat import Chat
from mistralai.classifiers import Classifiers
from mistralai.embeddings import Embeddings
from mistralai.files import Files
from mistralai.fim import Fim
from mistralai.fine_tuning import FineTuning
from mistralai.models_ import Models
from mistralai.ocr import Ocr
from mistralai.types import OptionalNullable, UNSET
from typing import Any, Callable, Dict, Optional, Union, cast
import weakref


class Mistral(BaseSDK):
    r"""Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it."""

    models: Models
    r"""Model Management API"""
    beta: Beta
    files: Files
    r"""Files API"""
    fine_tuning: FineTuning
    batch: Batch
    chat: Chat
    r"""Chat Completion API."""
    fim: Fim
    r"""Fill-in-the-middle API."""
    agents: Agents
    r"""Agents API."""
    embeddings: Embeddings
    r"""Embeddings API."""
    classifiers: Classifiers
    r"""Classifiers API."""
    ocr: Ocr
    r"""OCR API"""

    def __init__(
        self,
        api_key: Optional[Union[Optional[str], Callable[[], Optional[str]]]] = None,
        server: Optional[str] = None,
        server_url: Optional[str] = None,
        url_params: Optional[Dict[str, str]] = None,
        client: Optional[HttpClient] = None,
        async_client: Optional[AsyncHttpClient] = None,
        retry_config: OptionalNullable[RetryConfig] = UNSET,
        timeout_ms: Optional[int] = None,
        debug_logger: Optional[Logger] = None,
    ) -> None:
        r"""Instantiates the SDK configuring it with the provided parameters.

        :param api_key: The api_key required for authentication
        :param server: The server by name to use for all methods
        :param server_url: The server URL to use for all methods
        :param url_params: Parameters to optionally template the server URL with
        :param client: The HTTP client to use for all synchronous methods
        :param async_client: The Async HTTP client to use for all asynchronous methods
        :param retry_config: The retry configuration to use for all supported methods
        :param timeout_ms: Optional request timeout applied to each operation in milliseconds
        """
        client_supplied = True
        if client is None:
            client = httpx.Client()
            client_supplied = False

        assert issubclass(
            type(client), HttpClient
        ), "The provided client must implement the HttpClient protocol."

        async_client_supplied = True
        if async_client is None:
            async_client = httpx.AsyncClient()
            async_client_supplied = False

        if debug_logger is None:
            debug_logger = get_default_logger()

        assert issubclass(
            type(async_client), AsyncHttpClient
        ), "The provided async_client must implement the AsyncHttpClient protocol."

        security: Any = None
        if callable(api_key):
            # pylint: disable=unnecessary-lambda-assignment
            security = lambda: models.Security(api_key=api_key())
        else:
            security = models.Security(api_key=api_key)

        if server_url is not None:
            if url_params is not None:
                server_url = utils.template_url(server_url, url_params)

        BaseSDK.__init__(
            self,
            SDKConfiguration(
                client=client,
                client_supplied=client_supplied,
                async_client=async_client,
                async_client_supplied=async_client_supplied,
                security=security,
                server_url=server_url,
                server=server,
                retry_config=retry_config,
                timeout_ms=timeout_ms,
                debug_logger=debug_logger,
            ),
        )

        hooks = SDKHooks()

        current_server_url, *_ = self.sdk_configuration.get_server_details()
        server_url, self.sdk_configuration.client = hooks.sdk_init(
            current_server_url, client
        )
        if current_server_url != server_url:
            self.sdk_configuration.server_url = server_url

        # pylint: disable=protected-access
        self.sdk_configuration.__dict__["_hooks"] = hooks

        weakref.finalize(
            self,
            close_clients,
            cast(ClientOwner, self.sdk_configuration),
            self.sdk_configuration.client,
            self.sdk_configuration.client_supplied,
            self.sdk_configuration.async_client,
            self.sdk_configuration.async_client_supplied,
        )

        self._init_sdks()

    def _init_sdks(self):
        self.models = Models(self.sdk_configuration)
        self.beta = Beta(self.sdk_configuration)
        self.files = Files(self.sdk_configuration)
        self.fine_tuning = FineTuning(self.sdk_configuration)
        self.batch = Batch(self.sdk_configuration)
        self.chat = Chat(self.sdk_configuration)
        self.fim = Fim(self.sdk_configuration)
        self.agents = Agents(self.sdk_configuration)
        self.embeddings = Embeddings(self.sdk_configuration)
        self.classifiers = Classifiers(self.sdk_configuration)
        self.ocr = Ocr(self.sdk_configuration)

    def __enter__(self):
        return self

    async def __aenter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.client is not None
            and not self.sdk_configuration.client_supplied
        ):
            self.sdk_configuration.client.close()
        self.sdk_configuration.client = None

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if (
            self.sdk_configuration.async_client is not None
            and not self.sdk_configuration.async_client_supplied
        ):
            await self.sdk_configuration.async_client.aclose()
        self.sdk_configuration.async_client = None
</file>

<file path="src/mistralai/sdkconfiguration.py">
"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from ._hooks import SDKHooks
from ._version import (
    __gen_version__,
    __openapi_doc_version__,
    __user_agent__,
    __version__,
)
from .httpclient import AsyncHttpClient, HttpClient
from .utils import Logger, RetryConfig, remove_suffix
from dataclasses import dataclass
from mistralai import models
from mistralai.types import OptionalNullable, UNSET
from pydantic import Field
from typing import Callable, Dict, Optional, Tuple, Union


SERVER_EU = "eu"
r"""EU Production server"""
SERVERS = {
    SERVER_EU: "https://api.mistral.ai",
}
"""Contains the list of servers available to the SDK"""


@dataclass
class SDKConfiguration:
    client: Union[HttpClient, None]
    client_supplied: bool
    async_client: Union[AsyncHttpClient, None]
    async_client_supplied: bool
    debug_logger: Logger
    security: Optional[Union[models.Security, Callable[[], models.Security]]] = None
    server_url: Optional[str] = ""
    server: Optional[str] = ""
    language: str = "python"
    openapi_doc_version: str = __openapi_doc_version__
    sdk_version: str = __version__
    gen_version: str = __gen_version__
    user_agent: str = __user_agent__
    retry_config: OptionalNullable[RetryConfig] = Field(default_factory=lambda: UNSET)
    timeout_ms: Optional[int] = None

    def __post_init__(self):
        self._hooks = SDKHooks()

    def get_server_details(self) -> Tuple[str, Dict[str, str]]:
        if self.server_url is not None and self.server_url:
            return remove_suffix(self.server_url, "/"), {}
        if not self.server:
            self.server = SERVER_EU

        if self.server not in SERVERS:
            raise ValueError(f'Invalid server "{self.server}"')

        return SERVERS[self.server], {}

    def get_hooks(self) -> SDKHooks:
        return self._hooks
</file>

<file path="src/mistralai/version.py">
from importlib import metadata

try:
    __version__ = metadata.version(__package__)
except metadata.PackageNotFoundError:
    # Case where package metadata is not available.
    __version__ = ""
</file>

<file path=".genignore">
pyproject.toml
examples/*
src/mistral/extra/*
pylintrc
</file>

<file path=".gitattributes">
# This allows generated code to be indexed correctly
*.py linguist-generated=false
</file>

<file path=".gitignore">
.vscode/
.speakeasy/reports
README-PYPI.md
.venv/
pyrightconfig.json
src/*.egg-info/
.python-version
.DS_Store
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
# C extensions
*.so
# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec
# Installer logs
pip-log.txt
pip-delete-this-directory.txt
# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
# Translations
*.mo
*.pot
# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal
# Flask stuff:
instance/
.webassets-cache
# Scrapy stuff:
.scrapy
# Sphinx documentation
docs/_build/
# PyBuilder
.pybuilder/
target/
# Jupyter Notebook
.ipynb_checkpoints
# IPython
profile_default/
ipython_config.py
# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version
# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock
# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/
# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/
# Celery stuff
celerybeat-schedule
celerybeat.pid
# SageMath parsed files
*.sage.py
# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
# Spyder project settings
.spyderproject
.spyproject
# Rope project settings
.ropeproject
# mkdocs documentation
/site
# mypy
.mypy_cache/
.dmypy.json
dmypy.json
# Pyre type checker
.pyre/
# pytype static type analyzer
.pytype/
# Cython debug symbols
cython_debug/
# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
</file>

<file path=".pre-commit-config.yaml">
repos:
- repo: https://github.com/astral-sh/ruff-pre-commit
  rev: v0.11.10
  hooks:
    - id: ruff
      args: [--fix]
      files: ^(example/|src/mistralai/).*\.py$
      exclude: ^src/mistralai/(__init__|sdkhooks|types)\.py$
- repo: https://github.com/RobertCraigie/pyright-python
  rev: v1.1.401
  hooks:
    - id: pyright
      files: ^(example/|src/mistralai/).*\.py$
      exclude: ^src/mistralai/(__init__|sdkhooks|types)\.py$
- repo: https://github.com/pre-commit/mirrors-mypy
  rev: v1.15.0
  hooks:
    - id: mypy
      files: ^(example/|src/mistralai/).*\.py$
      exclude: ^src/mistralai/(__init__|sdkhooks|types)\.py$
</file>

<file path="CONTRIBUTING.md">
# Contributing to This Repository

Thank you for your interest in contributing to this repository. Please note that this repository contains generated code. As such, we do not accept direct changes or pull requests. Instead, we encourage you to follow the guidelines below to report issues and suggest improvements.

## How to Report Issues

If you encounter any bugs or have suggestions for improvements, please open an issue on GitHub. When reporting an issue, please provide as much detail as possible to help us reproduce the problem. This includes:

- A clear and descriptive title
- Steps to reproduce the issue
- Expected and actual behavior
- Any relevant logs, screenshots, or error messages
- Information about your environment (e.g., operating system, software versions)
    - For example can be collected using the `npx envinfo` command from your terminal if you have Node.js installed

## Issue Triage and Upstream Fixes

We will review and triage issues as quickly as possible. Our goal is to address bugs and incorporate improvements in the upstream source code. Fixes will be included in the next generation of the generated code.

## Contact

If you have any questions or need further assistance, please feel free to reach out by opening an issue.

Thank you for your understanding and cooperation!

The Maintainers
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2024 Mistral AI

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="MIGRATION.md">
# Migration Guide for MistralAI Client from 0.\*.\* to 1.0.0

We have made significant changes to the `mistralai` library to improve its usability and consistency. This guide will help you migrate your code from the old client to the new one.

## Major Changes

1. **Unified Client Class**:
   - The `MistralClient` and `MistralAsyncClient` classes have been consolidated into a single `Mistral` class.
   - This simplifies the API by providing a single entry point for both synchronous and asynchronous operations.

2. **Method Names and Structure**:
   - The method names and structure have been updated for better clarity and consistency.
   - For example:
      - `client.chat` is now `client.chat.complete` for non-streaming calls
      - `client.chat_stream` is now `client.chat.stream` for streaming calls
      - Async `client.chat` is now `client.chat.complete_async` for async non-streaming calls
      - Async `client.chat_stream` is now `client.chat.stream_async` for async streaming calls


## Method changes

### Sync

| Old Methods                | New Methods                      |
| -------------------------- | -------------------------------- |
| `MistralCLient`            | `Mistral`                        |
| `client.chat`              | `client.chat.complete`           |
| `client.chat_stream`       | `client.chat.stream`             |
| `client.completions`       | `client.fim.complete`            |
| `client.completions_stream`| `client.fim.stream`              |
| `client.embeddings`        | `client.embeddings.create`       |
| `client.list_models`       | `client.models.list`             |
| `client.delete_model`      | `client.models.delete`           |
| `client.files.create`      | `client.files.upload`            |
| `client.files.list`        | `client.files.list`              |
| `client.files.retrieve`    | `client.files.retrieve`          |
| `client.files.delete`      | `client.files.delete`            |
| `client.jobs.create`       | `client.fine_tuning.jobs.create` |
| `client.jobs.list`         | `client.fine_tuning.jobs.list`   |
| `client.jobs.retrieve`     | `client.fine_tuning.jobs.get`    |
| `client.jobs.cancel`       | `client.fine_tuning.jobs.cancel` |

### Async

| Old Methods                      | New Methods                            |
| -------------------------------- | -------------------------------------- |
| `MistralAsyncClient`             | `Mistral`                              |
| `async_client.chat`              | `client.chat.complete_async`           |
| `async_client.chat_stream`       | `client.chat.stream_async`             |
| `async_client.completions`       | `client.fim.complete_async`            |
| `async_client.completions_stream`| `client.fim.stream_async`              |
| `async_client.embeddings`        | `client.embeddings.create_async`       |
| `async_client.list_models`       | `client.models.list_async`             |
| `async_client.delete_model`      | `client.models.delete_async`           |
| `async_client.files.create`      | `client.files.upload_async`            |
| `async_client.files.list`        | `client.files.list_async`              |
| `async_client.files.retrieve`    | `client.files.retrieve_async`          |
| `async_client.files.delete`      | `client.files.delete_async`            |
| `async_client.jobs.create`       | `client.fine_tuning.jobs.create_async` |
| `async_client.jobs.list`         | `client.fine_tuning.jobs.list_async`   |
| `async_client.jobs.retrieve`     | `client.fine_tuning.jobs.get_async`    |
| `async_client.jobs.cancel`       | `client.fine_tuning.jobs.cancel_async` |

### Message Changes

The `ChatMessage` class has been replaced with a more flexible system. You can now use the `SystemMessage`, `UserMessage`, `AssistantMessage`, and `ToolMessage` classes to create messages.

The return object of the stream call methods have been modified to `chunk.data.choices[0].delta.content` from `chunk.choices[0].delta.content`. 

## Example Migrations

### Example 1: Non-Streaming Chat

**Old:**
```python
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = MistralClient(api_key=api_key)

messages = [
    ChatMessage(role="user", content="What is the best French cheese?")
]

# No streaming
chat_response = client.chat(
    model=model,
    messages=messages,
)

print(chat_response.choices[0].message.content)
```

**New:**

```python
import os

from mistralai import Mistral, UserMessage

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = Mistral(api_key=api_key)

messages = [
    {
        "role": "user",
        "content": "What is the best French cheese?",
    },
]
# Or using the new message classes
# messages = [
#     UserMessage(content="What is the best French cheese?"),
# ]

chat_response = client.chat.complete(
    model=model,
    messages=messages,
)

print(chat_response.choices[0].message.content)
```

### Example 2: Streaming Chat

**Old:**

```python
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = MistralClient(api_key=api_key)

messages = [
    ChatMessage(role="user", content="What is the best French cheese?")
]

# With streaming
stream_response = client.chat_stream(model=model, messages=messages)

for chunk in stream_response:
    print(chunk.choices[0].delta.content)
```
**New:**
```python
import os

from mistralai import Mistral, UserMessage

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = Mistral(api_key=api_key)

messages = [
    {
        "role": "user",
        "content": "What is the best French cheese?",
    },
]
# Or using the new message classes
# messages = [
#     UserMessage(content="What is the best French cheese?"),
# ]

stream_response = client.chat.stream(
    model=model,
    messages=messages,
)

for chunk in stream_response:
    print(chunk.data.choices[0].delta.content)

```

### Example 3: Async

**Old:**
```python
from mistralai.async_client import MistralAsyncClient
from mistralai.models.chat_completion import ChatMessage

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = MistralAsyncClient(api_key=api_key)

messages = [
    ChatMessage(role="user", content="What is the best French cheese?")
]

# With async
async_response = client.chat_stream(model=model, messages=messages)

async for chunk in async_response:
    print(chunk.choices[0].delta.content)
```

**New:**
```python
import asyncio
import os

from mistralai import Mistral, UserMessage


async def main():
    client = Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    )

    messages = [
        {
            "role": "user",
            "content": "What is the best French cheese?",
        },
    ]
    # Or using the new message classes
    # messages = [
    #     UserMessage(
    #         content="What is the best French cheese?",
    #     ),
    # ]
    async_response = await client.chat.stream_async(
        messages=messages,
        model="mistral-large-latest",
    )

    async for chunk in async_response:
        print(chunk.data.choices[0].delta.content)


asyncio.run(main())
```
</file>

<file path="OLD-README.md">
# Mistral Python Client

This client is inspired from [cohere-python](https://github.com/cohere-ai/cohere-python)

You can use the Mistral Python client to interact with the Mistral AI API.

## Installing

```bash
pip install mistralai
```

### From Source

This client uses `poetry` as a dependency and virtual environment manager.

You can install poetry with

```bash
pip install poetry
```

`poetry` will set up a virtual environment and install dependencies with the following command:

```bash
poetry install
```

## Run examples

You can run the examples in the `examples/` directory using `poetry run` or by entering the virtual environment using `poetry shell`.

### API Key Setup

Running the examples requires a Mistral AI API key.

1. Get your own Mistral API Key: <https://docs.mistral.ai/#api-access>
2. Set your Mistral API Key as an environment variable. You only need to do this once.

```bash
# set Mistral API Key (using zsh for example)
$ echo 'export MISTRAL_API_KEY=[your_key_here]' >> ~/.zshenv

# reload the environment (or just quit and open a new terminal)
$ source ~/.zshenv
```

### Using poetry run

```bash
cd examples
poetry run python chat_no_streaming.py
```

### Using poetry shell

```bash
poetry shell
cd examples

>> python chat_no_streaming.py
```
</file>

<file path="poetry.toml">
[virtualenvs]
in-project = true
</file>

<file path="py.typed">
# Marker file for PEP 561. The package enables type hints.
</file>

<file path="pylintrc">
[MAIN]

# Analyse import fallback blocks. This can be used to support both Python 2 and
# 3 compatible code, which means that the block might have code that exists
# only in one or another interpreter, leading to false positives when analysed.
analyse-fallback-blocks=no

# Clear in-memory caches upon conclusion of linting. Useful if running pylint
# in a server-like mode.
clear-cache-post-run=no

# Load and enable all available extensions. Use --list-extensions to see a list
# all available extensions.
#enable-all-extensions=

# In error mode, messages with a category besides ERROR or FATAL are
# suppressed, and no reports are done by default. Error mode is compatible with
# disabling specific errors.
#errors-only=

# Always return a 0 (non-error) status code, even if lint errors are found.
# This is primarily useful in continuous integration scripts.
#exit-zero=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-allow-list=

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code. (This is an alternative name to extension-pkg-allow-list
# for backward compatibility.)
extension-pkg-whitelist=

# Return non-zero exit code if any of these messages/categories are detected,
# even if score is above --fail-under value. Syntax same as enable. Messages
# specified are enabled, while categories only check already-enabled messages.
fail-on=

# Specify a score threshold under which the program will exit with error.
fail-under=10

# Interpret the stdin as a python script, whose filename needs to be passed as
# the module_or_package argument.
#from-stdin=

# Files or directories to be skipped. They should be base names, not paths.
ignore=CVS

# Add files or directories matching the regular expressions patterns to the
# ignore-list. The regex matches against paths and can be in Posix or Windows
# format. Because '\\' represents the directory delimiter on Windows systems,
# it can't be used as an escape character.
ignore-paths=^src/mistralai/extra/.*$,
             ^src/mistralai/conversations.py

# Files or directories matching the regular expression patterns are skipped.
# The regex matches against base names, not paths. The default value ignores
# Emacs file locks
ignore-patterns=^\.#

# List of module names for which member attributes should not be checked and
# will not be imported (useful for modules/projects where namespaces are
# manipulated during runtime and thus existing member attributes cannot be
# deduced by static analysis). It supports qualified module names, as well as
# Unix pattern matching.
ignored-modules=

# Python code to execute, usually for sys.path manipulation such as
# pygtk.require().
#init-hook=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use, and will cap the count on Windows to
# avoid hangs.
jobs=1

# Control the amount of potential inferred values when inferring a single
# object. This can help the performance when dealing with large functions or
# complex, nested conditions.
limit-inference-results=100

# List of plugins (as comma separated values of python module names) to load,
# usually to register additional checkers.
load-plugins=

# Pickle collected data for later comparisons.
persistent=yes

# Minimum Python version to use for version dependent checks. Will default to
# the version used to run pylint.
py-version=3.9

# Discover python modules and packages in the file system subtree.
recursive=no

# Add paths to the list of the source roots. Supports globbing patterns. The
# source root is an absolute path or a path relative to the current working
# directory used to determine a package namespace for modules located under the
# source root.
source-roots=src

# When enabled, pylint would attempt to guess common misconfiguration and emit
# user-friendly hints instead of false-positive error messages.
suggestion-mode=yes

# Allow loading of arbitrary C extensions. Extensions are imported into the
# active Python interpreter and may run arbitrary code.
unsafe-load-any-extension=no

# In verbose mode, extra non-checker-related info will be displayed.
#verbose=


[BASIC]

# Naming style matching correct argument names.
argument-naming-style=snake_case

# Regular expression matching correct argument names. Overrides argument-
# naming-style. If left empty, argument names will be checked with the set
# naming style.
#argument-rgx=

# Naming style matching correct attribute names.
#attr-naming-style=snake_case

# Regular expression matching correct attribute names. Overrides attr-naming-
# style. If left empty, attribute names will be checked with the set naming
# style.
attr-rgx=[^\W\d][^\W]*|__.*__$

# Bad variable names which should always be refused, separated by a comma.
bad-names=

# Bad variable names regexes, separated by a comma. If names match any regex,
# they will always be refused
bad-names-rgxs=

# Naming style matching correct class attribute names.
class-attribute-naming-style=any

# Regular expression matching correct class attribute names. Overrides class-
# attribute-naming-style. If left empty, class attribute names will be checked
# with the set naming style.
#class-attribute-rgx=

# Naming style matching correct class constant names.
class-const-naming-style=UPPER_CASE

# Regular expression matching correct class constant names. Overrides class-
# const-naming-style. If left empty, class constant names will be checked with
# the set naming style.
#class-const-rgx=

# Naming style matching correct class names.
class-naming-style=PascalCase

# Regular expression matching correct class names. Overrides class-naming-
# style. If left empty, class names will be checked with the set naming style.
#class-rgx=

# Naming style matching correct constant names.
const-naming-style=UPPER_CASE

# Regular expression matching correct constant names. Overrides const-naming-
# style. If left empty, constant names will be checked with the set naming
# style.
#const-rgx=

# Minimum line length for functions/classes that require docstrings, shorter
# ones are exempt.
docstring-min-length=-1

# Naming style matching correct function names.
function-naming-style=snake_case

# Regular expression matching correct function names. Overrides function-
# naming-style. If left empty, function names will be checked with the set
# naming style.
#function-rgx=

# Good variable names which should always be accepted, separated by a comma.
good-names=i,
           j,
           k,
           ex,
           Run,
           _,
           e,
           n,
           id

# Good variable names regexes, separated by a comma. If names match any regex,
# they will always be accepted
good-names-rgxs=

# Include a hint for the correct naming format with invalid-name.
include-naming-hint=no

# Naming style matching correct inline iteration names.
inlinevar-naming-style=any

# Regular expression matching correct inline iteration names. Overrides
# inlinevar-naming-style. If left empty, inline iteration names will be checked
# with the set naming style.
#inlinevar-rgx=

# Naming style matching correct method names.
method-naming-style=snake_case

# Regular expression matching correct method names. Overrides method-naming-
# style. If left empty, method names will be checked with the set naming style.
#method-rgx=

# Naming style matching correct module names.
module-naming-style=snake_case

# Regular expression matching correct module names. Overrides module-naming-
# style. If left empty, module names will be checked with the set naming style.
#module-rgx=

# Colon-delimited sets of names that determine each other's naming style when
# the name regexes allow several styles.
name-group=

# Regular expression which should only match function or class names that do
# not require a docstring.
no-docstring-rgx=^_

# List of decorators that produce properties, such as abc.abstractproperty. Add
# to this list to register other decorators that produce valid properties.
# These decorators are taken in consideration only for invalid-name.
property-classes=abc.abstractproperty

# Regular expression matching correct type alias names. If left empty, type
# alias names will be checked with the set naming style.
typealias-rgx=.*

# Regular expression matching correct type variable names. If left empty, type
# variable names will be checked with the set naming style.
#typevar-rgx=

# Naming style matching correct variable names.
variable-naming-style=snake_case

# Regular expression matching correct variable names. Overrides variable-
# naming-style. If left empty, variable names will be checked with the set
# naming style.
#variable-rgx=


[CLASSES]

# Warn about protected attribute access inside special methods
check-protected-access-in-special-methods=no

# List of method names used to declare (i.e. assign) instance attributes.
defining-attr-methods=__init__,
                      __new__,
                      setUp,
                      asyncSetUp,
                      __post_init__

# List of member names, which should be excluded from the protected access
# warning.
exclude-protected=_asdict,_fields,_replace,_source,_make,os._exit

# List of valid names for the first argument in a class method.
valid-classmethod-first-arg=cls

# List of valid names for the first argument in a metaclass class method.
valid-metaclass-classmethod-first-arg=mcs


[DESIGN]

# List of regular expressions of class ancestor names to ignore when counting
# public methods (see R0903)
exclude-too-few-public-methods=

# List of qualified class names to ignore when counting class parents (see
# R0901)
ignored-parents=

# Maximum number of arguments for function / method.
max-args=5

# Maximum number of attributes for a class (see R0902).
max-attributes=7

# Maximum number of boolean expressions in an if statement (see R0916).
max-bool-expr=5

# Maximum number of branch for function / method body.
max-branches=12

# Maximum number of locals for function / method body.
max-locals=15

# Maximum number of parents for a class (see R0901).
max-parents=7

# Maximum number of public methods for a class (see R0904).
max-public-methods=25

# Maximum number of return / yield for function / method body.
max-returns=6

# Maximum number of statements in function / method body.
max-statements=50

# Minimum number of public methods for a class (see R0903).
min-public-methods=2


[EXCEPTIONS]

# Exceptions that will emit a warning when caught.
overgeneral-exceptions=builtins.BaseException,builtins.Exception


[FORMAT]

# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
expected-line-ending-format=

# Regexp for a line that is allowed to be longer than the limit.
ignore-long-lines=^\s*(# )?<?https?://\S+>?$

# Number of spaces of indent required inside a hanging or continued line.
indent-after-paren=4

# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
# tab).
indent-string='    '

# Maximum number of characters on a single line.
max-line-length=100

# Maximum number of lines in a module.
max-module-lines=1000

# Allow the body of a class to be on the same line as the declaration if body
# contains single statement.
single-line-class-stmt=no

# Allow the body of an if to be on the same line as the test if there is no
# else.
single-line-if-stmt=no


[IMPORTS]

# List of modules that can be imported at any level, not just the top level
# one.
allow-any-import-level=

# Allow explicit reexports by alias from a package __init__.
allow-reexport-from-package=no

# Allow wildcard imports from modules that define __all__.
allow-wildcard-with-all=no

# Deprecated modules which should not be used, separated by a comma.
deprecated-modules=

# Output a graph (.gv or any supported image format) of external dependencies
# to the given file (report RP0402 must not be disabled).
ext-import-graph=

# Output a graph (.gv or any supported image format) of all (i.e. internal and
# external) dependencies to the given file (report RP0402 must not be
# disabled).
import-graph=

# Output a graph (.gv or any supported image format) of internal dependencies
# to the given file (report RP0402 must not be disabled).
int-import-graph=

# Force import order to recognize a module as part of the standard
# compatibility libraries.
known-standard-library=

# Force import order to recognize a module as part of a third party library.
known-third-party=enchant

# Couples of modules and preferred modules, separated by a comma.
preferred-modules=


[LOGGING]

# The type of string formatting that logging methods do. `old` means using %
# formatting, `new` is for `{}` formatting.
logging-format-style=old

# Logging modules to check that the string format arguments are in logging
# function parameter format.
logging-modules=logging


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
# UNDEFINED.
confidence=HIGH,
           CONTROL_FLOW,
           INFERENCE,
           INFERENCE_FAILURE,
           UNDEFINED

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then re-enable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=raw-checker-failed,
        bad-inline-option,
        locally-disabled,
        file-ignored,
        suppressed-message,
        useless-suppression,
        deprecated-pragma,
        use-implicit-booleaness-not-comparison-to-string,
        use-implicit-booleaness-not-comparison-to-zero,
        use-symbolic-message-instead,
        trailing-whitespace,
        line-too-long,
        missing-class-docstring,
        missing-module-docstring,
        missing-function-docstring,
        too-many-instance-attributes,
        wrong-import-order,
        too-many-arguments,
        broad-exception-raised,
        too-few-public-methods,
        too-many-branches,
        duplicate-code,
        trailing-newlines,
        too-many-public-methods,
        too-many-locals,
        too-many-lines,
        using-constant-test,
        too-many-statements,
        cyclic-import,
        too-many-nested-blocks,
        too-many-boolean-expressions,
        no-else-raise,
        bare-except,
        broad-exception-caught,
        fixme,
        relative-beyond-top-level,
        consider-using-with,
        wildcard-import,
        unused-wildcard-import

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=


[METHOD_ARGS]

# List of qualified names (i.e., library.method) which require a timeout
# parameter e.g. 'requests.api.get,requests.api.post'
timeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request


[MISCELLANEOUS]

# List of note tags to take in consideration, separated by a comma.
notes=FIXME,
      XXX,
      TODO

# Regular expression of note tags to take in consideration.
notes-rgx=


[REFACTORING]

# Maximum number of nested blocks for function / method body
max-nested-blocks=5

# Complete name of functions that never returns. When checking for
# inconsistent-return-statements if a never returning function is called then
# it will be considered as an explicit return statement and no message will be
# printed.
never-returning-functions=sys.exit,argparse.parse_error


[REPORTS]

# Python expression which should return a score less than or equal to 10. You
# have access to the variables 'fatal', 'error', 'warning', 'refactor',
# 'convention', and 'info' which contain the number of messages in each
# category, as well as 'statement' which is the total number of statements
# analyzed. This score is used by the global evaluation report (RP0004).
evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))

# Template used to display messages. This is a python new-style format string
# used to format the message information. See doc for all details.
msg-template=

# Set the output format. Available formats are: text, parseable, colorized,
# json2 (improved json format), json (old json format) and msvs (visual
# studio). You can also give a reporter class, e.g.
# mypackage.mymodule.MyReporterClass.
#output-format=

# Tells whether to display a full report or only the messages.
reports=no

# Activate the evaluation score.
score=yes


[SIMILARITIES]

# Comments are removed from the similarity computation
ignore-comments=yes

# Docstrings are removed from the similarity computation
ignore-docstrings=yes

# Imports are removed from the similarity computation
ignore-imports=yes

# Signatures are removed from the similarity computation
ignore-signatures=yes

# Minimum lines number of a similarity.
min-similarity-lines=4


[SPELLING]

# Limits count of emitted suggestions for spelling mistakes.
max-spelling-suggestions=4

# Spelling dictionary name. No available dictionaries : You need to install
# both the python package and the system dependency for enchant to work.
spelling-dict=

# List of comma separated words that should be considered directives if they
# appear at the beginning of a comment and should not be checked.
spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:

# List of comma separated words that should not be checked.
spelling-ignore-words=

# A path to a file that contains the private dictionary; one word per line.
spelling-private-dict-file=

# Tells whether to store unknown words to the private dictionary (see the
# --spelling-private-dict-file option) instead of raising a message.
spelling-store-unknown-words=no


[STRING]

# This flag controls whether inconsistent-quotes generates a warning when the
# character used as a quote delimiter is used inconsistently within a module.
check-quote-consistency=no

# This flag controls whether the implicit-str-concat should generate a warning
# on implicit string concatenation in sequences defined over several lines.
check-str-concat-over-line-jumps=no


[TYPECHECK]

# List of decorators that produce context managers, such as
# contextlib.contextmanager. Add to this list to register other decorators that
# produce valid context managers.
contextmanager-decorators=contextlib.contextmanager

# List of members which are set dynamically and missed by pylint inference
# system, and so shouldn't trigger E1101 when accessed. Python regular
# expressions are accepted.
generated-members=

# Tells whether to warn about missing members when the owner of the attribute
# is inferred to be None.
ignore-none=yes

# This flag controls whether pylint should warn about no-member and similar
# checks whenever an opaque object is returned when inferring. The inference
# can return multiple potential results while evaluating a Python object, but
# some branches might not be evaluated, which results in partial inference. In
# that case, it might be useful to still emit no-member and other checks for
# the rest of the inferred objects.
ignore-on-opaque-inference=yes

# List of symbolic message names to ignore for Mixin members.
ignored-checks-for-mixins=no-member,
                          not-async-context-manager,
                          not-context-manager,
                          attribute-defined-outside-init

# List of class names for which member attributes should not be checked (useful
# for classes with dynamically set attributes). This supports the use of
# qualified names.
ignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace

# Show a hint with possible names when a member name was not found. The aspect
# of finding the hint is based on edit distance.
missing-member-hint=yes

# The minimum edit distance a name should have in order to be considered a
# similar match for a missing member name.
missing-member-hint-distance=1

# The total number of similar names that should be taken in consideration when
# showing a hint for a missing member.
missing-member-max-choices=1

# Regex pattern to define which classes are considered mixins.
mixin-class-rgx=.*[Mm]ixin

# List of decorators that change the signature of a decorated function.
signature-mutators=


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of names allowed to shadow builtins
allowed-redefined-builtins=id,object

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored.
ignored-argument-names=_.*|^ignored_|^unused_

# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
</file>

<file path="pyproject.toml">
[project]
name = "mistralai"
version = "1.8.2"
description = "Python Client SDK for the Mistral AI API."
authors = [{ name = "Mistral" },]
readme = "README-PYPI.md"
requires-python = ">=3.9"
dependencies = [
    "eval-type-backport >=0.2.0",
    "httpx >=0.28.1",
    "pydantic >=2.10.3",
    "python-dateutil >=2.8.2",
    "typing-inspection >=0.4.0",
]

[tool.poetry]
repository = "https://github.com/mistralai/client-python.git"
packages = [
    { include = "mistralai", from = "src" },
    { include = "mistralai_azure", from = "packages/mistralai_azure/src" },
    { include = "mistralai_gcp", from = "packages/mistralai_gcp/src" },
]
include = ["py.typed", "src/mistralai/py.typed"]

[tool.setuptools.package-data]
"*" = ["py.typed", "src/mistralai/py.typed"]

[virtualenvs]
in-project = true

[tool.poetry.group.dev.dependencies]
mypy = "==1.15.0"
pylint = "==3.2.3"
pytest = "^8.2.2"
pytest-asyncio = "^0.23.7"
types-python-dateutil = "^2.9.0.20240316"
types-authlib = "^1.5.0.20250516"

[tool.poetry.group.lint.dependencies]
ruff = "^0.11.10"
pyright = "^1.1.401"
mypy = "==1.15.0"


[project.optional-dependencies]
gcp = [
    "google-auth >=2.27.0",
    "requests >=2.32.3"
]
agents = [
    "mcp >=1.0,<2.0; python_version >= '3.10'",
    "griffe >=1.7.3,<2.0",
    "authlib >=1.5.2,<2.0",
]

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["src"]

[tool.mypy]
disable_error_code = "misc"

[[tool.mypy.overrides]]
module = "typing_inspect"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = [
    "jsonpath.*",
    "typing_inspect.*",
    "authlib.*",
    "mcp.*",
    "griffe.*"
]
ignore_missing_imports = true

[tool.pyright]
venvPath = "."
venv = ".venv"
</file>

<file path="README.md">
# Mistral Python Client

## Migration warning
 
This documentation is for Mistral AI SDK v1. You can find more details on how to migrate from v0 to v1 [here](MIGRATION.md)

## API Key Setup

Before you begin, you will need a Mistral AI API key.

1. Get your own Mistral API Key: <https://docs.mistral.ai/#api-access>
2. Set your Mistral API Key as an environment variable. You only need to do this once.

```bash
# set Mistral API Key (using zsh for example)
$ echo 'export MISTRAL_API_KEY=[your_key_here]' >> ~/.zshenv

# reload the environment (or just quit and open a new terminal)
$ source ~/.zshenv
```

<!-- Start Summary [summary] -->
## Summary

Mistral AI API: Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it.
<!-- End Summary [summary] -->

<!-- Start Table of Contents [toc] -->
## Table of Contents
<!-- $toc-max-depth=2 -->
* [Mistral Python Client](#mistral-python-client)
  * [Migration warning](#migration-warning)
  * [API Key Setup](#api-key-setup)
  * [SDK Installation](#sdk-installation)
  * [SDK Example Usage](#sdk-example-usage)
  * [Providers' SDKs Example Usage](#providers-sdks-example-usage)
  * [Available Resources and Operations](#available-resources-and-operations)
  * [Server-sent event streaming](#server-sent-event-streaming)
  * [File uploads](#file-uploads)
  * [Retries](#retries)
  * [Error Handling](#error-handling)
  * [Server Selection](#server-selection)
  * [Custom HTTP Client](#custom-http-client)
  * [Authentication](#authentication)
  * [Resource Management](#resource-management)
  * [Debugging](#debugging)
  * [IDE Support](#ide-support)
* [Development](#development)
  * [Contributions](#contributions)

<!-- End Table of Contents [toc] -->

<!-- Start SDK Installation [installation] -->
## SDK Installation

> [!NOTE]
> **Python version upgrade policy**
>
> Once a Python version reaches its [official end of life date](https://devguide.python.org/versions/), a 3-month grace period is provided for users to upgrade. Following this grace period, the minimum python version supported in the SDK will be updated.

The SDK can be installed with either *pip* or *poetry* package managers.

### PIP

*PIP* is the default package installer for Python, enabling easy installation and management of packages from PyPI via the command line.

```bash
pip install mistralai
```

### Poetry

*Poetry* is a modern tool that simplifies dependency management and package publishing by using a single `pyproject.toml` file to handle project metadata and dependencies.

```bash
poetry add mistralai
```

### Shell and script usage with `uv`

You can use this SDK in a Python shell with [uv](https://docs.astral.sh/uv/) and the `uvx` command that comes with it like so:

```shell
uvx --from mistralai python
```

It's also possible to write a standalone Python script without needing to set up a whole project like so:

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = [
#     "mistralai",
# ]
# ///

from mistralai import Mistral

sdk = Mistral(
  # SDK arguments
)

# Rest of script here...
```

Once that is saved to a file, you can run it with `uv run script.py` where
`script.py` can be replaced with the actual file name.
<!-- End SDK Installation [installation] -->

### Agents extra dependencies

When using the agents related feature it is required to add the `agents` extra dependencies. This can be added when 
installing the package:

```bash
pip install "mistralai[agents]"
```

> Note: Because of some of our dependencies, these features are only available for python version higher or equal to 
> 3.10.

<!-- Start SDK Example Usage [usage] -->
## SDK Example Usage

### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.chat.complete(model="mistral-small-latest", messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ])

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.chat.complete_async(model="mistral-small-latest", messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ])

        # Handle response
        print(res)

asyncio.run(main())
```

### Upload a file

This example shows how to upload a file.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.upload(file={
        "file_name": "example.file",
        "content": open("example.file", "rb"),
    })

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.files.upload_async(file={
            "file_name": "example.file",
            "content": open("example.file", "rb"),
        })

        # Handle response
        print(res)

asyncio.run(main())
```

### Create Agents Completions

This example shows how to create agents completions.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.agents.complete(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], agent_id="<id>")

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.agents.complete_async(messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ], agent_id="<id>")

        # Handle response
        print(res)

asyncio.run(main())
```

### Create Embedding Request

This example shows how to create embedding request.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.embeddings.create(model="mistral-embed", inputs=[
        "Embed this sentence.",
        "As well as this one.",
    ])

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.embeddings.create_async(model="mistral-embed", inputs=[
            "Embed this sentence.",
            "As well as this one.",
        ])

        # Handle response
        print(res)

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->


### More examples

You can run the examples in the `examples/` directory using `poetry run` or by entering the virtual environment using `poetry shell`.


## Providers' SDKs Example Usage

### Azure AI

**Prerequisites**

Before you begin, ensure you have `AZUREAI_ENDPOINT` and an `AZURE_API_KEY`. To obtain these, you will need to deploy Mistral on Azure AI.
See [instructions for deploying Mistral on Azure AI here](https://docs.mistral.ai/deployment/cloud/azure/).

Here's a basic example to get you started. You can also run [the example in the `examples` directory](/examples/azure).

```python
import asyncio
import os

from mistralai_azure import MistralAzure

client = MistralAzure(
    azure_api_key=os.getenv("AZURE_API_KEY", ""),
    azure_endpoint=os.getenv("AZURE_ENDPOINT", "")
)

async def main() -> None:
    res = await client.chat.complete_async( 
        max_tokens= 100,
        temperature= 0.5,
        messages= [
            {
                "content": "Hello there!",
                "role": "user"
            }
        ]
    )
    print(res)

asyncio.run(main())
```
The documentation for the Azure SDK is available [here](packages/mistralai_azure/README.md).

### Google Cloud


**Prerequisites**

Before you begin, you will need to create a Google Cloud project and enable the Mistral API. To do this, follow the instructions [here](https://docs.mistral.ai/deployment/cloud/vertex/).

To run this locally you will also need to ensure you are authenticated with Google Cloud. You can do this by running

```bash
gcloud auth application-default login
```

**Step 1: Install**

Install the extras dependencies specific to Google Cloud:

```bash
pip install mistralai[gcp]
```

**Step 2: Example Usage**

Here's a basic example to get you started.

```python
import asyncio
from mistralai_gcp import MistralGoogleCloud

client = MistralGoogleCloud()


async def main() -> None:
    res = await client.chat.complete_async(
        model= "mistral-small-2402",
        messages= [
            {
                "content": "Hello there!",
                "role": "user"
            }
        ]
    )
    print(res)

asyncio.run(main())
```

The documentation for the GCP SDK is available [here](packages/mistralai_gcp/README.md).


<!-- Start Available Resources and Operations [operations] -->
## Available Resources and Operations

<details open>
<summary>Available methods</summary>

### [agents](docs/sdks/agents/README.md)

* [complete](docs/sdks/agents/README.md#complete) - Agents Completion
* [stream](docs/sdks/agents/README.md#stream) - Stream Agents completion

### [batch](docs/sdks/batch/README.md)


#### [batch.jobs](docs/sdks/mistraljobs/README.md)

* [list](docs/sdks/mistraljobs/README.md#list) - Get Batch Jobs
* [create](docs/sdks/mistraljobs/README.md#create) - Create Batch Job
* [get](docs/sdks/mistraljobs/README.md#get) - Get Batch Job
* [cancel](docs/sdks/mistraljobs/README.md#cancel) - Cancel Batch Job

### [beta](docs/sdks/beta/README.md)


#### [beta.agents](docs/sdks/mistralagents/README.md)

* [create](docs/sdks/mistralagents/README.md#create) - Create a agent that can be used within a conversation.
* [list](docs/sdks/mistralagents/README.md#list) - List agent entities.
* [get](docs/sdks/mistralagents/README.md#get) - Retrieve an agent entity.
* [update](docs/sdks/mistralagents/README.md#update) - Update an agent entity.
* [update_version](docs/sdks/mistralagents/README.md#update_version) - Update an agent version.

#### [beta.conversations](docs/sdks/conversations/README.md)

* [start](docs/sdks/conversations/README.md#start) - Create a conversation and append entries to it.
* [list](docs/sdks/conversations/README.md#list) - List all created conversations.
* [get](docs/sdks/conversations/README.md#get) - Retrieve a conversation information.
* [append](docs/sdks/conversations/README.md#append) - Append new entries to an existing conversation.
* [get_history](docs/sdks/conversations/README.md#get_history) - Retrieve all entries in a conversation.
* [get_messages](docs/sdks/conversations/README.md#get_messages) - Retrieve all messages in a conversation.
* [restart](docs/sdks/conversations/README.md#restart) - Restart a conversation starting from a given entry.
* [start_stream](docs/sdks/conversations/README.md#start_stream) - Create a conversation and append entries to it.
* [append_stream](docs/sdks/conversations/README.md#append_stream) - Append new entries to an existing conversation.
* [restart_stream](docs/sdks/conversations/README.md#restart_stream) - Restart a conversation starting from a given entry.

### [chat](docs/sdks/chat/README.md)

* [complete](docs/sdks/chat/README.md#complete) - Chat Completion
* [stream](docs/sdks/chat/README.md#stream) - Stream chat completion

### [classifiers](docs/sdks/classifiers/README.md)

* [moderate](docs/sdks/classifiers/README.md#moderate) - Moderations
* [moderate_chat](docs/sdks/classifiers/README.md#moderate_chat) - Chat Moderations
* [classify](docs/sdks/classifiers/README.md#classify) - Classifications
* [classify_chat](docs/sdks/classifiers/README.md#classify_chat) - Chat Classifications

### [embeddings](docs/sdks/embeddings/README.md)

* [create](docs/sdks/embeddings/README.md#create) - Embeddings

### [files](docs/sdks/files/README.md)

* [upload](docs/sdks/files/README.md#upload) - Upload File
* [list](docs/sdks/files/README.md#list) - List Files
* [retrieve](docs/sdks/files/README.md#retrieve) - Retrieve File
* [delete](docs/sdks/files/README.md#delete) - Delete File
* [download](docs/sdks/files/README.md#download) - Download File
* [get_signed_url](docs/sdks/files/README.md#get_signed_url) - Get Signed Url

### [fim](docs/sdks/fim/README.md)

* [complete](docs/sdks/fim/README.md#complete) - Fim Completion
* [stream](docs/sdks/fim/README.md#stream) - Stream fim completion

### [fine_tuning](docs/sdks/finetuning/README.md)


#### [fine_tuning.jobs](docs/sdks/jobs/README.md)

* [list](docs/sdks/jobs/README.md#list) - Get Fine Tuning Jobs
* [create](docs/sdks/jobs/README.md#create) - Create Fine Tuning Job
* [get](docs/sdks/jobs/README.md#get) - Get Fine Tuning Job
* [cancel](docs/sdks/jobs/README.md#cancel) - Cancel Fine Tuning Job
* [start](docs/sdks/jobs/README.md#start) - Start Fine Tuning Job


### [models](docs/sdks/models/README.md)

* [list](docs/sdks/models/README.md#list) - List Models
* [retrieve](docs/sdks/models/README.md#retrieve) - Retrieve Model
* [delete](docs/sdks/models/README.md#delete) - Delete Model
* [update](docs/sdks/models/README.md#update) - Update Fine Tuned Model
* [archive](docs/sdks/models/README.md#archive) - Archive Fine Tuned Model
* [unarchive](docs/sdks/models/README.md#unarchive) - Unarchive Fine Tuned Model

### [ocr](docs/sdks/ocr/README.md)

* [process](docs/sdks/ocr/README.md#process) - OCR

</details>
<!-- End Available Resources and Operations [operations] -->

<!-- Start Server-sent event streaming [eventstream] -->
## Server-sent event streaming

[Server-sent events][mdn-sse] are used to stream content from certain
operations. These operations will expose the stream as [Generator][generator] that
can be consumed using a simple `for` loop. The loop will
terminate when the server no longer has any events to send and closes the
underlying connection.  

The stream is also a [Context Manager][context-manager] and can be used with the `with` statement and will close the
underlying connection when the context is exited.

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.beta.conversations.start_stream(inputs="<value>")

    with res as event_stream:
        for event in event_stream:
            # handle event
            print(event, flush=True)

```

[mdn-sse]: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events
[generator]: https://book.pythontips.com/en/latest/generators.html
[context-manager]: https://book.pythontips.com/en/latest/context_managers.html
<!-- End Server-sent event streaming [eventstream] -->

<!-- Start File uploads [file-upload] -->
## File uploads

Certain SDK methods accept file objects as part of a request body or multi-part request. It is possible and typically recommended to upload files as a stream rather than reading the entire contents into memory. This avoids excessive memory consumption and potentially crashing with out-of-memory errors when working with very large files. The following example demonstrates how to attach a file stream to a request.

> [!TIP]
>
> For endpoints that handle file uploads bytes arrays can also be used. However, using streams is recommended for large files.
>

```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.upload(file={
        "file_name": "example.file",
        "content": open("example.file", "rb"),
    })

    # Handle response
    print(res)

```
<!-- End File uploads [file-upload] -->

<!-- Start Retries [retries] -->
## Retries

Some of the endpoints in this SDK support retries. If you use the SDK without any configuration, it will fall back to the default retry strategy provided by the API. However, the default retry strategy can be overridden on a per-operation basis, or across the entire SDK.

To change the default retry strategy for a single API call, simply provide a `RetryConfig` object to the call:
```python
from mistralai import Mistral
from mistralai.utils import BackoffStrategy, RetryConfig
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list(,
        RetryConfig("backoff", BackoffStrategy(1, 50, 1.1, 100), False))

    # Handle response
    print(res)

```

If you'd like to override the default retry strategy for all operations that support retries, you can use the `retry_config` optional parameter when initializing the SDK:
```python
from mistralai import Mistral
from mistralai.utils import BackoffStrategy, RetryConfig
import os


with Mistral(
    retry_config=RetryConfig("backoff", BackoffStrategy(1, 50, 1.1, 100), False),
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list()

    # Handle response
    print(res)

```
<!-- End Retries [retries] -->

<!-- Start Error Handling [errors] -->
## Error Handling

Handling errors in this SDK should largely match your expectations. All operations return a response object or raise an exception.

By default, an API error will raise a models.SDKError exception, which has the following properties:

| Property        | Type             | Description           |
|-----------------|------------------|-----------------------|
| `.status_code`  | *int*            | The HTTP status code  |
| `.message`      | *str*            | The error message     |
| `.raw_response` | *httpx.Response* | The raw HTTP response |
| `.body`         | *str*            | The response content  |

When custom error responses are specified for an operation, the SDK may also raise their associated exceptions. You can refer to respective *Errors* tables in SDK docs for more details on possible exception types for each operation. For example, the `list_async` method may raise the following exceptions:

| Error Type                 | Status Code | Content Type     |
| -------------------------- | ----------- | ---------------- |
| models.HTTPValidationError | 422         | application/json |
| models.SDKError            | 4XX, 5XX    | \*/\*            |

### Example

```python
from mistralai import Mistral, models
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:
    res = None
    try:

        res = mistral.models.list()

        # Handle response
        print(res)

    except models.HTTPValidationError as e:
        # handle e.data: models.HTTPValidationErrorData
        raise(e)
    except models.SDKError as e:
        # handle exception
        raise(e)
```
<!-- End Error Handling [errors] -->

<!-- Start Server Selection [server] -->
## Server Selection

### Select Server by Name

You can override the default server globally by passing a server name to the `server: str` optional parameter when initializing the SDK client instance. The selected server will then be used as the default on the operations that use it. This table lists the names associated with the available servers:

| Name | Server                   | Description          |
| ---- | ------------------------ | -------------------- |
| `eu` | `https://api.mistral.ai` | EU Production server |

#### Example

```python
from mistralai import Mistral
import os


with Mistral(
    server="eu",
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list()

    # Handle response
    print(res)

```

### Override Server URL Per-Client

The default server can also be overridden globally by passing a URL to the `server_url: str` optional parameter when initializing the SDK client instance. For example:
```python
from mistralai import Mistral
import os


with Mistral(
    server_url="https://api.mistral.ai",
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list()

    # Handle response
    print(res)

```
<!-- End Server Selection [server] -->

<!-- Start Custom HTTP Client [http-client] -->
## Custom HTTP Client

The Python SDK makes API calls using the [httpx](https://www.python-httpx.org/) HTTP library.  In order to provide a convenient way to configure timeouts, cookies, proxies, custom headers, and other low-level configuration, you can initialize the SDK client with your own HTTP client instance.
Depending on whether you are using the sync or async version of the SDK, you can pass an instance of `HttpClient` or `AsyncHttpClient` respectively, which are Protocol's ensuring that the client has the necessary methods to make API calls.
This allows you to wrap the client with your own custom logic, such as adding custom headers, logging, or error handling, or you can just pass an instance of `httpx.Client` or `httpx.AsyncClient` directly.

For example, you could specify a header for every request that this sdk makes as follows:
```python
from mistralai import Mistral
import httpx

http_client = httpx.Client(headers={"x-custom-header": "someValue"})
s = Mistral(client=http_client)
```

or you could wrap the client with your own custom logic:
```python
from mistralai import Mistral
from mistralai.httpclient import AsyncHttpClient
import httpx

class CustomClient(AsyncHttpClient):
    client: AsyncHttpClient

    def __init__(self, client: AsyncHttpClient):
        self.client = client

    async def send(
        self,
        request: httpx.Request,
        *,
        stream: bool = False,
        auth: Union[
            httpx._types.AuthTypes, httpx._client.UseClientDefault, None
        ] = httpx.USE_CLIENT_DEFAULT,
        follow_redirects: Union[
            bool, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
    ) -> httpx.Response:
        request.headers["Client-Level-Header"] = "added by client"

        return await self.client.send(
            request, stream=stream, auth=auth, follow_redirects=follow_redirects
        )

    def build_request(
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: Optional[httpx._types.RequestContent] = None,
        data: Optional[httpx._types.RequestData] = None,
        files: Optional[httpx._types.RequestFiles] = None,
        json: Optional[Any] = None,
        params: Optional[httpx._types.QueryParamTypes] = None,
        headers: Optional[httpx._types.HeaderTypes] = None,
        cookies: Optional[httpx._types.CookieTypes] = None,
        timeout: Union[
            httpx._types.TimeoutTypes, httpx._client.UseClientDefault
        ] = httpx.USE_CLIENT_DEFAULT,
        extensions: Optional[httpx._types.RequestExtensions] = None,
    ) -> httpx.Request:
        return self.client.build_request(
            method,
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )

s = Mistral(async_client=CustomClient(httpx.AsyncClient()))
```
<!-- End Custom HTTP Client [http-client] -->

<!-- Start Authentication [security] -->
## Authentication

### Per-Client Security Schemes

This SDK supports the following security scheme globally:

| Name      | Type | Scheme      | Environment Variable |
| --------- | ---- | ----------- | -------------------- |
| `api_key` | http | HTTP Bearer | `MISTRAL_API_KEY`    |

To authenticate with the API the `api_key` parameter must be set when initializing the SDK client instance. For example:
```python
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.models.list()

    # Handle response
    print(res)

```
<!-- End Authentication [security] -->

<!-- Start Resource Management [resource-management] -->
## Resource Management

The `Mistral` class implements the context manager protocol and registers a finalizer function to close the underlying sync and async HTTPX clients it uses under the hood. This will close HTTP connections, release memory and free up other resources held by the SDK. In short-lived Python programs and notebooks that make a few SDK method calls, resource management may not be a concern. However, in longer-lived programs, it is beneficial to create a single SDK instance via a [context manager][context-manager] and reuse it across the application.

[context-manager]: https://docs.python.org/3/reference/datamodel.html#context-managers

```python
from mistralai import Mistral
import os
def main():

    with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:
        # Rest of application here...


# Or when using async:
async def amain():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:
        # Rest of application here...
```
<!-- End Resource Management [resource-management] -->

<!-- Start Debugging [debug] -->
## Debugging

You can setup your SDK to emit debug logs for SDK requests and responses.

You can pass your own logger class directly into your SDK.
```python
from mistralai import Mistral
import logging

logging.basicConfig(level=logging.DEBUG)
s = Mistral(debug_logger=logging.getLogger("mistralai"))
```

You can also enable a default debug logger by setting an environment variable `MISTRAL_DEBUG` to true.
<!-- End Debugging [debug] -->

<!-- Start IDE Support [idesupport] -->
## IDE Support

### PyCharm

Generally, the SDK will work well with most IDEs out of the box. However, when using PyCharm, you can enjoy much better integration with Pydantic by installing an additional plugin.

- [PyCharm Pydantic Plugin](https://docs.pydantic.dev/latest/integrations/pycharm/)
<!-- End IDE Support [idesupport] -->

<!-- Placeholder for Future Speakeasy SDK Sections -->

# Development

## Contributions

While we value open-source contributions to this SDK, this library is generated programmatically. Any manual changes added to internal files will be overwritten on the next generation. 
We look forward to hearing your feedback. Feel free to open a PR or an issue with a proof of concept and we'll do our best to include it in a future release.
</file>

<file path="RELEASES.md">
## 2024-08-07 14:25:13
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.356.0 (2.388.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.0.0] .
### Releases
- [PyPI v1.0.0] https://pypi.org/project/mistralai/1.0.0 - .

## 2024-08-08 18:12:16
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.357.4 (2.390.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.0.1] .
### Releases
- [PyPI v1.0.1] https://pypi.org/project/mistralai/1.0.1 - .

## 2024-08-20 08:36:28
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.376.0 (2.402.5) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.0.2] .
### Releases
- [PyPI v1.0.2] https://pypi.org/project/mistralai/1.0.2 - .

## 2024-08-29 09:09:05
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.382.0 (2.404.11) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.0.3] .
### Releases
- [PyPI v1.0.3] https://pypi.org/project/mistralai/1.0.3 - .

## 2024-09-13 16:21:24
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.396.7 (2.415.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.1.0] .
### Releases
- [PyPI v1.1.0] https://pypi.org/project/mistralai/1.1.0 - .

## 2024-11-07 19:52:56
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.434.3 (2.452.0) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.0] .
### Releases
- [PyPI v1.2.0] https://pypi.org/project/mistralai/1.2.0 - .

## 2024-11-08 13:41:24
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.434.7 (2.452.0) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.1] .
### Releases
- [PyPI v1.2.1] https://pypi.org/project/mistralai/1.2.1 - .

## 2024-11-12 18:04:16
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.438.1 (2.457.2) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.2] .
### Releases
- [PyPI v1.2.2] https://pypi.org/project/mistralai/1.2.2 - .

## 2024-11-15 18:37:23
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.440.1 (2.460.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.3] .
### Releases
- [PyPI v1.2.3] https://pypi.org/project/mistralai/1.2.3 - .

## 2024-12-02 14:25:56
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.440.1 (2.460.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.4] .
### Releases
- [PyPI v1.2.4] https://pypi.org/project/mistralai/1.2.4 - .

## 2024-12-04 15:14:08
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.451.1 (2.470.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.5] .
### Releases
- [PyPI v1.2.5] https://pypi.org/project/mistralai/1.2.5 - .

## 2025-01-06 09:57:47
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.462.2 (2.486.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.2.6] .
### Releases
- [PyPI v1.2.6] https://pypi.org/project/mistralai/1.2.6 - .

## 2025-01-14 09:35:05
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.468.5 (2.493.11) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.3.0] .
### Releases
- [PyPI v1.3.0] https://pypi.org/project/mistralai/1.3.0 - .

## 2025-01-15 10:44:07
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.462.2 (2.486.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.3.1] .
### Releases
- [PyPI v1.3.1] https://pypi.org/project/mistralai/1.3.1 - .

## 2025-01-21 11:09:53
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.469.11 (2.493.32) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.4.0] .
### Releases
- [PyPI v1.4.0] https://pypi.org/project/mistralai/1.4.0 - .

## 2025-01-27 13:57:39
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.476.2 (2.495.1) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.5.0] .
### Releases
- [PyPI v1.5.0] https://pypi.org/project/mistralai/1.5.0 - .

## 2025-03-06 16:38:57
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.477.0 (2.497.0) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.5.1] .
### Releases
- [PyPI v1.5.1] https://pypi.org/project/mistralai/1.5.1 - .

## 2025-03-19 18:09:29
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.477.0 (2.497.0) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.5.2] .
### Releases
- [PyPI v1.5.2] https://pypi.org/project/mistralai/1.5.2 - .

## 2025-03-20 10:34:02
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.6.0] .
### Releases
- [PyPI v1.6.0] https://pypi.org/project/mistralai/1.6.0 - .

## 2025-04-16 18:35:19
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.7.0] .
### Releases
- [PyPI v1.7.0] https://pypi.org/project/mistralai/1.7.0 - .

## 2025-05-22 15:03:08
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.7.1] .
### Releases
- [PyPI v1.7.1] https://pypi.org/project/mistralai/1.7.1 - .

## 2025-05-26 11:05:08
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.8.0] .
### Releases
- [PyPI v1.8.0] https://pypi.org/project/mistralai/1.8.0 - .

## 2025-05-28 15:38:22
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.8.1] .
### Releases
- [PyPI v1.8.1] https://pypi.org/project/mistralai/1.8.1 - .

## 2025-06-10 16:42:28
### Changes
Based on:
- OpenAPI Doc  
- Speakeasy CLI 1.517.3 (2.548.6) https://github.com/speakeasy-api/speakeasy
### Generated
- [python v1.8.2] .
### Releases
- [PyPI v1.8.2] https://pypi.org/project/mistralai/1.8.2 - .
</file>

<file path="USAGE.md">
<!-- Start SDK Example Usage [usage] -->
### Create Chat Completions

This example shows how to create chat completions.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.chat.complete(model="mistral-small-latest", messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ])

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.chat.complete_async(model="mistral-small-latest", messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ])

        # Handle response
        print(res)

asyncio.run(main())
```

### Upload a file

This example shows how to upload a file.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.files.upload(file={
        "file_name": "example.file",
        "content": open("example.file", "rb"),
    })

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.files.upload_async(file={
            "file_name": "example.file",
            "content": open("example.file", "rb"),
        })

        # Handle response
        print(res)

asyncio.run(main())
```

### Create Agents Completions

This example shows how to create agents completions.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.agents.complete(messages=[
        {
            "content": "Who is the best French painter? Answer in one short sentence.",
            "role": "user",
        },
    ], agent_id="<id>")

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.agents.complete_async(messages=[
            {
                "content": "Who is the best French painter? Answer in one short sentence.",
                "role": "user",
            },
        ], agent_id="<id>")

        # Handle response
        print(res)

asyncio.run(main())
```

### Create Embedding Request

This example shows how to create embedding request.

```python
# Synchronous Example
from mistralai import Mistral
import os


with Mistral(
    api_key=os.getenv("MISTRAL_API_KEY", ""),
) as mistral:

    res = mistral.embeddings.create(model="mistral-embed", inputs=[
        "Embed this sentence.",
        "As well as this one.",
    ])

    # Handle response
    print(res)
```

</br>

The same SDK client can also be used to make asychronous requests by importing asyncio.
```python
# Asynchronous Example
import asyncio
from mistralai import Mistral
import os

async def main():

    async with Mistral(
        api_key=os.getenv("MISTRAL_API_KEY", ""),
    ) as mistral:

        res = await mistral.embeddings.create_async(model="mistral-embed", inputs=[
            "Embed this sentence.",
            "As well as this one.",
        ])

        # Handle response
        print(res)

asyncio.run(main())
```
<!-- End SDK Example Usage [usage] -->
</file>

</files>

<git_diffs>
<git_diff_work_tree>

</git_diff_work_tree>
<git_diff_staged>

</git_diff_staged>
</git_diffs>
