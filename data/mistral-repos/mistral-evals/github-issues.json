[
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/12",
    "id": 3012523851,
    "node_id": "PR_kwDOMxW0Oc6Tg9Rh",
    "number": 12,
    "title": "Bump vllm from 0.6.2 to 0.8.4",
    "user": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [
      {
        "id": 8063349909,
        "node_id": "LA_kwDOMxW0Oc8AAAAB4Jz0lQ",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/dependencies",
        "name": "dependencies",
        "color": "0366d6",
        "default": false,
        "description": "Pull requests that update a dependency file"
      },
      {
        "id": 8320044253,
        "node_id": "LA_kwDOMxW0Oc8AAAAB7-nM3Q",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/python",
        "name": "python",
        "color": "2b67c6",
        "default": false,
        "description": "Pull requests that update python code"
      }
    ],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2025-04-23T02:27:48Z",
    "updated_at": "2025-04-23T02:27:49Z",
    "closed_at": null,
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/12",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/12",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/12.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/12.patch",
      "merged_at": null
    },
    "body": "Bumps [vllm](https://github.com/vllm-project/vllm) from 0.6.2 to 0.8.4.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vllm-project/vllm/releases\">vllm's releases</a>.</em></p>\n<blockquote>\n<h2>v0.8.4</h2>\n<p>This release contains 180 commits from 84 contributors (25 new contributors!).</p>\n<h2>Highlights</h2>\n<p>This release includes important accuracy fixes for Llama4 models, if you are using it, we highly recommend you to update.</p>\n<h3>Model</h3>\n<ul>\n<li>Llama4 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16113\">#16113</a>,<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16509\">#16509</a>) bug fix and enhancements:\n<ul>\n<li>qknorm should be not shared across head (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16311\">#16311</a>)</li>\n<li>Enable attention temperature tuning by default for long context (&gt;32k) (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16439\">#16439</a>)</li>\n<li>Index Error When Single Request Near Max Context (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16209\">#16209</a>)</li>\n<li>Add tuned FusedMoE kernel config for Llama4 Scout, TP=8 on H100  (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16488\">#16488</a>)</li>\n<li>Update to transformers==4.51.1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16257\">#16257</a>)</li>\n<li>Added chat templates for LLaMa4 pythonic tool calling (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16463\">#16463</a>)</li>\n<li>Optimized topk for topk=1(<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16512\">#16512</a>)</li>\n<li>Add warning for Attention backends that do not support irope yet (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16212\">#16212</a>)</li>\n</ul>\n</li>\n<li>Support Qwen3 and Qwen3MoE (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15289\">#15289</a>), smolvlm (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16017\">#16017</a>), jinaai/jina-embeddings-v3 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16120\">#16120</a>), InternVL3 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16495\">#16495</a>), GLM-4-0414 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16338\">#16338</a>)</li>\n</ul>\n<h3>API</h3>\n<ul>\n<li>Estimate max-model-len use available KV cache memory. The error message nows hints at how to set <code>--max-model-len</code> (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16168\">#16168</a>)</li>\n<li>Add hf_token to EngineArgs (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16093\">#16093</a>)</li>\n<li>Enable regex support with xgrammar in V0 engine (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13228\">#13228</a>)</li>\n<li>Support matryoshka representation / support embedding API dimensions (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16331\">#16331</a>)</li>\n<li>Add bucket for <code>request_latency</code>, <code>time_to_first_token</code> and <code>time_per_output_token</code> (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15202\">#15202</a>)</li>\n<li>Support for TorchAO quantization (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14231\">#14231</a>)</li>\n</ul>\n<h3>Hardware</h3>\n<ul>\n<li>Intel-Gaudi: Multi-step scheduling implementation for HPU (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12779\">#12779</a>)</li>\n<li>TPU:\n<ul>\n<li>Make <a href=\"https://github.com/support\"><code>@\u200bsupport</code></a>_torch_compile work for XLA backend (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15782\">#15782</a>)</li>\n<li>Use <code>language_model</code> interface for getting text backbone in MM (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16410\">#16410</a>)</li>\n</ul>\n</li>\n</ul>\n<h3>Performance</h3>\n<ul>\n<li>DeepSeek MLA: a new merge_attn_states CUDA kernel, 3x speedup (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16173\">#16173</a>)</li>\n<li>MoE: Support W8A8 channel-wise weights and per-token activations in triton fused_moe_kernel (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16366\">#16366</a>)</li>\n<li>Add support to modelopt quantization of Mixtral model (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15961\">#15961</a>)</li>\n<li>Enable PTPC FP8 for CompressedTensorsW8A8Fp8MoEMethod (triton fused_moe) (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16537\">#16537</a>)</li>\n</ul>\n<h3>V1 Engine Core</h3>\n<ul>\n<li>Enable multi-input by default (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15799\">#15799</a>)</li>\n<li>Scatter and gather placeholders in the model runner (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16076\">#16076</a>)</li>\n<li>Set structured output backend to <code>auto</code> by default (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15724\">#15724</a>)</li>\n<li>Zero-copy tensor/ndarray serialization/transmission (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13790\">#13790</a>)</li>\n<li>Eagle Model loading (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16035\">#16035</a>)</li>\n<li>KV cache slots for eagle heads (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16370\">#16370</a>)</li>\n<li>Add <code>supports_structured_output()</code> method to Platform (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16148\">#16148</a>)</li>\n</ul>\n<h3>Developer Facing</h3>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/dc1b4a6f1300003ae27f033afbdff5e2683721ce\"><code>dc1b4a6</code></a> [Core][V0] Enable regex support with xgrammar (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13228\">#13228</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/63d2705edbe7fbf4d581ef49503725f3481e04c7\"><code>63d2705</code></a> [Benchmark][Bugfix] Fix SonnetDataset default values in benchmark_throughput....</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/d085a4408244502dd496cae0ccbc524352e87ef8\"><code>d085a44</code></a> Enable PTPC FP8 for CompressedTensorsW8A8Fp8MoEMethod (triton fused_moe) (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16\">#16</a>...</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/f49e5aff11c986ed4d45202b1716c5d74786efa9\"><code>f49e5af</code></a> [V1][Spec Decode] KV cache slots for eagle heads (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16370\">#16370</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/6c11ecf8d337d3b89e891588c81640a0bd30f6e1\"><code>6c11ecf</code></a> [Bugfix] Validate logit biases to prevent out of vocab ids crashing engine (#...</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/93e5f3c5fb4a4bbd49610efb96aad30df95fca66\"><code>93e5f3c</code></a> [Perf] Optimize Preparing Inputs for GPU Model Runner (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16484\">#16484</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/70363bccfac1a6a0818ea577ad9cf8123a0ec3ae\"><code>70363bc</code></a> Fix syntaxWarning: invalid escape sequence '\\s' (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16532\">#16532</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/3cdc57669f231a2c2bdbebb0480164d10cf89edb\"><code>3cdc576</code></a> [Misc] Delete redundant code (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16530\">#16530</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/68bb122eb458a42169dd9d1772a7b32b84b2be95\"><code>68bb122</code></a> [MISC] Make GroupCoordinator compatible with out-of-tree devices (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/16464\">#16464</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/d9fc8cd9da4a69cb4171efb7cb5a46308680c83c\"><code>d9fc8cd</code></a> [V1] Enable multi-input by default (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15799\">#15799</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vllm-project/vllm/compare/v0.6.2...v0.8.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vllm&package-manager=pip&previous-version=0.6.2&new-version=0.8.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/mistralai/mistral-evals/network/alerts).\n\n</details>",
    "closed_by": null,
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/12/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11/events",
    "html_url": "https://github.com/mistralai/mistral-evals/issues/11",
    "id": 2991381000,
    "node_id": "I_kwDOMxW0Oc6yTNoI",
    "number": 11,
    "title": "Memory leaks issue when loading evaluation datasets",
    "user": {
      "login": "Isotr0py",
      "id": 41363108,
      "node_id": "MDQ6VXNlcjQxMzYzMTA4",
      "avatar_url": "https://avatars.githubusercontent.com/u/41363108?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Isotr0py",
      "html_url": "https://github.com/Isotr0py",
      "followers_url": "https://api.github.com/users/Isotr0py/followers",
      "following_url": "https://api.github.com/users/Isotr0py/following{/other_user}",
      "gists_url": "https://api.github.com/users/Isotr0py/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Isotr0py/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Isotr0py/subscriptions",
      "organizations_url": "https://api.github.com/users/Isotr0py/orgs",
      "repos_url": "https://api.github.com/users/Isotr0py/repos",
      "events_url": "https://api.github.com/users/Isotr0py/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Isotr0py/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2025-04-13T17:34:30Z",
    "updated_at": "2025-04-13T17:34:30Z",
    "closed_at": null,
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "sub_issues_summary": {
      "total": 0,
      "completed": 0,
      "percent_completed": 0
    },
    "body": "When loading the docvqa datasets, RAM usage keeps going high and nearly reaches 30+GB, which almost crashed a machine set up with 32GB RAM.\n\nRunning commands:\n```shell\npython -m eval.run eval_vllm         --model_name HuggingFaceTB/SmolVLM-256M-Instruct         --url http://0.0.0.0:8000         --output_dir ~/tmp         --eval_name \"docvqa\"\n```\n\nMemory profiling results:\n\n\n![Image](https://github.com/user-attachments/assets/5ee182ab-8030-40e7-89e3-ccea33f67561)\n\n![Image](https://github.com/user-attachments/assets/9506b7b8-367a-4268-a0ba-ecf66d385e69)",
    "closed_by": null,
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/11/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/10",
    "id": 2932314436,
    "node_id": "PR_kwDOMxW0Oc6PThRt",
    "number": 10,
    "title": "Bump vllm from 0.6.2 to 0.8.0",
    "user": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [
      {
        "id": 8063349909,
        "node_id": "LA_kwDOMxW0Oc8AAAAB4Jz0lQ",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/dependencies",
        "name": "dependencies",
        "color": "0366d6",
        "default": false,
        "description": "Pull requests that update a dependency file"
      },
      {
        "id": 8320044253,
        "node_id": "LA_kwDOMxW0Oc8AAAAB7-nM3Q",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/python",
        "name": "python",
        "color": "2b67c6",
        "default": false,
        "description": "Pull requests that update python code"
      }
    ],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2025-03-19T15:54:20Z",
    "updated_at": "2025-04-23T02:27:52Z",
    "closed_at": "2025-04-23T02:27:50Z",
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/10",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/10",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/10.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/10.patch",
      "merged_at": null
    },
    "body": "Bumps [vllm](https://github.com/vllm-project/vllm) from 0.6.2 to 0.8.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vllm-project/vllm/releases\">vllm's releases</a>.</em></p>\n<blockquote>\n<p>v0.8.0 featured 523 commits from 166 total contributors (68 new contributors)!</p>\n<h2>Highlights</h2>\n<h3>V1</h3>\n<p>We have now enabled V1 engine by default (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13726\">#13726</a>) for supported use cases. Please refer to <a href=\"https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html\">V1 user guide</a> for more detail. We expect better performance for supported scenarios. If you'd like to disable V1 mode, please specify the environment variable <code>VLLM_USE_V1=0</code>, and send us a GitHub issue sharing the reason!</p>\n<ul>\n<li>Support variety of sampling parameters (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13376\">#13376</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/10980\">#10980</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13210\">#13210</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13774\">#13774</a>)</li>\n<li>Compatability prompt logprobs + prefix caching (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13949\">#13949</a>), sliding window + prefix caching (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13069\">#13069</a>)</li>\n<li>Stability fixes (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14380\">#14380</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14379\">#14379</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13298\">#13298</a>)</li>\n<li>Pluggable scheduler (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14466\">#14466</a>)</li>\n<li><code>SupportsV0Only</code> protocol for model definitions (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13959\">#13959</a>)</li>\n<li>Metrics enhancements (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13299\">#13299</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13504\">#13504</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14695\">#14695</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14082\">#14082</a>)</li>\n<li>V1 user guide (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13991\">#13991</a>) and design doc (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12745\">#12745</a>)</li>\n<li>Support for Structured Outputs (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12388\">#12388</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14590\">#14590</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14625\">#14625</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14630\">#14630</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14851\">#14851</a>)</li>\n<li>Support for LoRA (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13705\">#13705</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13096\">#13096</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14626\">#14626</a>)</li>\n<li>Enhance Pipeline Parallelism (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14585\">#14585</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14643\">#14643</a>)</li>\n<li>Ngram speculative decoding (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13729\">#13729</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13933\">#13933</a>)</li>\n</ul>\n<h3>DeepSeek Improvements</h3>\n<p>We observe state of the art performance with vLLM running DeepSeek model on latest version of vLLM:</p>\n<ul>\n<li>MLA Enhancements:\n<ul>\n<li>FlashMLA integration (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13747\">#13747</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/13867\">#13867</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14451\">#14451</a>)</li>\n<li>MLA support for V1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13789\">#13789</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14253\">#14253</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14384\">#14384</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14540\">#14540</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14921\">#14921</a>)</li>\n<li>MLA with chunked prefill (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12639\">#12639</a>)</li>\n<li>Holistic memory and performance optimization (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14769\">#14769</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14770\">#14770</a>,<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14842\">#14842</a>)</li>\n<li>Support MLA for CompressedTensorsWNA16 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13725\">#13725</a>)</li>\n</ul>\n</li>\n<li>Distributed Expert Parallelism (EP) and Data Parallelism (DP)\n<ul>\n<li>EP Support for DeepSeek Models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12583\">#12583</a>)</li>\n<li>Add enable_expert_parallel arg (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14305\">#14305</a>)</li>\n<li>EP/TP MoE + DP Attention (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13931\">#13931</a>)</li>\n<li>Set up data parallel communication (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13591\">#13591</a>)</li>\n</ul>\n</li>\n<li>MTP: Expand DeepSeek MTP code to support k &gt; n_predict (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13626\">#13626</a>)</li>\n<li>Pipeline Parallelism:\n<ul>\n<li>DeepSeek V2/V3/R1 only place <code>lm_head</code> on last pp rank (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13833\">#13833</a>)</li>\n<li>Improve pipeline partitioning (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13839\">#13839</a>)</li>\n</ul>\n</li>\n<li>GEMM\n<ul>\n<li>Add streamK for block-quantized CUTLASS kernels (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12978\">#12978</a>)</li>\n<li>Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13917\">#13917</a>)</li>\n<li>Add more tuned configs for H20 and others (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14877\">#14877</a>)</li>\n</ul>\n</li>\n</ul>\n<h3>New Models</h3>\n<ul>\n<li>Gemma 3 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14660\">#14660</a>)\n<ul>\n<li><strong>Note</strong>: You have to install transformers from main branch (<code>pip install git+https://github.com/huggingface/transformers.git</code>) to use this model. Also, there may be numerical instabilities for <code>float16</code>/<code>half</code> dtype. Please use <code>bfloat16</code> (preferred by HF) or <code>float32</code> dtype.</li>\n</ul>\n</li>\n<li>Mistral Small 3.1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14957\">#14957</a>)</li>\n<li>Phi-4-multimodal-instruct (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14119\">#14119</a>)</li>\n<li>Grok1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13795\">#13795</a>)</li>\n<li>QwQ-32B and toll calling (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14479\">#14479</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/14478\">#14478</a>)</li>\n<li>Zamba2 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13185\">#13185</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/966f933ee1cd7c9a41db60de5c7ff98657005251\"><code>966f933</code></a> [Bugfix] Fix LoRA extra vocab size (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15047\">#15047</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/1a504aff6c79878238e535d78f322e6cae40b71a\"><code>1a504af</code></a> [Bugfix] Fix broken CPU quantization due to triton import (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15038\">#15038</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/01ca85bbd8d773fc5c234763d969ed46e0efd004\"><code>01ca85b</code></a> [MODEL] Add support for Zamba2 models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/13185\">#13185</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/d82b9487eaae0aeac3931c4f1fba7030aa44535c\"><code>d82b948</code></a> [Bugfix] Register serializers for V0 MQ Engine (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15009\">#15009</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/be13281d4bed2582b75e783b0c711000012f860b\"><code>be13281</code></a> [Bugfix] Loosen type check to avoid errors in V1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/15021\">#15021</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/54e084f7fbbb5bcc9e8565b86fac541d4cc05fb6\"><code>54e084f</code></a> [Bugfix] torchrun compatibility (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14899\">#14899</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/9e8f089d08f1c773476a9c4f98eacdea5af42dc4\"><code>9e8f089</code></a> [Kernels] LoRA - Retire SGMV and BGMV Kernels (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14685\">#14685</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/16e9064f84c193cc9a3e02fa0daa035f21e0ed9c\"><code>16e9064</code></a> [V1] Guard Against Main Thread Usage (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14972\">#14972</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/5ac1a8e6e480639014a6b21748f225411e053fa0\"><code>5ac1a8e</code></a> [Bugfix] Fix interface for Olmo2 on V1 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14976\">#14976</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/37e380613220f607cb465fc15f72a4a033a98b23\"><code>37e3806</code></a> [Bugfix] Make Gemma3 MM V0 only for now (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/14971\">#14971</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vllm-project/vllm/compare/v0.6.2...v0.8.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vllm&package-manager=pip&previous-version=0.6.2&new-version=0.8.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/mistralai/mistral-evals/network/alerts).\n\n</details>",
    "closed_by": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/10/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/9",
    "id": 2920995985,
    "node_id": "PR_kwDOMxW0Oc6Otupf",
    "number": 9,
    "title": "Update README.md",
    "user": {
      "login": "ywang96",
      "id": 136131678,
      "node_id": "U_kgDOCB00Xg",
      "avatar_url": "https://avatars.githubusercontent.com/u/136131678?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ywang96",
      "html_url": "https://github.com/ywang96",
      "followers_url": "https://api.github.com/users/ywang96/followers",
      "following_url": "https://api.github.com/users/ywang96/following{/other_user}",
      "gists_url": "https://api.github.com/users/ywang96/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ywang96/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ywang96/subscriptions",
      "organizations_url": "https://api.github.com/users/ywang96/orgs",
      "repos_url": "https://api.github.com/users/ywang96/repos",
      "events_url": "https://api.github.com/users/ywang96/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ywang96/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2025-03-14T18:25:02Z",
    "updated_at": "2025-03-14T19:54:50Z",
    "closed_at": "2025-03-14T19:54:50Z",
    "author_association": "CONTRIBUTOR",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/9",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/9",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/9.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/9.patch",
      "merged_at": "2025-03-14T19:54:50Z"
    },
    "body": "One minor fix so that copy & paste works",
    "closed_by": {
      "login": "patrickvonplaten",
      "id": 23423619,
      "node_id": "MDQ6VXNlcjIzNDIzNjE5",
      "avatar_url": "https://avatars.githubusercontent.com/u/23423619?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/patrickvonplaten",
      "html_url": "https://github.com/patrickvonplaten",
      "followers_url": "https://api.github.com/users/patrickvonplaten/followers",
      "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}",
      "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions",
      "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs",
      "repos_url": "https://api.github.com/users/patrickvonplaten/repos",
      "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}",
      "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/9/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8/events",
    "html_url": "https://github.com/mistralai/mistral-evals/issues/8",
    "id": 2901478981,
    "node_id": "I_kwDOMxW0Oc6s8Q5F",
    "number": 8,
    "title": "Problem with judge prompt for MM-MT-Bench",
    "user": {
      "login": "gabegma",
      "id": 36087158,
      "node_id": "MDQ6VXNlcjM2MDg3MTU4",
      "avatar_url": "https://avatars.githubusercontent.com/u/36087158?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gabegma",
      "html_url": "https://github.com/gabegma",
      "followers_url": "https://api.github.com/users/gabegma/followers",
      "following_url": "https://api.github.com/users/gabegma/following{/other_user}",
      "gists_url": "https://api.github.com/users/gabegma/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gabegma/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gabegma/subscriptions",
      "organizations_url": "https://api.github.com/users/gabegma/orgs",
      "repos_url": "https://api.github.com/users/gabegma/repos",
      "events_url": "https://api.github.com/users/gabegma/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gabegma/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2025-03-06T21:32:07Z",
    "updated_at": "2025-03-06T21:32:07Z",
    "closed_at": null,
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "sub_issues_summary": {
      "total": 0,
      "completed": 0,
      "percent_completed": 0
    },
    "body": "Hi! \n\nI'm trying to reproduce the MM-MT-Bench results, and I'm struggling a little bit. I believe that the judge's prompt in MM-MT-Bench is incorrect. Below is an example and my interrogations\n\n1 - The user content is not being populated, as the code is looping on keys instead of values (hence we get `rolecontent`)\n2 - The reference answer has the full dictionary instead of only the text - is that on purpose? The Pixtral paper doesn't mention that.\n3 - The text often contains headers (#, ##, ###) which the model might confuse with ### User and ### Reference answer.\n4 - Lastly, in the Pixtral paper, it is hinted that the image might be sent to the judge model, but this is not happening in the code either - is the paper or the code the source of truth?\n\nThank you so much for the help!\n\n```\n'<|The Start of Conversation with User|>\n\n### User:\nrolecontent\n\n### Reference answer:\n[{\\'type\\': \\'text\\', \\'text\\': \"# Military Battalion Comparison (1990 - 2020)\\\\n\\\\nThis comparison chart outlines the variations in battalion numbers within different military branches \u2013 Armoured, Infantry, and Artillery \u2013 for a selection of countries between the years 1990 and 2020.\\\\n\\\\n## Key Trends and Statistics\\\\n\\\\n### Overall Reduction in Forces\\\\n- All highlighted countries have undergone substantial decreases in their total number of battalions across all branches, indicating a widespread trend of downsizing military forces over the span of three decades.\\\\n\\\\n### Germany\\'s Dramatic Decrease\\\\n- Germany\\'s reduction is particularly notable. In 1990, West Germany counted a total of 215 battalions:\\\\n  - **Armoured:** 85\\\\n  - **Infantry:** 67\\\\n  - **Artillery:** 63\\\\n- By 2020, unified Germany had only 33 battalions:\\\\n  - **Armoured:** 11\\\\n  - **Infantry:** 18\\\\n  - **Artillery:** 4\\\\n- This represents an approximate 85% cutback.\\\\n\\\\n### Shift in Force Composition\\\\n- Although all types saw reductions, shifts in the composition of forces occurred:\\\\n  - **Armoured** battalions faced the sharpest declines. Germany\\'s armoured battalions dropped by 87%, from 85 to 11.\\\\n  - **Infantry** battalions generally experienced smaller reductions. Britain\\'s infantry units reduced by 41%, from 58 to 34.\\\\n  - **Artillery** units were also significantly cut, often more so than infantry but to a lesser extent than armoured units. France\\'s artillery battalions fell from 23 to 7, a 70% decrease.\\\\n\\\\n### U.S. EUCOM (European Command) Reductions\\\\n- The U.S. military footprint in Europe diminished substantially, with a reduction from 99 battalions in 1990 to merely 16 in 2020, equating to an 84% decline.\\\\n\\\\n### Relative Positions Unchanged\\\\n- Despite the overall reductions, the relative standings in terms of battalion counts have remained approximately constant. Germany and Italy retained more battalions than France and Britain both in 1990 and 2020.\\\\n\\\\n## Analysis\\\\n\\\\nThe revealed trends likely mirror shifts in military strategies, budget allocations, and the changing geopolitical climate after the Cold War\\'s conclusion. The trend away from massed armoured formations infers a strategic pivot toward more nimble and deployable forces, potentially as a reaction to evolving threat landscapes and military operations in the post-Cold War environment.\"}]\n\n### Assistant's answer:\nThis assistant's answer [placeholder]\n\n<|The End of Conversation with User|>\n```\n\n![Image](https://github.com/user-attachments/assets/4c6985ba-458e-4f6b-9c39-a077ee1ca538)\n",
    "closed_by": null,
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/8/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/7",
    "id": 2836463514,
    "node_id": "PR_kwDOMxW0Oc6KVAaL",
    "number": 7,
    "title": "Bump vllm from 0.6.2 to 0.7.2",
    "user": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [
      {
        "id": 8063349909,
        "node_id": "LA_kwDOMxW0Oc8AAAAB4Jz0lQ",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/dependencies",
        "name": "dependencies",
        "color": "0366d6",
        "default": false,
        "description": "Pull requests that update a dependency file"
      }
    ],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2025-02-06T20:01:43Z",
    "updated_at": "2025-03-19T15:54:27Z",
    "closed_at": "2025-03-19T15:54:25Z",
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/7",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/7",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/7.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/7.patch",
      "merged_at": null
    },
    "body": "Bumps [vllm](https://github.com/vllm-project/vllm) from 0.6.2 to 0.7.2.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vllm-project/vllm/releases\">vllm's releases</a>.</em></p>\n<blockquote>\n<h2>v0.7.2</h2>\n<h2>Highlights</h2>\n<ul>\n<li>Qwen2.5-VL is now supported in vLLM. Please note that it requires a source installation from Hugging Face <code>transformers</code> library at the moment (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12604\">#12604</a>)</li>\n<li>Add <code>transformers</code> backend support via <code>--model-impl=transformers</code>. This allows vLLM to be ran with arbitrary Hugging Face text models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11330\">#11330</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12785\">#12785</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12727\">#12727</a>).</li>\n<li>Performance enhancement to DeepSeek models.\n<ul>\n<li>Align KV caches entries to start 256 byte boundaries, yielding 43% throughput enhancement (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12676\">#12676</a>)</li>\n<li>Apply <code>torch.compile</code> to fused_moe/grouped_topk, yielding 5% throughput enhancement (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12637\">#12637</a>)</li>\n<li>Enable MLA for DeepSeek VL2 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12729\">#12729</a>)</li>\n<li>Enable DeepSeek model on ROCm (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12662\">#12662</a>)</li>\n</ul>\n</li>\n</ul>\n<h3>Core Engine</h3>\n<ul>\n<li>Use <code>VLLM_LOGITS_PROCESSOR_THREADS</code> to speed up structured decoding in high batch size scenarios (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12368\">#12368</a>)</li>\n</ul>\n<h3>Security Update</h3>\n<ul>\n<li>Improve hash collision avoidance in prefix caching (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12621\">#12621</a>)</li>\n<li>Add SPDX-License-Identifier headers to python source files (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12628\">#12628</a>)</li>\n</ul>\n<h3>Other</h3>\n<ul>\n<li>Enable FusedSDPA support for Intel Gaudi (HPU) (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12359\">#12359</a>)</li>\n</ul>\n<h2>What's Changed</h2>\n<ul>\n<li>Apply torch.compile to fused_moe/grouped_topk by <a href=\"https://github.com/mgoin\"><code>@\u200bmgoin</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12637\">vllm-project/vllm#12637</a></li>\n<li>doc: fixing minor typo in readme.md by <a href=\"https://github.com/vicenteherrera\"><code>@\u200bvicenteherrera</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12643\">vllm-project/vllm#12643</a></li>\n<li>[Bugfix] fix moe_wna16 get_quant_method by <a href=\"https://github.com/jinzhen-lin\"><code>@\u200bjinzhen-lin</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12648\">vllm-project/vllm#12648</a></li>\n<li>[Core] Silence unnecessary deprecation warnings by <a href=\"https://github.com/russellb\"><code>@\u200brussellb</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12620\">vllm-project/vllm#12620</a></li>\n<li>[V1][Minor] Avoid frequently creating ConstantList by <a href=\"https://github.com/WoosukKwon\"><code>@\u200bWoosukKwon</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12653\">vllm-project/vllm#12653</a></li>\n<li>[Core][v1] Unify allocating slots in prefill and decode in KV cache manager by <a href=\"https://github.com/ShawnD200\"><code>@\u200bShawnD200</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12608\">vllm-project/vllm#12608</a></li>\n<li>[Hardware][Intel GPU] add XPU bf16 support by <a href=\"https://github.com/jikunshang\"><code>@\u200bjikunshang</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12392\">vllm-project/vllm#12392</a></li>\n<li>[Misc] Add SPDX-License-Identifier headers to python source files by <a href=\"https://github.com/russellb\"><code>@\u200brussellb</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12628\">vllm-project/vllm#12628</a></li>\n<li>[doc][misc] clarify VLLM_HOST_IP for multi-node inference by <a href=\"https://github.com/youkaichao\"><code>@\u200byoukaichao</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12667\">vllm-project/vllm#12667</a></li>\n<li>[Doc] Deprecate Discord by <a href=\"https://github.com/zhuohan123\"><code>@\u200bzhuohan123</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12668\">vllm-project/vllm#12668</a></li>\n<li>[Kernel] port sgl moe_align_block_size kernels by <a href=\"https://github.com/chenyang78\"><code>@\u200bchenyang78</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12574\">vllm-project/vllm#12574</a></li>\n<li>make sure mistral_common not imported for non-mistral models by <a href=\"https://github.com/youkaichao\"><code>@\u200byoukaichao</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12669\">vllm-project/vllm#12669</a></li>\n<li>Properly check if all fused layers are in the list of targets by <a href=\"https://github.com/eldarkurtic\"><code>@\u200beldarkurtic</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12666\">vllm-project/vllm#12666</a></li>\n<li>Fix for attention layers to remain unquantized during moe_wn16 quant by <a href=\"https://github.com/srikanthsrnvs\"><code>@\u200bsrikanthsrnvs</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12570\">vllm-project/vllm#12570</a></li>\n<li>[cuda] manually import the correct pynvml module by <a href=\"https://github.com/youkaichao\"><code>@\u200byoukaichao</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12679\">vllm-project/vllm#12679</a></li>\n<li>[ci/build] fix gh200 test by <a href=\"https://github.com/youkaichao\"><code>@\u200byoukaichao</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12681\">vllm-project/vllm#12681</a></li>\n<li>[Model]: Add <code>transformers</code> backend support by <a href=\"https://github.com/ArthurZucker\"><code>@\u200bArthurZucker</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11330\">vllm-project/vllm#11330</a></li>\n<li>[Misc] Fix improper placement of SPDX header in scripts by <a href=\"https://github.com/russellb\"><code>@\u200brussellb</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12694\">vllm-project/vllm#12694</a></li>\n<li>[Bugfix][Kernel] Fix per-token/per-channel quantization for Hopper scaled mm by <a href=\"https://github.com/tlrmchlsmth\"><code>@\u200btlrmchlsmth</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12696\">vllm-project/vllm#12696</a></li>\n<li>Squelch MLA warning for Compressed-Tensors Models by <a href=\"https://github.com/kylesayrs\"><code>@\u200bkylesayrs</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12704\">vllm-project/vllm#12704</a></li>\n<li>[Model] Add Deepseek V3 fp8_w8a8 configs for B200 by <a href=\"https://github.com/kushanam\"><code>@\u200bkushanam</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12707\">vllm-project/vllm#12707</a></li>\n<li>[MISC] Remove model input dumping when exception by <a href=\"https://github.com/comaniac\"><code>@\u200bcomaniac</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12582\">vllm-project/vllm#12582</a></li>\n<li>[V1] Revert <code>uncache_blocks</code> and support recaching full blocks by <a href=\"https://github.com/comaniac\"><code>@\u200bcomaniac</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12415\">vllm-project/vllm#12415</a></li>\n<li>[Core] Improve hash collision avoidance in prefix caching by <a href=\"https://github.com/russellb\"><code>@\u200brussellb</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12621\">vllm-project/vllm#12621</a></li>\n<li>Support Pixtral-Large HF by using llava multimodal_projector_bias config by <a href=\"https://github.com/mgoin\"><code>@\u200bmgoin</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12710\">vllm-project/vllm#12710</a></li>\n<li>[Doc] Replace ibm-fms with ibm-ai-platform by <a href=\"https://github.com/tdoublep\"><code>@\u200btdoublep</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12709\">vllm-project/vllm#12709</a></li>\n<li>[Quant] Fix use_mla TypeError and support loading pure-sparsity Compressed Tensors configs by <a href=\"https://github.com/kylesayrs\"><code>@\u200bkylesayrs</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12711\">vllm-project/vllm#12711</a></li>\n<li>[AMD][ROCm] Enable DeepSeek model on ROCm by <a href=\"https://github.com/hongxiayang\"><code>@\u200bhongxiayang</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/12662\">vllm-project/vllm#12662</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/0408efc6d0c17fba17b2be38d0d0f02e96d2bf9d\"><code>0408efc</code></a> [Misc] Improve error message for incorrect pynvml (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12809\">#12809</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/449d1bce029f87e0d1cf3f30483687ff659268f2\"><code>449d1bc</code></a> [Misc] Remove duplicated DeepSeek V2/V3 model definition (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12793\">#12793</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/1a6fcad4c933c89b4060a37d807a7f9e5a680cf3\"><code>1a6fcad</code></a> Improve <code>TransformersModel</code> UX (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12785\">#12785</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/56534cd577211c563b2c5b74098929b949fc4063\"><code>56534cd</code></a> [Bugfix] Fix the test_ultravox.py's license (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12806\">#12806</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/d88506dda45f69cf1f56c4325282c2a881eaaaf7\"><code>d88506d</code></a> [Model] LoRA Support for Ultravox model (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11253\">#11253</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/9cdea30b4fe0ebd23847371f51ea0a48b9615847\"><code>9cdea30</code></a> [Misc][Easy] Remove the space from the file name</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/76abd0c88143419826bfc13d2cd29669d0fdfa1b\"><code>76abd0c</code></a> [Bugfix] Better FP8 supported defaults</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/5b19b93082fc5ad0ce33752d8467337cbe93de21\"><code>5b19b93</code></a> [ROCm][Kernel] Using the correct warp_size value</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/75404d041be0d6e656b59cbbea23520d47d37b66\"><code>75404d0</code></a> [VLM] Update compatibility with transformers 4.49</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/bf3b79efb82676219a3275764d8fcf4c70097ce5\"><code>bf3b79e</code></a> [VLM] Qwen2.5-VL</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vllm-project/vllm/compare/v0.6.2...v0.7.2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vllm&package-manager=pip&previous-version=0.6.2&new-version=0.7.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/mistralai/mistral-evals/network/alerts).\n\n</details>",
    "closed_by": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/7/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/6",
    "id": 2814034232,
    "node_id": "PR_kwDOMxW0Oc6JI6NV",
    "number": 6,
    "title": "Bump vllm from 0.6.2 to 0.7.0",
    "user": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [
      {
        "id": 8063349909,
        "node_id": "LA_kwDOMxW0Oc8AAAAB4Jz0lQ",
        "url": "https://api.github.com/repos/mistralai/mistral-evals/labels/dependencies",
        "name": "dependencies",
        "color": "0366d6",
        "default": false,
        "description": "Pull requests that update a dependency file"
      }
    ],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2025-01-27T20:52:05Z",
    "updated_at": "2025-02-06T20:01:47Z",
    "closed_at": "2025-02-06T20:01:45Z",
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/6",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/6",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/6.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/6.patch",
      "merged_at": null
    },
    "body": "Bumps [vllm](https://github.com/vllm-project/vllm) from 0.6.2 to 0.7.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/vllm-project/vllm/releases\">vllm's releases</a>.</em></p>\n<blockquote>\n<h2>v0.7.0</h2>\n<h2>Highlights</h2>\n<ul>\n<li>vLLM's V1 engine is ready for testing! This is a rewritten engine designed for performance and architectural simplicity. You can turn it on by setting environment variable <code>VLLM_USE_V1=1</code>. See <a href=\"https://blog.vllm.ai/2025/01/27/v1-alpha-release.html\">our blog</a> for more details. (44 commits).</li>\n<li>New methods (<code>LLM.sleep</code>, <code>LLM.wake_up</code>, <code>LLM.collective_rpc</code>, <code>LLM.reset_prefix_cache</code>) in vLLM for the post training frameworks! (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12361\">#12361</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12084\">#12084</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12284\">#12284</a>).</li>\n<li><code>torch.compile</code> is now fully integrated in vLLM, and enabled by default in V1. You can turn it on via <code>-O3</code> engine parameter. (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11614\">#11614</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12243\">#12243</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12043\">#12043</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12191\">#12191</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11677\">#11677</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12182\">#12182</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12246\">#12246</a>).</li>\n</ul>\n<p>This release features</p>\n<ul>\n<li>400 commits from 132 contributors, including 57 new contributors.\n<ul>\n<li>28 CI and build enhancements, including testing for nightly torch (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12270\">#12270</a>) and inclusion of genai-perf for benchmark (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/10704\">#10704</a>).</li>\n<li>58 documentation enhancements, including reorganized documentation structure (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11645\">#11645</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11755\">#11755</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11766\">#11766</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11843\">#11843</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11896\">#11896</a>).</li>\n<li>more than 161 bug fixes and miscellaneous enhancements</li>\n</ul>\n</li>\n</ul>\n<h3>Features</h3>\n<p><em>Models</em></p>\n<ul>\n<li>New generative models: CogAgent (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11742\">#11742</a>), Deepseek-VL2 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11578\">#11578</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12068\">#12068</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12169\">#12169</a>), fairseq2 Llama (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11442\">#11442</a>), InternLM3 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12037\">#12037</a>), Whisper (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11280\">#11280</a>)</li>\n<li>New pooling models: Qwen2 PRM (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12202\">#12202</a>), InternLM2 reward models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11571\">#11571</a>)</li>\n<li>VLM: Merged multi-modal processor is now ready for model developers! (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11620\">#11620</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11900\">#11900</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11682\">#11682</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11717\">#11717</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11669\">#11669</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11396\">#11396</a>)\n<ul>\n<li>Any model that implements merged multi-modal processor and the <code>get_*_embeddings</code> methods according to <a href=\"https://docs.vllm.ai/en/latest/contributing/model/multimodal.html\">this guide</a> is automatically supported by V1 engine.</li>\n</ul>\n</li>\n</ul>\n<p><em>Hardwares</em></p>\n<ul>\n<li>Apple: Native support for macOS Apple Silicon (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11696\">#11696</a>)</li>\n<li>AMD: MI300 FP8 format for block_quant (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12134\">#12134</a>), Tuned MoE configurations for multiple models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12408\">#12408</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12049\">#12049</a>), block size heuristic for avg 2.8x speedup for int8 models (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11698\">#11698</a>)</li>\n<li>TPU: support for <code>W8A8</code> (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11785\">#11785</a>)</li>\n<li>x86: Multi-LoRA (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11100\">#11100</a>) and MoE Support (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11831\">#11831</a>)</li>\n<li>Progress in out-of-tree hardware support (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12009\">#12009</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11981\">#11981</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11948\">#11948</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11609\">#11609</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12264\">#12264</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11516\">#11516</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11503\">#11503</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11369\">#11369</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11602\">#11602</a>)</li>\n</ul>\n<p><em>Features</em></p>\n<ul>\n<li>Distributed:\n<ul>\n<li>Support torchrun and SPMD-style offline inference (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12071\">#12071</a>)</li>\n<li>New <code>collective_rpc</code> abstraction (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12151\">#12151</a>, <a href=\"https://redirect.github.com/vllm-project/vllm/issues/11256\">#11256</a>)</li>\n</ul>\n</li>\n<li>API Server: Jina- and Cohere-compatible Rerank API (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12376\">#12376</a>)</li>\n<li>Kernels:\n<ul>\n<li>Flash Attention 3 Support (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12093\">#12093</a>)</li>\n<li>Punica prefill kernels fusion (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11234\">#11234</a>)</li>\n<li>For Deepseek V3: optimize <code>moe_align_block_size</code> for cuda graph and large num_experts (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12222\">#12222</a>)</li>\n</ul>\n</li>\n</ul>\n<h3>Others</h3>\n<ul>\n<li>Benchmark: new script for CPU offloading  (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/11533\">#11533</a>)</li>\n<li>Security: Set <code>weights_only=True</code> when using <code>torch.load()</code> (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12366\">#12366</a>)</li>\n</ul>\n<h2>What's Changed</h2>\n<ul>\n<li>[Docs] Document Deepseek V3 support by <a href=\"https://github.com/simon-mo\"><code>@\u200bsimon-mo</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11535\">vllm-project/vllm#11535</a></li>\n<li>Update openai_compatible_server.md by <a href=\"https://github.com/robertgshaw2-redhat\"><code>@\u200brobertgshaw2-redhat</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11536\">vllm-project/vllm#11536</a></li>\n<li>[V1] Use FlashInfer Sampling Kernel for Top-P &amp; Top-K Sampling by <a href=\"https://github.com/WoosukKwon\"><code>@\u200bWoosukKwon</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11394\">vllm-project/vllm#11394</a></li>\n<li>[V1] Fix yapf by <a href=\"https://github.com/WoosukKwon\"><code>@\u200bWoosukKwon</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11538\">vllm-project/vllm#11538</a></li>\n<li>[CI] Fix broken CI by <a href=\"https://github.com/robertgshaw2-redhat\"><code>@\u200brobertgshaw2-redhat</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11543\">vllm-project/vllm#11543</a></li>\n<li>[misc] fix typing by <a href=\"https://github.com/youkaichao\"><code>@\u200byoukaichao</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11540\">vllm-project/vllm#11540</a></li>\n<li>[V1][3/N] API Server: Reduce Task Switching + Handle Abort Properly by <a href=\"https://github.com/robertgshaw2-redhat\"><code>@\u200brobertgshaw2-redhat</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11534\">vllm-project/vllm#11534</a></li>\n<li>[BugFix] Deepseekv3 broke quantization for all other methods by <a href=\"https://github.com/robertgshaw2-redhat\"><code>@\u200brobertgshaw2-redhat</code></a> in <a href=\"https://redirect.github.com/vllm-project/vllm/pull/11547\">vllm-project/vllm#11547</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/5204ff5c3feeb96e8a6eea65dfcb78395f90d4d8\"><code>5204ff5</code></a> [Bugfix] Fix Granite 3.0 MoE model loading (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12446\">#12446</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/0cc6b383d73eb662dfeec671d3b47cda301b2f47\"><code>0cc6b38</code></a> [Frontend] Support scores endpoint in run_batch (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12430\">#12430</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/28e0750847ded93158a66efdcbc869d87463b38f\"><code>28e0750</code></a> [V1] Avoid list creation in input preparation (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12457\">#12457</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/582cf78798a6fef9b69d0471df73d81e09a7d3d8\"><code>582cf78</code></a> [DOC] Add link to vLLM blog (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12460\">#12460</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/0034b09ceb7f578f2d097c7fb8c7042d17367c35\"><code>0034b09</code></a> [Frontend] Rerank API (Jina- and Cohere-compatible API)  (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12376\">#12376</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/72bac7306796b01c202d846da041f62ded3a26a9\"><code>72bac73</code></a> [Build/CI] Fix libcuda.so linkage (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12424\">#12424</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/68f11149d845a164c9bbf122ab3bee8c94290169\"><code>68f1114</code></a> [Bugfix][Kernel] Fix perf regression caused by PR <a href=\"https://redirect.github.com/vllm-project/vllm/issues/12405\">#12405</a> (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12434\">#12434</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/72f4880425edf06f105863b2389f9c46025e08ee\"><code>72f4880</code></a> [Bugfix/CI] Fix broken kernels/test_mha.py (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12450\">#12450</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/aa2cd2c43d1d19ece0f3b36ad716c3a9b8a2def0\"><code>aa2cd2c</code></a> [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 (<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12417\">#12417</a>)</li>\n<li><a href=\"https://github.com/vllm-project/vllm/commit/9ddc35220bee793eb445d9592a40bc4d3c081519\"><code>9ddc352</code></a> [Frontend] generation_config.json for  maximum tokens(<a href=\"https://redirect.github.com/vllm-project/vllm/issues/12242\">#12242</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/vllm-project/vllm/compare/v0.6.2...v0.7.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=vllm&package-manager=pip&previous-version=0.6.2&new-version=0.7.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/mistralai/mistral-evals/network/alerts).\n\n</details>",
    "closed_by": {
      "login": "dependabot[bot]",
      "id": 49699333,
      "node_id": "MDM6Qm90NDk2OTkzMzM=",
      "avatar_url": "https://avatars.githubusercontent.com/in/29110?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dependabot%5Bbot%5D",
      "html_url": "https://github.com/apps/dependabot",
      "followers_url": "https://api.github.com/users/dependabot%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dependabot%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/dependabot%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/dependabot%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dependabot%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/6/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5/events",
    "html_url": "https://github.com/mistralai/mistral-evals/issues/5",
    "id": 2594218382,
    "node_id": "I_kwDOMxW0Oc6aoKGO",
    "number": 5,
    "title": "micro_average or macro_average_score for MM-MT-Bench",
    "user": {
      "login": "yuhangzang",
      "id": 31362732,
      "node_id": "MDQ6VXNlcjMxMzYyNzMy",
      "avatar_url": "https://avatars.githubusercontent.com/u/31362732?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/yuhangzang",
      "html_url": "https://github.com/yuhangzang",
      "followers_url": "https://api.github.com/users/yuhangzang/followers",
      "following_url": "https://api.github.com/users/yuhangzang/following{/other_user}",
      "gists_url": "https://api.github.com/users/yuhangzang/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/yuhangzang/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/yuhangzang/subscriptions",
      "organizations_url": "https://api.github.com/users/yuhangzang/orgs",
      "repos_url": "https://api.github.com/users/yuhangzang/repos",
      "events_url": "https://api.github.com/users/yuhangzang/events{/privacy}",
      "received_events_url": "https://api.github.com/users/yuhangzang/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "open",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-10-17T09:53:14Z",
    "updated_at": "2024-10-17T09:53:14Z",
    "closed_at": null,
    "author_association": "NONE",
    "type": null,
    "active_lock_reason": null,
    "sub_issues_summary": {
      "total": 0,
      "completed": 0,
      "percent_completed": 0
    },
    "body": "The MM-MT-bench results in Table 2 of the [paper](https://arxiv.org/pdf/2410.07073) are presented without specifying the averaging method. Could you clarify if they are [micro-averaged](https://github.com/mistralai/mistral-evals/blob/main/eval/tasks/mm_mt_bench.py#L230) or [macro-averaged](https://github.com/mistralai/mistral-evals/blob/main/eval/tasks/mm_mt_bench.py#L231)?",
    "closed_by": null,
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/5/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/4",
    "id": 2581923680,
    "node_id": "PR_kwDOMxW0Oc5-Xp0v",
    "number": 4,
    "title": "Fix `eval.mm_mt_bench` -> `eval.run` example in README",
    "user": {
      "login": "mgoin",
      "id": 3195154,
      "node_id": "MDQ6VXNlcjMxOTUxNTQ=",
      "avatar_url": "https://avatars.githubusercontent.com/u/3195154?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/mgoin",
      "html_url": "https://github.com/mgoin",
      "followers_url": "https://api.github.com/users/mgoin/followers",
      "following_url": "https://api.github.com/users/mgoin/following{/other_user}",
      "gists_url": "https://api.github.com/users/mgoin/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/mgoin/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/mgoin/subscriptions",
      "organizations_url": "https://api.github.com/users/mgoin/orgs",
      "repos_url": "https://api.github.com/users/mgoin/repos",
      "events_url": "https://api.github.com/users/mgoin/events{/privacy}",
      "received_events_url": "https://api.github.com/users/mgoin/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 1,
    "created_at": "2024-10-11T18:04:29Z",
    "updated_at": "2024-10-12T00:57:06Z",
    "closed_at": "2024-10-12T00:57:01Z",
    "author_association": "CONTRIBUTOR",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/4",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/4",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/4.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/4.patch",
      "merged_at": "2024-10-12T00:57:01Z"
    },
    "body": "The current example in the README wasn't correct, so updated with `eval.run` which seems to work fine.",
    "closed_by": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/4/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/3",
    "id": 2555181529,
    "node_id": "PR_kwDOMxW0Oc59ChzW",
    "number": 3,
    "title": "Refactor to support MM-evals.",
    "user": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-09-29T21:55:52Z",
    "updated_at": "2024-10-08T05:03:05Z",
    "closed_at": "2024-10-08T05:03:00Z",
    "author_association": "CONTRIBUTOR",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/3",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/3",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/3.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/3.patch",
      "merged_at": "2024-10-08T05:03:00Z"
    },
    "body": "This PR refactors the code originally only created for MM-MT-Bench and instead supports a generic set of evals. This is meant to be lightweight and extensible.\r\n\r\nWe release prompt + metrics + eval runner for the minimal set of evals reported in the Pixtral-12B paper, and welcome contributions for more evals in a standardized format.\r\n* DocVQA\r\n* MMMU\r\n* MathVista\r\n* ChartQA\r\n\r\nMM-MT-Bench is also included from before.",
    "closed_by": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/3/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/2",
    "id": 2537933145,
    "node_id": "PR_kwDOMxW0Oc58HvcJ",
    "number": 2,
    "title": "Adds a readme",
    "user": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2024-09-20T05:50:51Z",
    "updated_at": "2024-09-20T22:26:18Z",
    "closed_at": "2024-09-20T22:26:18Z",
    "author_association": "CONTRIBUTOR",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/2",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/2",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/2.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/2.patch",
      "merged_at": "2024-09-20T22:26:18Z"
    },
    "body": null,
    "closed_by": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/2/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  },
  {
    "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1",
    "repository_url": "https://api.github.com/repos/mistralai/mistral-evals",
    "labels_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1/labels{/name}",
    "comments_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1/comments",
    "events_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1/events",
    "html_url": "https://github.com/mistralai/mistral-evals/pull/1",
    "id": 2532114171,
    "node_id": "PR_kwDOMxW0Oc570H2e",
    "number": 1,
    "title": "Adds MM-MT-Bench",
    "user": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 2,
    "created_at": "2024-09-17T20:42:24Z",
    "updated_at": "2024-09-19T06:29:30Z",
    "closed_at": "2024-09-19T06:25:15Z",
    "author_association": "CONTRIBUTOR",
    "type": null,
    "active_lock_reason": null,
    "draft": false,
    "pull_request": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/pulls/1",
      "html_url": "https://github.com/mistralai/mistral-evals/pull/1",
      "diff_url": "https://github.com/mistralai/mistral-evals/pull/1.diff",
      "patch_url": "https://github.com/mistralai/mistral-evals/pull/1.patch",
      "merged_at": "2024-09-19T06:25:15Z"
    },
    "body": null,
    "closed_by": {
      "login": "sohamghosh121",
      "id": 2398819,
      "node_id": "MDQ6VXNlcjIzOTg4MTk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2398819?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sohamghosh121",
      "html_url": "https://github.com/sohamghosh121",
      "followers_url": "https://api.github.com/users/sohamghosh121/followers",
      "following_url": "https://api.github.com/users/sohamghosh121/following{/other_user}",
      "gists_url": "https://api.github.com/users/sohamghosh121/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sohamghosh121/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sohamghosh121/subscriptions",
      "organizations_url": "https://api.github.com/users/sohamghosh121/orgs",
      "repos_url": "https://api.github.com/users/sohamghosh121/repos",
      "events_url": "https://api.github.com/users/sohamghosh121/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sohamghosh121/received_events",
      "type": "User",
      "user_view_type": "public",
      "site_admin": false
    },
    "reactions": {
      "url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "timeline_url": "https://api.github.com/repos/mistralai/mistral-evals/issues/1/timeline",
    "performed_via_github_app": null,
    "state_reason": null
  }
]